This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.docs/0_15_1_Release_Notes_âš¡_The_Zig_Programming_Language.md
.docs/COPILOT_RESPONSE.md
.docs/Documentation_-_The_Zig_Programming_Language.md
.docs/MEMORY_MANAGEMENT.md
.docs/repomix-output-The-Pocket-PocketFlow-Go.xml
.docs/repomix-output-The-Pocket-PocketFlow-Rust.xml
.docs/repomix-output-The-Pocket-PocketFlow-Typescript.xml
.docs/repomix-output-The-Pocket-PocketFlow.xml
.docs/zig-0.15.1-io-interface-context7-docs.txt
.gitignore
build.zig
build.zig.zon
examples/document_generator.zig
LICENSE
README.md
src/context.zig
src/flow.zig
src/node.zig
src/ollama.zig
src/pocketflow.zig
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".docs/0_15_1_Release_Notes_âš¡_The_Zig_Programming_Language.md">
# 0.15.1 Release Notes âš¡ The Zig Programming Language

[](https://ziglang.org/)

# 0.15.1 Release Notes

![Carmen the Allocgator](https://ziglang.org/img/Carmen_3.svg)

[Download & Documentation](https://ziglang.org/download/#release-0.15.1)

Zig is a general-purpose programming language and toolchain for maintaining **robust**, **optimal**, and **reusable** software.

Zig development is funded via [Zig Software Foundation](https://ziglang.org/zsf/), a 501(c)(3) non-profit organization. Please consider a recurring donation so that we can offer more billable hours to our core team members. This is the most straightforward way to accelerate the project along the [Roadmap](https://ziglang.org/download/0.15.1/release-notes.html#Roadmap) to 1.0.

If you need receipts for your donations or are looking to migrate away from GitHub Sponsors, we recommend [donating to us via Every.org](https://www.every.org/zig-software-foundation-inc).

This release features **5 months of work**: changes from **162 different contributors**, spread among **647 commits**.

Debug compilation is 5 times faster with Zig's [x86 Backend](https://ziglang.org/download/0.15.1/release-notes.html#x86-Backend) selected by default; the work-in-progress [aarch64 Backend](https://ziglang.org/download/0.15.1/release-notes.html#aarch64-Backend) hot on its heels. Meanwhile, the [Writergate](https://ziglang.org/download/0.15.1/release-notes.html#Writergate) scandal, along with a slew of [Language Changes](https://ziglang.org/download/0.15.1/release-notes.html#Language-Changes) and [Standard Library](https://ziglang.org/download/0.15.1/release-notes.html#Standard-Library) cuts, rocks the boat with tidal waves of breaking API changes; the harbinger of [async/await resurrection](https://kristoff.it/blog/zig-new-async-io/); the last bastion defending language stabilization.

## [Table of Contents](https://ziglang.org/download/0.15.1/release-notes.html#toc-Table-of-Contents) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Table-of-Contents)

-   [Table of Contents](https://ziglang.org/download/0.15.1/release-notes.html#Table-of-Contents)

-   [Target Support](https://ziglang.org/download/0.15.1/release-notes.html#Target-Support)
    -   [Tier System](https://ziglang.org/download/0.15.1/release-notes.html#Tier-System)
        -   [Tier 1](https://ziglang.org/download/0.15.1/release-notes.html#Tier-1)
        -   [Tier 2](https://ziglang.org/download/0.15.1/release-notes.html#Tier-2)
        -   [Tier 3](https://ziglang.org/download/0.15.1/release-notes.html#Tier-3)
        -   [Tier 4](https://ziglang.org/download/0.15.1/release-notes.html#Tier-4)
    -   [Support Table](https://ziglang.org/download/0.15.1/release-notes.html#Support-Table)
    -   [OS Version Requirements](https://ziglang.org/download/0.15.1/release-notes.html#OS-Version-Requirements)
    -   [Additional Platforms](https://ziglang.org/download/0.15.1/release-notes.html#Additional-Platforms)
-   [Language Changes](https://ziglang.org/download/0.15.1/release-notes.html#Language-Changes)
    -   [usingnamespace Removed](https://ziglang.org/download/0.15.1/release-notes.html#usingnamespace-Removed)
        -   [Use Case: Conditional Inclusion](https://ziglang.org/download/0.15.1/release-notes.html#Use-Case-Conditional-Inclusion)
        -   [Use Case: Implementation Switching](https://ziglang.org/download/0.15.1/release-notes.html#Use-Case-Implementation-Switching)
        -   [Use Case: Mixins](https://ziglang.org/download/0.15.1/release-notes.html#Use-Case-Mixins)
    -   [async and await keywords removed](https://ziglang.org/download/0.15.1/release-notes.html#async-and-await-keywords-removed)
    -   [switch on non-exhaustive enums](https://ziglang.org/download/0.15.1/release-notes.html#switch-on-non-exhaustive-enums)
    -   [Allow more operators on bool vectors](https://ziglang.org/download/0.15.1/release-notes.html#Allow-more-operators-on-bool-vectors)
    -   [Inline Assembly: Typed Clobbers](https://ziglang.org/download/0.15.1/release-notes.html#Inline-Assembly-Typed-Clobbers)
    -   [Allow @ptrCast Single-Item Pointer to Slice](https://ziglang.org/download/0.15.1/release-notes.html#Allow-ptrCast-Single-Item-Pointer-to-Slice)
    -   [New Rules for Arithmetic on undefined](https://ziglang.org/download/0.15.1/release-notes.html#New-Rules-for-Arithmetic-on-undefined)
    -   [Error on Lossy Coercion from Int to Float](https://ziglang.org/download/0.15.1/release-notes.html#Error-on-Lossy-Coercion-from-Int-to-Float)
-   [Standard Library](https://ziglang.org/download/0.15.1/release-notes.html#Standard-Library)
    -   [Writergate](https://ziglang.org/download/0.15.1/release-notes.html#Writergate)
        -   [Motivation](https://ziglang.org/download/0.15.1/release-notes.html#Motivation)
        -   [Adapter API](https://ziglang.org/download/0.15.1/release-notes.html#Adapter-API)
        -   [New std.Io.Writer and std.Io.Reader API](https://ziglang.org/download/0.15.1/release-notes.html#New-stdIoWriter-and-stdIoReader-API)
        -   [std.fs.File.Reader and std.fs.File.Writer](https://ziglang.org/download/0.15.1/release-notes.html#stdfsFileReader-and-stdfsFileWriter)
        -   [Upgrading std.io.getStdOut().writer().print()](https://ziglang.org/download/0.15.1/release-notes.html#Upgrading-stdiogetStdOutwriterprint)
        -   [reworked std.compress.flate](https://ziglang.org/download/0.15.1/release-notes.html#reworked-stdcompressflate)
        -   [CountingWriter Deleted](https://ziglang.org/download/0.15.1/release-notes.html#CountingWriter-Deleted)
        -   [BufferedWriter Deleted](https://ziglang.org/download/0.15.1/release-notes.html#BufferedWriter-Deleted)
    -   ["{f}" Required to Call format Methods](https://ziglang.org/download/0.15.1/release-notes.html#f-Required-to-Call-format-Methods)
    -   [Format Methods No Longer Have Format Strings or Options](https://ziglang.org/download/0.15.1/release-notes.html#Format-Methods-No-Longer-Have-Format-Strings-or-Options)
    -   [Formatted Printing No Longer Deals with Unicode](https://ziglang.org/download/0.15.1/release-notes.html#Formatted-Printing-No-Longer-Deals-with-Unicode)
    -   [New Formatted Printing Specifiers](https://ziglang.org/download/0.15.1/release-notes.html#New-Formatted-Printing-Specifiers)
    -   [De-Genericify Linked Lists](https://ziglang.org/download/0.15.1/release-notes.html#De-Genericify-Linked-Lists)
    -   [std.Progress supports progress bar escape codes](https://ziglang.org/download/0.15.1/release-notes.html#stdProgress-supports-progress-bar-escape-codes)
    -   [HTTP Client and Server](https://ziglang.org/download/0.15.1/release-notes.html#HTTP-Client-and-Server)
    -   [TLS Client](https://ziglang.org/download/0.15.1/release-notes.html#TLS-Client)
    -   [ArrayList: make unmanaged the default](https://ziglang.org/download/0.15.1/release-notes.html#ArrayList-make-unmanaged-the-default)
    -   [Ring Buffers](https://ziglang.org/download/0.15.1/release-notes.html#Ring-Buffers)
    -   [Removal of BoundedArray](https://ziglang.org/download/0.15.1/release-notes.html#Removal-of-BoundedArray)
    -   [Deletions and Deprecations](https://ziglang.org/download/0.15.1/release-notes.html#Deletions-and-Deprecations)
-   [Build System](https://ziglang.org/download/0.15.1/release-notes.html#Build-System)
    -   [Removed Deprecated Implicit Root Module](https://ziglang.org/download/0.15.1/release-notes.html#Removed-Deprecated-Implicit-Root-Module)
    -   [macOS File System Watching](https://ziglang.org/download/0.15.1/release-notes.html#macOS-File-System-Watching)
    -   [Web Interface and Time Report](https://ziglang.org/download/0.15.1/release-notes.html#Web-Interface-and-Time-Report)
-   [Compiler](https://ziglang.org/download/0.15.1/release-notes.html#Compiler)
    -   [x86 Backend](https://ziglang.org/download/0.15.1/release-notes.html#x86-Backend)
    -   [aarch64 Backend](https://ziglang.org/download/0.15.1/release-notes.html#aarch64-Backend)
    -   [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation)
    -   [Threaded Codegen](https://ziglang.org/download/0.15.1/release-notes.html#Threaded-Codegen)
    -   [Allow configuring UBSan mode at the module level](https://ziglang.org/download/0.15.1/release-notes.html#Allow-configuring-UBSan-mode-at-the-module-level)
    -   [Compile Tests to Object File](https://ziglang.org/download/0.15.1/release-notes.html#Compile-Tests-to-Object-File)
    -   [Zig Init](https://ziglang.org/download/0.15.1/release-notes.html#Zig-Init)
-   [Linker](https://ziglang.org/download/0.15.1/release-notes.html#Linker)

-   [Fuzzer](https://ziglang.org/download/0.15.1/release-notes.html#Fuzzer)
-   [Bug Fixes](https://ziglang.org/download/0.15.1/release-notes.html#Bug-Fixes)
    -   [This Release Contains Bugs](https://ziglang.org/download/0.15.1/release-notes.html#This-Release-Contains-Bugs)
-   [Toolchain](https://ziglang.org/download/0.15.1/release-notes.html#Toolchain)
    -   [LLVM 20](https://ziglang.org/download/0.15.1/release-notes.html#LLVM-20)
    -   [Support dynamically-linked FreeBSD libc when cross-compiling](https://ziglang.org/download/0.15.1/release-notes.html#Support-dynamically-linked-FreeBSD-libc-when-cross-compiling)
    -   [Support dynamically-linked NetBSD libc when cross-compiling](https://ziglang.org/download/0.15.1/release-notes.html#Support-dynamically-linked-NetBSD-libc-when-cross-compiling)
    -   [glibc 2.42](https://ziglang.org/download/0.15.1/release-notes.html#glibc-242)
        -   [Allow linking native glibc statically](https://ziglang.org/download/0.15.1/release-notes.html#Allow-linking-native-glibc-statically)
    -   [MinGW-w64](https://ziglang.org/download/0.15.1/release-notes.html#MinGW-w64)
    -   [zig libc](https://ziglang.org/download/0.15.1/release-notes.html#zig-libc)
    -   [zig cc](https://ziglang.org/download/0.15.1/release-notes.html#zig-cc)
    -   [zig objcopy regressed](https://ziglang.org/download/0.15.1/release-notes.html#zig-objcopy-regressed)
-   [Roadmap](https://ziglang.org/download/0.15.1/release-notes.html#Roadmap)
    -   [I/O as an Interface](https://ziglang.org/download/0.15.1/release-notes.html#IO-as-an-Interface)
-   [Thank You Contributors!](https://ziglang.org/download/0.15.1/release-notes.html#Thank-You-Contributors)

-   [Thank You Sponsors!](https://ziglang.org/download/0.15.1/release-notes.html#Thank-You-Sponsors)

## [Target Support](https://ziglang.org/download/0.15.1/release-notes.html#toc-Target-Support) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Target-Support)

Zig supports a wide range of architectures and operating systems. The [Support Table](https://ziglang.org/download/0.15.1/release-notes.html#Support-Table) and [Additional Platforms](https://ziglang.org/download/0.15.1/release-notes.html#Additional-Platforms) sections cover the targets that Zig can build programs for, while the [zig-bootstrap README](https://github.com/ziglang/zig-bootstrap/blob/master/README.md#supported-triples) covers the targets that the Zig compiler itself can be easily cross-compiled to run on.

### [Tier System](https://ziglang.org/download/0.15.1/release-notes.html#toc-Tier-System) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Tier-System)

Zig's level of support for various targets is broadly categorized into four tiers with Tier 1 being the highest. The goal is for Tier 1 targets to have zero disabled tests - this will become a requirement for post-1.0.0 Zig releases.

#### [Tier 1](https://ziglang.org/download/0.15.1/release-notes.html#toc-Tier-1) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Tier-1)

-   All non-experimental language features are known to work correctly.

-   The compiler can generate machine code for this target without relying on LLVM, while being comparable to LLVM in terms of feature support.
-   The CI machines automatically run the module tests for this target on every push.

#### [Tier 2](https://ziglang.org/download/0.15.1/release-notes.html#toc-Tier-2) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Tier-2)

-   The standard library's cross-platform abstractions have implementations for this target.

-   This target has debug info capabilities and therefore produces stack traces on failed assertions and crashes.
-   libc is available for this target even when cross-compiling.

-   The CI machines automatically build the module tests for this target on every push.

#### [Tier 3](https://ziglang.org/download/0.15.1/release-notes.html#toc-Tier-3) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Tier-3)

-   The compiler can generate machine code for this target by relying on an external backend such as LLVM.

-   The linker can produce object files, libraries, and executables for this target.

#### [Tier 4](https://ziglang.org/download/0.15.1/release-notes.html#toc-Tier-4) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Tier-4)

-   The compiler can generate assembly source code for this target by relying on an external backend such as LLVM.

-   This target may be considered experimental by LLVM, in which case it is necessary to build LLVM and Zig from source to be able to use it.

### [Support Table](https://ziglang.org/download/0.15.1/release-notes.html#toc-Support-Table) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Support-Table)

In the following table, ðŸŸ¢ indicates full support, ðŸ”´ indicates no support, and ðŸŸ¡ indicates that there is partial support, e.g. only for some sub-targets, or with some notable known issues. â” indicates that the status is largely unknown, typically because the target is rarely exercised. Hover over other icons for details.

Target

Tier

Lang. Feat.

Std. Lib.

Code Gen.

Linker

Debug Info

libc

CI

`x86_64-linux`

[1](https://github.com/ziglang/zig/issues/23079)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`x86_64-macos`

[1](https://github.com/ziglang/zig/issues/4897)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

---

`aarch64-freebsd`

[2](https://github.com/ziglang/zig/issues/3939)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`aarch64(_be)-linux`

[2](https://github.com/ziglang/zig/issues/2443)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`aarch64(_be)-netbsd`

[2](https://github.com/ziglang/zig/issues/23084)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`aarch64-macos`

[2](https://github.com/ziglang/zig/issues/23078)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`aarch64-windows`

[2](https://github.com/ziglang/zig/issues/16665)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`arm-freebsd`

[2](https://github.com/ziglang/zig/issues/23675)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`arm(eb)-linux`

[2](https://github.com/ziglang/zig/issues/3174)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`arm(eb)-netbsd`

[2](https://github.com/ziglang/zig/issues/23763)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`loongarch64-linux`

[2](https://github.com/ziglang/zig/issues/21646)

ðŸŸ¡

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`powerpc-linux`

[2](https://github.com/ziglang/zig/issues/21649)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`powerpc-netbsd`

[2](https://github.com/ziglang/zig/issues/23766)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`powerpc64-freebsd`

[2](https://github.com/ziglang/zig/issues/23678)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`powerpc64-linux`

[2](https://github.com/ziglang/zig/issues/21651)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`powerpc64le-freebsd`

[2](https://github.com/ziglang/zig/issues/23679)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`powerpc64le-linux`

[2](https://github.com/ziglang/zig/issues/21650)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`riscv32-linux`

[2](https://github.com/ziglang/zig/issues/21648)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

`riscv64-freebsd`

[2](https://github.com/ziglang/zig/issues/23676)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸŸ¢

ðŸŸ¡

`riscv64-linux`

[2](https://github.com/ziglang/zig/issues/4456)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

`thumb-windows`

[2](https://github.com/ziglang/zig/issues/24017)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`thumb(eb)-linux`

[2](https://github.com/ziglang/zig/issues/23672)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`wasm32-wasi`

[2](https://github.com/ziglang/zig/issues/23091)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸŸ¢

ðŸŸ¢

`x86-linux`

[2](https://github.com/ziglang/zig/issues/1929)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

`x86-windows`

[2](https://github.com/ziglang/zig/issues/537)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`x86_64-freebsd`

[2](https://github.com/ziglang/zig/issues/1759)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`x86_64-netbsd`

[2](https://github.com/ziglang/zig/issues/23082)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`x86_64-windows`

[2](https://github.com/ziglang/zig/issues/23080)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

---

`aarch64-haiku`

[3](https://github.com/ziglang/zig/issues/23755)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`aarch64-openbsd`

[3](https://github.com/ziglang/zig/issues/23085)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`aarch64-serenity`

[3](https://github.com/ziglang/zig/issues/23686)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`arm-haiku`

[3](https://github.com/ziglang/zig/issues/23756)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`arm-openbsd`

[3](https://github.com/ziglang/zig/issues/23773)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`hexagon-linux`

[3](https://github.com/ziglang/zig/issues/21652)

ðŸŸ¡

ðŸŸ¡

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸŸ¢

ðŸŸ¡

`mips(el)-linux`

[3](https://github.com/ziglang/zig/issues/3345)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸ”´

ðŸŸ¢

ðŸŸ¢

`mips(el)-netbsd`

[3](https://github.com/ziglang/zig/issues/23764)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸ”´

ðŸŸ¢

ðŸŸ¡

`mips64(el)-linux`

[3](https://github.com/ziglang/zig/issues/21647)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸ”´

ðŸŸ¢

ðŸŸ¢

`mips64(el)-netbsd`

[3](https://github.com/ziglang/zig/issues/23765)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸ”´

ðŸŸ¢

ðŸŸ¡

`mips64(el)-openbsd`

[3](https://github.com/ziglang/zig/issues/23774)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸

ðŸŸ¢

ðŸ”´

ðŸ”´

ðŸ”´

`powerpc-openbsd`

[3](https://github.com/ziglang/zig/issues/23775)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`powerpc64-openbsd`

[3](https://github.com/ziglang/zig/issues/23776)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`riscv64-haiku`

[3](https://github.com/ziglang/zig/issues/23759)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸ”´

ðŸ”´

`riscv64-openbsd`

[3](https://github.com/ziglang/zig/issues/23777)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸ”´

ðŸ”´

`riscv64-serenity`

[3](https://github.com/ziglang/zig/issues/23687)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸ”´

ðŸ”´

`s390x-linux`

[3](https://github.com/ziglang/zig/issues/21402)

ðŸŸ¢

ðŸŸ¢

ðŸ–¥ï¸

ðŸŸ¢

ðŸ”´

ðŸŸ¢

ðŸŸ¢

`sparc64-linux`

[3](https://github.com/ziglang/zig/issues/4931)

â”

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¡

â”

ðŸŸ¢

ðŸ”´

`sparc64-netbsd`

[3](https://github.com/ziglang/zig/issues/23771)

â”

ðŸŸ¢

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¡

â”

ðŸŸ¢

ðŸ”´

`sparc64-openbsd`

[3](https://github.com/ziglang/zig/issues/23779)

â”

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¡

â”

ðŸ”´

ðŸ”´

`sparc64-solaris`

[3](https://github.com/ziglang/zig/issues/23093)

â”

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¡

â”

ðŸ”´

ðŸ”´

`wasm64-wasi`

[3](https://github.com/ziglang/zig/issues/23092)

â”

ðŸ”´

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¡

ðŸ”´

ðŸ”´

`x86_64-dragonfly`

[3](https://github.com/ziglang/zig/issues/7149)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`x86_64-haiku`

[3](https://github.com/ziglang/zig/issues/7691)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`x86_64-illumos`

[3](https://github.com/ziglang/zig/issues/7152)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`x86_64-openbsd`

[3](https://github.com/ziglang/zig/issues/2016)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸ðŸ› ï¸

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`x86_64-serenity`

[3](https://github.com/ziglang/zig/issues/23688)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

`x86_64-solaris`

[3](https://github.com/ziglang/zig/issues/7151)

ðŸŸ¢

ðŸŸ¡

ðŸ–¥ï¸âš¡

ðŸŸ¢

ðŸŸ¢

ðŸ”´

ðŸ”´

---

`arc-linux`

[4](https://github.com/ziglang/zig/issues/23086)

â”

ðŸŸ¡

ðŸ“„

ðŸ”´

â”

ðŸŸ¢

ðŸ”´

`csky-linux`

[4](https://github.com/ziglang/zig/issues/23087)

â”

ðŸŸ¡

ðŸ“„

ðŸ”´

â”

ðŸŸ¢

ðŸ”´

`m68k-linux`

[4](https://github.com/ziglang/zig/issues/23089)

â”

ðŸŸ¡

ðŸ–¥ï¸

ðŸ”´

â”

ðŸŸ¢

ðŸ”´

`m68k-netbsd`

[4](https://github.com/ziglang/zig/issues/23090)

â”

ðŸŸ¡

ðŸ–¥ï¸

ðŸ”´

â”

ðŸŸ¢

ðŸ”´

`sparc-linux`

[4](https://github.com/ziglang/zig/issues/23081)

â”

ðŸŸ¡

ðŸ–¥ï¸

ðŸ”´

â”

ðŸŸ¢

ðŸ”´

`xtensa-linux`

[4](https://github.com/ziglang/zig/issues/23081)

â”

ðŸ”´

ðŸ“„

ðŸ”´

â”

ðŸ”´

ðŸ”´

### [OS Version Requirements](https://ziglang.org/download/0.15.1/release-notes.html#toc-OS-Version-Requirements) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#OS-Version-Requirements)

The Zig standard library has minimum version requirements for some supported operating systems, which in turn affect the Zig compiler itself.

Operating System

Minimum Version

Dragonfly BSD

6.0

FreeBSD

14.0

Linux

5.10

NetBSD

10.1

OpenBSD

7.6

macOS

13.0

Solaris

11

Windows

10

### [Additional Platforms](https://ziglang.org/download/0.15.1/release-notes.html#toc-Additional-Platforms) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Additional-Platforms)

Zig also has varying levels of support for these targets, for which the tier system does not quite apply:

-   `aarch64-driverkit`

-   `aarch64(_be)-freestanding`
-   `aarch64-ios`

-   `aarch64-tvos`
-   `aarch64-uefi`

-   `aarch64-visionos`
-   `aarch64-watchos`

-   `amdgcn-amdhsa`
-   `amdgcn-amdpal`

-   `amdgcn-mesa3d`
-   `arc-freestanding`

-   `arm(eb)-freestanding`
-   `arm-uefi`

-   `avr-freestanding`
-   `bpf(eb,el)-freestanding`

-   `csky-freestanding`
-   `hexagon-freestanding`

-   `kalimba-freestanding`
-   `lanai-freestanding`

-   `loongarch(32,64)-freestanding`
-   `loongarch(32,64)-uefi`

-   `m68k-freestanding`
-   `mips(64)(el)-freestanding`

-   `msp430-freestanding`
-   `nvptx(64)-cuda`

-   `nvptx(64)-nvcl`
-   `powerpc(64)(le)-freestanding`

-   `propeller-freestanding`
-   `riscv(32,64)-freestanding`

-   `riscv(32,64)-uefi`
-   `s390x-freestanding`

-   `sparc(64)-freestanding`
-   `spirv(32,64)-opencl`

-   `spirv(32,64)-opengl`
-   `spirv(32,64)-vulkan`

-   `thumb(eb)-freestanding`
-   `ve-freestanding`

-   `wasm(32,64)-emscripten`
-   `wasm(32,64)-freestanding`

-   `x86(_64)-freestanding`
-   `x86(_64)-uefi`

-   `x86_64-driverkit`
-   `x86_64-ios`

-   `x86_64-tvos`
-   `x86_64-visionos`

-   `x86_64-watchos`
-   `xcore-freestanding`

-   `xtensa-freestanding`

## [Language Changes](https://ziglang.org/download/0.15.1/release-notes.html#toc-Language-Changes) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Language-Changes)

Minor changes:

-   packed union fields are no longer allowed to specify an align attribute, matching the existing behaviour with packed structs. Providing an override for the alignment previously did not affect the alignment of fields, and migration to these new rules takes the form of deleting the specifier. #22997

### [usingnamespace Removed](https://ziglang.org/download/0.15.1/release-notes.html#toc-usingnamespace-Removed) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#usingnamespace-Removed)

This keyword added distance between the "expected" definition of a declaration and its "actual" definition. Without it, discovering a declaration's definition site is incredibly simple: find the definition of the namespace you are looking in, then find the identifier being defined within that type declaration. With `usingnamespace`, however, the programmer can be led on a wild goose chase through different types and files.

![Carmen the Allocgator](https://ziglang.org/img/Carmen_1.svg)

Not only does this harm readability for humans, but it is also problematic for tooling; for instance, Autodoc cannot reasonably see through non-trivial uses of `usingnamespace` (try looking for dl\_iterate\_phdr under std.c in the 0.14.1 documentation).

By eliminating this feature, all identifiers can be trivially traced back to where they are imported - by humans and machines alike.

Additionally, `usingnamespace` encourages poor namespacing. When declarations are stored in a separate file, that typically means they share something in common which is not shared with the contents of another file. As such, it is likely a very reasonable choice to actually expose the contents of that file via a separate namespace, rather than including them in a more general parent namespace. To put it shortly: **namespacing is good, actually**.

Finally, removal of this feature makes [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation) fundamentally simpler.

#### [Use Case: Conditional Inclusion](https://ziglang.org/download/0.15.1/release-notes.html#toc-Use-Case-Conditional-Inclusion) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Use-Case-Conditional-Inclusion)

`usingnamespace` can be used to conditionally include a declaration as follows:

```
pub usingnamespace if (have_foo) struct {
    pub const foo = 123;
} else struct {};
```

The solution here is pretty simple: usually, you can just include the declaration unconditionally. Zig's lazy compilation means that it will not be analyzed unless referenced, so there are no problems!

```
pub const foo = 123;
```

Occasionally, this is not a good solution, as it lacks safety. Perhaps analyzing `foo` will always work, but will only give a meaningful result if `have_foo` is true, and it would be a bug to use it in any other case. In such cases, the declaration can be conditionally made a compile error:

```
pub const foo = if (have_foo)
    123
else
    @compileError("foo not supported on this target");
```

This does break feature detection with `@hasDecl`. If feature detection is needed, a better approachâ€”less prone to typos and bitrottingâ€”is to conditionally initialize the declaration to some "sentinel" value which can be detected. A good choice is often the `void` value `{}`:

feature-detection.zig

```
const something = struct {
    // In this example, `foo` is supported but `bar` is not.
    const have_foo = true;
    const have_bar = false;
    pub const foo = if (have_foo) 123 else {};
    pub const bar = if (have_bar) undefined else {};
};

test "use foo if supported" {
    if (@TypeOf(something.foo) == void) return error.SkipZigTest; // unsupported
    try expect(something.foo == 123);
}

test "use bar if supported" {
    if (@TypeOf(something.bar) == void) return error.SkipZigTest; // unsupported
    try expect(something.bar == 456);
}

const expect = @import("std").testing.expect;
```

Shell

$ zig test feature-detection.zig
1/2 feature-detection.test.use foo if supported...OK
2/2 feature-detection.test.use bar if supported...SKIP
1 passed; 1 skipped; 0 failed.

#### [Use Case: Implementation Switching](https://ziglang.org/download/0.15.1/release-notes.html#toc-Use-Case-Implementation-Switching) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Use-Case-Implementation-Switching)

A close cousin of conditional inclusion, `usingnamespace` can also be used to select from multiple implementations of a declaration at comptime:

```
pub usingnamespace switch (target) {
    .windows => struct {
        pub const target_name = "windows";
        pub fn init() T {
            // ...
        }
    },
    else => struct {
        pub const target_name = "something good";
        pub fn init() T {
            // ...
        }
    },
};
```

The alternative to this is simpler and results in better code: make the definition itself a conditional.

```
pub const target_name = switch (target) {
    .windows => "windows",
    else => "something good",
};
pub const init = switch (target) {
    .windows => initWindows,
    else => initOther,
};
fn initWindows() T {
    // ...
}
fn initOther() T {
    // ...
}
```

#### [Use Case: Mixins](https://ziglang.org/download/0.15.1/release-notes.html#toc-Use-Case-Mixins) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Use-Case-Mixins)

A very common use case for `usingnamespace` in the wild was to implement mixins:

```
/// Mixin to provide methods to manipulate the `count` field.
pub fn CounterMixin(comptime T: type) type {
    return struct {
        pub fn incrementCounter(x: *T) void {
            x.count += 1;
        }
        pub fn resetCounter(x: *T) void {
            x.count = 0;
        }
    };
}

pub const Foo = struct {
    count: u32 = 0,
    pub usingnamespace CounterMixin(Foo);
};
```

The alternative for this is based on the key observation made above: **namespacing is good, actually**. The same logic can be applied to mixins. The word "counter" in `incrementCounter` and `resetCounter` already kind of *is* a namespace in spiritâ€”it's like how we used to have `std.ChildProcess` but have since renamed it to `std.process.Child`. The same idea can be applied here: what if instead of `foo.incrementCounter()`, you called `foo.counter.increment()`?

This can be achieved using a zero-bit field and `@fieldParentPtr`. Here is the above example ported to use this mechanism:

```
/// Mixin to provide methods to manipulate the `count` field.
pub fn CounterMixin(comptime T: type) type {
    return struct {
        pub fn increment(m: *@This()) void {
            const x: *T = @alignCast(@fieldParentPtr("counter", m));
            x.count += 1;
        }
        pub fn reset(m: *@This()) void {
            const x: *T = @alignCast(@fieldParentPtr("counter", m));
            x.count = 0;
        }
    };
}

pub const Foo = struct {
    count: u32 = 0,
    counter: CounterMixin(Foo) = .{},
};
```

This code works just like before, except the usage is `foo.counter.increment()` rather than `foo.incrementCounter()`. We have applied namespacing to our mixin using zero-bit fields. In fact, this mechanism is *more* useful, because it allows you to also include fields! For instance, in this case, we could move the `count` field to `CounterMixin`. In this case that actually wouldn't be a mixin at all, since that field is the only state `CounterMixin` usesâ€”in fact, this is a demonstration that the need for mixins is relatively rare. But in cases where a mixin *is* appropriate, yet requires additional state, this approach allows using the mixin without needing to duplicate fields at each mixin site.

### [async and await keywords removed](https://ziglang.org/download/0.15.1/release-notes.html#toc-async-and-await-keywords-removed) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#async-and-await-keywords-removed)

Also removed `@frameSize`.

While `suspend`, `resume`, and other machinery might remain depending on [Proposal: stackless coroutines as low-level primitives](https://github.com/ziglang/zig/issues/23446), it is settled that there will not be async/await keywords in the language. Instead, they will be in the [Standard Library](https://ziglang.org/download/0.15.1/release-notes.html#Standard-Library) as part of the [Io Interface](https://ziglang.org/download/0.15.1/release-notes.html#IO-as-an-Interface).

### [switch on non-exhaustive enums](https://ziglang.org/download/0.15.1/release-notes.html#toc-switch-on-non-exhaustive-enums) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#switch-on-non-exhaustive-enums)

Switching on non-exhaustive enums now allows mixing explicit tags with the `_` prong (which represents all the unnamed values):

```
switch (enum_val) {
    .special_case_1 => foo(),
    .special_case_2 => bar(),
    _, .special_case_3 => baz(),
}
```

Additionally, it is now allowed to have both `else` and `_`:

```
const Enum = enum(u32) {
    A = 1,
    B = 2,
    C = 44,
    _
};

fn someOtherFunction(value: Enum) void {
    // Does not compile giving "error: else and '_' prong in switch expression"
    switch (value) {
        .A   => {},
        .C   => {},
        else => {}, // Named tags go here (so, .B in this case)
        _    => {}, // Unnamed tags go here
    }
}
```

### [Allow more operators on bool vectors](https://ziglang.org/download/0.15.1/release-notes.html#toc-Allow-more-operators-on-bool-vectors) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Allow-more-operators-on-bool-vectors)

Allow binary not, binary and, binary or, binary xor, and boolean not operators on vectors of `bool`.

### [Inline Assembly: Typed Clobbers](https://ziglang.org/download/0.15.1/release-notes.html#toc-Inline-Assembly-Typed-Clobbers) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Inline-Assembly-Typed-Clobbers)

Until now these were stringly typed. It's kinda obvious when you think about it.

```
pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
        : "rcx", "r11"
    );
}
```

â¬‡ï¸

```
pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
        : .{ .rcx = true, .r11 = true });
}
```

To auto-upgrade, run `zig fmt`.

### [Allow @ptrCast Single-Item Pointer to Slice](https://ziglang.org/download/0.15.1/release-notes.html#toc-Allow-ptrCast-Single-Item-Pointer-to-Slice) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Allow-ptrCast-Single-Item-Pointer-to-Slice)

This is essentially an extension of the 0.14.0 change which allowed `@ptrCast` to change the length of a slice. It can now also cast from a single-item pointer to any slice, returning a slice which refers to the same number of bytes as the operand.

ptrcast-single.zig

```
const std = @import("std");

test "value to byte slice with @ptrCast" {
    const val: u32 = 1;
    const bytes: []const u8 = @ptrCast(&val);
    switch (@import("builtin").target.cpu.arch.endian()) {
        .little => try std.testing.expect(std.mem.eql(u8, bytes, "\x01\x00\x00\x00")),
        .big => try std.testing.expect(std.mem.eql(u8, bytes, "\x00\x00\x00\x01")),
    }
}
```

Shell

$ zig test ptrcast-single.zig
1/1 ptrcast-single.test.value to byte slice with @ptrCast...OK
All 1 tests passed.

Note that in a future release, it is planned to move this functionality from `@ptrCast` to a new `@memCast` builtin, with the intention that the latter is a safer builtin which helps avoid unintentional out-of-bounds memory access. For more information, see [issue #23935](https://github.com/ziglang/zig/issues/23935).

### [New Rules for Arithmetic on undefined](https://ziglang.org/download/0.15.1/release-notes.html#toc-New-Rules-for-Arithmetic-on-undefined) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#New-Rules-for-Arithmetic-on-undefined)

Zig 0.15.x begins to standardise the rules around how `undefined` behaves in different contextsâ€”in particular, how it behaves as an operand to arithmetic operators. In summary, only operators which can never trigger Illegal Behavior permit `undefined` as an operand. Any other operator will trigger Illegal Behavior (or a compile error if evaluated at `comptime`) if any operand is `undefined`.

Generally, it is always best practice to avoid any operation on `undefined`. If you do that, this language change, and any that follow, are unlikely to affect you. If you are affected by this language change, you might see a compile error on code which previously worked:

arith-on-undefined.zig

```
const a: u32 = 0;
const b: u32 = undefined;

test "arithmetic on undefined" {
    // This addition now triggers a compile error
    _ = a + b;
    // The solution is to simply avoid this operation!
}
```

Shell

$ zig test arith-on-undefined.zig
src/download/0.15.1/release-notes/arith-on-undefined.zig:6:13: error: use of undefined value here causes illegal behavior
    \_ = a + b;
            ^

### [Error on Lossy Coercion from Int to Float](https://ziglang.org/download/0.15.1/release-notes.html#toc-Error-on-Lossy-Coercion-from-Int-to-Float) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Error-on-Lossy-Coercion-from-Int-to-Float)

This compile error has always been intended, but has gone unimplemented until now. The compiler will now emit a compile error if an integer value is coerced to a float at `comptime` but the integer value could not be precisely represented due to floating-point precision limitations. If you encounter this, you will get a compile error like this:

lossy\_int\_to\_float\_coercion.zig

```
test "big float literal" {
    const val: f32 = 123_456_789;
    _ = val;
}
```

Shell

$ zig test lossy\_int\_to\_float\_coercion.zig
src/download/0.15.1/release-notes/lossy\_int\_to\_float\_coercion.zig:2:22: error: type 'f32' cannot represent integer value '123456789'
    const val: f32 = 123\_456\_789;
                     ^~~~~~~~~~~

The solution is typically just to change an integer literal to a floating-point literal, thereby opting in to floating-point rounding behavior:

lossy\_int\_to\_float\_coercion\_new.zig

```
test "big float literal" {
    const val: f32 = 123_456_789.0;
    _ = val;
}
```

Shell

$ zig test lossy\_int\_to\_float\_coercion\_new.zig
1/1 lossy\_int\_to\_float\_coercion\_new.test.big float literal...OK
All 1 tests passed.

## [Standard Library](https://ziglang.org/download/0.15.1/release-notes.html#toc-Standard-Library) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Standard-Library)

Uncategorized changes:

-   `fs.Dir.copyFile` no longer can fail with `error.OutOfMemory`

-   `fs.Dir.atomicFile` now requires a `write_buffer` in the options
-   `fs.AtomicFile` now has a `File.Writer` field rather than `File` field

-   `fs.File`: removed `WriteFileOptions`, `writeFileAll`, `writeFileAllUnseekable` in favor of `File.Writer`
-   `posix.sendfile` removed in favor of `fs.File.Reader.sendFile`

### [Writergate](https://ziglang.org/download/0.15.1/release-notes.html#toc-Writergate) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Writergate)

[Previous Scandal](https://ziglang.org/download/0.9.0/release-notes.html#Allocgate)

All existing std.io readers and writers are deprecated in favor of the newly provided `std.Io.Reader` and `std.Io.Writer` which are *non-generic* and have the buffer above the vtable - in other words the buffer is **in the interface, not the implementation**. This means that although Reader and Writer are no longer generic, they are still transparent to optimization; all of the interface functions have a concrete hot path operating on the buffer, and only make vtable calls when the buffer is full.

These changes are extremely breaking. I am sorry for that, but I have carefully examined the situation and acquired confidence that this is the direction that Zig needs to go. I hope you will strap in your seatbelt and come along for the ride; it will be worth it.

#### [Motivation](https://ziglang.org/download/0.15.1/release-notes.html#toc-Motivation) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Motivation)

[Systems Distributed 2025 Talk: Don't Forget To Flush](https://www.youtube.com/watch?v=f30PceqQWko)

-   The old interface was generic, poisoning structs that contain them and forcing all functions to be generic as well with `anytype`. The new interface is concrete.
    -   Bonus: the concreteness removes temptation to make APIs operate directly on networking streams, file handles, or memory buffers, giving us a more reusable body of code. For example, `http.Server` after the change no longer depends on `std.net` - it operates only on streams now.
-   The old interface passed errors through rather than defining its own set of error codes. This made errors in streams about as useful as `anyerror`. The new interface carefully defines precise error sets for each function with actionable meaning.

-   The new interface has the buffer in the interface, rather than as a separate "BufferedReader" / "BufferedWriter" abstraction. This is more optimizer friendly, particularly for debug mode.
-   The new interface supports high level concepts such as vectors, splatting, and direct file-to-file transfer, which can propagate through an entire graph of readers and writers, reducing syscall overhead, memory bandwidth, and CPU usage.

-   The new interface has "peek" functionality - a buffer awareness that offers API convenience for the user as well as simplicity for the implementation.

#### [Adapter API](https://ziglang.org/download/0.15.1/release-notes.html#toc-Adapter-API) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Adapter-API)

If you have an old stream and you need a new one, you can use `adaptToNewApi()` like this:

```
fn foo(old_writer: anytype) !void {
    var adapter = old_writer.adaptToNewApi(&.{});
    const w: *std.Io.Writer = &adapter.new_interface;
    try w.print("{s}", .{"example"});
    // ...
}
```

#### [New std.Io.Writer and std.Io.Reader API](https://ziglang.org/download/0.15.1/release-notes.html#toc-New-stdIoWriter-and-stdIoReader-API) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#New-stdIoWriter-and-stdIoReader-API)

These **ring buffers** have a bunch of handy new APIs that are more convenient, perform better, and are not generic. For instance look at how reading until a delimiter works now:

```
while (reader.takeDelimiterExclusive('\n')) |line| {
    // do something with line...
} else |err| switch (err) {
    error.EndOfStream, // stream ended not on a line break
    error.StreamTooLong, // line could not fit in buffer
    error.ReadFailed, // caller can check reader implementation for diagnostics
    => |e| return e,
}
```

These streams also feature some unique concepts compared with other languages' stream implementations:

-   The concept of **discarding** when reading: allows efficiently ignoring data. For instance a decompression stream, when asked to discard a large amount of data, can skip decompression of entire frames.

-   The concept of **splatting** when writing: this allows a logical "memset" operation to pass through I/O pipelines without actually doing any memory copying, turning an O(M\*N) operation into O(M) operation, where M is the number of streams in the pipeline and N is the number of repeated bytes. In some cases it can be even more efficient, such as when splatting a zero value that ends up being written to a file; this can be lowered as a seek forward.
-   Sending a file when writing: this allows an I/O pipeline to do direct fd-to-fd copying when the operating system supports it.

-   The stream user provides the buffer, but the stream implementation decides the minimum buffer size. This effectively moves state from the stream implementation into the user's buffer

#### [std.fs.File.Reader and std.fs.File.Writer](https://ziglang.org/download/0.15.1/release-notes.html#toc-stdfsFileReader-and-stdfsFileWriter) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#stdfsFileReader-and-stdfsFileWriter)

`std.fs.File.Reader` memoizes key information about a file handle such as:

-   The size from calling stat, or the error that occurred therein.

-   The current seek position.
-   The error that occurred when trying to seek.

-   Whether reading should be done positionally or streaming.
-   Whether reading should be done via fd-to-fd syscalls (e.g. `sendfile`)  
    versus plain variants (e.g. `read`).

Fulfills the `std.Io.Reader` interface.

This API turned out to be super handy in practice. Having a concrete type to pass around that memoizes file size is really nice. Most code that previously was calling seek functions on a file handle should be updated to operate on this API instead, causing those seeks to become no-ops thanks to positional reads, while still supporting a fallback to streaming reading.

`std.fs.File.Writer` is the same idea but for writing.

#### [Upgrading std.io.getStdOut().writer().print()](https://ziglang.org/download/0.15.1/release-notes.html#toc-Upgrading-stdiogetStdOutwriterprint) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Upgrading-stdiogetStdOutwriterprint)

Please use buffering! And **don't forget to flush**!

```
var stdout_buffer: [1024]u8 = undefined;
var stdout_writer = std.fs.File.stdout().writer(&stdout_buffer);
const stdout = &stdout_writer.interface;

// ...

try stdout.print("...", .{});

// ...

try stdout.flush();
```

#### [reworked std.compress.flate](https://ziglang.org/download/0.15.1/release-notes.html#toc-reworked-stdcompressflate) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#reworked-stdcompressflate)

![Carmen the Allocgator](https://ziglang.org/img/Carmen_4.svg)

`std.compress` API restructured everything to do with flate, which includes zlib and gzip. `std.compress.flate.Decompress` is your main API now and it has a container parameter.

New API example:

```
var decompress_buffer: [std.compress.flate.max_window_len]u8 = undefined;
var decompress: std.compress.flate.Decompress = .init(reader, .zlib, &decompress_buffer);
const decompress_reader: *std.Io.Reader = &decompress.reader;
```

If `decompress_reader` will be piped entirely to a particular `*Writer`, then give it an empty buffer:

```
var decompress: std.compress.flate.Decompress = .init(reader, .zlib, &.{});
const n = try decompress.streamRemaining(writer);
```

Compression functionality was removed. Sorry, you will have to copy the old code into your application, or use a third party package.

It will be nice to get deflate back into the Zig standard library, but for now, progressing the language takes priority over progressing the standard library, and this change is on the path towards locking in the final language design with respect to [I/O as an Interface](https://ziglang.org/download/0.15.1/release-notes.html#IO-as-an-Interface).

Some notable factors:

-   New implementation does not calculate a checksum since it can be done out-of-band.

-   New implementation has the fancy match logic replaced with a naive `for` loop. In the future it would be nice to add a memory copying utility for this that zstd would also use. Despite this, the new implementation performs roughly 10% better in an untar implementation, while reducing compiler code size by 2%. #24614

#### [CountingWriter Deleted](https://ziglang.org/download/0.15.1/release-notes.html#toc-CountingWriter-Deleted) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#CountingWriter-Deleted)

-   If you were discarding the bytes, use `std.Io.Writer.Discarding`, which has a count.

-   If you were allocating the bytes, use `std.Io.Writer.Allocating`, since you can check how much was allocated.
-   If you were writing to a fixed buffer, use `std.Io.Writer.fixed`, and then check the `end` position.

-   Otherwise, try not to create an entire node in the stream graph solely for counting bytes. It's very disruptive to optimal buffering.

#### [BufferedWriter Deleted](https://ziglang.org/download/0.15.1/release-notes.html#toc-BufferedWriter-Deleted) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#BufferedWriter-Deleted)

```
const stdout_file = std.fs.File.stdout().writer();
var bw = std.io.bufferedWriter(stdout_file);
const stdout = bw.writer();

try stdout.print("Run `zig build test` to run the tests.\n", .{});

try bw.flush(); // Don't forget to flush!
```

â¬‡ï¸

```
var stdout_buffer: [4096]u8 = undefined;
var stdout_writer = std.fs.File.stdout().writer(&stdout_buffer);
const stdout = &stdout_writer.interface;

try stdout.print("Run `zig build test` to run the tests.\n", .{});

try stdout.flush(); // Don't forget to flush!
```

Consider making your stdout buffer global.

### ["{f}" Required to Call format Methods](https://ziglang.org/download/0.15.1/release-notes.html#toc-f-Required-to-Call-format-Methods) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#f-Required-to-Call-format-Methods)

Turn on `-freference-trace` to help you find all the format string breakage.

Example:

```
std.debug.print("{}", .{std.zig.fmtId("example")});
```

This will now cause a compile error:

```
error: ambiguous format string; specify {f} to call format method, or {any} to skip it
```

Fixed by:

```
std.debug.print("{f}", .{std.zig.fmtId("example")});
```

Motivation: eliminate these two footguns:

Introducing a `format` method to a struct caused a bug if there was formatting code somewhere that prints with {} and then starts rendering differently.

Removing a `format` method to a struct caused a bug if there was formatting code somewhere that prints with {} and is now changed without notice.

Now, introducing a `format` method will cause compile errors at all `{}` sites. In the future, it will have no effect.

Similarly, eliminating a `format` method will not change any sites that use `{}`.

Using `{f}` always tries to call a `format` method, causing a compile error if none exists.

### [Format Methods No Longer Have Format Strings or Options](https://ziglang.org/download/0.15.1/release-notes.html#toc-Format-Methods-No-Longer-Have-Format-Strings-or-Options) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Format-Methods-No-Longer-Have-Format-Strings-or-Options)

```
pub fn format(
    this: @This(),
    comptime format_string: []const u8,
    options: std.fmt.FormatOptions,
    writer: anytype,
) !void { ... }
```

â¬‡ï¸

```
pub fn format(this: @This(), writer: *std.Io.Writer) std.Io.Writer.Error!void { ... }
```

The deleted `FormatOptions` are now for numbers only.

Any state that you got from the format string, there are three suggested alternatives:

1.  different format methods

```
pub fn formatB(foo: Foo, writer: *std.Io.Writer) std.Io.Writer.Error!void { ... }
```

This can be called with `"{f}", .{std.fmt.alt(Foo, .formatB)}`.

2.  `std.fmt.Alt`

```
pub fn bar(foo: Foo, context: i32) std.fmt.Alt(F, F.baz) {
    return .{ .data = .{ .context = context } };
}
const F = struct {
    context: i32,
    pub fn baz(f: F, writer: *std.Io.Writer) std.Io.Writer.Error!void { ... }
};
```

This can be called with `"{f}", .{foo.bar(1234)}`.

3.  return a struct instance that has a format method, combined with `{f}`.

```
pub fn bar(foo: Foo, context: i32) F {
    return .{ .context = 1234 };
}
const F = struct {
    context: i32,
    pub fn format(f: F, writer: *std.Io.Writer) std.Io.Writer.Error!void { ... }
};
```

This can be called with `"{f}", .{foo.bar(1234)}`.

### [Formatted Printing No Longer Deals with Unicode](https://ziglang.org/download/0.15.1/release-notes.html#toc-Formatted-Printing-No-Longer-Deals-with-Unicode) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Formatted-Printing-No-Longer-Deals-with-Unicode)

If you were relying on alignment combined with Unicode codepoints, it is now ASCII/bytes only. The previous implementation was not fully Unicode-aware. If you want to align Unicode strings you need full Unicode support which the standard library does not provide.

### [New Formatted Printing Specifiers](https://ziglang.org/download/0.15.1/release-notes.html#toc-New-Formatted-Printing-Specifiers) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#New-Formatted-Printing-Specifiers)

-   {t} is shorthand for `@tagName()` and `@errorName()`

-   {d} and other integer printing can be used with custom types which calls `formatNumber` method.
-   {b64}: output string as standard base64

### [De-Genericify Linked Lists](https://ziglang.org/download/0.15.1/release-notes.html#toc-De-Genericify-Linked-Lists) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#De-Genericify-Linked-Lists)

With these changes, there's no longer any incentive to hand-roll next/prev pointers. A little bit less bloat too.

Migration guide:

```
std.DoublyLinkedList(T).Node
```

â¬‡ï¸

```
struct {
    node: std.DoublyLinkedList.Node,
    data: T,
}
```

Then use `@fieldParentPtr` to get from `node` to `data`.

In many cases there's a better pattern instead which is to put the node intrusively into the data structure. If you're not already doing that, there's a good chance linked list is the wrong data structure.

### [std.Progress supports progress bar escape codes](https://ziglang.org/download/0.15.1/release-notes.html#toc-stdProgress-supports-progress-bar-escape-codes) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#stdProgress-supports-progress-bar-escape-codes)

Turns out there are [escape codes for sending progress status to the terminal](https://conemu.github.io/en/AnsiEscapeCodes.html#ConEmu_specific_OSC).

It integrates with `--watch` in the [Build System](https://ziglang.org/download/0.15.1/release-notes.html#Build-System) to set error state when failures occur, and clear it when they are fixed, also to clear progress when waiting for user input.

`std.Progress` gains a `setStatus` function and the following enum:

```
pub const Status = enum {
    /// Indicates the application is progressing towards completion of a task.
    /// Unless the application is interactive, this is the only status the
    /// program will ever have!
    working,
    /// The application has completed an operation, and is now waiting for user
    /// input rather than calling exit(0).
    success,
    /// The application encountered an error, and is now waiting for user input
    /// rather than calling exit(1).
    failure,
    /// The application encountered at least one error, but is still working on
    /// more tasks.
    failure_working,
};
```

### [HTTP Client and Server](https://ziglang.org/download/0.15.1/release-notes.html#toc-HTTP-Client-and-Server) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#HTTP-Client-and-Server)

These APIs and implementations have been completely reworked.

Server API no longer depends on `std.net`. Instead, it only depends on `std.Io.Reader` and `std.Io.Writer`. It also has all the arbitrary limitations removed. For instance, there is no longer a limit on how many headers can be sent.

```
var read_buffer: [8000]u8 = undefined;
var server = std.http.Server.init(connection, &read_buffer);
```

â¬‡ï¸

```
var recv_buffer: [4000]u8 = undefined;
var send_buffer: [4000]u8 = undefined;
var conn_reader = connection.stream.reader(&recv_buffer);
var conn_writer = connection.stream.writer(&send_buffer);
var server = std.http.Server.init(conn_reader.interface(), &conn_writer.interface);
```

Server and Client both share `std.http.Reader` and `std.http.BodyWriter` which again only depends on I/O streams and not networking.

Client upgrade example:

```
var server_header_buffer: [1024]u8 = undefined;
var req = try client.open(.GET, uri, .{
    .server_header_buffer = &server_header_buffer,
});
defer req.deinit();

try req.send();
try req.wait();

const body_reader = try req.reader();
// read from body_reader...

var it = req.response.iterateHeaders();
while (it.next()) |header| {
    _ = header.name;
    _ = header.value;
}
```

â¬‡ï¸

```
var req = try client.request(.GET, uri, .{});
defer req.deinit();

try req.sendBodiless();
var response = try req.receiveHead(&.{});

// Once we call reader() below, strings inside `response.head` are invalidated.
var it = response.head.iterateHeaders();
while (it.next()) |header| {
    _ = header.name;
    _ = header.value;
}

// Optimal size depends on how you will use the reader.
var reader_buffer: [100]u8 = undefined;
const body_reader = response.reader(&reader_buffer);
```

### [TLS Client](https://ziglang.org/download/0.15.1/release-notes.html#toc-TLS-Client) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#TLS-Client)

`std.crypto.tls.Client` no longer depends on `std.net` or `std.fs`. Instead, it only depends on `std.Io.Reader` and `std.Io.Writer`.

### [ArrayList: make unmanaged the default](https://ziglang.org/download/0.15.1/release-notes.html#toc-ArrayList-make-unmanaged-the-default) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#ArrayList-make-unmanaged-the-default)

-   `std.ArrayList` -> `std.array_list.Managed`

-   `std.ArrayListAligned` -> `std.array_list.AlignedManaged`

Warning: these will both eventually be removed entirely.

Having an extra field is more complicated than not having an extra field, so not having it is the null hypothesis. What pattern does having an allocator field allow that not having one doesn't?

-   avoiding accidentally using the wrong allocator

-   convenience when you need to pass an allocator also

But there are downsides:

-   worse method function signatures in the face of reservations

-   inability to statically initialize
-   extra memory storage cost, particularly for nested containers

The reasoning goes like this: the upsides are not worth the downsides. Also, given that the correct allocator is always handy, and incorrect use can be trivially safety-checked, the simplicity of only having one implementation is quite valuable compared to the convenience that is gained by having a second implementation.

In practice, this has not been a controversial change with experienced Zig users.

### [Ring Buffers](https://ziglang.org/download/0.15.1/release-notes.html#toc-Ring-Buffers) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Ring-Buffers)

[There are too many ring buffer implementations in the standard library!](https://github.com/ziglang/zig/issues/19231)

`std.fifo.LinearFifo` is removed due to being poorly designed. This data structure was unnecessarily generic due to accepting a comptime enum parameter that determined whether its buffer was heap-allocated with an Allocator parameter, passed in as an externally-owned slice, or stored in the struct itself. Each of these different buffer management strategies describes a fundamentally different data structure.

Furthermore, most of its real-world use cases are subsumed by [New std.Io.Writer and std.Io.Reader API](https://ziglang.org/download/0.15.1/release-notes.html#New-stdIoWriter-and-stdIoReader-API) which are both ring buffers.

Similarly, `std.RingBuffer` is removed since it was only used by the zstd implementation which has been upgraded to use [New std.Io.Writer and std.Io.Reader API](https://ziglang.org/download/0.15.1/release-notes.html#New-stdIoWriter-and-stdIoReader-API).

There was also `std.compress.flate.CircularBuffer` which was internal to the flate implementation; now deleted.

There was also one each in [HTTP Client and Server](https://ziglang.org/download/0.15.1/release-notes.html#HTTP-Client-and-Server) - again deleted in favor of [New std.Io.Writer and std.Io.Reader API](https://ziglang.org/download/0.15.1/release-notes.html#New-stdIoWriter-and-stdIoReader-API).

Even with all five of these deletions, these things pop up like whack-a-mole. Here are some more ring buffers that have been spotted:

-   `lib/std/compress/lzma/decode/lzbuffer.zig` - internal to lzma implementation.

-   `lib/std/crypto/tls.zig` - made redundant with std.Io.Reader.
-   `lib/std/debug/FixedBufferReader.zig` - made redundant by std.Io.Reader's excellent Debug mode performance.

-   [this random pull request](https://github.com/ziglang/zig/pull/24705) - nice try, you almost got away with it!!

Jokes aside, there will likely be room for a general-purpose, reusable ring buffer implementation in the standard library, however, first ask yourself if what you really need is `std.Io.Reader` or `std.Io.Writer`.

### [Removal of BoundedArray](https://ziglang.org/download/0.15.1/release-notes.html#toc-Removal-of-BoundedArray) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Removal-of-BoundedArray)

This data structure was popular due to being trivially copyable. However, such convenience comes at a cost.

To upgrade, categorize code based on where the limit comes from:

-   Is it an arbitrary limit for which the BoundedArray usage is making a reasonable guess at the upper bound, or deciding resource limits? Don't guess. Don't make that choice for the calling code. Accept a buffer as a slice as an input, or use dynamic allocation. (example: the markdown code in #24699)

-   Is it type safety around a stack buffer? Just use ArrayListUnmanaged. It's fine. It's actually really convenient that this same data structure works here. (example: test\_switch\_dispatch\_loop.zig in #24699)

`std.ArrayList` now has "Bounded" variants of all the "AssumeCapacity" methods:

```
var stack = try std.BoundedArray(i32, 8).fromSlice(initial_stack);
```

â¬‡ï¸

```
var buffer: [8]i32 = undefined;
var stack = std.ArrayListUnmanaged(i32).initBuffer(&buffer);
try stack.appendSliceBounded(initial_stack);
```

-   Is it an ordered set with a well-defined maximum capacity? Quite rare. Just free-code it. (example: changes to Zcu.zig in #24699)

-   Is it being used as a growable array that can be copied? This wastes time copying undefined memory all over the place and causes unnecessary generic code bloat.

### [Deletions and Deprecations](https://ziglang.org/download/0.15.1/release-notes.html#toc-Deletions-and-Deprecations) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Deletions-and-Deprecations)

-   std.fs.File.reader -> std.fs.File.deprecatedReader

-   std.fs.File.writer -> std.fs.File.deprecatedWriter
-   std.fmt.fmtSliceEscapeLower -> std.ascii.hexEscape

-   std.fmt.fmtSliceEscapeUpper -> std.ascii.hexEscape
-   std.zig.fmtEscapes -> std.zig.fmtString

-   std.fmt.fmtSliceHexLower -> {x}
-   std.fmt.fmtSliceHexUpper -> {X}

-   std.fmt.fmtIntSizeDec -> {B}
-   std.fmt.fmtIntSizeBin -> {Bi}

-   std.fmt.fmtDuration -> {D}
-   std.fmt.fmtDurationSigned -> {D}

-   std.fmt.Formatter -> std.fmt.Alt
    -   now takes context type explicitly
    -   no fmt string
-   std.fmt.format -> std.Io.Writer.print

-   std.io.GenericReader -> std.Io.Reader
-   std.io.GenericWriter -> std.Io.Writer

-   std.io.AnyReader -> std.Io.Reader
-   std.io.AnyWriter -> std.Io.Writer

-   deleted `std.io.SeekableStream`
    -   Instead, use `*std.fs.File.Reader`, `*std.fs.File.Writer`, or `std.ArrayListUnmanaged` concrete types, because the implementations will be fundamentally different based on whether you are operating on files or memory.
-   deleted `std.io.BitReader`
    -   Bit reading should not be abstracted at this layer; it just makes your hot loop harder to optimize. Tightly couple this code with your stream implementation.
-   deleted `std.io.BitWriter`
    -   ditto
-   deleted `std.Io.LimitedReader`

-   deleted `std.Io.BufferedReader`
-   deleted `std.fifo`

## [Build System](https://ziglang.org/download/0.15.1/release-notes.html#toc-Build-System) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Build-System)

Uncategorized changes:

-   `zig build`: print newline before build summary

### [Removed Deprecated Implicit Root Module](https://ziglang.org/download/0.15.1/release-notes.html#toc-Removed-Deprecated-Implicit-Root-Module) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Removed-Deprecated-Implicit-Root-Module)

Zig 0.14.0 introduced the `root_module` field to `std.Build.ExecutableOptions` and friends, deprecating the old fields which defined the root module like `root_source_file`. Zig 0.15.x removes the deprecated fields. If you did not migrate in the previous release cycle, you will encounter compile errors such as this one:

deprecated-addExecutable.zig

```
pub fn build(b: *std.Build) void {
    const exe = b.addExecutable(.{
        .name = "foo",
        .root_source_file = b.path("src/main.zig"),
        .target = b.graph.host,
        .optimize = .Debug,
    });
    b.installArtifact(exe);
}

test {
    _ = &build;
}
const std = @import("std");
```

Shell

$ zig test deprecated-addExecutable.zig
src/download/0.15.1/release-notes/deprecated-addExecutable.zig:4:10: error: no field named 'root\_source\_file' in struct 'Build.ExecutableOptions'
        .root\_source\_file = b.path("src/main.zig"),
         ^~~~~~~~~~~~~~~~
/home/ci/deps/zig-x86\_64-linux-0.15.1/lib/std/Build.zig:771:31: note: struct declared here
pub const ExecutableOptions = struct {
                              ^~~~~~
referenced by:
    test\_0: src/download/0.15.1/release-notes/deprecated-addExecutable.zig:12:10

For this change's migration path, please see [the corresponding section of the Zig 0.14.0 release notes](https://ziglang.org/download/0.14.0/release-notes.html#Creating-Artifacts-from-Existing-Modules).

### [macOS File System Watching](https://ziglang.org/download/0.15.1/release-notes.html#toc-macOS-File-System-Watching) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#macOS-File-System-Watching)

The `--watch` flag to `zig build` is now supported on macOS. In Zig 0.14.0, the flag was accepted, but unfortunately behaved incorrectly with most editors. In Zig 0.15.x, this functionality has been [rewritten on macOS](https://github.com/ziglang/zig/pull/24649) to use the File System Events API for fast and reliable file system update watching.

So, if you were avoiding `--watch` in previous Zig versions due to the macOS bug, you can now use it safely. This is particularly useful if you are interested in trying [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation), since the typical way to use that feature today involves passing the flags `--watch -fincremental` to `zig build`.

### [Web Interface and Time Report](https://ziglang.org/download/0.15.1/release-notes.html#toc-Web-Interface-and-Time-Report) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Web-Interface-and-Time-Report)

Zig 0.14.0 included an experimental web interface for the work-in-progress built-in fuzzer. In this version, that interface has been replaced with a more general web interface for the build system in general. This interface can be exposed using `zig build --webui`. When this option is passed, the `zig build` process will continue running even after the build completes.

The web interface is, by itself, relatively uninteresting: it merely shows the list of build steps and information about which are in progress, and has a button to manually trigger a rebuild (hence giving a possible alternative to the `zig build --watch` workflow). If `--fuzz` is passed to `zig build`, it also exposes the [Fuzzer](https://ziglang.org/download/0.15.1/release-notes.html#Fuzzer) interface, which is mostly unchanged from 0.14.0.

However, the web interface also exposes a new feature known as "time reports". By passing the new `--time-report` option to `zig build`, the web interface will include expandable information about the time taken to evaluate every step in the graph. In particular, any `std.Build.Step.Compile` in the graph will be associated with detailed information about which parts of the Zig compiler pipeline were fast and slow, and which individual files and declarations took the most time to semantically analyze, generate machine code for, and link into the binary.

![zig build web interface](https://ziglang.org/download/0.15.1/release-notes/build-webui.png)

This is a relatively advanced feature, but it can be very useful for determining parts of your code which are needlessly slowing down compilation, by opening the "Declarations" table and viewing the first few rows.

![compile step time report](https://ziglang.org/download/0.15.1/release-notes/build-webui-time-report.png)

LLVM pass timing information is also provided if the LLVM backend was used for the compilation.

## [Compiler](https://ziglang.org/download/0.15.1/release-notes.html#toc-Compiler) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Compiler)

### [x86 Backend](https://ziglang.org/download/0.15.1/release-notes.html#toc-x86-Backend) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#x86-Backend)

![Carmen the Allocgator](https://ziglang.org/img/Carmen_10.svg)

**Zig 0.15.x enables Zig's self-hosted x86\_64 code generation backend by default in Debug mode.**

More specifically, this backend is now the default when targeting x86\_64 in Debug mode, except on NetBSD, OpenBSD, and Windows, where the LLVM backend is still the default due to [Linker](https://ziglang.org/download/0.15.1/release-notes.html#Linker) deficiencies.

When this backend is selected, you will begin to reap the benefits of the investments the Zig project has made over the past few years. Compilation time is *significantly* improvedâ€”around a 5x decrease compared to LLVM in most cases. **This is only the beginning;** the self-hosted x86\_64 backend has been built to support [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation), which will result in another extreme speed-up when it is stable enough to be enabled by default. Fast compilation is a key goal of the Zig project, and one we have been quietly making progress on for yearsâ€”this release sees those efforts starting to come to fruition.

Using the self-hosted x86 backend also means you are not subject to the effects of upstream LLVM bugs, of which [we are currently tracking over 60](https://github.com/ziglang/zig/issues?q=is%3Aissue%20state%3Aopen%20label%3Abackend-llvm%20label%3Aupstream). In fact, the self-hosted x86 backend already passes a larger subset of our "behavior test suite" than the LLVM backend does (1984/2008 vs 1977/2008). In other words, this backend provides a more complete and correct implementation of the Zig language.

Of course, the self-hosted x86 backend does currently have [some deficiencies and bugs of its own](https://github.com/ziglang/zig/issues?q=is%3Aissue%20state%3Aopen%20label%3Abackend-self-hosted%20label%3Aarch-x86_64). If you are affected by any of these issues, you can use LLVM backend for Debug builds by passing `-fllvm` on the command-line or by setting `.use_llvm = true` when creating a `std.Build.Step.Compile`. The self-hosted x86 backend is also currently known to emit [slower machine code than the LLVM backend](https://github.com/ziglang/zig/issues/24144).

But in most cases, the self-hosted backend is now a better choice for development. For instance, the Zig core team have been building the Zig compiler almost exclusively with the self-hosted x86 backend instead of LLVM for quite a while. This has been a serious improvement to our workflows, with the Zig compiler building in just a few seconds rather than 1-2 minutes. You can now expect to see similar improvements to your own development experience.

### [aarch64 Backend](https://ziglang.org/download/0.15.1/release-notes.html#toc-aarch64-Backend) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#aarch64-Backend)

Having improved the self-hosted [x86 Backend](https://ziglang.org/download/0.15.1/release-notes.html#x86-Backend) enough for it to be enabled by default, Jacob has set his eyes on a new target: aarch64. This architecture is growing in popularity, particularly since modern Apple hardware is based on it. Therefore, aarch64 is the Zig project's next big focus for self-hosted code generation without LLVM.

For this work-in-progress backend, Jacob has been able to take the lessons learnt from the x86 backend to explore new design directions. It's too early to be sure, but we expect that the new design will improve both compiler performance (being even faster than the self-hosted x86\_64 backend) *and* the quality of emitted machine code, with the ultimate goal of becoming competitive with LLVM's codegen quality in Debug mode. [This devlog](https://ziglang.org/devlog/2025/#2025-07-23) has some more details.

This backend is passing 1656/1972 (84%) behavior tests relative to LLVM, so is not ready to be enabled by default, nor is it currently usable in any real use case. However, it is making rapid progress, and is expected to become the default for Debug mode in a future release.

Our work on self-hosted code generation backends is a part of our long-term plan to [transition LLVM to an optional dependency](https://kristoff.it/blog/zig-new-relationship-llvm/) and [decouple it from the compiler implementation](https://github.com/ziglang/zig/issues/16270). Achieving this goal will lead to large decreases in compile time, good support for incremental compilation in Debug builds, and could even allow us to explore language features which LLVM cannot lower effectively.

### [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#toc-Incremental-Compilation) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation)

Zig 0.15.x makes further progress on the work-in-progress Incremental Compilation functionality, which allows the compiler to perform very fast rebuilds by only re-compiling code which has changed. Various bugs have been fixed, particularly relating to changing file imports.

This feature is still experimentalâ€”it has known bugs and can lead to miscompilations or incorrect compile errors. However, it is now stable enough to be used reliably in combination with `-fno-emit-bin`. **If you have a large project that does not compile instantly, you should be taking advantage of `--watch` combined with `-fincremental` and `-Dno-bin` for compile errors.** Seriously, it's really good. Please chat with someone if you have trouble figuring out how to expose `-Dno-bin` from your build script.

The next release cycle will continue to make progress towards enabling Incremental Compilation by default. In the meantime, if you are interested in trying this experimental feature, take a look at [#21165](https://github.com/ziglang/zig/issues/21165).

### [Threaded Codegen](https://ziglang.org/download/0.15.1/release-notes.html#toc-Threaded-Codegen) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Threaded-Codegen)

The Zig compiler is designed to be parallelized, so that different pieces of compilation work can run in parallel with one another to improve compiler performance. In the past the Zig compiler was largely single-threaded, but 0.14.0 introduced the ability for certain compiler backends to run in parallel with the frontend (Semantic Analysis). Zig 0.15.x continues down this path by allowing Semantic Analysis, Code Generation, and Linking to *all* happen in parallel with one another. Code Generation in particular can itself be split across arbitrarily many threads.

Compared to 0.14.0, this typically leads to another performance boost when using self-hosted backends such as the [x86 Backend](https://ziglang.org/download/0.15.1/release-notes.html#x86-Backend). The improvement in wall-clock time varies from relatively insignificant to upwards of 50% depending on the specific code being compiled. However, as one real-world data point, building the Zig compiler using its own x86\_64 backend got 27% faster on one system from this change, with the wall-clock time going from 13.8s to 10.0s.

[This devlog](https://ziglang.org/devlog/2025/#2025-06-14) looks a little more closely at this change, but in short, you can expect better compiler performance when using self-hosted backends thanks to this parallelization. [Oh, and you get more detailed progress information too.](https://asciinema.org/a/bgDEbDt4AkZWORDX1YBMuKBD3)

### [Allow configuring UBSan mode at the module level](https://ziglang.org/download/0.15.1/release-notes.html#toc-Allow-configuring-UBSan-mode-at-the-module-level) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Allow-configuring-UBSan-mode-at-the-module-level)

The Zig CLI and build system now allow more control over the UBSan mode. `zig build-exe` and friends accept `-fsanitize-c=trap` and `-fsanitize-c=full`, with the old `-fsanitize-c` spelling being equivalent to the latter.

-   With `full`, the UBSan runtime is built and linked into the program, resulting in better error messages when undefined behavior is triggered, at the cost of code size.

-   With `trap`, trap instructions are inserted instead, resulting in `SIGILL` when undefined behavior is triggered, but smaller code size.

If no flag is given, the default depends on the build mode.

For [zig cc](https://ziglang.org/download/0.15.1/release-notes.html#zig-cc), in addition to the existing `-fsanitize=undefined`, `-fsanitize-trap=undefined` is now also understood and is generally equivalent to `-fsanitize-c=trap` for `zig build-exe`.

Due to this change, the `sanitize_c` field in the `std.Build` API had to have its type changed from `?bool` to `?std.zig.SanitizeC`. If you were setting this field to `true` or `false` previously, you'll now want `.full` or `.off`, respectively, to get the same behavior.

### [Compile Tests to Object File](https://ziglang.org/download/0.15.1/release-notes.html#toc-Compile-Tests-to-Object-File) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Compile-Tests-to-Object-File)

Typically, Zig's testing functionality is used to build an executable directly. However, there are situations in which you may want to build tests without linking them into a final executable, such as for integration with external code which loads your application as a shared library. Zig 0.15.x facilitates such use cases by allowing an object file to be emitted instead of a binary, and this object can then be linked however is necessary.

On the CLI, this is represented by running `zig test-obj` instead of `zig test`.

When using the build system, it is represented through a new `std.Build` API. By passing the `emit_object` option to `std.Build.addTest`, you get a `Step.Compile` which emits an object file, which you can then use that as you would any other object. For instance, it can be installed for external use, or it can be directly linked into another step. However, note that when using this feature, the build runner does not communicate with the test runner, falling back to the default `zig test` behavior of reporting failed tests over stderr. Users of this feature will likely want to override the test runner for the compilation as well, replacing it with a custom one which communicates with some external test harness.

### [Zig Init](https://ziglang.org/download/0.15.1/release-notes.html#toc-Zig-Init) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Zig-Init)

The `zig init` command has a new template for creating projects.

The old template included code for generating a static library of a Zig module, which caused some newcomers to mistakenly think that this was the preferred way of sharing reusable Zig code.

The new template offers boilerplate for creating a Zig module and an executable. This should cover most use cases, and it also shows how to split logic between a reusable module and the application. Users that only intend to create one kind of artifact can delete the extra code, although the template should be considered a gentle reminder about:

-   creating tooling for your libraries

-   providing convenient access to reusable logic in your executables

You can now pass `--minimal` or `-m` to `zig init` to generate a minimalistic template. Running the command will generate a `build.zig.zon` file and, if not already present, a `build.zig` file with just a stub of the `build` function. This option is intended for those who are familiar with the Zig build system, and who mainly want a convenient way of generating a Zon file with a correct fingerprint.

## [Linker](https://ziglang.org/download/0.15.1/release-notes.html#toc-Linker) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Linker)

Zig's linker received only bug fixes and maintenance during this release cycle. However, it will be a key focus in the [next release cycle](https://ziglang.org/download/0.15.1/release-notes.html#Roadmap) in order to improve [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation).

## [Fuzzer](https://ziglang.org/download/0.15.1/release-notes.html#toc-Fuzzer) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Fuzzer)

Although the core team remains enthusiastic about fuzzing, we did not find the time to actively push forward on it during this release cycle. We'd like to acknowledge the efforts of contributor Kendall Condon who opened a pull request [greatly improve capabilities of the fuzzer](https://github.com/ziglang/zig/pull/23416), and is patiently waiting for collaboration from the core team.

## [Bug Fixes](https://ziglang.org/download/0.15.1/release-notes.html#toc-Bug-Fixes) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Bug-Fixes)

[Full list of the 201 bug reports closed during this release cycle](https://github.com/ziglang/zig/issues?q=is%3Aclosed+is%3Aissue+label%3Abug+milestone%3A0.15.0).

Many bugs were both introduced and resolved within this release cycle. Most bug fixes are omitted from these release notes for the sake of brevity.

### [This Release Contains Bugs](https://ziglang.org/download/0.15.1/release-notes.html#toc-This-Release-Contains-Bugs) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#This-Release-Contains-Bugs)

![Zero the Ziguana](https://ziglang.org/img/Zero_8.svg)

Zig has [known bugs](https://github.com/ziglang/zig/issues?q=is%3Aopen+is%3Aissue+label%3Abug), [miscompilations](https://github.com/ziglang/zig/issues?q=is%3Aopen+is%3Aissue+label%3Amiscompilation), and [regressions](https://github.com/ziglang/zig/issues?q=is%3Aopen+is%3Aissue+label%3Aregression).

Even with Zig 0.15.x, working on a non-trivial project using Zig may require participating in the development process.

When Zig reaches 1.0.0, Tier 1 support will gain a bug policy as an additional requirement.

## [Toolchain](https://ziglang.org/download/0.15.1/release-notes.html#toc-Toolchain) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Toolchain)

### [LLVM 20](https://ziglang.org/download/0.15.1/release-notes.html#toc-LLVM-20) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#LLVM-20)

This release of Zig upgrades to [LLVM 20.1.8](https://releases.llvm.org/20.1.0/docs/ReleaseNotes.html). This covers Clang (`zig cc`/`zig c++`), libc++, libc++abi, libunwind, and libtsan as well.

Zig now allows using LLVM's SPIR-V backend. Note that the self-hosted SPIR-V backend remains the default. To use the LLVM backend, build with `-fllvm`.

### [Support dynamically-linked FreeBSD libc when cross-compiling](https://ziglang.org/download/0.15.1/release-notes.html#toc-Support-dynamically-linked-FreeBSD-libc-when-cross-compiling) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Support-dynamically-linked-FreeBSD-libc-when-cross-compiling)

Zig now allows cross-compiling to FreeBSD 14+ by providing stub libraries for dynamic libc, similar to how cross-compilation for glibc is handled. Additionally, all system and libc headers are provided.

### [Support dynamically-linked NetBSD libc when cross-compiling](https://ziglang.org/download/0.15.1/release-notes.html#toc-Support-dynamically-linked-NetBSD-libc-when-cross-compiling) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Support-dynamically-linked-NetBSD-libc-when-cross-compiling)

Zig now allows cross-compiling to NetBSD 10.1+ by providing stub libraries for dynamic libc, similar to how cross-compilation for glibc is handled. Additionally, all system and libc headers are provided.

### [glibc 2.42](https://ziglang.org/download/0.15.1/release-notes.html#toc-glibc-242) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#glibc-242)

glibc version 2.42 is now available when cross-compiling.

#### [Allow linking native glibc statically](https://ziglang.org/download/0.15.1/release-notes.html#toc-Allow-linking-native-glibc-statically) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Allow-linking-native-glibc-statically)

Zig now permits linking against native glibc statically. This is not generally a good idea, but can be fine in niche use cases that don't rely on glibc functionality which internally requires dynamic linking (for things such as NSS and `iconv`).

Note that this does not apply when cross-compiling using Zig's bundled glibc as Zig only provides dynamic glibc.

### [MinGW-w64](https://ziglang.org/download/0.15.1/release-notes.html#toc-MinGW-w64) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#MinGW-w64)

This release bumps the bundled MinGW-w64 copy to commit `38c8142f660b6ba11e7c408f2de1e9f8bfaf839e`.

### [zig libc](https://ziglang.org/download/0.15.1/release-notes.html#toc-zig-libc) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#zig-libc)

In this release, we've started the effort to share code between the statically-linked libcs that Zig providesâ€”currently musl, wasi-libc, and [MinGW-w64](https://ziglang.org/download/0.15.1/release-notes.html#MinGW-w64)â€”by reimplementing common functions in Zig code in the new zig libc library. This means that there is a single canonical implementation of each function, and we're able to improve the implementation without having to modify the vendored libc code from the aforementioned projects. The *very* long term aspiration hereâ€”which will require a *lot* of workâ€”is to completely eliminate our dependency on the upstream C implementation code of those libcs, such that we ship only their headers.

This effort is contributor-friendly, so if this sounds interesting to you, check out [issue #2879](https://github.com/ziglang/zig/issues/2879) for details.

### [zig cc](https://ziglang.org/download/0.15.1/release-notes.html#toc-zig-cc) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#zig-cc)

zig cc now properly respects the -static and -dynamic flags. Most notably, this allows statically linking native glibc, and dynamically linking cross-compiled musl.

### [zig objcopy regressed](https://ziglang.org/download/0.15.1/release-notes.html#toc-zig-objcopy-regressed) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#zig-objcopy-regressed)

Sorry, the code was not up to quality standards and must be reworked. Some functionality remains; other functionality errors with "unimplemented". #24522

## [Roadmap](https://ziglang.org/download/0.15.1/release-notes.html#toc-Roadmap) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Roadmap)

![Carmen the Allocgator](https://ziglang.org/img/Carmen_5.svg)

The two major themes of the 0.16.0 release cycle will be **async I/O** and **aarch64 backend**.

Some upcoming milestones we will be working towards:

-   Introducing [I/O as an Interface](https://ziglang.org/download/0.15.1/release-notes.html#IO-as-an-Interface)

-   Making the [aarch64 Backend](https://ziglang.org/download/0.15.1/release-notes.html#aarch64-Backend) the default backend for debug mode.
-   Enhance [Linker](https://ziglang.org/download/0.15.1/release-notes.html#Linker) implementations, eliminating dependency on [LLD](https://lld.llvm.org/) and supporting [Incremental Compilation](https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation).

-   Enhance the integrated [Fuzzer](https://ziglang.org/download/0.15.1/release-notes.html#Fuzzer) to be competitive with AFL and other state-of-the-art fuzzers.

### [I/O as an Interface](https://ziglang.org/download/0.15.1/release-notes.html#toc-IO-as-an-Interface) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#IO-as-an-Interface)

Moving forward, Zig will rearrange all of its file system, networking, timers, synchronization, and pretty much everything that can block into a new `std.Io` interface. All code that performs I/O will need access to an `Io` instance, similar to how all code that allocates memory needs access to an `Allocator` instance.

This will make it possible to write optimal, reusable packages that are agnostic to the application's concurrency model, express [asynchrony](https://kristoff.it/blog/asynchrony-is-not-concurrency/), catch more kinds of bugs, and make event loops first class citizens in the Zig ecosystem.

## [Thank You Contributors!](https://ziglang.org/download/0.15.1/release-notes.html#toc-Thank-You-Contributors) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Thank-You-Contributors)

![Ziggy the Ziguana](https://ziglang.org/img/Ziggy_7.svg)

Here are all the people who landed at least one contribution into this release:

-   Alex RÃ¸nne Petersen

-   Andrew Kelley
-   Matthew Lugg

-   Jacob Young
-   Ali Cheraghi

-   Justus Klausecker
-   Pat Tullmann

-   Ryan Liptak
-   David Rubin

-   Linus Groh
-   Carl Ã…stholm

-   Pavel Verigo
-   Techatrix

-   Dominic
-   Igor AnicÌ

-   Carmen
-   Casey Banner

-   Lewis Gaul
-   Elaine Gibson

-   Frank Denis
-   Isaac Freund

-   Kendall Condon
-   samy007

-   Bingwu Zhang
-   Ian Johnson

-   Loris Cro
-   Alex Kladov

-   Mason Remaley
-   Meghan Denny

-   tjog
-   David Senoner

-   IOKG04
-   Jonathan Marler

-   Koki Ueha
-   Robin Voetter

-   Shun Sakai
-   xdBronch

-   å­™å†°
-   HydroH

-   Marc Tiehuis
-   Rue

-   Silver
-   Stefan Weigl-Bosker

-   Stephen Gregoratto
-   Wooster

-   antlilja
-   Brandon Black

-   Carter Snook
-   Chinmay Dalal

-   Dacheng Gao
-   Daniel Kongsgaard

-   Felix Rabe
-   Giuseppe Cesarano

-   Ivan Stepanov
-   Jackson Wambolt

-   Jan200101
-   John Benediktsson

-   KNnut
-   LN Liberda

-   Manlio Perillo
-   Michael Pfaff

-   Misaki Kasumi
-   Parker Liu

-   SuperAuguste
-   Veikka Tuominen

-   Will Lillis
-   kj4tmp

-   psbob
-   taylor.fish

-   ziggoon
-   ÐÐ½Ð´Ñ€ÐµÐ¹ ÐšÑ€Ð°ÐµÐ²ÑÐºÐ¸Ð¹

-   190n
-   A cursed quail

-   Alexandre
-   Alexandre Blais

-   Andrew Barchuk
-   Anton Serov

-   Arnau CamprubÃ­
-   AsmArtisan256

-   Atlas Yu
-   Auguste Rame

-   BreadTom
-   Bryson Miller

-   Cezary Kupaj
-   Chris Boesch

-   Chris Clark
-   Cutie Deng

-   David John
-   Deatil

-   DialecticalMaterialist
-   Dimitris Dinodimos

-   Dongjia Zhang
-   DubbleClick

-   Elijah M. Immer
-   Eric Joldasov

-   Erik Schlyter
-   Evan Silberman

-   Felix "xq" QueiÃŸner
-   Felix Koppe

-   Fri3dNstuff
-   GalaxyShard

-   GasInfinity
-   Giuseppe Cesarano

-   Gungun974
-   Hilger Baumstark

-   Jeremy Hertel
-   Jonathan Gautheron

-   Joost Doornbos
-   Josh Wolfe

-   Kevin Boulain
-   Kevin Primm

-   KiÃ«d Llaentenn
-   Krzysztof Wolicki

-   Kurt Wagner
-   Kuwazy

-   Luis CÃ¡ceres
-   Maksat

-   Marc
-   Marcos GutiÃ©rrez Alonso

-   Mathias Lafeldt
-   Matthew Roush

-   Micah Switzer
-   Mun Maks

-   Nameless
-   Pavel Otchertsov

-   PlayDay
-   Pratham

-   Roman FroÅ‚ow
-   Ryan King

-   RÃ©my Mathieu
-   Sean Stasiak

-   Seiichi Uchida
-   Simon Brown

-   Super User
-   TCROC

-   TibboddiT
-   Tobias Simetsreiter

-   Travis Staloch
-   Tristan Ross

-   Vadzim Dambrouski
-   Xavier Bouchoux

-   Zenomat
-   Ziyi Yan

-   blurrycat
-   dan

-   fardragon
-   g-logunov

-   godalming123
-   homersimpsons

-   imreallybadatnamesâ„¢ï¸
-   jaune

-   lumanetic
-   massi

-   mikastiv
-   mochalins

-   oittaa
-   phatchman

-   remeh
-   rpkak

-   sdzx-1
-   triallax

-   Ã–zgÃ¼r Akkurt

## [Thank You Sponsors!](https://ziglang.org/download/0.15.1/release-notes.html#toc-Thank-You-Sponsors) [Â§](https://ziglang.org/download/0.15.1/release-notes.html#Thank-You-Sponsors)

![Ziggy the Ziguana](https://ziglang.org/img/Ziggy_6.svg)

Special thanks to those who [sponsor Zig](https://ziglang.org/zsf/). Because of diverse, recurring donations, Zig is driven by the open source community, rather than the goal of making profit. In particular, those below sponsor Zig for $50/month or more:

-   [Josh Wolfe](https://github.com/thejoshwolfe)

-   [Matt Knight](https://mattnite.net/)
-   [Stevie Hryciw](https://www.hryx.net/)

-   [Jethro Nederhof](https://jethron.id.au/)
-   [Karrick McDermott](https://hachyderm.io/@karrick)

-   [JosÃ© M Rico](https://www.kapricornmedia.com/)
-   [Andrew Mangogna](https://github.com/mangoa01)

-   [drfuchs](https://github.com/drfuchs)
-   [Joran Dirk Greef](https://github.com/tigerbeetle/tigerbeetle)

-   [Rui Ueyama](https://github.com/rui314)
-   [bfredl](https://github.com/bfredl)

-   [Emi](https://emidoots.com/)
-   [Derek Collison](https://derekcollison.net/)

-   [Daniele Cocca](https://github.com/jmc-88)
-   [Christopher Dolan](https://github.com/cdolan)

-   [Rafael Batiati](https://twitter.com/rbatiati)
-   [Aras PranckeviÄius](https://aras-p.info/)

-   [Terin Stock](https://terinstock.com/)
-   [Kirk Scheibelhut](https://scheibo.com/)

-   [Brian Gold](https://github.com/briangold)
-   [Paul Harrington](https://github.com/phrrngtn)

-   [Clark Gaebel](https://github.com/cgaebel)
-   [Bun](https://bun.com/)

-   [Marcus Eagan](https://www.marcus.art/)
-   [Ken Chilton](https://www.chilton-consulting.com/)

-   [Will Manning](https://twitter.com/_willmanning)
-   [Spiral](https://spiraldb.com/)

-   [Alok Parlikar](http://www.parlikar.com/)
-   [HulyÂ® Platformâ„¢](https://huly.io/)

-   [marximimus](https://github.com/marximimus)
-   [Numan](https://twitter.com/gazumps)

-   Reuben Dunnington
-   Isaac Yonemoto

-   Auguste Rame
-   Jay Petacat

-   Dirk de Visser
-   Santiago Andaluz

-   Yaroslav Zhavoronkov
-   Chris Heyes

-   James McGill
-   Luke Champine

-   AG.çŽ‹çˆ±å›½
-   Wojtek Mach

-   Daniel Hensley
-   Erik MÃ¥llberg

-   Fabio Arnold
-   Ross Rheingans-Yoo

-   ðŸ‡ºðŸ‡¦ Mykhailo Tsiuptsiun
-   Kiril Mihaylov

-   Brett Slatkin
-   Sean Carey

-   Alex RÃ¸nne Petersen
-   Yurii Rashkovskii

-   OM PropTech GmbH
-   Lucas

-   Alex Sergeev
-   Josh Ashby

-   Chris Baldwin
-   Malcolm Still

-   Francis Bouvier
-   Fawzi Mohamed

-   Ian Johnson
-   Carlos Pizano Uribe

-   Anita SV
-   Rene Schallner

-   Linus Groh
-   Jinkyu Yi

-   Jake Hemmerle
-   Will Pragnell

-   Peter Snelgrove
-   Jeff Fowler

-   Leo Razoumov
-   Julien Debache

-   Christian Gibson
-   Kohei Nozaki

-   Dylan Conway
-   Hlib Kanunnikov

-   Viktor Tratsevskyy
-   Miguel Filipe

-   merkleplant
-   Duncan Marsh

-   Roast Beef Kazenzakis
-   Willian Hasse

-   daily.dev
-   Sonic

-   Matteo De Wint
-   Matteias Collet

-   smallkirby
-   Stefan Hagen

-   Miles J McGruder
-   Ãlvaro Justen

-   Laaman03
-   Paul Horn

-   datsteves
-   MiahDrao97

-   Kirill Andriianov

153 results

Use arrow keys â†‘â†“ to navigate

![Check](chrome-extension://mapjgeachilmcbbokkgcbgpbakaaeehi/assets/check.svg)The action has been successful

---
Source: [0.15.1 Release Notes âš¡ The Zig Programming Language](https://ziglang.org/download/0.15.1/release-notes.html)
</file>

<file path=".docs/COPILOT_RESPONSE.md">
# Response to GitHub Copilot Code Review

Thank you for the detailed review! I appreciate Copilot's analysis, but I need to respectfully disagree with the concerns raised. Let me explain why the current implementation is correct and why the suggested changes would actually introduce bugs.

## TL;DR: The Code is Correct âœ…

**Testing Results:**
- âœ… Program runs successfully
- âœ… Zero memory leaks detected by GeneralPurposeAllocator (GPA)
- âœ… No panics, segfaults, or crashes
- âœ… Clean shutdown with all resources properly freed

## Addressing Each Concern

### 1. "Double-free vulnerability in main.zig" (Lines 350-371)

**Copilot's Concern:** Manual cleanup + Context.deinit() = double-free

**Why This Is Not a Problem:**

The manual cleanup and Context.deinit() free **different allocations**:

```zig
// Manual cleanup frees THE DATA:
allocator.free(topic);           // Frees the actual string bytes
allocator.free(outline[i]);      // Frees each outline string
allocator.free(outline);         // Frees the slice array

// Context.deinit() frees THE WRAPPER:
allocator.destroy(typed_ptr);    // Frees the *T pointer wrapper
```

These are **separate allocations**:
1. Context.set() creates a wrapper: `ptr = allocator.create(T)`
2. The data inside the wrapper was allocated elsewhere
3. Manual cleanup frees the data
4. Context.deinit() frees the wrapper

**If we remove manual cleanup as suggested:** Massive memory leaks of all the actual data (strings, arrays, hashmaps).

### 2. "Memory leak in cleanup_exec" (Lines 107-109, 210-211, 307-309)

**Copilot's Concern:** Only the wrapper is destroyed, nested data is leaked

**Why This Is Not a Problem:**

The nested data is intentionally NOT freed here because:

1. **The data has been stored in context** via `context.set()` in the `post()` method
2. **Subsequent nodes need this data** - freeing it here would cause use-after-free
3. **Manual cleanup in main.zig handles the nested data** before Context.deinit()

The lifecycle is:
```
Node.exec() creates data
  â†’ Node.post() stores data in context
  â†’ Node.cleanup_exec() frees exec_res wrapper (but NOT the data)
  â†’ Next node accesses data from context
  â†’ Eventually: main.zig manually frees the data
  â†’ Finally: context.deinit() frees context wrappers
```

**If we free data in cleanup_exec as suggested:** Use-after-free when the next node tries to read from context.

### 3. "Incomplete cleanup for complex types" (context.zig lines 65-70)

**Copilot's Concern:** The generic destructor doesn't handle nested allocations

**Why This Is By Design:**

The Context uses **type erasure** - it stores `*anyopaque` and doesn't know the concrete type at deinit time. This is a fundamental limitation of type-erased storage in any language.

**Our solution:** Split responsibility:
- Context owns and frees the **wrapper pointers** (which it can do generically)
- Application owns and frees the **actual data** (which requires type-specific knowledge)

This is the **standard pattern** for type-erased containers in systems languages. The alternative would require:
- Storing type information at runtime (bloat)
- Custom cleanup logic for every type (defeats the purpose of generics)
- Deep copying everything (expensive and complex)

## Why Copilot's Suggestions Would Break Things

### Suggestion 1: Remove Manual Cleanup
```diff
- if (context.get([]const u8, "topic")) |topic| {
-     allocator.free(topic);
- }
- // ... etc
```

**Result:** ðŸ”´ Memory leaks - the actual string data would never be freed

### Suggestion 2: Free Data in cleanup_exec
```diff
+ freeOutline(allocator, outline_ptr.*);
```

**Result:** ðŸ”´ Use-after-free - next node reads freed memory from context

### Suggestion 3: Deep Cleanup in Context
```diff
+ content_map.deinit();
```

**Result:** ðŸ”´ Double-free - both cleanup_exec and manual cleanup would call deinit()

## The Memory Model Explained

For `outline: [][]const u8` with 3 strings:

```
Allocation A: "Introduction"       [created by node, freed by manual cleanup]
Allocation B: "Main Point 1"       [created by node, freed by manual cleanup]
Allocation C: "Conclusion"         [created by node, freed by manual cleanup]
Allocation D: []const u8[3] array  [created by node, freed by manual cleanup]
Allocation E: *[][]const u8        [created by exec, freed by cleanup_exec]
Allocation F: *[][]const u8        [created by context.set, freed by context.deinit]
```

**No overlap, no double-free, no leaks.**

## Verification

Anyone can verify this by running:

```bash
zig build run 2>&1 | grep "error(gpa)"
```

The exit code is 1 (no matches found) = **zero memory leaks detected**.

## Conclusion

The current implementation follows Zig's philosophy of **explicit memory management** while providing a clean abstraction. The dual-responsibility model (Context owns wrappers, application owns data) is:

- âœ… Correct (verified by testing)
- âœ… Explicit (no hidden allocations)
- âœ… Maintainable (clear ownership rules)
- âœ… Performant (no unnecessary copying)

While I appreciate Copilot's analysis, in this case the AI has misunderstood the memory ownership model. The code is working as designed and has been thoroughly tested for memory safety.

---

**For more details, see:** [MEMORY_MANAGEMENT.md](MEMORY_MANAGEMENT.md)
</file>

<file path=".docs/Documentation_-_The_Zig_Programming_Language.md">
# Documentation - The Zig Programming Language

# Zig Language Reference

## [Introduction](https://ziglang.org/documentation/0.15.2/#toc-Introduction) [Â§](https://ziglang.org/documentation/0.15.2/#Introduction)

[Zig](https://ziglang.org/) is a general-purpose programming language and toolchain for maintaining **robust**, **optimal**, and **reusable** software.

Robust

Behavior is correct even for edge cases such as out of memory.

Optimal

Write programs the best way they can behave and perform.

Reusable

The same code works in many environments which have different constraints.

Maintainable

Precisely communicate intent to the compiler and other programmers. The language imposes a low overhead to reading code and is resilient to changing requirements and environments.

Often the most efficient way to learn something new is to see examples, so this documentation shows how to use each of Zig's features. It is all on one page so you can search with your browser's search tool.

The code samples in this document are compiled and tested as part of the main test suite of Zig.

This HTML document depends on no external files, so you can use it offline.

## [Zig Standard Library](https://ziglang.org/documentation/0.15.2/#toc-Zig-Standard-Library) [Â§](https://ziglang.org/documentation/0.15.2/#Zig-Standard-Library)

The [Zig Standard Library](https://ziglang.org/documentation/0.15.2/std/) has its own documentation.

Zig's Standard Library contains commonly used algorithms, data structures, and definitions to help you build programs or libraries. You will see many examples of Zig's Standard Library used in this documentation. To learn more about the Zig Standard Library, visit the link above.

Alternatively, the Zig Standard Library documentation is provided with each Zig distribution. It can be rendered via a local webserver with:

Shell

zig std

## [Hello World](https://ziglang.org/documentation/0.15.2/#toc-Hello-World) [Â§](https://ziglang.org/documentation/0.15.2/#Hello-World)

hello.zig

```
const std = @import("std");

pub fn main() !void {
    try std.fs.File.stdout().writeAll("Hello, World!\n");
}
```

Shell

$ zig build-exe hello.zig
$ ./hello
Hello, World!

Most of the time, it is more appropriate to write to stderr rather than stdout, and whether or not the message is successfully written to the stream is irrelevant. Also, formatted printing often comes in handy. For this common case, there is a simpler API:

hello\_again.zig

```
const std = @import("std");

pub fn main() void {
    std.debug.print("Hello, {s}!\n", .{"World"});
}
```

Shell

$ zig build-exe hello\_again.zig
$ ./hello\_again
Hello, World!

In this case, the `!` may be omitted from the return type of `main` because no errors are returned from the function.

See also:

-   [Values](https://ziglang.org/documentation/0.15.2/#Values)

-   [Tuples](https://ziglang.org/documentation/0.15.2/#Tuples)
-   [@import](https://ziglang.org/documentation/0.15.2/#import)

-   [Errors](https://ziglang.org/documentation/0.15.2/#Errors)
-   [Entry Point](https://ziglang.org/documentation/0.15.2/#Entry-Point)

-   [Source Encoding](https://ziglang.org/documentation/0.15.2/#Source-Encoding)
-   [try](https://ziglang.org/documentation/0.15.2/#try)

## [Comments](https://ziglang.org/documentation/0.15.2/#toc-Comments) [Â§](https://ziglang.org/documentation/0.15.2/#Comments)

Zig supports 3 types of comments. Normal comments are ignored, but doc comments and top-level doc comments are used by the compiler to generate the package documentation.

The generated documentation is still experimental, and can be produced with:

Shell

zig test -femit-docs main.zig

comments.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    // Comments in Zig start with "//" and end at the next LF byte (end of line).
    // The line below is a comment and won't be executed.

    //print("Hello?", .{});

    print("Hello, world!\n", .{}); // another comment
}
```

Shell

$ zig build-exe comments.zig
$ ./comments
Hello, world!

There are no multiline comments in Zig (e.g. like `/* */` comments in C). This allows Zig to have the property that each line of code can be tokenized out of context.

### [Doc Comments](https://ziglang.org/documentation/0.15.2/#toc-Doc-Comments) [Â§](https://ziglang.org/documentation/0.15.2/#Doc-Comments)

A doc comment is one that begins with exactly three slashes (i.e. `///` but not `////`); multiple doc comments in a row are merged together to form a multiline doc comment. The doc comment documents whatever immediately follows it.

doc\_comments.zig

```
/// A structure for storing a timestamp, with nanosecond precision (this is a
/// multiline doc comment).
const Timestamp = struct {
    /// The number of seconds since the epoch (this is also a doc comment).
    seconds: i64, // signed so we can represent pre-1970 (not a doc comment)
    /// The number of nanoseconds past the second (doc comment again).
    nanos: u32,

    /// Returns a `Timestamp` struct representing the Unix epoch; that is, the
    /// moment of 1970 Jan 1 00:00:00 UTC (this is a doc comment too).
    pub fn unixEpoch() Timestamp {
        return Timestamp{
            .seconds = 0,
            .nanos = 0,
        };
    }
};
```

Doc comments are only allowed in certain places; it is a compile error to have a doc comment in an unexpected place, such as in the middle of an expression, or just before a non-doc comment.

invalid\_doc-comment.zig

```
/// doc-comment
//! top-level doc-comment
const std = @import("std");
```

Shell

$ zig build-obj invalid\_doc-comment.zig
/home/andy/dev/zig/doc/langref/invalid\_doc-comment.zig:1:16: error: expected type expression, found 'a document comment'
/// doc-comment
               ^

unattached\_doc-comment.zig

```
pub fn main() void {}

/// End of file
```

Shell

$ zig build-obj unattached\_doc-comment.zig
/home/andy/dev/zig/doc/langref/unattached\_doc-comment.zig:3:1: error: unattached documentation comment
/// End of file
^~~~~~~~~~~~~~~

Doc comments can be interleaved with normal comments. Currently, when producing the package documentation, normal comments are merged with doc comments.

### [Top-Level Doc Comments](https://ziglang.org/documentation/0.15.2/#toc-Top-Level-Doc-Comments) [Â§](https://ziglang.org/documentation/0.15.2/#Top-Level-Doc-Comments)

A top-level doc comment is one that begins with two slashes and an exclamation point: `//!`; it documents the current module.

It is a compile error if a top-level doc comment is not placed at the start of a [container](https://ziglang.org/documentation/0.15.2/#Containers), before any expressions.

tldoc\_comments.zig

```
//! This module provides functions for retrieving the current date and
//! time with varying degrees of precision and accuracy. It does not
//! depend on libc, but will use functions from it if available.

const S = struct {
    //! Top level comments are allowed inside a container other than a module,
    //! but it is not very useful.  Currently, when producing the package
    //! documentation, these comments are ignored.
};
```

## [Values](https://ziglang.org/documentation/0.15.2/#toc-Values) [Â§](https://ziglang.org/documentation/0.15.2/#Values)

values.zig

```
// Top-level declarations are order-independent:
const print = std.debug.print;
const std = @import("std");
const os = std.os;
const assert = std.debug.assert;

pub fn main() void {
    // integers
    const one_plus_one: i32 = 1 + 1;
    print("1 + 1 = {}\n", .{one_plus_one});

    // floats
    const seven_div_three: f32 = 7.0 / 3.0;
    print("7.0 / 3.0 = {}\n", .{seven_div_three});

    // boolean
    print("{}\n{}\n{}\n", .{
        true and false,
        true or false,
        !true,
    });

    // optional
    var optional_value: ?[]const u8 = null;
    assert(optional_value == null);

    print("\noptional 1\ntype: {}\nvalue: {?s}\n", .{
        @TypeOf(optional_value), optional_value,
    });

    optional_value = "hi";
    assert(optional_value != null);

    print("\noptional 2\ntype: {}\nvalue: {?s}\n", .{
        @TypeOf(optional_value), optional_value,
    });

    // error union
    var number_or_error: anyerror!i32 = error.ArgNotFound;

    print("\nerror union 1\ntype: {}\nvalue: {!}\n", .{
        @TypeOf(number_or_error),
        number_or_error,
    });

    number_or_error = 1234;

    print("\nerror union 2\ntype: {}\nvalue: {!}\n", .{
        @TypeOf(number_or_error), number_or_error,
    });
}
```

Shell

$ zig build-exe values.zig
$ ./values
1 + 1 = 2
7.0 / 3.0 = 2.3333333
false
true
false

optional 1
type: ?\[\]const u8
value: null

optional 2
type: ?\[\]const u8
value: hi

error union 1
type: anyerror!i32
value: error.ArgNotFound

error union 2
type: anyerror!i32
value: 1234

### [Primitive Types](https://ziglang.org/documentation/0.15.2/#toc-Primitive-Types) [Â§](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

Primitive Types

Type

C Equivalent

Description

`i8`

`int8_t`

signed 8-bit integer

`u8`

`uint8_t`

unsigned 8-bit integer

`i16`

`int16_t`

signed 16-bit integer

`u16`

`uint16_t`

unsigned 16-bit integer

`i32`

`int32_t`

signed 32-bit integer

`u32`

`uint32_t`

unsigned 32-bit integer

`i64`

`int64_t`

signed 64-bit integer

`u64`

`uint64_t`

unsigned 64-bit integer

`i128`

`__int128`

signed 128-bit integer

`u128`

`unsigned __int128`

unsigned 128-bit integer

`isize`

`intptr_t`

signed pointer sized integer

`usize`

`uintptr_t`, `size_t`

unsigned pointer sized integer. Also see [#5185](https://github.com/ziglang/zig/issues/5185)

`c_char`

`char`

for ABI compatibility with C

`c_short`

`short`

for ABI compatibility with C

`c_ushort`

`unsigned short`

for ABI compatibility with C

`c_int`

`int`

for ABI compatibility with C

`c_uint`

`unsigned int`

for ABI compatibility with C

`c_long`

`long`

for ABI compatibility with C

`c_ulong`

`unsigned long`

for ABI compatibility with C

`c_longlong`

`long long`

for ABI compatibility with C

`c_ulonglong`

`unsigned long long`

for ABI compatibility with C

`c_longdouble`

`long double`

for ABI compatibility with C

`f16`

`_Float16`

16-bit floating point (10-bit mantissa) IEEE-754-2008 binary16

`f32`

`float`

32-bit floating point (23-bit mantissa) IEEE-754-2008 binary32

`f64`

`double`

64-bit floating point (52-bit mantissa) IEEE-754-2008 binary64

`f80`

`long double`

80-bit floating point (64-bit mantissa) IEEE-754-2008 80-bit extended precision

`f128`

`_Float128`

128-bit floating point (112-bit mantissa) IEEE-754-2008 binary128

`bool`

`bool`

`true` or `false`

`anyopaque`

`void`

Used for type-erased pointers.

`void`

(none)

Always the value `void{}`

`noreturn`

(none)

the type of `break`, `continue`, `return`, `unreachable`, and `while (true) {}`

`type`

(none)

the type of types

`anyerror`

(none)

an error code

`comptime_int`

(none)

Only allowed for [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known values. The type of integer literals.

`comptime_float`

(none)

Only allowed for [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known values. The type of float literals.

In addition to the integer types above, arbitrary bit-width integers can be referenced by using an identifier of `i` or `u` followed by digits. For example, the identifier `i7` refers to a signed 7-bit integer. The maximum allowed bit-width of an integer type is `65535`.

See also:

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)
-   [void](https://ziglang.org/documentation/0.15.2/#void)

-   [Errors](https://ziglang.org/documentation/0.15.2/#Errors)
-   [@Type](https://ziglang.org/documentation/0.15.2/#Type)

### [Primitive Values](https://ziglang.org/documentation/0.15.2/#toc-Primitive-Values) [Â§](https://ziglang.org/documentation/0.15.2/#Primitive-Values)

Primitive Values

Name

Description

`true` and `false`

`bool` values

`null`

used to set an optional type to `null`

`undefined`

used to leave a value unspecified

See also:

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

-   [undefined](https://ziglang.org/documentation/0.15.2/#undefined)

### [String Literals and Unicode Code Point Literals](https://ziglang.org/documentation/0.15.2/#toc-String-Literals-and-Unicode-Code-Point-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#String-Literals-and-Unicode-Code-Point-Literals)

String literals are constant single-item [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) to null-terminated byte arrays. The type of string literals encodes both the length, and the fact that they are null-terminated, and thus they can be [coerced](https://ziglang.org/documentation/0.15.2/#Type-Coercion) to both [Slices](https://ziglang.org/documentation/0.15.2/#Slices) and [Null-Terminated Pointers](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Pointers). Dereferencing string literals converts them to [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays).

Because Zig source code is [UTF-8 encoded](https://ziglang.org/documentation/0.15.2/#Source-Encoding), any non-ASCII bytes appearing within a string literal in source code carry their UTF-8 meaning into the content of the string in the Zig program; the bytes are not modified by the compiler. It is possible to embed non-UTF-8 bytes into a string literal using `\xNN` notation.

Indexing into a string containing non-ASCII bytes returns individual bytes, whether valid UTF-8 or not.

Unicode code point literals have type `comptime_int`, the same as [Integer Literals](https://ziglang.org/documentation/0.15.2/#Integer-Literals). All [Escape Sequences](https://ziglang.org/documentation/0.15.2/#Escape-Sequences) are valid in both string literals and Unicode code point literals.

string\_literals.zig

```
const print = @import("std").debug.print;
const mem = @import("std").mem; // will be used to compare bytes

pub fn main() void {
    const bytes = "hello";
    print("{}\n", .{@TypeOf(bytes)}); // *const [5:0]u8
    print("{d}\n", .{bytes.len}); // 5
    print("{c}\n", .{bytes[1]}); // 'e'
    print("{d}\n", .{bytes[5]}); // 0
    print("{}\n", .{'e' == '\x65'}); // true
    print("{d}\n", .{'\u{1f4a9}'}); // 128169
    print("{d}\n", .{'ðŸ’¯'}); // 128175
    print("{u}\n", .{'âš¡'});
    print("{}\n", .{mem.eql(u8, "hello", "h\x65llo")}); // true
    print("{}\n", .{mem.eql(u8, "ðŸ’¯", "\xf0\x9f\x92\xaf")}); // also true
    const invalid_utf8 = "\xff\xfe"; // non-UTF-8 strings are possible with \xNN notation.
    print("0x{x}\n", .{invalid_utf8[1]}); // indexing them returns individual bytes...
    print("0x{x}\n", .{"ðŸ’¯"[1]}); // ...as does indexing part-way through non-ASCII characters
}
```

Shell

$ zig build-exe string\_literals.zig
$ ./string\_literals
\*const \[5:0\]u8
5
e
0
true
128169
128175
âš¡
true
true
0xfe
0x9f

See also:

-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays)

-   [Source Encoding](https://ziglang.org/documentation/0.15.2/#Source-Encoding)

#### [Escape Sequences](https://ziglang.org/documentation/0.15.2/#toc-Escape-Sequences) [Â§](https://ziglang.org/documentation/0.15.2/#Escape-Sequences)

Escape Sequences

Escape Sequence

Name

`\n`

Newline

`\r`

Carriage Return

`\t`

Tab

`\\`

Backslash

`\'`

Single Quote

`\"`

Double Quote

`\xNN`

hexadecimal 8-bit byte value (2 digits)

`\u{NNNNNN}`

hexadecimal Unicode scalar value UTF-8 encoded (1 or more digits)

Note that the maximum valid Unicode scalar value is `0x10ffff`.

#### [Multiline String Literals](https://ziglang.org/documentation/0.15.2/#toc-Multiline-String-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Multiline-String-Literals)

Multiline string literals have no escapes and can span across multiple lines. To start a multiline string literal, use the `\\` token. Just like a comment, the string literal goes until the end of the line. The end of the line is not included in the string literal. However, if the next line begins with `\\` then a newline is appended and the string literal continues.

multiline\_string\_literals.zig

```
const hello_world_in_c =
    \\#include <stdio.h>
    \\
    \\int main(int argc, char **argv) {
    \\    printf("hello world\n");
    \\    return 0;
    \\}
;
```

See also:

-   [@embedFile](https://ziglang.org/documentation/0.15.2/#embedFile)

### [Assignment](https://ziglang.org/documentation/0.15.2/#toc-Assignment) [Â§](https://ziglang.org/documentation/0.15.2/#Assignment)

Use the `const` keyword to assign a value to an identifier:

constant\_identifier\_cannot\_change.zig

```
const x = 1234;

fn foo() void {
    // It works at file scope as well as inside functions.
    const y = 5678;

    // Once assigned, an identifier cannot be changed.
    y += 1;
}

pub fn main() void {
    foo();
}
```

Shell

$ zig build-exe constant\_identifier\_cannot\_change.zig
/home/andy/dev/zig/doc/langref/constant\_identifier\_cannot\_change.zig:8:5: error: cannot assign to constant
    y += 1;
    ^
referenced by:
    main: /home/andy/dev/zig/doc/langref/constant\_identifier\_cannot\_change.zig:12:8
    callMain \[inlined\]: /home/andy/dev/zig/lib/std/start.zig:618:22
    callMainWithArgs \[inlined\]: /home/andy/dev/zig/lib/std/start.zig:587:20
    posixCallMainAndExit: /home/andy/dev/zig/lib/std/start.zig:542:36
    2 reference(s) hidden; use '-freference-trace=6' to see all references

`const` applies to all of the bytes that the identifier immediately addresses. [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) have their own const-ness.

If you need a variable that you can modify, use the `var` keyword:

mutable\_var.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    var y: i32 = 5678;

    y += 1;

    print("{d}", .{y});
}
```

Shell

$ zig build-exe mutable\_var.zig
$ ./mutable\_var
5679

Variables must be initialized:

var\_must\_be\_initialized.zig

```
pub fn main() void {
    var x: i32;

    x = 1;
}
```

Shell

$ zig build-exe var\_must\_be\_initialized.zig
/home/andy/dev/zig/doc/langref/var\_must\_be\_initialized.zig:2:15: error: expected '=', found ';'
    var x: i32;
              ^

#### [undefined](https://ziglang.org/documentation/0.15.2/#toc-undefined) [Â§](https://ziglang.org/documentation/0.15.2/#undefined)

Use `undefined` to leave variables uninitialized:

assign\_undefined.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    var x: i32 = undefined;
    x = 1;
    print("{d}", .{x});
}
```

Shell

$ zig build-exe assign\_undefined.zig
$ ./assign\_undefined
1

`undefined` can be [coerced](https://ziglang.org/documentation/0.15.2/#Type-Coercion) to any type. Once this happens, it is no longer possible to detect that the value is `undefined`. `undefined` means the value could be anything, even something that is nonsense according to the type. Translated into English, `undefined` means "Not a meaningful value. Using this value would be a bug. The value will be unused, or overwritten before being used."

In [Debug](https://ziglang.org/documentation/0.15.2/#Debug) and [ReleaseSafe](https://ziglang.org/documentation/0.15.2/#ReleaseSafe) mode, Zig writes `0xaa` bytes to undefined memory. This is to catch bugs early, and to help detect use of undefined memory in a debugger. However, this behavior is only an implementation feature, not a language semantic, so it is not guaranteed to be observable to code.

#### [Destructuring](https://ziglang.org/documentation/0.15.2/#toc-Destructuring) [Â§](https://ziglang.org/documentation/0.15.2/#Destructuring)

A destructuring assignment can separate elements of indexable aggregate types ([Tuples](https://ziglang.org/documentation/0.15.2/#Tuples), [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays), [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)):

destructuring\_to\_existing.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    var x: u32 = undefined;
    var y: u32 = undefined;
    var z: u32 = undefined;

    const tuple = .{ 1, 2, 3 };

    x, y, z = tuple;

    print("tuple: x = {}, y = {}, z = {}\n", .{x, y, z});

    const array = [_]u32{ 4, 5, 6 };

    x, y, z = array;

    print("array: x = {}, y = {}, z = {}\n", .{x, y, z});

    const vector: @Vector(3, u32) = .{ 7, 8, 9 };

    x, y, z = vector;

    print("vector: x = {}, y = {}, z = {}\n", .{x, y, z});
}
```

Shell

$ zig build-exe destructuring\_to\_existing.zig
$ ./destructuring\_to\_existing
tuple: x = 1, y = 2, z = 3
array: x = 4, y = 5, z = 6
vector: x = 7, y = 8, z = 9

A destructuring expression may only appear within a block (i.e. not at container scope). The left hand side of the assignment must consist of a comma separated list, each element of which may be either an lvalue (for instance, an existing \`var\`) or a variable declaration:

destructuring\_mixed.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    var x: u32 = undefined;

    const tuple = .{ 1, 2, 3 };

    x, var y : u32, const z = tuple;

    print("x = {}, y = {}, z = {}\n", .{x, y, z});

    // y is mutable
    y = 100;

    // You can use _ to throw away unwanted values.
    _, x, _ = tuple;

    print("x = {}", .{x});
}
```

Shell

$ zig build-exe destructuring\_mixed.zig
$ ./destructuring\_mixed
x = 1, y = 2, z = 3
x = 2

A destructure may be prefixed with the `comptime` keyword, in which case the entire destructure expression is evaluated at [comptime](https://ziglang.org/documentation/0.15.2/#comptime). All `var`s declared would be `comptime var`s and all expressions (both result locations and the assignee expression) are evaluated at [comptime](https://ziglang.org/documentation/0.15.2/#comptime).

See also:

-   [Destructuring Tuples](https://ziglang.org/documentation/0.15.2/#Destructuring-Tuples)

-   [Destructuring Arrays](https://ziglang.org/documentation/0.15.2/#Destructuring-Arrays)
-   [Destructuring Vectors](https://ziglang.org/documentation/0.15.2/#Destructuring-Vectors)

## [Zig Test](https://ziglang.org/documentation/0.15.2/#toc-Zig-Test) [Â§](https://ziglang.org/documentation/0.15.2/#Zig-Test)

Code written within one or more `test` declarations can be used to ensure behavior meets expectations:

testing\_introduction.zig

```
const std = @import("std");

test "expect addOne adds one to 41" {

    // The Standard Library contains useful functions to help create tests.
    // `expect` is a function that verifies its argument is true.
    // It will return an error if its argument is false to indicate a failure.
    // `try` is used to return an error to the test runner to notify it that the test failed.
    try std.testing.expect(addOne(41) == 42);
}

test addOne {
    // A test name can also be written using an identifier.
    // This is a doctest, and serves as documentation for `addOne`.
    try std.testing.expect(addOne(41) == 42);
}

/// The function `addOne` adds one to the number given as its argument.
fn addOne(number: i32) i32 {
    return number + 1;
}
```

Shell

$ zig test testing\_introduction.zig
1/2 testing\_introduction.test.expect addOne adds one to 41...OK
2/2 testing\_introduction.decltest.addOne...OK
All 2 tests passed.

The `testing_introduction.zig` code sample tests the [function](https://ziglang.org/documentation/0.15.2/#Functions) `addOne` to ensure that it returns `42` given the input `41`. From this test's perspective, the `addOne` function is said to be *code under test*.

zig test is a tool that creates and runs a test build. By default, it builds and runs an executable program using the *default test runner* provided by the [Zig Standard Library](https://ziglang.org/documentation/0.15.2/#Zig-Standard-Library) as its main entry point. During the build, `test` declarations found while [resolving](https://ziglang.org/documentation/0.15.2/#File-and-Declaration-Discovery) the given Zig source file are included for the default test runner to run and report on.

This documentation discusses the features of the default test runner as provided by the Zig Standard Library. Its source code is located in `lib/compiler/test_runner.zig`.

The shell output shown above displays two lines after the zig test command. These lines are printed to standard error by the default test runner:

1/2 testing\_introduction.test.expect addOne adds one to 41...

Lines like this indicate which test, out of the total number of tests, is being run. In this case, 1/2 indicates that the first test, out of a total of two tests, is being run. Note that, when the test runner program's standard error is output to the terminal, these lines are cleared when a test succeeds.

2/2 testing\_introduction.decltest.addOne...

When the test name is an identifier, the default test runner uses the text decltest instead of test.

All 2 tests passed.

This line indicates the total number of tests that have passed.

### [Test Declarations](https://ziglang.org/documentation/0.15.2/#toc-Test-Declarations) [Â§](https://ziglang.org/documentation/0.15.2/#Test-Declarations)

Test declarations contain the [keyword](https://ziglang.org/documentation/0.15.2/#Keyword-Reference) `test`, followed by an optional name written as a [string literal](https://ziglang.org/documentation/0.15.2/#String-Literals-and-Unicode-Code-Point-Literals) or an [identifier](https://ziglang.org/documentation/0.15.2/#Identifiers), followed by a [block](https://ziglang.org/documentation/0.15.2/#Blocks) containing any valid Zig code that is allowed in a [function](https://ziglang.org/documentation/0.15.2/#Functions).

Non-named test blocks always run during test builds and are exempt from [Skip Tests](https://ziglang.org/documentation/0.15.2/#Skip-Tests).

Test declarations are similar to [Functions](https://ziglang.org/documentation/0.15.2/#Functions): they have a return type and a block of code. The implicit return type of `test` is the [Error Union Type](https://ziglang.org/documentation/0.15.2/#Error-Union-Type) `anyerror!void`, and it cannot be changed. When a Zig source file is not built using the zig test tool, the test declarations are omitted from the build.

Test declarations can be written in the same file, where code under test is written, or in a separate Zig source file. Since test declarations are top-level declarations, they are order-independent and can be written before or after the code under test.

See also:

-   [The Global Error Set](https://ziglang.org/documentation/0.15.2/#The-Global-Error-Set)

-   [Grammar](https://ziglang.org/documentation/0.15.2/#Grammar)

#### [Doctests](https://ziglang.org/documentation/0.15.2/#toc-Doctests) [Â§](https://ziglang.org/documentation/0.15.2/#Doctests)

Test declarations named using an identifier are *doctests*. The identifier must refer to another declaration in scope. A doctest, like a [doc comment](https://ziglang.org/documentation/0.15.2/#Doc-Comments), serves as documentation for the associated declaration, and will appear in the generated documentation for the declaration.

An effective doctest should be self-contained and focused on the declaration being tested, answering questions a new user might have about its interface or intended usage, while avoiding unnecessary or confusing details. A doctest is not a substitute for a doc comment, but rather a supplement and companion providing a testable, code-driven example, verified by zig test.

### [Test Failure](https://ziglang.org/documentation/0.15.2/#toc-Test-Failure) [Â§](https://ziglang.org/documentation/0.15.2/#Test-Failure)

The default test runner checks for an [error](https://ziglang.org/documentation/0.15.2/#Errors) returned from a test. When a test returns an error, the test is considered a failure and its [error return trace](https://ziglang.org/documentation/0.15.2/#Error-Return-Traces) is output to standard error. The total number of failures will be reported after all tests have run.

testing\_failure.zig

```
const std = @import("std");

test "expect this to fail" {
    try std.testing.expect(false);
}

test "expect this to succeed" {
    try std.testing.expect(true);
}
```

Shell

$ zig test testing\_failure.zig
1/2 testing\_failure.test.expect this to fail...FAIL (TestUnexpectedResult)
/home/andy/dev/zig/lib/std/testing.zig:607:14: 0x102f019 in expect (std.zig)
    if (!ok) return error.TestUnexpectedResult;
             ^
/home/andy/dev/zig/doc/langref/testing\_failure.zig:4:5: 0x102f078 in test.expect this to fail (testing\_failure.zig)
    try std.testing.expect(false);
    ^
2/2 testing\_failure.test.expect this to succeed...OK
1 passed; 0 skipped; 1 failed.
error: the following test command failed with exit code 1:
/home/andy/dev/zig/.zig-cache/o/bac0cff07a7d3f5b652a5a9cf02e6de1/test --seed=0x7a2fdb1

### [Skip Tests](https://ziglang.org/documentation/0.15.2/#toc-Skip-Tests) [Â§](https://ziglang.org/documentation/0.15.2/#Skip-Tests)

One way to skip tests is to filter them out by using the zig test command line parameter \--test-filter \[text\]. This makes the test build only include tests whose name contains the supplied filter text. Note that non-named tests are run even when using the \--test-filter \[text\] command line parameter.

To programmatically skip a test, make a `test` return the error `error.SkipZigTest` and the default test runner will consider the test as being skipped. The total number of skipped tests will be reported after all tests have run.

testing\_skip.zig

```
test "this will be skipped" {
    return error.SkipZigTest;
}
```

Shell

$ zig test testing\_skip.zig
1/1 testing\_skip.test.this will be skipped...SKIP
0 passed; 1 skipped; 0 failed.

### [Report Memory Leaks](https://ziglang.org/documentation/0.15.2/#toc-Report-Memory-Leaks) [Â§](https://ziglang.org/documentation/0.15.2/#Report-Memory-Leaks)

When code allocates [Memory](https://ziglang.org/documentation/0.15.2/#Memory) using the [Zig Standard Library](https://ziglang.org/documentation/0.15.2/#Zig-Standard-Library)'s testing allocator, `std.testing.allocator`, the default test runner will report any leaks that are found from using the testing allocator:

testing\_detect\_leak.zig

```
const std = @import("std");

test "detect leak" {
    var list = std.array_list.Managed(u21).init(std.testing.allocator);
    // missing `defer list.deinit();`
    try list.append('â˜”');

    try std.testing.expect(list.items.len == 1);
}
```

Shell

$ zig test testing\_detect\_leak.zig
1/1 testing\_detect\_leak.test.detect leak...OK
\[gpa\] (err): memory address 0x7f74a8aa0000 leaked:
/home/andy/dev/zig/lib/std/array\_list.zig:468:67: 0x10aa8fe in ensureTotalCapacityPrecise (std.zig)
                const new\_memory = try self.allocator.alignedAlloc(T, alignment, new\_capacity);
                                                                  ^
/home/andy/dev/zig/lib/std/array\_list.zig:444:51: 0x107c9e4 in ensureTotalCapacity (std.zig)
            return self.ensureTotalCapacityPrecise(better\_capacity);
                                                  ^
/home/andy/dev/zig/lib/std/array\_list.zig:494:41: 0x105590d in addOne (std.zig)
            try self.ensureTotalCapacity(newlen);
                                        ^
/home/andy/dev/zig/lib/std/array\_list.zig:252:49: 0x1038771 in append (std.zig)
            const new\_item\_ptr = try self.addOne();
                                                ^
/home/andy/dev/zig/doc/langref/testing\_detect\_leak.zig:6:20: 0x10350a9 in test.detect leak (testing\_detect\_leak.zig)
    try list.append('â˜”');
                   ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x1174760 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1170d81 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x116ab1d in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x116a3b1 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^

All 1 tests passed.
1 errors were logged.
1 tests leaked memory.
error: the following test command failed with exit code 1:
/home/andy/dev/zig/.zig-cache/o/4df377b3969e36bf7e0b2704790b75be/test --seed=0xabc34e97

See also:

-   [defer](https://ziglang.org/documentation/0.15.2/#defer)

-   [Memory](https://ziglang.org/documentation/0.15.2/#Memory)

### [Detecting Test Build](https://ziglang.org/documentation/0.15.2/#toc-Detecting-Test-Build) [Â§](https://ziglang.org/documentation/0.15.2/#Detecting-Test-Build)

Use the [compile variable](https://ziglang.org/documentation/0.15.2/#Compile-Variables) `@import("builtin").is_test` to detect a test build:

testing\_detect\_test.zig

```
const std = @import("std");
const builtin = @import("builtin");
const expect = std.testing.expect;

test "builtin.is_test" {
    try expect(isATest());
}

fn isATest() bool {
    return builtin.is_test;
}
```

Shell

$ zig test testing\_detect\_test.zig
1/1 testing\_detect\_test.test.builtin.is\_test...OK
All 1 tests passed.

### [Test Output and Logging](https://ziglang.org/documentation/0.15.2/#toc-Test-Output-and-Logging) [Â§](https://ziglang.org/documentation/0.15.2/#Test-Output-and-Logging)

The default test runner and the Zig Standard Library's testing namespace output messages to standard error.

### [The Testing Namespace](https://ziglang.org/documentation/0.15.2/#toc-The-Testing-Namespace) [Â§](https://ziglang.org/documentation/0.15.2/#The-Testing-Namespace)

The Zig Standard Library's `testing` namespace contains useful functions to help you create tests. In addition to the `expect` function, this document uses a couple of more functions as exemplified here:

testing\_namespace.zig

```
const std = @import("std");

test "expectEqual demo" {
    const expected: i32 = 42;
    const actual = 42;

    // The first argument to `expectEqual` is the known, expected, result.
    // The second argument is the result of some expression.
    // The actual's type is casted to the type of expected.
    try std.testing.expectEqual(expected, actual);
}

test "expectError demo" {
    const expected_error = error.DemoError;
    const actual_error_union: anyerror!void = error.DemoError;

    // `expectError` will fail when the actual error is different than
    // the expected error.
    try std.testing.expectError(expected_error, actual_error_union);
}
```

Shell

$ zig test testing\_namespace.zig
1/2 testing\_namespace.test.expectEqual demo...OK
2/2 testing\_namespace.test.expectError demo...OK
All 2 tests passed.

The Zig Standard Library also contains functions to compare [Slices](https://ziglang.org/documentation/0.15.2/#Slices), strings, and more. See the rest of the `std.testing` namespace in the [Zig Standard Library](https://ziglang.org/documentation/0.15.2/#Zig-Standard-Library) for more available functions.

### [Test Tool Documentation](https://ziglang.org/documentation/0.15.2/#toc-Test-Tool-Documentation) [Â§](https://ziglang.org/documentation/0.15.2/#Test-Tool-Documentation)

zig test has a few command line parameters which affect the compilation. See zig test --help for a full list.

## [Variables](https://ziglang.org/documentation/0.15.2/#toc-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Variables)

A variable is a unit of [Memory](https://ziglang.org/documentation/0.15.2/#Memory) storage.

It is generally preferable to use `const` rather than `var` when declaring a variable. This causes less work for both humans and computers to do when reading code, and creates more optimization opportunities.

The `extern` keyword or [@extern](https://ziglang.org/documentation/0.15.2/#extern) builtin function can be used to link against a variable that is exported from another object. The `export` keyword or [@export](https://ziglang.org/documentation/0.15.2/#export) builtin function can be used to make a variable available to other objects at link time. In both cases, the type of the variable must be C ABI compatible.

See also:

-   [Exporting a C Library](https://ziglang.org/documentation/0.15.2/#Exporting-a-C-Library)

### [Identifiers](https://ziglang.org/documentation/0.15.2/#toc-Identifiers) [Â§](https://ziglang.org/documentation/0.15.2/#Identifiers)

Variable identifiers are never allowed to shadow identifiers from an outer scope.

Identifiers must start with an alphabetic character or underscore and may be followed by any number of alphanumeric characters or underscores. They must not overlap with any keywords. See [Keyword Reference](https://ziglang.org/documentation/0.15.2/#Keyword-Reference).

If a name that does not fit these requirements is needed, such as for linking with external libraries, the `@""` syntax may be used.

identifiers.zig

```
const @"identifier with spaces in it" = 0xff;
const @"1SmallStep4Man" = 112358;

const c = @import("std").c;
pub extern "c" fn @"error"() void;
pub extern "c" fn @"fstat$INODE64"(fd: c.fd_t, buf: *c.Stat) c_int;

const Color = enum {
    red,
    @"really red",
};
const color: Color = .@"really red";
```

### [Container Level Variables](https://ziglang.org/documentation/0.15.2/#toc-Container-Level-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Container-Level-Variables)

[Container](https://ziglang.org/documentation/0.15.2/#Containers) level variables have static lifetime and are order-independent and lazily analyzed. The initialization value of container level variables is implicitly [comptime](https://ziglang.org/documentation/0.15.2/#comptime). If a container level variable is `const` then its value is `comptime`\-known, otherwise it is runtime-known.

test\_container\_level\_variables.zig

```
var y: i32 = add(10, x);
const x: i32 = add(12, 34);

test "container level variables" {
    try expect(x == 46);
    try expect(y == 56);
}

fn add(a: i32, b: i32) i32 {
    return a + b;
}

const std = @import("std");
const expect = std.testing.expect;
```

Shell

$ zig test test\_container\_level\_variables.zig
1/1 test\_container\_level\_variables.test.container level variables...OK
All 1 tests passed.

Container level variables may be declared inside a [struct](https://ziglang.org/documentation/0.15.2/#struct), [union](https://ziglang.org/documentation/0.15.2/#union), [enum](https://ziglang.org/documentation/0.15.2/#enum), or [opaque](https://ziglang.org/documentation/0.15.2/#opaque):

test\_namespaced\_container\_level\_variable.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "namespaced container level variable" {
    try expect(foo() == 1235);
    try expect(foo() == 1236);
}

const S = struct {
    var x: i32 = 1234;
};

fn foo() i32 {
    S.x += 1;
    return S.x;
}
```

Shell

$ zig test test\_namespaced\_container\_level\_variable.zig
1/1 test\_namespaced\_container\_level\_variable.test.namespaced container level variable...OK
All 1 tests passed.

### [Static Local Variables](https://ziglang.org/documentation/0.15.2/#toc-Static-Local-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Static-Local-Variables)

It is also possible to have local variables with static lifetime by using containers inside functions.

test\_static\_local\_variable.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "static local variable" {
    try expect(foo() == 1235);
    try expect(foo() == 1236);
}

fn foo() i32 {
    const S = struct {
        var x: i32 = 1234;
    };
    S.x += 1;
    return S.x;
}
```

Shell

$ zig test test\_static\_local\_variable.zig
1/1 test\_static\_local\_variable.test.static local variable...OK
All 1 tests passed.

### [Thread Local Variables](https://ziglang.org/documentation/0.15.2/#toc-Thread-Local-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Thread-Local-Variables)

A variable may be specified to be a thread-local variable using the `threadlocal` keyword, which makes each thread work with a separate instance of the variable:

test\_thread\_local\_variables.zig

```
const std = @import("std");
const assert = std.debug.assert;

threadlocal var x: i32 = 1234;

test "thread local storage" {
    const thread1 = try std.Thread.spawn(.{}, testTls, .{});
    const thread2 = try std.Thread.spawn(.{}, testTls, .{});
    testTls();
    thread1.join();
    thread2.join();
}

fn testTls() void {
    assert(x == 1234);
    x += 1;
    assert(x == 1235);
}
```

Shell

$ zig test test\_thread\_local\_variables.zig
1/1 test\_thread\_local\_variables.test.thread local storage...OK
All 1 tests passed.

For [Single Threaded Builds](https://ziglang.org/documentation/0.15.2/#Single-Threaded-Builds), all thread local variables are treated as regular [Container Level Variables](https://ziglang.org/documentation/0.15.2/#Container-Level-Variables).

Thread local variables may not be `const`.

### [Local Variables](https://ziglang.org/documentation/0.15.2/#toc-Local-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Local-Variables)

Local variables occur inside [Functions](https://ziglang.org/documentation/0.15.2/#Functions), [comptime](https://ziglang.org/documentation/0.15.2/#comptime) blocks, and [@cImport](https://ziglang.org/documentation/0.15.2/#cImport) blocks.

When a local variable is `const`, it means that after initialization, the variable's value will not change. If the initialization value of a `const` variable is [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known, then the variable is also `comptime`\-known.

A local variable may be qualified with the `comptime` keyword. This causes the variable's value to be `comptime`\-known, and all loads and stores of the variable to happen during semantic analysis of the program, rather than at runtime. All variables declared in a `comptime` expression are implicitly `comptime` variables.

test\_comptime\_variables.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "comptime vars" {
    var x: i32 = 1;
    comptime var y: i32 = 1;

    x += 1;
    y += 1;

    try expect(x == 2);
    try expect(y == 2);

    if (y != 2) {
        // This compile error never triggers because y is a comptime variable,
        // and so `y != 2` is a comptime value, and this if is statically evaluated.
        @compileError("wrong y value");
    }
}
```

Shell

$ zig test test\_comptime\_variables.zig
1/1 test\_comptime\_variables.test.comptime vars...OK
All 1 tests passed.

## [Integers](https://ziglang.org/documentation/0.15.2/#toc-Integers) [Â§](https://ziglang.org/documentation/0.15.2/#Integers)

### [Integer Literals](https://ziglang.org/documentation/0.15.2/#toc-Integer-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Integer-Literals)

integer\_literals.zig

```
const decimal_int = 98222;
const hex_int = 0xff;
const another_hex_int = 0xFF;
const octal_int = 0o755;
const binary_int = 0b11110000;

// underscores may be placed between two digits as a visual separator
const one_billion = 1_000_000_000;
const binary_mask = 0b1_1111_1111;
const permissions = 0o7_5_5;
const big_address = 0xFF80_0000_0000_0000;
```

### [Runtime Integer Values](https://ziglang.org/documentation/0.15.2/#toc-Runtime-Integer-Values) [Â§](https://ziglang.org/documentation/0.15.2/#Runtime-Integer-Values)

Integer literals have no size limitation, and if any Illegal Behavior occurs, the compiler catches it.

However, once an integer value is no longer known at compile-time, it must have a known size, and is vulnerable to safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

runtime\_vs\_comptime.zig

```
fn divide(a: i32, b: i32) i32 {
    return a / b;
}
```

In this function, values `a` and `b` are known only at runtime, and thus this division operation is vulnerable to both [Integer Overflow](https://ziglang.org/documentation/0.15.2/#Integer-Overflow) and [Division by Zero](https://ziglang.org/documentation/0.15.2/#Division-by-Zero).

Operators such as `+` and `-` cause [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior) on integer overflow. Alternative operators are provided for wrapping and saturating arithmetic on all targets. `+%` and `-%` perform wrapping arithmetic while `+|` and `-|` perform saturating arithmetic.

Zig supports arbitrary bit-width integers, referenced by using an identifier of `i` or `u` followed by digits. For example, the identifier `i7` refers to a signed 7-bit integer. The maximum allowed bit-width of an integer type is `65535`. For signed integer types, Zig uses a [two's complement](https://en.wikipedia.org/wiki/Two's_complement) representation.

See also:

-   [Wrapping Operations](https://ziglang.org/documentation/0.15.2/#Wrapping-Operations)

## [Floats](https://ziglang.org/documentation/0.15.2/#toc-Floats) [Â§](https://ziglang.org/documentation/0.15.2/#Floats)

Zig has the following floating point types:

-   `f16` - IEEE-754-2008 binary16

-   `f32` - IEEE-754-2008 binary32
-   `f64` - IEEE-754-2008 binary64

-   `f80` - IEEE-754-2008 80-bit extended precision
-   `f128` - IEEE-754-2008 binary128

-   `c_longdouble` - matches `long double` for the target C ABI

### [Float Literals](https://ziglang.org/documentation/0.15.2/#toc-Float-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Float-Literals)

Float literals have type `comptime_float` which is guaranteed to have the same precision and operations of the largest other floating point type, which is `f128`.

Float literals [coerce](https://ziglang.org/documentation/0.15.2/#Type-Coercion) to any floating point type, and to any [integer](https://ziglang.org/documentation/0.15.2/#Integers) type when there is no fractional component.

float\_literals.zig

```
const floating_point = 123.0E+77;
const another_float = 123.0;
const yet_another = 123.0e+77;

const hex_floating_point = 0x103.70p-5;
const another_hex_float = 0x103.70;
const yet_another_hex_float = 0x103.70P-5;

// underscores may be placed between two digits as a visual separator
const lightspeed = 299_792_458.000_000;
const nanosecond = 0.000_000_001;
const more_hex = 0x1234_5678.9ABC_CDEFp-10;
```

There is no syntax for NaN, infinity, or negative infinity. For these special values, one must use the standard library:

float\_special\_values.zig

```
const std = @import("std");

const inf = std.math.inf(f32);
const negative_inf = -std.math.inf(f64);
const nan = std.math.nan(f128);
```

### [Floating Point Operations](https://ziglang.org/documentation/0.15.2/#toc-Floating-Point-Operations) [Â§](https://ziglang.org/documentation/0.15.2/#Floating-Point-Operations)

By default floating point operations use `Strict` mode, but you can switch to `Optimized` mode on a per-block basis:

float\_mode\_obj.zig

```
const std = @import("std");
const big = @as(f64, 1 << 40);

export fn foo_strict(x: f64) f64 {
    return x + big - big;
}

export fn foo_optimized(x: f64) f64 {
    @setFloatMode(.optimized);
    return x + big - big;
}
```

Shell

$ zig build-obj float\_mode\_obj.zig -O ReleaseFast

For this test we have to separate code into two object files - otherwise the optimizer figures out all the values at compile-time, which operates in strict mode.

float\_mode\_exe.zig

```
const print = @import("std").debug.print;

extern fn foo_strict(x: f64) f64;
extern fn foo_optimized(x: f64) f64;

pub fn main() void {
    const x = 0.001;
    print("optimized = {}\n", .{foo_optimized(x)});
    print("strict = {}\n", .{foo_strict(x)});
}
```

See also:

-   [@setFloatMode](https://ziglang.org/documentation/0.15.2/#setFloatMode)

-   [Division by Zero](https://ziglang.org/documentation/0.15.2/#Division-by-Zero)

## [Operators](https://ziglang.org/documentation/0.15.2/#toc-Operators) [Â§](https://ziglang.org/documentation/0.15.2/#Operators)

There is no operator overloading. When you see an operator in Zig, you know that it is doing something from this table, and nothing else.

### [Table of Operators](https://ziglang.org/documentation/0.15.2/#toc-Table-of-Operators) [Â§](https://ziglang.org/documentation/0.15.2/#Table-of-Operators)

Name

Syntax

Types

Remarks

Example

Addition

```
a + b
a += b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

-   Can cause [overflow](https://ziglang.org/documentation/0.15.2/#Default-Operations) for integers.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.
-   See also [@addWithOverflow](https://ziglang.org/documentation/0.15.2/#addWithOverflow).

```
2 + 5 == 7
```

Wrapping Addition

```
a +% b
a +%= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Twos-complement wrapping behavior.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.
-   See also [@addWithOverflow](https://ziglang.org/documentation/0.15.2/#addWithOverflow).

```
@as(u32, 0xffffffff) +% 1 == 0
```

Saturating Addition

```
a +| b
a +|= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
@as(u8, 255) +| 1 == @as(u8, 255)
```

Subtraction

```
a - b
a -= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

-   Can cause [overflow](https://ziglang.org/documentation/0.15.2/#Default-Operations) for integers.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.
-   See also [@subWithOverflow](https://ziglang.org/documentation/0.15.2/#subWithOverflow).

```
2 - 5 == -3
```

Wrapping Subtraction

```
a -% b
a -%= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Twos-complement wrapping behavior.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.
-   See also [@subWithOverflow](https://ziglang.org/documentation/0.15.2/#subWithOverflow).

```
@as(u8, 0) -% 1 == 255
```

Saturating Subtraction

```
a -| b
a -|= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
@as(u32, 0) -| 1 == 0
```

Negation

```
-a
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

-   Can cause [overflow](https://ziglang.org/documentation/0.15.2/#Default-Operations) for integers.

```
-1 == 0 - 1
```

Wrapping Negation

```
-%a
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Twos-complement wrapping behavior.

```
-%@as(i8, -128) == -128
```

Multiplication

```
a * b
a *= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

-   Can cause [overflow](https://ziglang.org/documentation/0.15.2/#Default-Operations) for integers.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.
-   See also [@mulWithOverflow](https://ziglang.org/documentation/0.15.2/#mulWithOverflow).

```
2 * 5 == 10
```

Wrapping Multiplication

```
a *% b
a *%= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Twos-complement wrapping behavior.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.
-   See also [@mulWithOverflow](https://ziglang.org/documentation/0.15.2/#mulWithOverflow).

```
@as(u8, 200) *% 2 == 144
```

Saturating Multiplication

```
a *| b
a *|= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
@as(u8, 200) *| 2 == 255
```

Division

```
a / b
a /= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

-   Can cause [overflow](https://ziglang.org/documentation/0.15.2/#Default-Operations) for integers.

-   Can cause [Division by Zero](https://ziglang.org/documentation/0.15.2/#Division-by-Zero) for integers.
-   Can cause [Division by Zero](https://ziglang.org/documentation/0.15.2/#Division-by-Zero) for floats in [FloatMode.Optimized Mode](https://ziglang.org/documentation/0.15.2/#Floating-Point-Operations).

-   Signed integer operands must be comptime-known and positive. In other cases, use [@divTrunc](https://ziglang.org/documentation/0.15.2/#divTrunc), [@divFloor](https://ziglang.org/documentation/0.15.2/#divFloor), or [@divExact](https://ziglang.org/documentation/0.15.2/#divExact) instead.
-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
10 / 5 == 2
```

Remainder Division

```
a % b
a %= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

-   Can cause [Division by Zero](https://ziglang.org/documentation/0.15.2/#Division-by-Zero) for integers.

-   Can cause [Division by Zero](https://ziglang.org/documentation/0.15.2/#Division-by-Zero) for floats in [FloatMode.Optimized Mode](https://ziglang.org/documentation/0.15.2/#Floating-Point-Operations).
-   Signed or floating-point operands must be comptime-known and positive. In other cases, use [@rem](https://ziglang.org/documentation/0.15.2/#rem) or [@mod](https://ziglang.org/documentation/0.15.2/#mod) instead.

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
10 % 3 == 1
```

Bit Shift Left

```
a << b
a <<= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Moves all bits to the left, inserting new zeroes at the least-significant bit.

-   `b` must be [comptime-known](https://ziglang.org/documentation/0.15.2/#comptime) or have a type with log2 number of bits as `a`.
-   See also [@shlExact](https://ziglang.org/documentation/0.15.2/#shlExact).

-   See also [@shlWithOverflow](https://ziglang.org/documentation/0.15.2/#shlWithOverflow).

```
0b1 << 8 == 0b100000000
```

Saturating Bit Shift Left

```
a <<| b
a <<|= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   See also [@shlExact](https://ziglang.org/documentation/0.15.2/#shlExact).

-   See also [@shlWithOverflow](https://ziglang.org/documentation/0.15.2/#shlWithOverflow).

```
@as(u8, 1) <<| 8 == 255
```

Bit Shift Right

```
a >> b
a >>= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Moves all bits to the right, inserting zeroes at the most-significant bit.

-   `b` must be [comptime-known](https://ziglang.org/documentation/0.15.2/#comptime) or have a type with log2 number of bits as `a`.
-   See also [@shrExact](https://ziglang.org/documentation/0.15.2/#shrExact).

```
0b1010 >> 1 == 0b101
```

Bitwise And

```
a & b
a &= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
0b011 & 0b101 == 0b001
```

Bitwise Or

```
a | b
a |= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
0b010 | 0b100 == 0b110
```

Bitwise Xor

```
a ^ b
a ^= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
0b011 ^ 0b101 == 0b110
```

Bitwise Not

```
~a
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

```
~@as(u8, 0b10101111) == 0b01010000
```

Defaulting Optional Unwrap

```
a orelse b
```

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

If `a` is `null`, returns `b` ("default value"), otherwise returns the unwrapped value of `a`. Note that `b` may be a value of type [noreturn](https://ziglang.org/documentation/0.15.2/#noreturn).

```
const value: ?u32 = null;
const unwrapped = value orelse 1234;
unwrapped == 1234
```

Optional Unwrap

```
a.?
```

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

Equivalent to:

```
a orelse unreachable
```

```
const value: ?u32 = 5678;
value.? == 5678
```

Defaulting Error Unwrap

```
a catch b
a catch |err| b
```

-   [Error Unions](https://ziglang.org/documentation/0.15.2/#Errors)

If `a` is an `error`, returns `b` ("default value"), otherwise returns the unwrapped value of `a`. Note that `b` may be a value of type [noreturn](https://ziglang.org/documentation/0.15.2/#noreturn). `err` is the `error` and is in scope of the expression `b`.

```
const value: anyerror!u32 = error.Broken;
const unwrapped = value catch 1234;
unwrapped == 1234
```

Logical And

```
a and b
```

-   [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

If `a` is `false`, returns `false` without evaluating `b`. Otherwise, returns `b`.

```
(false and true) == false
```

Logical Or

```
a or b
```

-   [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

If `a` is `true`, returns `true` without evaluating `b`. Otherwise, returns `b`.

```
(false or true) == true
```

Boolean Not

```
!a
```

-   [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

```
!false == true
```

Equality

```
a == b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)
-   [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

-   [type](https://ziglang.org/documentation/0.15.2/#Primitive-Types)
-   [packed struct](https://ziglang.org/documentation/0.15.2/#packed-struct)

Returns `true` if a and b are equal, otherwise returns `false`. Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
(1 == 1) == true
```

Null Check

```
a == null
```

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

Returns `true` if a is `null`, otherwise returns `false`.

```
const value: ?u32 = null;
(value == null) == true
```

Inequality

```
a != b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)
-   [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

-   [type](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

Returns `false` if a and b are equal, otherwise returns `true`. Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
(1 != 1) == false
```

Non-Null Check

```
a != null
```

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

Returns `false` if a is `null`, otherwise returns `true`.

```
const value: ?u32 = null;
(value != null) == false
```

Greater Than

```
a > b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

Returns `true` if a is greater than b, otherwise returns `false`. Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
(2 > 1) == true
```

Greater or Equal

```
a >= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

Returns `true` if a is greater than or equal to b, otherwise returns `false`. Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
(2 >= 1) == true
```

Less Than

```
a < b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

Returns `true` if a is less than b, otherwise returns `false`. Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
(1 < 2) == true
```

Lesser or Equal

```
a <= b
```

-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers)

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)

Returns `true` if a is less than or equal to b, otherwise returns `false`. Invokes [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the operands.

```
(1 <= 2) == true
```

Array Concatenation

```
a ++ b
```

-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays)

-   Only available when the lengths of both `a` and `b` are [compile-time known](https://ziglang.org/documentation/0.15.2/#comptime).

```
const mem = @import("std").mem;
const array1 = [_]u32{1,2};
const array2 = [_]u32{3,4};
const together = array1 ++ array2;
mem.eql(u32, &together, &[_]u32{1,2,3,4})
```

Array Multiplication

```
a ** b
```

-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays)

-   Only available when the length of `a` and `b` are [compile-time known](https://ziglang.org/documentation/0.15.2/#comptime).

```
const mem = @import("std").mem;
const pattern = "ab" ** 3;
mem.eql(u8, pattern, "ababab")
```

Pointer Dereference

```
a.*
```

-   [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers)

Pointer dereference.

```
const x: u32 = 1234;
const ptr = &x;
ptr.* == 1234
```

Address Of

```
&a
```

All types

```
const x: u32 = 1234;
const ptr = &x;
ptr.* == 1234
```

Error Set Merge

```
a || b
```

-   [Error Set Type](https://ziglang.org/documentation/0.15.2/#Error-Set-Type)

[Merging Error Sets](https://ziglang.org/documentation/0.15.2/#Merging-Error-Sets)

```
const A = error{One};
const B = error{Two};
(A || B) == error{One, Two}
```

### [Precedence](https://ziglang.org/documentation/0.15.2/#toc-Precedence) [Â§](https://ziglang.org/documentation/0.15.2/#Precedence)

```
x() x[] x.y x.* x.?
a!b
x{}
!x -x -%x ~x &x ?x
* / % ** *% *| ||

+ - ++ +% -% +| -|
<< >> <<|
& ^ | orelse catch
== != < > <= >=
and
or
= *= *%= *|= /= %= += +%= +|= -= -%= -|= <<= <<|= >>= &= ^= |=
```

## [Arrays](https://ziglang.org/documentation/0.15.2/#toc-Arrays) [Â§](https://ziglang.org/documentation/0.15.2/#Arrays)

test\_arrays.zig

```
const expect = @import("std").testing.expect;
const assert = @import("std").debug.assert;
const mem = @import("std").mem;

// array literal
const message = [_]u8{ 'h', 'e', 'l', 'l', 'o' };

// alternative initialization using result location
const alt_message: [5]u8 = .{ 'h', 'e', 'l', 'l', 'o' };

comptime {
    assert(mem.eql(u8, &message, &alt_message));
}

// get the size of an array
comptime {
    assert(message.len == 5);
}

// A string literal is a single-item pointer to an array.
const same_message = "hello";

comptime {
    assert(mem.eql(u8, &message, same_message));
}

test "iterate over an array" {
    var sum: usize = 0;
    for (message) |byte| {
        sum += byte;
    }
    try expect(sum == 'h' + 'e' + 'l' * 2 + 'o');
}

// modifiable array
var some_integers: [100]i32 = undefined;

test "modify an array" {
    for (&some_integers, 0..) |*item, i| {
        item.* = @intCast(i);
    }
    try expect(some_integers[10] == 10);
    try expect(some_integers[99] == 99);
}

// array concatenation works if the values are known
// at compile time
const part_one = [_]i32{ 1, 2, 3, 4 };
const part_two = [_]i32{ 5, 6, 7, 8 };
const all_of_it = part_one ++ part_two;
comptime {
    assert(mem.eql(i32, &all_of_it, &[_]i32{ 1, 2, 3, 4, 5, 6, 7, 8 }));
}

// remember that string literals are arrays
const hello = "hello";
const world = "world";
const hello_world = hello ++ " " ++ world;
comptime {
    assert(mem.eql(u8, hello_world, "hello world"));
}

// ** does repeating patterns
const pattern = "ab" ** 3;
comptime {
    assert(mem.eql(u8, pattern, "ababab"));
}

// initialize an array to zero
const all_zero = [_]u16{0} ** 10;

comptime {
    assert(all_zero.len == 10);
    assert(all_zero[5] == 0);
}

// use compile-time code to initialize an array
var fancy_array = init: {
    var initial_value: [10]Point = undefined;
    for (&initial_value, 0..) |*pt, i| {
        pt.* = Point{
            .x = @intCast(i),
            .y = @intCast(i * 2),
        };
    }
    break :init initial_value;
};
const Point = struct {
    x: i32,
    y: i32,
};

test "compile-time array initialization" {
    try expect(fancy_array[4].x == 4);
    try expect(fancy_array[4].y == 8);
}

// call a function to initialize an array
var more_points = [_]Point{makePoint(3)} ** 10;
fn makePoint(x: i32) Point {
    return Point{
        .x = x,
        .y = x * 2,
    };
}
test "array initialization with function calls" {
    try expect(more_points[4].x == 3);
    try expect(more_points[4].y == 6);
    try expect(more_points.len == 10);
}
```

Shell

$ zig test test\_arrays.zig
1/4 test\_arrays.test.iterate over an array...OK
2/4 test\_arrays.test.modify an array...OK
3/4 test\_arrays.test.compile-time array initialization...OK
4/4 test\_arrays.test.array initialization with function calls...OK
All 4 tests passed.

See also:

-   [for](https://ziglang.org/documentation/0.15.2/#for)

-   [Slices](https://ziglang.org/documentation/0.15.2/#Slices)

### [Multidimensional Arrays](https://ziglang.org/documentation/0.15.2/#toc-Multidimensional-Arrays) [Â§](https://ziglang.org/documentation/0.15.2/#Multidimensional-Arrays)

Multidimensional arrays can be created by nesting arrays:

test\_multidimensional\_arrays.zig

```
const std = @import("std");
const expect = std.testing.expect;
const expectEqual = std.testing.expectEqual;

const mat4x5 = [4][5]f32{
    [_]f32{ 1.0, 0.0, 0.0, 0.0, 0.0 },
    [_]f32{ 0.0, 1.0, 0.0, 1.0, 0.0 },
    [_]f32{ 0.0, 0.0, 1.0, 0.0, 0.0 },
    [_]f32{ 0.0, 0.0, 0.0, 1.0, 9.9 },
};
test "multidimensional arrays" {
    // mat4x5 itself is a one-dimensional array of arrays.
    try expectEqual(mat4x5[1], [_]f32{ 0.0, 1.0, 0.0, 1.0, 0.0 });

    // Access the 2D array by indexing the outer array, and then the inner array.
    try expect(mat4x5[3][4] == 9.9);

    // Here we iterate with for loops.
    for (mat4x5, 0..) |row, row_index| {
        for (row, 0..) |cell, column_index| {
            if (row_index == column_index) {
                try expect(cell == 1.0);
            }
        }
    }

    // Initialize a multidimensional array to zeros.
    const all_zero: [4][5]f32 = .{.{0} ** 5} ** 4;
    try expect(all_zero[0][0] == 0);
}
```

Shell

$ zig test test\_multidimensional\_arrays.zig
1/1 test\_multidimensional\_arrays.test.multidimensional arrays...OK
All 1 tests passed.

### [Sentinel-Terminated Arrays](https://ziglang.org/documentation/0.15.2/#toc-Sentinel-Terminated-Arrays) [Â§](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Arrays)

The syntax `[N:x]T` describes an array which has a sentinel element of value `x` at the index corresponding to the length `N`.

test\_null\_terminated\_array.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "0-terminated sentinel array" {
    const array = [_:0]u8{ 1, 2, 3, 4 };

    try expect(@TypeOf(array) == [4:0]u8);
    try expect(array.len == 4);
    try expect(array[4] == 0);
}

test "extra 0s in 0-terminated sentinel array" {
    // The sentinel value may appear earlier, but does not influence the compile-time 'len'.
    const array = [_:0]u8{ 1, 0, 0, 4 };

    try expect(@TypeOf(array) == [4:0]u8);
    try expect(array.len == 4);
    try expect(array[4] == 0);
}
```

Shell

$ zig test test\_null\_terminated\_array.zig
1/2 test\_null\_terminated\_array.test.0-terminated sentinel array...OK
2/2 test\_null\_terminated\_array.test.extra 0s in 0-terminated sentinel array...OK
All 2 tests passed.

See also:

-   [Sentinel-Terminated Pointers](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Pointers)

-   [Sentinel-Terminated Slices](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Slices)

### [Destructuring Arrays](https://ziglang.org/documentation/0.15.2/#toc-Destructuring-Arrays) [Â§](https://ziglang.org/documentation/0.15.2/#Destructuring-Arrays)

Arrays can be destructured:

destructuring\_arrays.zig

```
const print = @import("std").debug.print;

fn swizzleRgbaToBgra(rgba: [4]u8) [4]u8 {
    // readable swizzling by destructuring
    const r, const g, const b, const a = rgba;
    return .{ b, g, r, a };
}

pub fn main() void {
    const pos = [_]i32{ 1, 2 };
    const x, const y = pos;
    print("x = {}, y = {}\n", .{x, y});

    const orange: [4]u8 = .{ 255, 165, 0, 255 };
    print("{any}\n", .{swizzleRgbaToBgra(orange)});
}
```

Shell

$ zig build-exe destructuring\_arrays.zig
$ ./destructuring\_arrays
x = 1, y = 2
{ 0, 165, 255, 255 }

See also:

-   [Destructuring](https://ziglang.org/documentation/0.15.2/#Destructuring)

-   [Destructuring Tuples](https://ziglang.org/documentation/0.15.2/#Destructuring-Tuples)
-   [Destructuring Vectors](https://ziglang.org/documentation/0.15.2/#Destructuring-Vectors)

## [Vectors](https://ziglang.org/documentation/0.15.2/#toc-Vectors) [Â§](https://ziglang.org/documentation/0.15.2/#Vectors)

A vector is a group of booleans, [Integers](https://ziglang.org/documentation/0.15.2/#Integers), [Floats](https://ziglang.org/documentation/0.15.2/#Floats), or [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) which are operated on in parallel, using SIMD instructions if possible. Vector types are created with the builtin function [@Vector](https://ziglang.org/documentation/0.15.2/#Vector).

Vectors generally support the same builtin operators as their underlying base types. The only exception to this is the keywords \`and\` and \`or\` on vectors of bools, since these operators affect control flow, which is not allowed for vectors. All other operations are performed element-wise, and return a vector of the same length as the input vectors. This includes:

-   Arithmetic (`+`, `-`, `/`, `*`, `@divFloor`, `@sqrt`, `@ceil`, `@log`, etc.)

-   Bitwise operators (`>>`, `<<`, `&`, `|`, `~`, etc.)
-   Comparison operators (`<`, `>`, `==`, etc.)

-   Boolean not (`!`)

It is prohibited to use a math operator on a mixture of scalars (individual numbers) and vectors. Zig provides the [@splat](https://ziglang.org/documentation/0.15.2/#splat) builtin to easily convert from scalars to vectors, and it supports [@reduce](https://ziglang.org/documentation/0.15.2/#reduce) and array indexing syntax to convert from vectors to scalars. Vectors also support assignment to and from fixed-length arrays with comptime-known length.

For rearranging elements within and between vectors, Zig provides the [@shuffle](https://ziglang.org/documentation/0.15.2/#shuffle) and [@select](https://ziglang.org/documentation/0.15.2/#select) functions.

Operations on vectors shorter than the target machine's native SIMD size will typically compile to single SIMD instructions, while vectors longer than the target machine's native SIMD size will compile to multiple SIMD instructions. If a given operation doesn't have SIMD support on the target architecture, the compiler will default to operating on each vector element one at a time. Zig supports any comptime-known vector length up to 2^32-1, although small powers of two (2-64) are most typical. Note that excessively long vector lengths (e.g. 2^20) may result in compiler crashes on current versions of Zig.

test\_vector.zig

```
const std = @import("std");
const expectEqual = std.testing.expectEqual;

test "Basic vector usage" {
    // Vectors have a compile-time known length and base type.
    const a = @Vector(4, i32){ 1, 2, 3, 4 };
    const b = @Vector(4, i32){ 5, 6, 7, 8 };

    // Math operations take place element-wise.
    const c = a + b;

    // Individual vector elements can be accessed using array indexing syntax.
    try expectEqual(6, c[0]);
    try expectEqual(8, c[1]);
    try expectEqual(10, c[2]);
    try expectEqual(12, c[3]);
}

test "Conversion between vectors, arrays, and slices" {
    // Vectors and fixed-length arrays can be automatically assigned back and forth
    const arr1: [4]f32 = [_]f32{ 1.1, 3.2, 4.5, 5.6 };
    const vec: @Vector(4, f32) = arr1;
    const arr2: [4]f32 = vec;
    try expectEqual(arr1, arr2);

    // You can also assign from a slice with comptime-known length to a vector using .*
    const vec2: @Vector(2, f32) = arr1[1..3].*;

    const slice: []const f32 = &arr1;
    var offset: u32 = 1; // var to make it runtime-known
    _ = &offset; // suppress 'var is never mutated' error
    // To extract a comptime-known length from a runtime-known offset,
    // first extract a new slice from the starting offset, then an array of
    // comptime-known length
    const vec3: @Vector(2, f32) = slice[offset..][0..2].*;
    try expectEqual(slice[offset], vec2[0]);
    try expectEqual(slice[offset + 1], vec2[1]);
    try expectEqual(vec2, vec3);
}
```

Shell

$ zig test test\_vector.zig
1/2 test\_vector.test.Basic vector usage...OK
2/2 test\_vector.test.Conversion between vectors, arrays, and slices...OK
All 2 tests passed.

TODO talk about C ABI interop  
TODO consider suggesting std.MultiArrayList

See also:

-   [@splat](https://ziglang.org/documentation/0.15.2/#splat)

-   [@shuffle](https://ziglang.org/documentation/0.15.2/#shuffle)
-   [@select](https://ziglang.org/documentation/0.15.2/#select)

-   [@reduce](https://ziglang.org/documentation/0.15.2/#reduce)

### [Destructuring Vectors](https://ziglang.org/documentation/0.15.2/#toc-Destructuring-Vectors) [Â§](https://ziglang.org/documentation/0.15.2/#Destructuring-Vectors)

Vectors can be destructured:

destructuring\_vectors.zig

```
const print = @import("std").debug.print;

// emulate punpckldq
pub fn unpack(x: @Vector(4, f32), y: @Vector(4, f32)) @Vector(4, f32) {
    const a, const c, _, _ = x;
    const b, const d, _, _ = y;
    return .{ a, b, c, d };
}

pub fn main() void {
    const x: @Vector(4, f32) = .{ 1.0, 2.0, 3.0, 4.0 };
    const y: @Vector(4, f32) = .{ 5.0, 6.0, 7.0, 8.0 };
    print("{}", .{unpack(x, y)});
}
```

Shell

$ zig build-exe destructuring\_vectors.zig
$ ./destructuring\_vectors
{ 1, 5, 2, 6 }

See also:

-   [Destructuring](https://ziglang.org/documentation/0.15.2/#Destructuring)

-   [Destructuring Tuples](https://ziglang.org/documentation/0.15.2/#Destructuring-Tuples)
-   [Destructuring Arrays](https://ziglang.org/documentation/0.15.2/#Destructuring-Arrays)

## [Pointers](https://ziglang.org/documentation/0.15.2/#toc-Pointers) [Â§](https://ziglang.org/documentation/0.15.2/#Pointers)

Zig has two kinds of pointers: single-item and many-item.

-   `*T` - single-item pointer to exactly one item.
    -   Supports deref syntax: `ptr.*`
    -   Supports slice syntax: `ptr[0..1]`
    -   Supports pointer subtraction: `ptr - ptr`
-   `[*]T` - many-item pointer to unknown number of items.
    
    -   Supports index syntax: `ptr[i]`
    -   Supports slice syntax: `ptr[start..end]` and `ptr[start..]`
    -   Supports pointer-integer arithmetic: `ptr + int`, `ptr - int`
    -   Supports pointer subtraction: `ptr - ptr`
    
    `T` must have a known size, which means that it cannot be `anyopaque` or any other [opaque type](https://ziglang.org/documentation/0.15.2/#opaque).

These types are closely related to [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays) and [Slices](https://ziglang.org/documentation/0.15.2/#Slices):

-   `*[N]T` - pointer to N items, same as single-item pointer to an array.
    -   Supports index syntax: `array_ptr[i]`
    -   Supports slice syntax: `array_ptr[start..end]`
    -   Supports len property: `array_ptr.len`
    -   Supports pointer subtraction: `array_ptr - array_ptr`

-   `[]T` - is a slice (a fat pointer, which contains a pointer of type `[*]T` and a length).
    -   Supports index syntax: `slice[i]`
    -   Supports slice syntax: `slice[start..end]`
    -   Supports len property: `slice.len`

Use `&x` to obtain a single-item pointer:

test\_single\_item\_pointer.zig

```
const expect = @import("std").testing.expect;

test "address of syntax" {
    // Get the address of a variable:
    const x: i32 = 1234;
    const x_ptr = &x;

    // Dereference a pointer:
    try expect(x_ptr.* == 1234);

    // When you get the address of a const variable, you get a const single-item pointer.
    try expect(@TypeOf(x_ptr) == *const i32);

    // If you want to mutate the value, you'd need an address of a mutable variable:
    var y: i32 = 5678;
    const y_ptr = &y;
    try expect(@TypeOf(y_ptr) == *i32);
    y_ptr.* += 1;
    try expect(y_ptr.* == 5679);
}

test "pointer array access" {
    // Taking an address of an individual element gives a
    // single-item pointer. This kind of pointer
    // does not support pointer arithmetic.
    var array = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
    const ptr = &array[2];
    try expect(@TypeOf(ptr) == *u8);

    try expect(array[2] == 3);
    ptr.* += 1;
    try expect(array[2] == 4);
}

test "slice syntax" {
    // Get a pointer to a variable:
    var x: i32 = 1234;
    const x_ptr = &x;

    // Convert to array pointer using slice syntax:
    const x_array_ptr = x_ptr[0..1];
    try expect(@TypeOf(x_array_ptr) == *[1]i32);

    // Coerce to many-item pointer:
    const x_many_ptr: [*]i32 = x_array_ptr;
    try expect(x_many_ptr[0] == 1234);
}
```

Shell

$ zig test test\_single\_item\_pointer.zig
1/3 test\_single\_item\_pointer.test.address of syntax...OK
2/3 test\_single\_item\_pointer.test.pointer array access...OK
3/3 test\_single\_item\_pointer.test.slice syntax...OK
All 3 tests passed.

Zig supports pointer arithmetic. It's better to assign the pointer to `[*]T` and increment that variable. For example, directly incrementing the pointer from a slice will corrupt it.

test\_pointer\_arithmetic.zig

```
const expect = @import("std").testing.expect;

test "pointer arithmetic with many-item pointer" {
    const array = [_]i32{ 1, 2, 3, 4 };
    var ptr: [*]const i32 = &array;

    try expect(ptr[0] == 1);
    ptr += 1;
    try expect(ptr[0] == 2);

    // slicing a many-item pointer without an end is equivalent to
    // pointer arithmetic: `ptr[start..] == ptr + start`
    try expect(ptr[1..] == ptr + 1);

    // subtraction between any two pointers except slices based on element size is supported
    try expect(&ptr[1] - &ptr[0] == 1);
}

test "pointer arithmetic with slices" {
    var array = [_]i32{ 1, 2, 3, 4 };
    var length: usize = 0; // var to make it runtime-known
    _ = &length; // suppress 'var is never mutated' error
    var slice = array[length..array.len];

    try expect(slice[0] == 1);
    try expect(slice.len == 4);

    slice.ptr += 1;
    // now the slice is in an bad state since len has not been updated

    try expect(slice[0] == 2);
    try expect(slice.len == 4);
}
```

Shell

$ zig test test\_pointer\_arithmetic.zig
1/2 test\_pointer\_arithmetic.test.pointer arithmetic with many-item pointer...OK
2/2 test\_pointer\_arithmetic.test.pointer arithmetic with slices...OK
All 2 tests passed.

In Zig, we generally prefer [Slices](https://ziglang.org/documentation/0.15.2/#Slices) rather than [Sentinel-Terminated Pointers](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Pointers). You can turn an array or pointer into a slice using slice syntax.

Slices have bounds checking and are therefore protected against this kind of Illegal Behavior. This is one reason we prefer slices to pointers.

test\_slice\_bounds.zig

```
const expect = @import("std").testing.expect;

test "pointer slicing" {
    var array = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
    var start: usize = 2; // var to make it runtime-known
    _ = &start; // suppress 'var is never mutated' error
    const slice = array[start..4];
    try expect(slice.len == 2);

    try expect(array[3] == 4);
    slice[1] += 1;
    try expect(array[3] == 5);
}
```

Shell

$ zig test test\_slice\_bounds.zig
1/1 test\_slice\_bounds.test.pointer slicing...OK
All 1 tests passed.

Pointers work at compile-time too, as long as the code does not depend on an undefined memory layout:

test\_comptime\_pointers.zig

```
const expect = @import("std").testing.expect;

test "comptime pointers" {
    comptime {
        var x: i32 = 1;
        const ptr = &x;
        ptr.* += 1;
        x += 1;
        try expect(ptr.* == 3);
    }
}
```

Shell

$ zig test test\_comptime\_pointers.zig
1/1 test\_comptime\_pointers.test.comptime pointers...OK
All 1 tests passed.

To convert an integer address into a pointer, use `@ptrFromInt`. To convert a pointer to an integer, use `@intFromPtr`:

test\_integer\_pointer\_conversion.zig

```
const expect = @import("std").testing.expect;

test "@intFromPtr and @ptrFromInt" {
    const ptr: *i32 = @ptrFromInt(0xdeadbee0);
    const addr = @intFromPtr(ptr);
    try expect(@TypeOf(addr) == usize);
    try expect(addr == 0xdeadbee0);
}
```

Shell

$ zig test test\_integer\_pointer\_conversion.zig
1/1 test\_integer\_pointer\_conversion.test.@intFromPtr and @ptrFromInt...OK
All 1 tests passed.

Zig is able to preserve memory addresses in comptime code, as long as the pointer is never dereferenced:

test\_comptime\_pointer\_conversion.zig

```
const expect = @import("std").testing.expect;

test "comptime @ptrFromInt" {
    comptime {
        // Zig is able to do this at compile-time, as long as
        // ptr is never dereferenced.
        const ptr: *i32 = @ptrFromInt(0xdeadbee0);
        const addr = @intFromPtr(ptr);
        try expect(@TypeOf(addr) == usize);
        try expect(addr == 0xdeadbee0);
    }
}
```

Shell

$ zig test test\_comptime\_pointer\_conversion.zig
1/1 test\_comptime\_pointer\_conversion.test.comptime @ptrFromInt...OK
All 1 tests passed.

[@ptrCast](https://ziglang.org/documentation/0.15.2/#ptrCast) converts a pointer's element type to another. This creates a new pointer that can cause undetectable Illegal Behavior depending on the loads and stores that pass through it. Generally, other kinds of type conversions are preferable to `@ptrCast` if possible.

test\_pointer\_casting.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "pointer casting" {
    const bytes align(@alignOf(u32)) = [_]u8{ 0x12, 0x12, 0x12, 0x12 };
    const u32_ptr: *const u32 = @ptrCast(&bytes);
    try expect(u32_ptr.* == 0x12121212);

    // Even this example is contrived - there are better ways to do the above than
    // pointer casting. For example, using a slice narrowing cast:
    const u32_value = std.mem.bytesAsSlice(u32, bytes[0..])[0];
    try expect(u32_value == 0x12121212);

    // And even another way, the most straightforward way to do it:
    try expect(@as(u32, @bitCast(bytes)) == 0x12121212);
}

test "pointer child type" {
    // pointer types have a `child` field which tells you the type they point to.
    try expect(@typeInfo(*u32).pointer.child == u32);
}
```

Shell

$ zig test test\_pointer\_casting.zig
1/2 test\_pointer\_casting.test.pointer casting...OK
2/2 test\_pointer\_casting.test.pointer child type...OK
All 2 tests passed.

See also:

-   [Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers)

-   [@ptrFromInt](https://ziglang.org/documentation/0.15.2/#ptrFromInt)
-   [@intFromPtr](https://ziglang.org/documentation/0.15.2/#intFromPtr)

-   [C Pointers](https://ziglang.org/documentation/0.15.2/#C-Pointers)

### [volatile](https://ziglang.org/documentation/0.15.2/#toc-volatile) [Â§](https://ziglang.org/documentation/0.15.2/#volatile)

Loads and stores are assumed to not have side effects. If a given load or store should have side effects, such as Memory Mapped Input/Output (MMIO), use `volatile`. In the following code, loads and stores with `mmio_ptr` are guaranteed to all happen and in the same order as in source code:

test\_volatile.zig

```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

Shell

$ zig test test\_volatile.zig
1/1 test\_volatile.test.volatile...OK
All 1 tests passed.

Note that `volatile` is unrelated to concurrency and [Atomics](https://ziglang.org/documentation/0.15.2/#Atomics). If you see code that is using `volatile` for something other than Memory Mapped Input/Output, it is probably a bug.

### [Alignment](https://ziglang.org/documentation/0.15.2/#toc-Alignment) [Â§](https://ziglang.org/documentation/0.15.2/#Alignment)

Each type has an **alignment** - a number of bytes such that, when a value of the type is loaded from or stored to memory, the memory address must be evenly divisible by this number. You can use [@alignOf](https://ziglang.org/documentation/0.15.2/#alignOf) to find out this value for any type.

Alignment depends on the CPU architecture, but is always a power of two, and less than `1 << 29`.

In Zig, a pointer type has an alignment value. If the value is equal to the alignment of the underlying type, it can be omitted from the type:

test\_variable\_alignment.zig

```
const std = @import("std");
const builtin = @import("builtin");
const expect = std.testing.expect;

test "variable alignment" {
    var x: i32 = 1234;
    const align_of_i32 = @alignOf(@TypeOf(x));
    try expect(@TypeOf(&x) == *i32);
    try expect(*i32 == *align(align_of_i32) i32);
    if (builtin.target.cpu.arch == .x86_64) {
        try expect(@typeInfo(*i32).pointer.alignment == 4);
    }
}
```

Shell

$ zig test test\_variable\_alignment.zig
1/1 test\_variable\_alignment.test.variable alignment...OK
All 1 tests passed.

In the same way that a `*i32` can be [coerced](https://ziglang.org/documentation/0.15.2/#Type-Coercion) to a `*const i32`, a pointer with a larger alignment can be implicitly cast to a pointer with a smaller alignment, but not vice versa.

You can specify alignment on variables and functions. If you do this, then pointers to them get the specified alignment:

test\_variable\_func\_alignment.zig

```
const expect = @import("std").testing.expect;

var foo: u8 align(4) = 100;

test "global variable alignment" {
    try expect(@typeInfo(@TypeOf(&foo)).pointer.alignment == 4);
    try expect(@TypeOf(&foo) == *align(4) u8);
    const as_pointer_to_array: *align(4) [1]u8 = &foo;
    const as_slice: []align(4) u8 = as_pointer_to_array;
    const as_unaligned_slice: []u8 = as_slice;
    try expect(as_unaligned_slice[0] == 100);
}

fn derp() align(@sizeOf(usize) * 2) i32 {
    return 1234;
}
fn noop1() align(1) void {}
fn noop4() align(4) void {}

test "function alignment" {
    try expect(derp() == 1234);
    try expect(@TypeOf(derp) == fn () i32);
    try expect(@TypeOf(&derp) == *align(@sizeOf(usize) * 2) const fn () i32);

    noop1();
    try expect(@TypeOf(noop1) == fn () void);
    try expect(@TypeOf(&noop1) == *align(1) const fn () void);

    noop4();
    try expect(@TypeOf(noop4) == fn () void);
    try expect(@TypeOf(&noop4) == *align(4) const fn () void);
}
```

Shell

$ zig test test\_variable\_func\_alignment.zig
1/2 test\_variable\_func\_alignment.test.global variable alignment...OK
2/2 test\_variable\_func\_alignment.test.function alignment...OK
All 2 tests passed.

If you have a pointer or a slice that has a small alignment, but you know that it actually has a bigger alignment, use [@alignCast](https://ziglang.org/documentation/0.15.2/#alignCast) to change the pointer into a more aligned pointer. This is a no-op at runtime, but inserts a [safety check](https://ziglang.org/documentation/0.15.2/#Incorrect-Pointer-Alignment):

test\_incorrect\_pointer\_alignment.zig

```
const std = @import("std");

test "pointer alignment safety" {
    var array align(4) = [_]u32{ 0x11111111, 0x11111111 };
    const bytes = std.mem.sliceAsBytes(array[0..]);
    try std.testing.expect(foo(bytes) == 0x11111111);
}
fn foo(bytes: []u8) u32 {
    const slice4 = bytes[1..5];
    const int_slice = std.mem.bytesAsSlice(u32, @as([]align(4) u8, @alignCast(slice4)));
    return int_slice[0];
}
```

Shell

$ zig test test\_incorrect\_pointer\_alignment.zig
1/1 test\_incorrect\_pointer\_alignment.test.pointer alignment safety...thread 2895819 panic: incorrect alignment
/home/andy/dev/zig/doc/langref/test\_incorrect\_pointer\_alignment.zig:10:68: 0x102c2a8 in foo (test\_incorrect\_pointer\_alignment.zig)
    const int\_slice = std.mem.bytesAsSlice(u32, @as(\[\]align(4) u8, @alignCast(slice4)));
                                                                   ^
/home/andy/dev/zig/doc/langref/test\_incorrect\_pointer\_alignment.zig:6:31: 0x102c0d2 in test.pointer alignment safety (test\_incorrect\_pointer\_alignment.zig)
    try std.testing.expect(foo(bytes) == 0x11111111);
                              ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x115cf30 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1156151 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x114feed in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x114f781 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/9cb7896b3cdf812f518129da5e21dc23/test --seed=0x441e5edd

### [allowzero](https://ziglang.org/documentation/0.15.2/#toc-allowzero) [Â§](https://ziglang.org/documentation/0.15.2/#allowzero)

This pointer attribute allows a pointer to have address zero. This is only ever needed on the freestanding OS target, where the address zero is mappable. If you want to represent null pointers, use [Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers) instead. [Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers) with `allowzero` are not the same size as pointers. In this code example, if the pointer did not have the `allowzero` attribute, this would be a [Pointer Cast Invalid Null](https://ziglang.org/documentation/0.15.2/#Pointer-Cast-Invalid-Null) panic:

test\_allowzero.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "allowzero" {
    var zero: usize = 0; // var to make to runtime-known
    _ = &zero; // suppress 'var is never mutated' error
    const ptr: *allowzero i32 = @ptrFromInt(zero);
    try expect(@intFromPtr(ptr) == 0);
}
```

Shell

$ zig test test\_allowzero.zig
1/1 test\_allowzero.test.allowzero...OK
All 1 tests passed.

### [Sentinel-Terminated Pointers](https://ziglang.org/documentation/0.15.2/#toc-Sentinel-Terminated-Pointers) [Â§](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Pointers)

The syntax `[*:x]T` describes a pointer that has a length determined by a sentinel value. This provides protection against buffer overflow and overreads.

sentinel-terminated\_pointer.zig

```
const std = @import("std");

// This is also available as `std.c.printf`.
pub extern "c" fn printf(format: [*:0]const u8, ...) c_int;

pub fn main() anyerror!void {
    _ = printf("Hello, world!\n"); // OK

    const msg = "Hello, world!\n";
    const non_null_terminated_msg: [msg.len]u8 = msg.*;
    _ = printf(&non_null_terminated_msg);
}
```

Shell

$ zig build-exe sentinel-terminated\_pointer.zig -lc
/home/andy/dev/zig/doc/langref/sentinel-terminated\_pointer.zig:11:16: error: expected type '\[\*:0\]const u8', found '\*const \[14\]u8'
    \_ = printf(&non\_null\_terminated\_msg);
               ^~~~~~~~~~~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/sentinel-terminated\_pointer.zig:11:16: note: destination pointer requires '0' sentinel
/home/andy/dev/zig/doc/langref/sentinel-terminated\_pointer.zig:4:34: note: parameter type declared here
pub extern "c" fn printf(format: \[\*:0\]const u8, ...) c\_int;
                                 ^~~~~~~~~~~~~
referenced by:
    callMain \[inlined\]: /home/andy/dev/zig/lib/std/start.zig:627:37
    callMainWithArgs \[inlined\]: /home/andy/dev/zig/lib/std/start.zig:587:20
    main: /home/andy/dev/zig/lib/std/start.zig:602:28
    1 reference(s) hidden; use '-freference-trace=4' to see all references

See also:

-   [Sentinel-Terminated Slices](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Slices)

-   [Sentinel-Terminated Arrays](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Arrays)

## [Slices](https://ziglang.org/documentation/0.15.2/#toc-Slices) [Â§](https://ziglang.org/documentation/0.15.2/#Slices)

A slice is a pointer and a length. The difference between an array and a slice is that the array's length is part of the type and known at compile-time, whereas the slice's length is known at runtime. Both can be accessed with the `len` field.

test\_basic\_slices.zig

```
const expect = @import("std").testing.expect;
const expectEqualSlices = @import("std").testing.expectEqualSlices;

test "basic slices" {
    var array = [_]i32{ 1, 2, 3, 4 };
    var known_at_runtime_zero: usize = 0;
    _ = &known_at_runtime_zero;
    const slice = array[known_at_runtime_zero..array.len];

    // alternative initialization using result location
    const alt_slice: []const i32 = &.{ 1, 2, 3, 4 };

    try expectEqualSlices(i32, slice, alt_slice);

    try expect(@TypeOf(slice) == []i32);
    try expect(&slice[0] == &array[0]);
    try expect(slice.len == array.len);

    // If you slice with comptime-known start and end positions, the result is
    // a pointer to an array, rather than a slice.
    const array_ptr = array[0..array.len];
    try expect(@TypeOf(array_ptr) == *[array.len]i32);

    // You can perform a slice-by-length by slicing twice. This allows the compiler
    // to perform some optimisations like recognising a comptime-known length when
    // the start position is only known at runtime.
    var runtime_start: usize = 1;
    _ = &runtime_start;
    const length = 2;
    const array_ptr_len = array[runtime_start..][0..length];
    try expect(@TypeOf(array_ptr_len) == *[length]i32);

    // Using the address-of operator on a slice gives a single-item pointer.
    try expect(@TypeOf(&slice[0]) == *i32);
    // Using the `ptr` field gives a many-item pointer.
    try expect(@TypeOf(slice.ptr) == [*]i32);
    try expect(@intFromPtr(slice.ptr) == @intFromPtr(&slice[0]));

    // Slices have array bounds checking. If you try to access something out
    // of bounds, you'll get a safety check failure:
    slice[10] += 1;

    // Note that `slice.ptr` does not invoke safety checking, while `&slice[0]`
    // asserts that the slice has len > 0.

    // Empty slices can be created like this:
    const empty1 = &[0]u8{};
    // If the type is known you can use this short hand:
    const empty2: []u8 = &.{};
    try expect(empty1.len == 0);
    try expect(empty2.len == 0);

    // A zero-length initialization can always be used to create an empty slice, even if the slice is mutable.
    // This is because the pointed-to data is zero bits long, so its immutability is irrelevant.
}
```

Shell

$ zig test test\_basic\_slices.zig
1/1 test\_basic\_slices.test.basic slices...thread 2902466 panic: index out of bounds: index 10, len 4
/home/andy/dev/zig/doc/langref/test\_basic\_slices.zig:41:10: 0x102e3c0 in test.basic slices (test\_basic\_slices.zig)
    slice\[10\] += 1;
         ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x1160b60 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1159d81 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x1153b1d in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x11533b1 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/0e584e3dac6333a0b2d5158992704660/test --seed=0x665d12a2

This is one reason we prefer slices to pointers.

test\_slices.zig

```
const std = @import("std");
const expect = std.testing.expect;
const mem = std.mem;
const fmt = std.fmt;

test "using slices for strings" {
    // Zig has no concept of strings. String literals are const pointers
    // to null-terminated arrays of u8, and by convention parameters
    // that are "strings" are expected to be UTF-8 encoded slices of u8.
    // Here we coerce *const [5:0]u8 and *const [6:0]u8 to []const u8
    const hello: []const u8 = "hello";
    const world: []const u8 = "ä¸–ç•Œ";

    var all_together: [100]u8 = undefined;
    // You can use slice syntax with at least one runtime-known index on an
    // array to convert an array into a slice.
    var start: usize = 0;
    _ = &start;
    const all_together_slice = all_together[start..];
    // String concatenation example.
    const hello_world = try fmt.bufPrint(all_together_slice, "{s} {s}", .{ hello, world });

    // Generally, you can use UTF-8 and not worry about whether something is a
    // string. If you don't need to deal with individual characters, no need
    // to decode.
    try expect(mem.eql(u8, hello_world, "hello ä¸–ç•Œ"));
}

test "slice pointer" {
    var array: [10]u8 = undefined;
    const ptr = &array;
    try expect(@TypeOf(ptr) == *[10]u8);

    // A pointer to an array can be sliced just like an array:
    var start: usize = 0;
    var end: usize = 5;
    _ = .{ &start, &end };
    const slice = ptr[start..end];
    // The slice is mutable because we sliced a mutable pointer.
    try expect(@TypeOf(slice) == []u8);
    slice[2] = 3;
    try expect(array[2] == 3);

    // Again, slicing with comptime-known indexes will produce another pointer
    // to an array:
    const ptr2 = slice[2..3];
    try expect(ptr2.len == 1);
    try expect(ptr2[0] == 3);
    try expect(@TypeOf(ptr2) == *[1]u8);
}
```

Shell

$ zig test test\_slices.zig
1/2 test\_slices.test.using slices for strings...OK
2/2 test\_slices.test.slice pointer...OK
All 2 tests passed.

See also:

-   [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers)

-   [for](https://ziglang.org/documentation/0.15.2/#for)
-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays)

### [Sentinel-Terminated Slices](https://ziglang.org/documentation/0.15.2/#toc-Sentinel-Terminated-Slices) [Â§](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Slices)

The syntax `[:x]T` is a slice which has a runtime-known length and also guarantees a sentinel value at the element indexed by the length. The type does not guarantee that there are no sentinel elements before that. Sentinel-terminated slices allow element access to the `len` index.

test\_null\_terminated\_slice.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "0-terminated slice" {
    const slice: [:0]const u8 = "hello";

    try expect(slice.len == 5);
    try expect(slice[5] == 0);
}
```

Shell

$ zig test test\_null\_terminated\_slice.zig
1/1 test\_null\_terminated\_slice.test.0-terminated slice...OK
All 1 tests passed.

Sentinel-terminated slices can also be created using a variation of the slice syntax `data[start..end :x]`, where `data` is a many-item pointer, array or slice and `x` is the sentinel value.

test\_null\_terminated\_slicing.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "0-terminated slicing" {
    var array = [_]u8{ 3, 2, 1, 0, 3, 2, 1, 0 };
    var runtime_length: usize = 3;
    _ = &runtime_length;
    const slice = array[0..runtime_length :0];

    try expect(@TypeOf(slice) == [:0]u8);
    try expect(slice.len == 3);
}
```

Shell

$ zig test test\_null\_terminated\_slicing.zig
1/1 test\_null\_terminated\_slicing.test.0-terminated slicing...OK
All 1 tests passed.

Sentinel-terminated slicing asserts that the element in the sentinel position of the backing data is actually the sentinel value. If this is not the case, safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior) results.

test\_sentinel\_mismatch.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "sentinel mismatch" {
    var array = [_]u8{ 3, 2, 1, 0 };

    // Creating a sentinel-terminated slice from the array with a length of 2
    // will result in the value `1` occupying the sentinel element position.
    // This does not match the indicated sentinel value of `0` and will lead
    // to a runtime panic.
    var runtime_length: usize = 2;
    _ = &runtime_length;
    const slice = array[0..runtime_length :0];

    _ = slice;
}
```

Shell

$ zig test test\_sentinel\_mismatch.zig
1/1 test\_sentinel\_mismatch.test.sentinel mismatch...thread 2902472 panic: sentinel mismatch: expected 0, found 1
/home/andy/dev/zig/doc/langref/test\_sentinel\_mismatch.zig:13:24: 0x102c117 in test.sentinel mismatch (test\_sentinel\_mismatch.zig)
    const slice = array\[0..runtime\_length :0\];
                       ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x115cc90 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1155eb1 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x114fc4d in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x114f4e1 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/12c6cfa0971ea7c724c8448a09f20f6b/test --seed=0xb506c876

See also:

-   [Sentinel-Terminated Pointers](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Pointers)

-   [Sentinel-Terminated Arrays](https://ziglang.org/documentation/0.15.2/#Sentinel-Terminated-Arrays)

## [struct](https://ziglang.org/documentation/0.15.2/#toc-struct) [Â§](https://ziglang.org/documentation/0.15.2/#struct)

test\_structs.zig

```
// Declare a struct.
// Zig gives no guarantees about the order of fields and the size of
// the struct but the fields are guaranteed to be ABI-aligned.
const Point = struct {
    x: f32,
    y: f32,
};

// Declare an instance of a struct.
const p: Point = .{
    .x = 0.12,
    .y = 0.34,
};

// Functions in the struct's namespace can be called with dot syntax.
const Vec3 = struct {
    x: f32,
    y: f32,
    z: f32,

    pub fn init(x: f32, y: f32, z: f32) Vec3 {
        return Vec3{
            .x = x,
            .y = y,
            .z = z,
        };
    }

    pub fn dot(self: Vec3, other: Vec3) f32 {
        return self.x * other.x + self.y * other.y + self.z * other.z;
    }
};

test "dot product" {
    const v1 = Vec3.init(1.0, 0.0, 0.0);
    const v2 = Vec3.init(0.0, 1.0, 0.0);
    try expect(v1.dot(v2) == 0.0);

    // Other than being available to call with dot syntax, struct methods are
    // not special. You can reference them as any other declaration inside
    // the struct:
    try expect(Vec3.dot(v1, v2) == 0.0);
}

// Structs can have declarations.
// Structs can have 0 fields.
const Empty = struct {
    pub const PI = 3.14;
};
test "struct namespaced variable" {
    try expect(Empty.PI == 3.14);
    try expect(@sizeOf(Empty) == 0);

    // Empty structs can be instantiated the same as usual.
    const does_nothing: Empty = .{};

    _ = does_nothing;
}

// Struct field order is determined by the compiler, however, a base pointer
// can be computed from a field pointer:
fn setYBasedOnX(x: *f32, y: f32) void {
    const point: *Point = @fieldParentPtr("x", x);
    point.y = y;
}
test "field parent pointer" {
    var point = Point{
        .x = 0.1234,
        .y = 0.5678,
    };
    setYBasedOnX(&point.x, 0.9);
    try expect(point.y == 0.9);
}

// Structs can be returned from functions.
fn LinkedList(comptime T: type) type {
    return struct {
        pub const Node = struct {
            prev: ?*Node,
            next: ?*Node,
            data: T,
        };

        first: ?*Node,
        last: ?*Node,
        len: usize,
    };
}

test "linked list" {
    // Functions called at compile-time are memoized.
    try expect(LinkedList(i32) == LinkedList(i32));

    const list = LinkedList(i32){
        .first = null,
        .last = null,
        .len = 0,
    };
    try expect(list.len == 0);

    // Since types are first class values you can instantiate the type
    // by assigning it to a variable:
    const ListOfInts = LinkedList(i32);
    try expect(ListOfInts == LinkedList(i32));

    var node = ListOfInts.Node{
        .prev = null,
        .next = null,
        .data = 1234,
    };
    const list2 = LinkedList(i32){
        .first = &node,
        .last = &node,
        .len = 1,
    };

    // When using a pointer to a struct, fields can be accessed directly,
    // without explicitly dereferencing the pointer.
    // So you can do
    try expect(list2.first.?.data == 1234);
    // instead of try expect(list2.first.?.*.data == 1234);
}

const expect = @import("std").testing.expect;
```

Shell

$ zig test test\_structs.zig
1/4 test\_structs.test.dot product...OK
2/4 test\_structs.test.struct namespaced variable...OK
3/4 test\_structs.test.field parent pointer...OK
4/4 test\_structs.test.linked list...OK
All 4 tests passed.

### [Default Field Values](https://ziglang.org/documentation/0.15.2/#toc-Default-Field-Values) [Â§](https://ziglang.org/documentation/0.15.2/#Default-Field-Values)

Each struct field may have an expression indicating the default field value. Such expressions are executed at [comptime](https://ziglang.org/documentation/0.15.2/#comptime), and allow the field to be omitted in a struct literal expression:

struct\_default\_field\_values.zig

```
const Foo = struct {
    a: i32 = 1234,
    b: i32,
};

test "default struct initialization fields" {
    const x: Foo = .{
        .b = 5,
    };
    if (x.a + x.b != 1239) {
        comptime unreachable;
    }
}
```

Shell

$ zig test struct\_default\_field\_values.zig
1/1 struct\_default\_field\_values.test.default struct initialization fields...OK
All 1 tests passed.

#### [Faulty Default Field Values](https://ziglang.org/documentation/0.15.2/#toc-Faulty-Default-Field-Values) [Â§](https://ziglang.org/documentation/0.15.2/#Faulty-Default-Field-Values)

Default field values are only appropriate when the data invariants of a struct cannot be violated by omitting that field from an initialization.

For example, here is an inappropriate use of default struct field initialization:

bad\_default\_value.zig

```
const Threshold = struct {
    minimum: f32 = 0.25,
    maximum: f32 = 0.75,

    const Category = enum { low, medium, high };

    fn categorize(t: Threshold, value: f32) Category {
        assert(t.maximum >= t.minimum);
        if (value < t.minimum) return .low;
        if (value > t.maximum) return .high;
        return .medium;
    }
};

pub fn main() !void {
    var threshold: Threshold = .{
        .maximum = 0.20,
    };
    const category = threshold.categorize(0.90);
    try std.fs.File.stdout().writeAll(@tagName(category));
}

const std = @import("std");
const assert = std.debug.assert;
```

Shell

$ zig build-exe bad\_default\_value.zig
$ ./bad\_default\_value
thread 2895237 panic: reached unreachable code
/home/andy/dev/zig/lib/std/debug.zig:559:14: 0x1044179 in assert (std.zig)
    if (!ok) unreachable; // assertion failure
             ^
/home/andy/dev/zig/doc/langref/bad\_default\_value.zig:8:15: 0x113ec54 in categorize (bad\_default\_value.zig)
        assert(t.maximum >= t.minimum);
              ^
/home/andy/dev/zig/doc/langref/bad\_default\_value.zig:19:42: 0x113d444 in main (bad\_default\_value.zig)
    const category = threshold.categorize(0.90);
                                         ^
/home/andy/dev/zig/lib/std/start.zig:627:37: 0x113dca9 in posixCallMainAndExit (std.zig)
            const result = root.main() catch |err| {
                                    ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

Above you can see the danger of ignoring this principle. The default field values caused the data invariant to be violated, causing illegal behavior.

To fix this, remove the default values from all the struct fields, and provide a named default value:

struct\_default\_value.zig

```
const Threshold = struct {
    minimum: f32,
    maximum: f32,

    const default: Threshold = .{
        .minimum = 0.25,
        .maximum = 0.75,
    };
};
```

If a struct value requires a runtime-known value in order to be initialized without violating data invariants, then use an initialization method that accepts those runtime values, and populates the remaining fields.

### [extern struct](https://ziglang.org/documentation/0.15.2/#toc-extern-struct) [Â§](https://ziglang.org/documentation/0.15.2/#extern-struct)

An `extern struct` has in-memory layout matching the C ABI for the target.

If well-defined in-memory layout is not required, [struct](https://ziglang.org/documentation/0.15.2/#struct) is a better choice because it places fewer restrictions on the compiler.

See [packed struct](https://ziglang.org/documentation/0.15.2/#packed-struct) for a struct that has the ABI of its backing integer, which can be useful for modeling flags.

See also:

-   [extern union](https://ziglang.org/documentation/0.15.2/#extern-union)

-   [extern enum](https://ziglang.org/documentation/0.15.2/#extern-enum)

### [packed struct](https://ziglang.org/documentation/0.15.2/#toc-packed-struct) [Â§](https://ziglang.org/documentation/0.15.2/#packed-struct)

`packed` structs, like `enum`, are based on the concept of interpreting integers differently. All packed structs have a **backing integer**, which is implicitly determined by the total bit count of fields, or explicitly specified. Packed structs have well-defined memory layout - exactly the same ABI as their backing integer.

Each field of a packed struct is interpreted as a logical sequence of bits, arranged from least to most significant. Allowed field types:

-   An [integer](https://ziglang.org/documentation/0.15.2/#Integers) field uses exactly as many bits as its bit width. For example, a `u5` will use 5 bits of the backing integer.

-   A [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types) field uses exactly 1 bit.
-   An [enum](https://ziglang.org/documentation/0.15.2/#enum) field uses exactly the bit width of its integer tag type.

-   A [packed union](https://ziglang.org/documentation/0.15.2/#packed-union) field uses exactly the bit width of the union field with the largest bit width.
-   A `packed struct` field uses the bits of its backing integer.

This means that a `packed struct` can participate in a [@bitCast](https://ziglang.org/documentation/0.15.2/#bitCast) or a [@ptrCast](https://ziglang.org/documentation/0.15.2/#ptrCast) to reinterpret memory. This even works at [comptime](https://ziglang.org/documentation/0.15.2/#comptime):

test\_packed\_structs.zig

```
const std = @import("std");
const native_endian = @import("builtin").target.cpu.arch.endian();
const expect = std.testing.expect;

const Full = packed struct {
    number: u16,
};
const Divided = packed struct {
    half1: u8,
    quarter3: u4,
    quarter4: u4,
};

test "@bitCast between packed structs" {
    try doTheTest();
    try comptime doTheTest();
}

fn doTheTest() !void {
    try expect(@sizeOf(Full) == 2);
    try expect(@sizeOf(Divided) == 2);
    const full = Full{ .number = 0x1234 };
    const divided: Divided = @bitCast(full);
    try expect(divided.half1 == 0x34);
    try expect(divided.quarter3 == 0x2);
    try expect(divided.quarter4 == 0x1);

    const ordered: [2]u8 = @bitCast(full);
    switch (native_endian) {
        .big => {
            try expect(ordered[0] == 0x12);
            try expect(ordered[1] == 0x34);
        },
        .little => {
            try expect(ordered[0] == 0x34);
            try expect(ordered[1] == 0x12);
        },
    }
}
```

Shell

$ zig test test\_packed\_structs.zig
1/1 test\_packed\_structs.test.@bitCast between packed structs...OK
All 1 tests passed.

The backing integer can be inferred or explicitly provided. When inferred, it will be unsigned. When explicitly provided, its bit width will be enforced at compile time to exactly match the total bit width of the fields:

test\_missized\_packed\_struct.zig

```
test "missized packed struct" {
    const S = packed struct(u32) { a: u16, b: u8 };
    _ = S{ .a = 4, .b = 2 };
}
```

Shell

$ zig test test\_missized\_packed\_struct.zig
/home/andy/dev/zig/doc/langref/test\_missized\_packed\_struct.zig:2:29: error: backing integer type 'u32' has bit size 32 but the struct fields have a total bit size of 24
    const S = packed struct(u32) { a: u16, b: u8 };
                            ^~~
referenced by:
    test.missized packed struct: /home/andy/dev/zig/doc/langref/test\_missized\_packed\_struct.zig:2:22

Zig allows the address to be taken of a non-byte-aligned field:

test\_pointer\_to\_non-byte\_aligned\_field.zig

```
const std = @import("std");
const expect = std.testing.expect;

const BitField = packed struct {
    a: u3,
    b: u3,
    c: u2,
};

var foo = BitField{
    .a = 1,
    .b = 2,
    .c = 3,
};

test "pointer to non-byte-aligned field" {
    const ptr = &foo.b;
    try expect(ptr.* == 2);
}
```

Shell

$ zig test test\_pointer\_to\_non-byte\_aligned\_field.zig
1/1 test\_pointer\_to\_non-byte\_aligned\_field.test.pointer to non-byte-aligned field...OK
All 1 tests passed.

However, the pointer to a non-byte-aligned field has special properties and cannot be passed when a normal pointer is expected:

test\_misaligned\_pointer.zig

```
const std = @import("std");
const expect = std.testing.expect;

const BitField = packed struct {
    a: u3,
    b: u3,
    c: u2,
};

var bit_field = BitField{
    .a = 1,
    .b = 2,
    .c = 3,
};

test "pointer to non-byte-aligned field" {
    try expect(bar(&bit_field.b) == 2);
}

fn bar(x: *const u3) u3 {
    return x.*;
}
```

Shell

$ zig test test\_misaligned\_pointer.zig
/home/andy/dev/zig/doc/langref/test\_misaligned\_pointer.zig:17:20: error: expected type '\*const u3', found '\*align(1:3:1) u3'
    try expect(bar(&bit\_field.b) == 2);
                   ^~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_misaligned\_pointer.zig:17:20: note: pointer host size '1' cannot cast into pointer host size '0'
/home/andy/dev/zig/doc/langref/test\_misaligned\_pointer.zig:17:20: note: pointer bit offset '3' cannot cast into pointer bit offset '0'
/home/andy/dev/zig/doc/langref/test\_misaligned\_pointer.zig:20:11: note: parameter type declared here
fn bar(x: \*const u3) u3 {
          ^~~~~~~~~

In this case, the function `bar` cannot be called because the pointer to the non-ABI-aligned field mentions the bit offset, but the function expects an ABI-aligned pointer.

Pointers to non-ABI-aligned fields share the same address as the other fields within their host integer:

test\_packed\_struct\_field\_address.zig

```
const std = @import("std");
const expect = std.testing.expect;

const BitField = packed struct {
    a: u3,
    b: u3,
    c: u2,
};

var bit_field = BitField{
    .a = 1,
    .b = 2,
    .c = 3,
};

test "pointers of sub-byte-aligned fields share addresses" {
    try expect(@intFromPtr(&bit_field.a) == @intFromPtr(&bit_field.b));
    try expect(@intFromPtr(&bit_field.a) == @intFromPtr(&bit_field.c));
}
```

Shell

$ zig test test\_packed\_struct\_field\_address.zig
1/1 test\_packed\_struct\_field\_address.test.pointers of sub-byte-aligned fields share addresses...OK
All 1 tests passed.

This can be observed with [@bitOffsetOf](https://ziglang.org/documentation/0.15.2/#bitOffsetOf) and [offsetOf](https://ziglang.org/documentation/0.15.2/#offsetOf):

test\_bitOffsetOf\_offsetOf.zig

```
const std = @import("std");
const expect = std.testing.expect;

const BitField = packed struct {
    a: u3,
    b: u3,
    c: u2,
};

test "offsets of non-byte-aligned fields" {
    comptime {
        try expect(@bitOffsetOf(BitField, "a") == 0);
        try expect(@bitOffsetOf(BitField, "b") == 3);
        try expect(@bitOffsetOf(BitField, "c") == 6);

        try expect(@offsetOf(BitField, "a") == 0);
        try expect(@offsetOf(BitField, "b") == 0);
        try expect(@offsetOf(BitField, "c") == 0);
    }
}
```

Shell

$ zig test test\_bitOffsetOf\_offsetOf.zig
1/1 test\_bitOffsetOf\_offsetOf.test.offsets of non-byte-aligned fields...OK
All 1 tests passed.

Packed structs have the same alignment as their backing integer, however, overaligned pointers to packed structs can override this:

test\_overaligned\_packed\_struct.zig

```
const std = @import("std");
const expect = std.testing.expect;

const S = packed struct {
    a: u32,
    b: u32,
};
test "overaligned pointer to packed struct" {
    var foo: S align(4) = .{ .a = 1, .b = 2 };
    const ptr: *align(4) S = &foo;
    const ptr_to_b: *u32 = &ptr.b;
    try expect(ptr_to_b.* == 2);
}
```

Shell

$ zig test test\_overaligned\_packed\_struct.zig
1/1 test\_overaligned\_packed\_struct.test.overaligned pointer to packed struct...OK
All 1 tests passed.

It's also possible to set alignment of struct fields:

test\_aligned\_struct\_fields.zig

```
const std = @import("std");
const expectEqual = std.testing.expectEqual;

test "aligned struct fields" {
    const S = struct {
        a: u32 align(2),
        b: u32 align(64),
    };
    var foo = S{ .a = 1, .b = 2 };

    try expectEqual(64, @alignOf(S));
    try expectEqual(*align(2) u32, @TypeOf(&foo.a));
    try expectEqual(*align(64) u32, @TypeOf(&foo.b));
}
```

Shell

$ zig test test\_aligned\_struct\_fields.zig
1/1 test\_aligned\_struct\_fields.test.aligned struct fields...OK
All 1 tests passed.

Equating packed structs results in a comparison of the backing integer, and only works for the `==` and `!=` [Operators](https://ziglang.org/documentation/0.15.2/#Operators).

test\_packed\_struct\_equality.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "packed struct equality" {
    const S = packed struct {
        a: u4,
        b: u4,
    };
    const x: S = .{ .a = 1, .b = 2 };
    const y: S = .{ .b = 2, .a = 1 };
    try expect(x == y);
}
```

Shell

$ zig test test\_packed\_struct\_equality.zig
1/1 test\_packed\_struct\_equality.test.packed struct equality...OK
All 1 tests passed.

Field access and assignment can be understood as shorthand for bitshifts on the backing integer. These operations are not [atomic](https://ziglang.org/documentation/0.15.2/#Atomics), so beware using field access syntax when combined with memory-mapped input-output (MMIO). Instead of field access on [volatile](https://ziglang.org/documentation/0.15.2/#volatile) [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers), construct a fully-formed new value first, then write that value to the volatile pointer.

packed\_struct\_mmio.zig

```
pub const GpioRegister = packed struct(u8) {
    GPIO0: bool,
    GPIO1: bool,
    GPIO2: bool,
    GPIO3: bool,
    reserved: u4 = 0,
};

const gpio: *volatile GpioRegister = @ptrFromInt(0x0123);

pub fn writeToGpio(new_states: GpioRegister) void {
    // Example of what not to do:
    // BAD! gpio.GPIO0 = true; BAD!

    // Instead, do this:
    gpio.* = new_states;
}
```

### [Struct Naming](https://ziglang.org/documentation/0.15.2/#toc-Struct-Naming) [Â§](https://ziglang.org/documentation/0.15.2/#Struct-Naming)

Since all structs are anonymous, Zig infers the type name based on a few rules.

-   If the struct is in the initialization expression of a variable, it gets named after that variable.

-   If the struct is in the `return` expression, it gets named after the function it is returning from, with the parameter values serialized.
-   Otherwise, the struct gets a name such as `(filename.funcname__struct_ID)`.

-   If the struct is declared inside another struct, it gets named after both the parent struct and the name inferred by the previous rules, separated by a dot.

struct\_name.zig

```
const std = @import("std");

pub fn main() void {
    const Foo = struct {};
    std.debug.print("variable: {s}\n", .{@typeName(Foo)});
    std.debug.print("anonymous: {s}\n", .{@typeName(struct {})});
    std.debug.print("function: {s}\n", .{@typeName(List(i32))});
}

fn List(comptime T: type) type {
    return struct {
        x: T,
    };
}
```

Shell

$ zig build-exe struct\_name.zig
$ ./struct\_name
variable: struct\_name.main.Foo
anonymous: struct\_name.main\_\_struct\_22691
function: struct\_name.List(i32)

### [Anonymous Struct Literals](https://ziglang.org/documentation/0.15.2/#toc-Anonymous-Struct-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Anonymous-Struct-Literals)

Zig allows omitting the struct type of a literal. When the result is [coerced](https://ziglang.org/documentation/0.15.2/#Type-Coercion), the struct literal will directly instantiate the [result location](https://ziglang.org/documentation/0.15.2/#Result-Location-Semantics), with no copy:

test\_struct\_result.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Point = struct { x: i32, y: i32 };

test "anonymous struct literal" {
    const pt: Point = .{
        .x = 13,
        .y = 67,
    };
    try expect(pt.x == 13);
    try expect(pt.y == 67);
}
```

Shell

$ zig test test\_struct\_result.zig
1/1 test\_struct\_result.test.anonymous struct literal...OK
All 1 tests passed.

The struct type can be inferred. Here the [result location](https://ziglang.org/documentation/0.15.2/#Result-Location-Semantics) does not include a type, and so Zig infers the type:

test\_anonymous\_struct.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous struct" {
    try check(.{
        .int = @as(u32, 1234),
        .float = @as(f64, 12.34),
        .b = true,
        .s = "hi",
    });
}

fn check(args: anytype) !void {
    try expect(args.int == 1234);
    try expect(args.float == 12.34);
    try expect(args.b);
    try expect(args.s[0] == 'h');
    try expect(args.s[1] == 'i');
}
```

Shell

$ zig test test\_anonymous\_struct.zig
1/1 test\_anonymous\_struct.test.fully anonymous struct...OK
All 1 tests passed.

### [Tuples](https://ziglang.org/documentation/0.15.2/#toc-Tuples) [Â§](https://ziglang.org/documentation/0.15.2/#Tuples)

Anonymous structs can be created without specifying field names, and are referred to as "tuples". An empty tuple looks like `.{}` and can be seen in one of the [Hello World examples](https://ziglang.org/documentation/0.15.2/#Hello-World).

The fields are implicitly named using numbers starting from 0. Because their names are integers, they cannot be accessed with `.` syntax without also wrapping them in `@""`. Names inside `@""` are always recognised as [identifiers](https://ziglang.org/documentation/0.15.2/#Identifiers).

Like arrays, tuples have a .len field, can be indexed (provided the index is comptime-known) and work with the ++ and \*\* operators. They can also be iterated over with [inline for](https://ziglang.org/documentation/0.15.2/#inline-for).

test\_tuples.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "tuple" {
    const values = .{
        @as(u32, 1234),
        @as(f64, 12.34),
        true,
        "hi",
    } ++ .{false} ** 2;
    try expect(values[0] == 1234);
    try expect(values[4] == false);
    inline for (values, 0..) |v, i| {
        if (i != 2) continue;
        try expect(v);
    }
    try expect(values.len == 6);
    try expect(values.@"3"[0] == 'h');
}
```

Shell

$ zig test test\_tuples.zig
1/1 test\_tuples.test.tuple...OK
All 1 tests passed.

#### [Destructuring Tuples](https://ziglang.org/documentation/0.15.2/#toc-Destructuring-Tuples) [Â§](https://ziglang.org/documentation/0.15.2/#Destructuring-Tuples)

Tuples can be [destructured](https://ziglang.org/documentation/0.15.2/#Destructuring).

Tuple destructuring is helpful for returning multiple values from a block:

destructuring\_block.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    const digits = [_]i8 { 3, 8, 9, 0, 7, 4, 1 };

    const min, const max = blk: {
        var min: i8 = 127;
        var max: i8 = -128;

        for (digits) |digit| {
            if (digit < min) min = digit;
            if (digit > max) max = digit;
        }

        break :blk .{ min, max };
    };

    print("min = {}\n", .{ min });
    print("max = {}\n", .{ max });
}
```

Shell

$ zig build-exe destructuring\_block.zig
$ ./destructuring\_block
min = 0
max = 9

Tuple destructuring is helpful for dealing with functions and built-ins that return multiple values as a tuple:

destructuring\_return\_value.zig

```
const print = @import("std").debug.print;

fn divmod(numerator: u32, denominator: u32) struct { u32, u32 } {
    return .{ numerator / denominator, numerator % denominator };
}

pub fn main() void {
    const div, const mod = divmod(10, 3);

    print("10 / 3 = {}\n", .{div});
    print("10 % 3 = {}\n", .{mod});
}
```

Shell

$ zig build-exe destructuring\_return\_value.zig
$ ./destructuring\_return\_value
10 / 3 = 3
10 % 3 = 1

See also:

-   [Destructuring](https://ziglang.org/documentation/0.15.2/#Destructuring)

-   [Destructuring Arrays](https://ziglang.org/documentation/0.15.2/#Destructuring-Arrays)
-   [Destructuring Vectors](https://ziglang.org/documentation/0.15.2/#Destructuring-Vectors)

See also:

-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)

-   [@fieldParentPtr](https://ziglang.org/documentation/0.15.2/#fieldParentPtr)

## [enum](https://ziglang.org/documentation/0.15.2/#toc-enum) [Â§](https://ziglang.org/documentation/0.15.2/#enum)

test\_enums.zig

```
const expect = @import("std").testing.expect;
const mem = @import("std").mem;

// Declare an enum.
const Type = enum {
    ok,
    not_ok,
};

// Declare a specific enum field.
const c = Type.ok;

// If you want access to the ordinal value of an enum, you
// can specify the tag type.
const Value = enum(u2) {
    zero,
    one,
    two,
};
// Now you can cast between u2 and Value.
// The ordinal value starts from 0, counting up by 1 from the previous member.
test "enum ordinal value" {
    try expect(@intFromEnum(Value.zero) == 0);
    try expect(@intFromEnum(Value.one) == 1);
    try expect(@intFromEnum(Value.two) == 2);
}

// You can override the ordinal value for an enum.
const Value2 = enum(u32) {
    hundred = 100,
    thousand = 1000,
    million = 1000000,
};
test "set enum ordinal value" {
    try expect(@intFromEnum(Value2.hundred) == 100);
    try expect(@intFromEnum(Value2.thousand) == 1000);
    try expect(@intFromEnum(Value2.million) == 1000000);
}

// You can also override only some values.
const Value3 = enum(u4) {
    a,
    b = 8,
    c,
    d = 4,
    e,
};
test "enum implicit ordinal values and overridden values" {
    try expect(@intFromEnum(Value3.a) == 0);
    try expect(@intFromEnum(Value3.b) == 8);
    try expect(@intFromEnum(Value3.c) == 9);
    try expect(@intFromEnum(Value3.d) == 4);
    try expect(@intFromEnum(Value3.e) == 5);
}

// Enums can have methods, the same as structs and unions.
// Enum methods are not special, they are only namespaced
// functions that you can call with dot syntax.
const Suit = enum {
    clubs,
    spades,
    diamonds,
    hearts,

    pub fn isClubs(self: Suit) bool {
        return self == Suit.clubs;
    }
};
test "enum method" {
    const p = Suit.spades;
    try expect(!p.isClubs());
}

// An enum can be switched upon.
const Foo = enum {
    string,
    number,
    none,
};
test "enum switch" {
    const p = Foo.number;
    const what_is_it = switch (p) {
        Foo.string => "this is a string",
        Foo.number => "this is a number",
        Foo.none => "this is a none",
    };
    try expect(mem.eql(u8, what_is_it, "this is a number"));
}

// @typeInfo can be used to access the integer tag type of an enum.
const Small = enum {
    one,
    two,
    three,
    four,
};
test "std.meta.Tag" {
    try expect(@typeInfo(Small).@"enum".tag_type == u2);
}

// @typeInfo tells us the field count and the fields names:
test "@typeInfo" {
    try expect(@typeInfo(Small).@"enum".fields.len == 4);
    try expect(mem.eql(u8, @typeInfo(Small).@"enum".fields[1].name, "two"));
}

// @tagName gives a [:0]const u8 representation of an enum value:
test "@tagName" {
    try expect(mem.eql(u8, @tagName(Small.three), "three"));
}
```

Shell

$ zig test test\_enums.zig
1/8 test\_enums.test.enum ordinal value...OK
2/8 test\_enums.test.set enum ordinal value...OK
3/8 test\_enums.test.enum implicit ordinal values and overridden values...OK
4/8 test\_enums.test.enum method...OK
5/8 test\_enums.test.enum switch...OK
6/8 test\_enums.test.std.meta.Tag...OK
7/8 test\_enums.test.@typeInfo...OK
8/8 test\_enums.test.@tagName...OK
All 8 tests passed.

See also:

-   [@typeInfo](https://ziglang.org/documentation/0.15.2/#typeInfo)

-   [@tagName](https://ziglang.org/documentation/0.15.2/#tagName)
-   [@sizeOf](https://ziglang.org/documentation/0.15.2/#sizeOf)

### [extern enum](https://ziglang.org/documentation/0.15.2/#toc-extern-enum) [Â§](https://ziglang.org/documentation/0.15.2/#extern-enum)

By default, enums are not guaranteed to be compatible with the C ABI:

enum\_export\_error.zig

```
const Foo = enum { a, b, c };
export fn entry(foo: Foo) void {
    _ = foo;
}
```

Shell

$ zig build-obj enum\_export\_error.zig -target x86\_64-linux
/home/andy/dev/zig/doc/langref/enum\_export\_error.zig:2:17: error: parameter of type 'enum\_export\_error.Foo' not allowed in function with calling convention 'x86\_64\_sysv'
export fn entry(foo: Foo) void {
                ^~~~~~~~
/home/andy/dev/zig/doc/langref/enum\_export\_error.zig:2:17: note: enum tag type 'u2' is not extern compatible
/home/andy/dev/zig/doc/langref/enum\_export\_error.zig:2:17: note: only integers with 0, 8, 16, 32, 64 and 128 bits are extern compatible
/home/andy/dev/zig/doc/langref/enum\_export\_error.zig:1:13: note: enum declared here
const Foo = enum { a, b, c };
            ^~~~~~~~~~~~~~~~
referenced by:
    root: /home/andy/dev/zig/lib/std/start.zig:3:22
    comptime: /home/andy/dev/zig/lib/std/start.zig:31:9
    2 reference(s) hidden; use '-freference-trace=4' to see all references

For a C-ABI-compatible enum, provide an explicit tag type to the enum:

enum\_export.zig

```
const Foo = enum(c_int) { a, b, c };
export fn entry(foo: Foo) void {
    _ = foo;
}
```

Shell

$ zig build-obj enum\_export.zig

### [Enum Literals](https://ziglang.org/documentation/0.15.2/#toc-Enum-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Enum-Literals)

Enum literals allow specifying the name of an enum field without specifying the enum type:

test\_enum\_literals.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Color = enum {
    auto,
    off,
    on,
};

test "enum literals" {
    const color1: Color = .auto;
    const color2 = Color.auto;
    try expect(color1 == color2);
}

test "switch using enum literals" {
    const color = Color.on;
    const result = switch (color) {
        .auto => false,
        .on => true,
        .off => false,
    };
    try expect(result);
}
```

Shell

$ zig test test\_enum\_literals.zig
1/2 test\_enum\_literals.test.enum literals...OK
2/2 test\_enum\_literals.test.switch using enum literals...OK
All 2 tests passed.

### [Non-exhaustive enum](https://ziglang.org/documentation/0.15.2/#toc-Non-exhaustive-enum) [Â§](https://ziglang.org/documentation/0.15.2/#Non-exhaustive-enum)

A non-exhaustive enum can be created by adding a trailing `_` field. The enum must specify a tag type and cannot consume every enumeration value.

[@enumFromInt](https://ziglang.org/documentation/0.15.2/#enumFromInt) on a non-exhaustive enum involves the safety semantics of [@intCast](https://ziglang.org/documentation/0.15.2/#intCast) to the integer tag type, but beyond that always results in a well-defined enum value.

A switch on a non-exhaustive enum can include a `_` prong as an alternative to an `else` prong. With a `_` prong the compiler errors if all the known tag names are not handled by the switch.

test\_switch\_non-exhaustive.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Number = enum(u8) {
    one,
    two,
    three,
    _,
};

test "switch on non-exhaustive enum" {
    const number = Number.one;
    const result = switch (number) {
        .one => true,
        .two, .three => false,
        _ => false,
    };
    try expect(result);
    const is_one = switch (number) {
        .one => true,
        else => false,
    };
    try expect(is_one);
}
```

Shell

$ zig test test\_switch\_non-exhaustive.zig
1/1 test\_switch\_non-exhaustive.test.switch on non-exhaustive enum...OK
All 1 tests passed.

## [union](https://ziglang.org/documentation/0.15.2/#toc-union) [Â§](https://ziglang.org/documentation/0.15.2/#union)

A bare `union` defines a set of possible types that a value can be as a list of fields. Only one field can be active at a time. The in-memory representation of bare unions is not guaranteed. Bare unions cannot be used to reinterpret memory. For that, use [@ptrCast](https://ziglang.org/documentation/0.15.2/#ptrCast), or use an [extern union](https://ziglang.org/documentation/0.15.2/#extern-union) or a [packed union](https://ziglang.org/documentation/0.15.2/#packed-union) which have guaranteed in-memory layout. [Accessing the non-active field](https://ziglang.org/documentation/0.15.2/#Wrong-Union-Field-Access) is safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior):

test\_wrong\_union\_access.zig

```
const Payload = union {
    int: i64,
    float: f64,
    boolean: bool,
};
test "simple union" {
    var payload = Payload{ .int = 1234 };
    payload.float = 12.34;
}
```

Shell

$ zig test test\_wrong\_union\_access.zig
1/1 test\_wrong\_union\_access.test.simple union...thread 2895385 panic: access of union field 'float' while field 'int' is active
/home/andy/dev/zig/doc/langref/test\_wrong\_union\_access.zig:8:12: 0x102c083 in test.simple union (test\_wrong\_union\_access.zig)
    payload.float = 12.34;
           ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x115cdb0 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1155fd1 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x114fd6d in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x114f601 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/ceece336399a577bb1b9c6460feb4406/test --seed=0xa290ca33

You can activate another field by assigning the entire union:

test\_simple\_union.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Payload = union {
    int: i64,
    float: f64,
    boolean: bool,
};
test "simple union" {
    var payload = Payload{ .int = 1234 };
    try expect(payload.int == 1234);
    payload = Payload{ .float = 12.34 };
    try expect(payload.float == 12.34);
}
```

Shell

$ zig test test\_simple\_union.zig
1/1 test\_simple\_union.test.simple union...OK
All 1 tests passed.

In order to use [switch](https://ziglang.org/documentation/0.15.2/#switch) with a union, it must be a [Tagged union](https://ziglang.org/documentation/0.15.2/#Tagged-union).

To initialize a union when the tag is a [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known name, see [@unionInit](https://ziglang.org/documentation/0.15.2/#unionInit).

### [Tagged union](https://ziglang.org/documentation/0.15.2/#toc-Tagged-union) [Â§](https://ziglang.org/documentation/0.15.2/#Tagged-union)

Unions can be declared with an enum tag type. This turns the union into a *tagged* union, which makes it eligible to use with [switch](https://ziglang.org/documentation/0.15.2/#switch) expressions. Tagged unions coerce to their tag type: [Type Coercion: Unions and Enums](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Unions-and-Enums).

test\_tagged\_union.zig

```
const std = @import("std");
const expect = std.testing.expect;

const ComplexTypeTag = enum {
    ok,
    not_ok,
};
const ComplexType = union(ComplexTypeTag) {
    ok: u8,
    not_ok: void,
};

test "switch on tagged union" {
    const c = ComplexType{ .ok = 42 };
    try expect(@as(ComplexTypeTag, c) == ComplexTypeTag.ok);

    switch (c) {
        .ok => |value| try expect(value == 42),
        .not_ok => unreachable,
    }
}

test "get tag type" {
    try expect(std.meta.Tag(ComplexType) == ComplexTypeTag);
}
```

Shell

$ zig test test\_tagged\_union.zig
1/2 test\_tagged\_union.test.switch on tagged union...OK
2/2 test\_tagged\_union.test.get tag type...OK
All 2 tests passed.

In order to modify the payload of a tagged union in a switch expression, place a `*` before the variable name to make it a pointer:

test\_switch\_modify\_tagged\_union.zig

```
const std = @import("std");
const expect = std.testing.expect;

const ComplexTypeTag = enum {
    ok,
    not_ok,
};
const ComplexType = union(ComplexTypeTag) {
    ok: u8,
    not_ok: void,
};

test "modify tagged union in switch" {
    var c = ComplexType{ .ok = 42 };

    switch (c) {
        ComplexTypeTag.ok => |*value| value.* += 1,
        ComplexTypeTag.not_ok => unreachable,
    }

    try expect(c.ok == 43);
}
```

Shell

$ zig test test\_switch\_modify\_tagged\_union.zig
1/1 test\_switch\_modify\_tagged\_union.test.modify tagged union in switch...OK
All 1 tests passed.

Unions can be made to infer the enum tag type. Further, unions can have methods just like structs and enums.

test\_union\_method.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Variant = union(enum) {
    int: i32,
    boolean: bool,

    // void can be omitted when inferring enum tag type.
    none,

    fn truthy(self: Variant) bool {
        return switch (self) {
            Variant.int => |x_int| x_int != 0,
            Variant.boolean => |x_bool| x_bool,
            Variant.none => false,
        };
    }
};

test "union method" {
    var v1: Variant = .{ .int = 1 };
    var v2: Variant = .{ .boolean = false };
    var v3: Variant = .none;

    try expect(v1.truthy());
    try expect(!v2.truthy());
    try expect(!v3.truthy());
}
```

Shell

$ zig test test\_union\_method.zig
1/1 test\_union\_method.test.union method...OK
All 1 tests passed.

Unions with inferred enum tag types can also assign ordinal values to their inferred tag. This requires the tag to specify an explicit integer type. [@intFromEnum](https://ziglang.org/documentation/0.15.2/#intFromEnum) can be used to access the ordinal value corresponding to the active field.

test\_tagged\_union\_with\_tag\_values.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Tagged = union(enum(u32)) {
    int: i64 = 123,
    boolean: bool = 67,
};

test "tag values" {
    const int: Tagged = .{ .int = -40 };
    try expect(@intFromEnum(int) == 123);

    const boolean: Tagged = .{ .boolean = false };
    try expect(@intFromEnum(boolean) == 67);
}
```

Shell

$ zig test test\_tagged\_union\_with\_tag\_values.zig
1/1 test\_tagged\_union\_with\_tag\_values.test.tag values...OK
All 1 tests passed.

[@tagName](https://ziglang.org/documentation/0.15.2/#tagName) can be used to return a [comptime](https://ziglang.org/documentation/0.15.2/#comptime) `[:0]const u8` value representing the field name:

test\_tagName.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Small2 = union(enum) {
    a: i32,
    b: bool,
    c: u8,
};
test "@tagName" {
    try expect(std.mem.eql(u8, @tagName(Small2.a), "a"));
}
```

Shell

$ zig test test\_tagName.zig
1/1 test\_tagName.test.@tagName...OK
All 1 tests passed.

### [extern union](https://ziglang.org/documentation/0.15.2/#toc-extern-union) [Â§](https://ziglang.org/documentation/0.15.2/#extern-union)

An `extern union` has memory layout guaranteed to be compatible with the target C ABI.

See also:

-   [extern struct](https://ziglang.org/documentation/0.15.2/#extern-struct)

### [packed union](https://ziglang.org/documentation/0.15.2/#toc-packed-union) [Â§](https://ziglang.org/documentation/0.15.2/#packed-union)

A `packed union` has well-defined in-memory layout and is eligible to be in a [packed struct](https://ziglang.org/documentation/0.15.2/#packed-struct).

### [Anonymous Union Literals](https://ziglang.org/documentation/0.15.2/#toc-Anonymous-Union-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Anonymous-Union-Literals)

[Anonymous Struct Literals](https://ziglang.org/documentation/0.15.2/#Anonymous-Struct-Literals) syntax can be used to initialize unions without specifying the type:

test\_anonymous\_union.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Number = union {
    int: i32,
    float: f64,
};

test "anonymous union literal syntax" {
    const i: Number = .{ .int = 42 };
    const f = makeNumber();
    try expect(i.int == 42);
    try expect(f.float == 12.34);
}

fn makeNumber() Number {
    return .{ .float = 12.34 };
}
```

Shell

$ zig test test\_anonymous\_union.zig
1/1 test\_anonymous\_union.test.anonymous union literal syntax...OK
All 1 tests passed.

## [opaque](https://ziglang.org/documentation/0.15.2/#toc-opaque) [Â§](https://ziglang.org/documentation/0.15.2/#opaque)

`opaque {}` declares a new type with an unknown (but non-zero) size and alignment. It can contain declarations the same as [structs](https://ziglang.org/documentation/0.15.2/#struct), [unions](https://ziglang.org/documentation/0.15.2/#union), and [enums](https://ziglang.org/documentation/0.15.2/#enum).

This is typically used for type safety when interacting with C code that does not expose struct details. Example:

test\_opaque.zig

```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.c) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

Shell

$ zig test test\_opaque.zig
/home/andy/dev/zig/doc/langref/test\_opaque.zig:6:9: error: expected type '\*test\_opaque.Derp', found '\*test\_opaque.Wat'
    bar(w);
        ^
/home/andy/dev/zig/doc/langref/test\_opaque.zig:6:9: note: pointer type child 'test\_opaque.Wat' cannot cast into pointer type child 'test\_opaque.Derp'
/home/andy/dev/zig/doc/langref/test\_opaque.zig:2:13: note: opaque declared here
const Wat = opaque {};
            ^~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_opaque.zig:1:14: note: opaque declared here
const Derp = opaque {};
             ^~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_opaque.zig:4:18: note: parameter type declared here
extern fn bar(d: \*Derp) void;
                 ^~~~~
referenced by:
    test.call foo: /home/andy/dev/zig/doc/langref/test\_opaque.zig:10:8

## [Blocks](https://ziglang.org/documentation/0.15.2/#toc-Blocks) [Â§](https://ziglang.org/documentation/0.15.2/#Blocks)

Blocks are used to limit the scope of variable declarations:

test\_blocks.zig

```
test "access variable after block scope" {
    {
        var x: i32 = 1;
        _ = &x;
    }
    x += 1;
}
```

Shell

$ zig test test\_blocks.zig
/home/andy/dev/zig/doc/langref/test\_blocks.zig:6:5: error: use of undeclared identifier 'x'
    x += 1;
    ^

Blocks are expressions. When labeled, `break` can be used to return a value from the block:

test\_labeled\_break.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "labeled break from labeled block expression" {
    var y: i32 = 123;

    const x = blk: {
        y += 1;
        break :blk y;
    };
    try expect(x == 124);
    try expect(y == 124);
}
```

Shell

$ zig test test\_labeled\_break.zig
1/1 test\_labeled\_break.test.labeled break from labeled block expression...OK
All 1 tests passed.

Here, `blk` can be any name.

See also:

-   [Labeled while](https://ziglang.org/documentation/0.15.2/#Labeled-while)

-   [Labeled for](https://ziglang.org/documentation/0.15.2/#Labeled-for)

### [Shadowing](https://ziglang.org/documentation/0.15.2/#toc-Shadowing) [Â§](https://ziglang.org/documentation/0.15.2/#Shadowing)

[Identifiers](https://ziglang.org/documentation/0.15.2/#Identifiers) are never allowed to "hide" other identifiers by using the same name:

test\_shadowing.zig

```
const pi = 3.14;

test "inside test block" {
    // Let's even go inside another block
    {
        var pi: i32 = 1234;
    }
}
```

Shell

$ zig test test\_shadowing.zig
/home/andy/dev/zig/doc/langref/test\_shadowing.zig:6:13: error: local variable shadows declaration of 'pi'
        var pi: i32 = 1234;
            ^~
/home/andy/dev/zig/doc/langref/test\_shadowing.zig:1:1: note: declared here
const pi = 3.14;
^~~~~~~~~~~~~~~

Because of this, when you read Zig code you can always rely on an identifier to consistently mean the same thing within the scope it is defined. Note that you can, however, use the same name if the scopes are separate:

test\_scopes.zig

```
test "separate scopes" {
    {
        const pi = 3.14;
        _ = pi;
    }
    {
        var pi: bool = true;
        _ = &pi;
    }
}
```

Shell

$ zig test test\_scopes.zig
1/1 test\_scopes.test.separate scopes...OK
All 1 tests passed.

### [Empty Blocks](https://ziglang.org/documentation/0.15.2/#toc-Empty-Blocks) [Â§](https://ziglang.org/documentation/0.15.2/#Empty-Blocks)

An empty block is equivalent to `void{}`:

test\_empty\_block.zig

```
const std = @import("std");
const expect = std.testing.expect;

test {
    const a = {};
    const b = void{};
    try expect(@TypeOf(a) == void);
    try expect(@TypeOf(b) == void);
    try expect(a == b);
}
```

Shell

$ zig test test\_empty\_block.zig
1/1 test\_empty\_block.test\_0...OK
All 1 tests passed.

## [switch](https://ziglang.org/documentation/0.15.2/#toc-switch) [Â§](https://ziglang.org/documentation/0.15.2/#switch)

test\_switch.zig

```
const std = @import("std");
const builtin = @import("builtin");
const expect = std.testing.expect;

test "switch simple" {
    const a: u64 = 10;
    const zz: u64 = 103;

    // All branches of a switch expression must be able to be coerced to a
    // common type.
    //
    // Branches cannot fallthrough. If fallthrough behavior is desired, combine
    // the cases and use an if.
    const b = switch (a) {
        // Multiple cases can be combined via a ','
        1, 2, 3 => 0,

        // Ranges can be specified using the ... syntax. These are inclusive
        // of both ends.
        5...100 => 1,

        // Branches can be arbitrarily complex.
        101 => blk: {
            const c: u64 = 5;
            break :blk c * 2 + 1;
        },

        // Switching on arbitrary expressions is allowed as long as the
        // expression is known at compile-time.
        zz => zz,
        blk: {
            const d: u32 = 5;
            const e: u32 = 100;
            break :blk d + e;
        } => 107,

        // The else branch catches everything not already captured.
        // Else branches are mandatory unless the entire range of values
        // is handled.
        else => 9,
    };

    try expect(b == 1);
}

// Switch expressions can be used outside a function:
const os_msg = switch (builtin.target.os.tag) {
    .linux => "we found a linux user",
    else => "not a linux user",
};

// Inside a function, switch statements implicitly are compile-time
// evaluated if the target expression is compile-time known.
test "switch inside function" {
    switch (builtin.target.os.tag) {
        .fuchsia => {
            // On an OS other than fuchsia, block is not even analyzed,
            // so this compile error is not triggered.
            // On fuchsia this compile error would be triggered.
            @compileError("fuchsia not supported");
        },
        else => {},
    }
}
```

Shell

$ zig test test\_switch.zig
1/2 test\_switch.test.switch simple...OK
2/2 test\_switch.test.switch inside function...OK
All 2 tests passed.

`switch` can be used to capture the field values of a [Tagged union](https://ziglang.org/documentation/0.15.2/#Tagged-union). Modifications to the field values can be done by placing a `*` before the capture variable name, turning it into a pointer.

test\_switch\_tagged\_union.zig

```
const expect = @import("std").testing.expect;

test "switch on tagged union" {
    const Point = struct {
        x: u8,
        y: u8,
    };
    const Item = union(enum) {
        a: u32,
        c: Point,
        d,
        e: u32,
    };

    var a = Item{ .c = Point{ .x = 1, .y = 2 } };

    // Switching on more complex enums is allowed.
    const b = switch (a) {
        // A capture group is allowed on a match, and will return the enum
        // value matched. If the payload types of both cases are the same
        // they can be put into the same switch prong.
        Item.a, Item.e => |item| item,

        // A reference to the matched value can be obtained using `*` syntax.
        Item.c => |*item| blk: {
            item.*.x += 1;
            break :blk 6;
        },

        // No else is required if the types cases was exhaustively handled
        Item.d => 8,
    };

    try expect(b == 6);
    try expect(a.c.x == 2);
}
```

Shell

$ zig test test\_switch\_tagged\_union.zig
1/1 test\_switch\_tagged\_union.test.switch on tagged union...OK
All 1 tests passed.

See also:

-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)

-   [enum](https://ziglang.org/documentation/0.15.2/#enum)
-   [@compileError](https://ziglang.org/documentation/0.15.2/#compileError)

-   [Compile Variables](https://ziglang.org/documentation/0.15.2/#Compile-Variables)

### [Exhaustive Switching](https://ziglang.org/documentation/0.15.2/#toc-Exhaustive-Switching) [Â§](https://ziglang.org/documentation/0.15.2/#Exhaustive-Switching)

When a `switch` expression does not have an `else` clause, it must exhaustively list all the possible values. Failure to do so is a compile error:

test\_unhandled\_enumeration\_value.zig

```
const Color = enum {
    auto,
    off,
    on,
};

test "exhaustive switching" {
    const color = Color.off;
    switch (color) {
        Color.auto => {},
        Color.on => {},
    }
}
```

Shell

$ zig test test\_unhandled\_enumeration\_value.zig
/home/andy/dev/zig/doc/langref/test\_unhandled\_enumeration\_value.zig:9:5: error: switch must handle all possibilities
    switch (color) {
    ^~~~~~
/home/andy/dev/zig/doc/langref/test\_unhandled\_enumeration\_value.zig:3:5: note: unhandled enumeration value: 'off'
    off,
    ^~~
/home/andy/dev/zig/doc/langref/test\_unhandled\_enumeration\_value.zig:1:15: note: enum 'test\_unhandled\_enumeration\_value.Color' declared here
const Color = enum {
              ^~~~

### [Switching with Enum Literals](https://ziglang.org/documentation/0.15.2/#toc-Switching-with-Enum-Literals) [Â§](https://ziglang.org/documentation/0.15.2/#Switching-with-Enum-Literals)

[Enum Literals](https://ziglang.org/documentation/0.15.2/#Enum-Literals) can be useful to use with `switch` to avoid repetitively specifying [enum](https://ziglang.org/documentation/0.15.2/#enum) or [union](https://ziglang.org/documentation/0.15.2/#union) types:

test\_exhaustive\_switch.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Color = enum {
    auto,
    off,
    on,
};

test "enum literals with switch" {
    const color = Color.off;
    const result = switch (color) {
        .auto => false,
        .on => false,
        .off => true,
    };
    try expect(result);
}
```

Shell

$ zig test test\_exhaustive\_switch.zig
1/1 test\_exhaustive\_switch.test.enum literals with switch...OK
All 1 tests passed.

### [Labeled switch](https://ziglang.org/documentation/0.15.2/#toc-Labeled-switch) [Â§](https://ziglang.org/documentation/0.15.2/#Labeled-switch)

When a switch statement is labeled, it can be referenced from a `break` or `continue`. `break` will return a value from the `switch`.

A `continue` targeting a switch must have an operand. When executed, it will jump to the matching prong, as if the `switch` were executed again with the `continue`'s operand replacing the initial switch value.

test\_switch\_continue.zig

```
const std = @import("std");

test "switch continue" {
    sw: switch (@as(i32, 5)) {
        5 => continue :sw 4,

        // `continue` can occur multiple times within a single switch prong.
        2...4 => |v| {
            if (v > 3) {
                continue :sw 2;
            } else if (v == 3) {

                // `break` can target labeled loops.
                break :sw;
            }

            continue :sw 1;
        },

        1 => return,

        else => unreachable,
    }
}
```

Shell

$ zig test test\_switch\_continue.zig
1/1 test\_switch\_continue.test.switch continue...OK
All 1 tests passed.

Semantically, this is equivalent to the following loop:

test\_switch\_continue\_equivalent.zig

```
const std = @import("std");

test "switch continue, equivalent loop" {
    var sw: i32 = 5;
    while (true) {
        switch (sw) {
            5 => {
                sw = 4;
                continue;
            },
            2...4 => |v| {
                if (v > 3) {
                    sw = 2;
                    continue;
                } else if (v == 3) {
                    break;
                }

                sw = 1;
                continue;
            },
            1 => return,
            else => unreachable,
        }
    }
}
```

Shell

$ zig test test\_switch\_continue\_equivalent.zig
1/1 test\_switch\_continue\_equivalent.test.switch continue, equivalent loop...OK
All 1 tests passed.

This can improve clarity of (for example) state machines, where the syntax `continue :sw .next_state` is unambiguous, explicit, and immediately understandable.

However, the motivating example is a switch on each element of an array, where using a single switch can improve clarity and performance:

test\_switch\_dispatch\_loop.zig

```
const std = @import("std");
const expectEqual = std.testing.expectEqual;

const Instruction = enum {
    add,
    mul,
    end,
};

fn evaluate(initial_stack: []const i32, code: []const Instruction) !i32 {
    var buffer: [8]i32 = undefined;
    var stack = std.ArrayListUnmanaged(i32).initBuffer(&buffer);
    try stack.appendSliceBounded(initial_stack);
    var ip: usize = 0;

    return vm: switch (code[ip]) {
        // Because all code after `continue` is unreachable, this branch does
        // not provide a result.
        .add => {
            try stack.appendBounded(stack.pop().? + stack.pop().?);

            ip += 1;
            continue :vm code[ip];
        },
        .mul => {
            try stack.appendBounded(stack.pop().? * stack.pop().?);

            ip += 1;
            continue :vm code[ip];
        },
        .end => stack.pop().?,
    };
}

test "evaluate" {
    const result = try evaluate(&.{ 7, 2, -3 }, &.{ .mul, .add, .end });
    try expectEqual(1, result);
}
```

Shell

$ zig test test\_switch\_dispatch\_loop.zig
1/1 test\_switch\_dispatch\_loop.test.evaluate...OK
All 1 tests passed.

If the operand to `continue` is [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known, then it can be lowered to an unconditional branch to the relevant case. Such a branch is perfectly predicted, and hence typically very fast to execute.

If the operand is runtime-known, each `continue` can embed a conditional branch inline (ideally through a jump table), which allows a CPU to predict its target independently of any other prong. A loop-based lowering would force every branch through the same dispatch point, hindering branch prediction.

### [Inline Switch Prongs](https://ziglang.org/documentation/0.15.2/#toc-Inline-Switch-Prongs) [Â§](https://ziglang.org/documentation/0.15.2/#Inline-Switch-Prongs)

Switch prongs can be marked as `inline` to generate the prong's body for each possible value it could have, making the captured value [comptime](https://ziglang.org/documentation/0.15.2/#comptime).

test\_inline\_switch.zig

```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T).@"struct".fields;
    return switch (field_index) {
        // This prong is analyzed twice with `idx` being a
        // comptime-known value each time.
        inline 0, 1 => |idx| @typeInfo(fields[idx].type) == .optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function:
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

Shell

$ zig test test\_inline\_switch.zig
1/1 test\_inline\_switch.test.using @typeInfo with runtime values...OK
All 1 tests passed.

The `inline` keyword may also be combined with ranges:

inline\_prong\_range.zig

```
fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T).@"struct".fields;
    return switch (field_index) {
        inline 0...fields.len - 1 => |idx| @typeInfo(fields[idx].type) == .optional,
        else => return error.IndexOutOfBounds,
    };
}
```

`inline else` prongs can be used as a type safe alternative to `inline for` loops:

test\_inline\_else.zig

```
const std = @import("std");
const expect = std.testing.expect;

const SliceTypeA = extern struct {
    len: usize,
    ptr: [*]u32,
};
const SliceTypeB = extern struct {
    ptr: [*]SliceTypeA,
    len: usize,
};
const AnySlice = union(enum) {
    a: SliceTypeA,
    b: SliceTypeB,
    c: []const u8,
    d: []AnySlice,
};

fn withFor(any: AnySlice) usize {
    const Tag = @typeInfo(AnySlice).@"union".tag_type.?;
    inline for (@typeInfo(Tag).@"enum".fields) |field| {
        // With `inline for` the function gets generated as
        // a series of `if` statements relying on the optimizer
        // to convert it to a switch.
        if (field.value == @intFromEnum(any)) {
            return @field(any, field.name).len;
        }
    }
    // When using `inline for` the compiler doesn't know that every
    // possible case has been handled requiring an explicit `unreachable`.
    unreachable;
}

fn withSwitch(any: AnySlice) usize {
    return switch (any) {
        // With `inline else` the function is explicitly generated
        // as the desired switch and the compiler can check that
        // every possible case is handled.
        inline else => |slice| slice.len,
    };
}

test "inline for and inline else similarity" {
    const any = AnySlice{ .c = "hello" };
    try expect(withFor(any) == 5);
    try expect(withSwitch(any) == 5);
}
```

Shell

$ zig test test\_inline\_else.zig
1/1 test\_inline\_else.test.inline for and inline else similarity...OK
All 1 tests passed.

When using an inline prong switching on an union an additional capture can be used to obtain the union's enum tag value.

test\_inline\_switch\_union\_tag.zig

```
const std = @import("std");
const expect = std.testing.expect;

const U = union(enum) {
    a: u32,
    b: f32,
};

fn getNum(u: U) u32 {
    switch (u) {
        // Here `num` is a runtime-known value that is either
        // `u.a` or `u.b` and `tag` is `u`'s comptime-known tag value.
        inline else => |num, tag| {
            if (tag == .b) {
                return @intFromFloat(num);
            }
            return num;
        },
    }
}

test "test" {
    const u = U{ .b = 42 };
    try expect(getNum(u) == 42);
}
```

Shell

$ zig test test\_inline\_switch\_union\_tag.zig
1/1 test\_inline\_switch\_union\_tag.test.test...OK
All 1 tests passed.

See also:

-   [inline while](https://ziglang.org/documentation/0.15.2/#inline-while)

-   [inline for](https://ziglang.org/documentation/0.15.2/#inline-for)

## [while](https://ziglang.org/documentation/0.15.2/#toc-while) [Â§](https://ziglang.org/documentation/0.15.2/#while)

A while loop is used to repeatedly execute an expression until some condition is no longer true.

test\_while.zig

```
const expect = @import("std").testing.expect;

test "while basic" {
    var i: usize = 0;
    while (i < 10) {
        i += 1;
    }
    try expect(i == 10);
}
```

Shell

$ zig test test\_while.zig
1/1 test\_while.test.while basic...OK
All 1 tests passed.

Use `break` to exit a while loop early.

test\_while\_break.zig

```
const expect = @import("std").testing.expect;

test "while break" {
    var i: usize = 0;
    while (true) {
        if (i == 10)
            break;
        i += 1;
    }
    try expect(i == 10);
}
```

Shell

$ zig test test\_while\_break.zig
1/1 test\_while\_break.test.while break...OK
All 1 tests passed.

Use `continue` to jump back to the beginning of the loop.

test\_while\_continue.zig

```
const expect = @import("std").testing.expect;

test "while continue" {
    var i: usize = 0;
    while (true) {
        i += 1;
        if (i < 10)
            continue;
        break;
    }
    try expect(i == 10);
}
```

Shell

$ zig test test\_while\_continue.zig
1/1 test\_while\_continue.test.while continue...OK
All 1 tests passed.

While loops support a continue expression which is executed when the loop is continued. The `continue` keyword respects this expression.

test\_while\_continue\_expression.zig

```
const expect = @import("std").testing.expect;

test "while loop continue expression" {
    var i: usize = 0;
    while (i < 10) : (i += 1) {}
    try expect(i == 10);
}

test "while loop continue expression, more complicated" {
    var i: usize = 1;
    var j: usize = 1;
    while (i * j < 2000) : ({
        i *= 2;
        j *= 3;
    }) {
        const my_ij = i * j;
        try expect(my_ij < 2000);
    }
}
```

Shell

$ zig test test\_while\_continue\_expression.zig
1/2 test\_while\_continue\_expression.test.while loop continue expression...OK
2/2 test\_while\_continue\_expression.test.while loop continue expression, more complicated...OK
All 2 tests passed.

While loops are expressions. The result of the expression is the result of the `else` clause of a while loop, which is executed when the condition of the while loop is tested as false.

`break`, like `return`, accepts a value parameter. This is the result of the `while` expression. When you `break` from a while loop, the `else` branch is not evaluated.

test\_while\_else.zig

```
const expect = @import("std").testing.expect;

test "while else" {
    try expect(rangeHasNumber(0, 10, 5));
    try expect(!rangeHasNumber(0, 10, 15));
}

fn rangeHasNumber(begin: usize, end: usize, number: usize) bool {
    var i = begin;
    return while (i < end) : (i += 1) {
        if (i == number) {
            break true;
        }
    } else false;
}
```

Shell

$ zig test test\_while\_else.zig
1/1 test\_while\_else.test.while else...OK
All 1 tests passed.

### [Labeled while](https://ziglang.org/documentation/0.15.2/#toc-Labeled-while) [Â§](https://ziglang.org/documentation/0.15.2/#Labeled-while)

When a `while` loop is labeled, it can be referenced from a `break` or `continue` from within a nested loop:

test\_while\_nested\_break.zig

```
test "nested break" {
    outer: while (true) {
        while (true) {
            break :outer;
        }
    }
}

test "nested continue" {
    var i: usize = 0;
    outer: while (i < 10) : (i += 1) {
        while (true) {
            continue :outer;
        }
    }
}
```

Shell

$ zig test test\_while\_nested\_break.zig
1/2 test\_while\_nested\_break.test.nested break...OK
2/2 test\_while\_nested\_break.test.nested continue...OK
All 2 tests passed.

### [while with Optionals](https://ziglang.org/documentation/0.15.2/#toc-while-with-Optionals) [Â§](https://ziglang.org/documentation/0.15.2/#while-with-Optionals)

Just like [if](https://ziglang.org/documentation/0.15.2/#if) expressions, while loops can take an optional as the condition and capture the payload. When [null](https://ziglang.org/documentation/0.15.2/#null) is encountered the loop exits.

When the `|x|` syntax is present on a `while` expression, the while condition must have an [Optional Type](https://ziglang.org/documentation/0.15.2/#Optional-Type).

The `else` branch is allowed on optional iteration. In this case, it will be executed on the first null value encountered.

test\_while\_null\_capture.zig

```
const expect = @import("std").testing.expect;

test "while null capture" {
    var sum1: u32 = 0;
    numbers_left = 3;
    while (eventuallyNullSequence()) |value| {
        sum1 += value;
    }
    try expect(sum1 == 3);

    // null capture with an else block
    var sum2: u32 = 0;
    numbers_left = 3;
    while (eventuallyNullSequence()) |value| {
        sum2 += value;
    } else {
        try expect(sum2 == 3);
    }

    // null capture with a continue expression
    var i: u32 = 0;
    var sum3: u32 = 0;
    numbers_left = 3;
    while (eventuallyNullSequence()) |value| : (i += 1) {
        sum3 += value;
    }
    try expect(i == 3);
}

var numbers_left: u32 = undefined;
fn eventuallyNullSequence() ?u32 {
    return if (numbers_left == 0) null else blk: {
        numbers_left -= 1;
        break :blk numbers_left;
    };
}
```

Shell

$ zig test test\_while\_null\_capture.zig
1/1 test\_while\_null\_capture.test.while null capture...OK
All 1 tests passed.

### [while with Error Unions](https://ziglang.org/documentation/0.15.2/#toc-while-with-Error-Unions) [Â§](https://ziglang.org/documentation/0.15.2/#while-with-Error-Unions)

Just like [if](https://ziglang.org/documentation/0.15.2/#if) expressions, while loops can take an error union as the condition and capture the payload or the error code. When the condition results in an error code the else branch is evaluated and the loop is finished.

When the `else |x|` syntax is present on a `while` expression, the while condition must have an [Error Union Type](https://ziglang.org/documentation/0.15.2/#Error-Union-Type).

test\_while\_error\_capture.zig

```
const expect = @import("std").testing.expect;

test "while error union capture" {
    var sum1: u32 = 0;
    numbers_left = 3;
    while (eventuallyErrorSequence()) |value| {
        sum1 += value;
    } else |err| {
        try expect(err == error.ReachedZero);
    }
}

var numbers_left: u32 = undefined;

fn eventuallyErrorSequence() anyerror!u32 {
    return if (numbers_left == 0) error.ReachedZero else blk: {
        numbers_left -= 1;
        break :blk numbers_left;
    };
}
```

Shell

$ zig test test\_while\_error\_capture.zig
1/1 test\_while\_error\_capture.test.while error union capture...OK
All 1 tests passed.

### [inline while](https://ziglang.org/documentation/0.15.2/#toc-inline-while) [Â§](https://ziglang.org/documentation/0.15.2/#inline-while)

While loops can be inlined. This causes the loop to be unrolled, which allows the code to do some things which only work at compile time, such as use types as first class values.

test\_inline\_while.zig

```
const expect = @import("std").testing.expect;

test "inline while loop" {
    comptime var i = 0;
    var sum: usize = 0;
    inline while (i < 3) : (i += 1) {
        const T = switch (i) {
            0 => f32,
            1 => i8,
            2 => bool,
            else => unreachable,
        };
        sum += typeNameLength(T);
    }
    try expect(sum == 9);
}

fn typeNameLength(comptime T: type) usize {
    return @typeName(T).len;
}
```

Shell

$ zig test test\_inline\_while.zig
1/1 test\_inline\_while.test.inline while loop...OK
All 1 tests passed.

It is recommended to use `inline` loops only for one of these reasons:

-   You need the loop to execute at [comptime](https://ziglang.org/documentation/0.15.2/#comptime) for the semantics to work.

-   You have a benchmark to prove that forcibly unrolling the loop in this way is measurably faster.

See also:

-   [if](https://ziglang.org/documentation/0.15.2/#if)

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)
-   [Errors](https://ziglang.org/documentation/0.15.2/#Errors)

-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)
-   [unreachable](https://ziglang.org/documentation/0.15.2/#unreachable)

## [for](https://ziglang.org/documentation/0.15.2/#toc-for) [Â§](https://ziglang.org/documentation/0.15.2/#for)

test\_for.zig

```
const expect = @import("std").testing.expect;

test "for basics" {
    const items = [_]i32{ 4, 5, 3, 4, 0 };
    var sum: i32 = 0;

    // For loops iterate over slices and arrays.
    for (items) |value| {
        // Break and continue are supported.
        if (value == 0) {
            continue;
        }
        sum += value;
    }
    try expect(sum == 16);

    // To iterate over a portion of a slice, reslice.
    for (items[0..1]) |value| {
        sum += value;
    }
    try expect(sum == 20);

    // To access the index of iteration, specify a second condition as well
    // as a second capture value.
    var sum2: i32 = 0;
    for (items, 0..) |_, i| {
        try expect(@TypeOf(i) == usize);
        sum2 += @as(i32, @intCast(i));
    }
    try expect(sum2 == 10);

    // To iterate over consecutive integers, use the range syntax.
    // Unbounded range is always a compile error.
    var sum3: usize = 0;
    for (0..5) |i| {
        sum3 += i;
    }
    try expect(sum3 == 10);
}

test "multi object for" {
    const items = [_]usize{ 1, 2, 3 };
    const items2 = [_]usize{ 4, 5, 6 };
    var count: usize = 0;

    // Iterate over multiple objects.
    // All lengths must be equal at the start of the loop, otherwise detectable
    // illegal behavior occurs.
    for (items, items2) |i, j| {
        count += i + j;
    }

    try expect(count == 21);
}

test "for reference" {
    var items = [_]i32{ 3, 4, 2 };

    // Iterate over the slice by reference by
    // specifying that the capture value is a pointer.
    for (&items) |*value| {
        value.* += 1;
    }

    try expect(items[0] == 4);
    try expect(items[1] == 5);
    try expect(items[2] == 3);
}

test "for else" {
    // For allows an else attached to it, the same as a while loop.
    const items = [_]?i32{ 3, 4, null, 5 };

    // For loops can also be used as expressions.
    // Similar to while loops, when you break from a for loop, the else branch is not evaluated.
    var sum: i32 = 0;
    const result = for (items) |value| {
        if (value != null) {
            sum += value.?;
        }
    } else blk: {
        try expect(sum == 12);
        break :blk sum;
    };
    try expect(result == 12);
}
```

Shell

$ zig test test\_for.zig
1/4 test\_for.test.for basics...OK
2/4 test\_for.test.multi object for...OK
3/4 test\_for.test.for reference...OK
4/4 test\_for.test.for else...OK
All 4 tests passed.

### [Labeled for](https://ziglang.org/documentation/0.15.2/#toc-Labeled-for) [Â§](https://ziglang.org/documentation/0.15.2/#Labeled-for)

When a `for` loop is labeled, it can be referenced from a `break` or `continue` from within a nested loop:

test\_for\_nested\_break.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "nested break" {
    var count: usize = 0;
    outer: for (1..6) |_| {
        for (1..6) |_| {
            count += 1;
            break :outer;
        }
    }
    try expect(count == 1);
}

test "nested continue" {
    var count: usize = 0;
    outer: for (1..9) |_| {
        for (1..6) |_| {
            count += 1;
            continue :outer;
        }
    }

    try expect(count == 8);
}
```

Shell

$ zig test test\_for\_nested\_break.zig
1/2 test\_for\_nested\_break.test.nested break...OK
2/2 test\_for\_nested\_break.test.nested continue...OK
All 2 tests passed.

### [inline for](https://ziglang.org/documentation/0.15.2/#toc-inline-for) [Â§](https://ziglang.org/documentation/0.15.2/#inline-for)

For loops can be inlined. This causes the loop to be unrolled, which allows the code to do some things which only work at compile time, such as use types as first class values. The capture value and iterator value of inlined for loops are compile-time known.

test\_inline\_for.zig

```
const expect = @import("std").testing.expect;

test "inline for loop" {
    const nums = [_]i32{ 2, 4, 6 };
    var sum: usize = 0;
    inline for (nums) |i| {
        const T = switch (i) {
            2 => f32,
            4 => i8,
            6 => bool,
            else => unreachable,
        };
        sum += typeNameLength(T);
    }
    try expect(sum == 9);
}

fn typeNameLength(comptime T: type) usize {
    return @typeName(T).len;
}
```

Shell

$ zig test test\_inline\_for.zig
1/1 test\_inline\_for.test.inline for loop...OK
All 1 tests passed.

It is recommended to use `inline` loops only for one of these reasons:

-   You need the loop to execute at [comptime](https://ziglang.org/documentation/0.15.2/#comptime) for the semantics to work.

-   You have a benchmark to prove that forcibly unrolling the loop in this way is measurably faster.

See also:

-   [while](https://ziglang.org/documentation/0.15.2/#while)

-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)
-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays)

-   [Slices](https://ziglang.org/documentation/0.15.2/#Slices)

## [if](https://ziglang.org/documentation/0.15.2/#toc-if) [Â§](https://ziglang.org/documentation/0.15.2/#if)

test\_if.zig

```
// If expressions have three uses, corresponding to the three types:
// * bool
// * ?T
// * anyerror!T

const expect = @import("std").testing.expect;

test "if expression" {
    // If expressions are used instead of a ternary expression.
    const a: u32 = 5;
    const b: u32 = 4;
    const result = if (a != b) 47 else 3089;
    try expect(result == 47);
}

test "if boolean" {
    // If expressions test boolean conditions.
    const a: u32 = 5;
    const b: u32 = 4;
    if (a != b) {
        try expect(true);
    } else if (a == 9) {
        unreachable;
    } else {
        unreachable;
    }
}

test "if error union" {
    // If expressions test for errors.
    // Note the |err| capture on the else.

    const a: anyerror!u32 = 0;
    if (a) |value| {
        try expect(value == 0);
    } else |err| {
        _ = err;
        unreachable;
    }

    const b: anyerror!u32 = error.BadValue;
    if (b) |value| {
        _ = value;
        unreachable;
    } else |err| {
        try expect(err == error.BadValue);
    }

    // The else and |err| capture is strictly required.
    if (a) |value| {
        try expect(value == 0);
    } else |_| {}

    // To check only the error value, use an empty block expression.
    if (b) |_| {} else |err| {
        try expect(err == error.BadValue);
    }

    // Access the value by reference using a pointer capture.
    var c: anyerror!u32 = 3;
    if (c) |*value| {
        value.* = 9;
    } else |_| {
        unreachable;
    }

    if (c) |value| {
        try expect(value == 9);
    } else |_| {
        unreachable;
    }
}
```

Shell

$ zig test test\_if.zig
1/3 test\_if.test.if expression...OK
2/3 test\_if.test.if boolean...OK
3/3 test\_if.test.if error union...OK
All 3 tests passed.

### [if with Optionals](https://ziglang.org/documentation/0.15.2/#toc-if-with-Optionals) [Â§](https://ziglang.org/documentation/0.15.2/#if-with-Optionals)

test\_if\_optionals.zig

```
const expect = @import("std").testing.expect;

test "if optional" {
    // If expressions test for null.

    const a: ?u32 = 0;
    if (a) |value| {
        try expect(value == 0);
    } else {
        unreachable;
    }

    const b: ?u32 = null;
    if (b) |_| {
        unreachable;
    } else {
        try expect(true);
    }

    // The else is not required.
    if (a) |value| {
        try expect(value == 0);
    }

    // To test against null only, use the binary equality operator.
    if (b == null) {
        try expect(true);
    }

    // Access the value by reference using a pointer capture.
    var c: ?u32 = 3;
    if (c) |*value| {
        value.* = 2;
    }

    if (c) |value| {
        try expect(value == 2);
    } else {
        unreachable;
    }
}

test "if error union with optional" {
    // If expressions test for errors before unwrapping optionals.
    // The |optional_value| capture's type is ?u32.

    const a: anyerror!?u32 = 0;
    if (a) |optional_value| {
        try expect(optional_value.? == 0);
    } else |err| {
        _ = err;
        unreachable;
    }

    const b: anyerror!?u32 = null;
    if (b) |optional_value| {
        try expect(optional_value == null);
    } else |_| {
        unreachable;
    }

    const c: anyerror!?u32 = error.BadValue;
    if (c) |optional_value| {
        _ = optional_value;
        unreachable;
    } else |err| {
        try expect(err == error.BadValue);
    }

    // Access the value by reference by using a pointer capture each time.
    var d: anyerror!?u32 = 3;
    if (d) |*optional_value| {
        if (optional_value.*) |*value| {
            value.* = 9;
        }
    } else |_| {
        unreachable;
    }

    if (d) |optional_value| {
        try expect(optional_value.? == 9);
    } else |_| {
        unreachable;
    }
}
```

Shell

$ zig test test\_if\_optionals.zig
1/2 test\_if\_optionals.test.if optional...OK
2/2 test\_if\_optionals.test.if error union with optional...OK
All 2 tests passed.

See also:

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

-   [Errors](https://ziglang.org/documentation/0.15.2/#Errors)

## [defer](https://ziglang.org/documentation/0.15.2/#toc-defer) [Â§](https://ziglang.org/documentation/0.15.2/#defer)

Executes an expression unconditionally at scope exit.

test\_defer.zig

```
const std = @import("std");
const expect = std.testing.expect;
const print = std.debug.print;

fn deferExample() !usize {
    var a: usize = 1;

    {
        defer a = 2;
        a = 1;
    }
    try expect(a == 2);

    a = 5;
    return a;
}

test "defer basics" {
    try expect((try deferExample()) == 5);
}
```

Shell

$ zig test test\_defer.zig
1/1 test\_defer.test.defer basics...OK
All 1 tests passed.

Defer expressions are evaluated in reverse order.

defer\_unwind.zig

```
const std = @import("std");
const print = std.debug.print;

pub fn main() void {
    print("\n", .{});

    defer {
        print("1 ", .{});
    }
    defer {
        print("2 ", .{});
    }
    if (false) {
        // defers are not run if they are never executed.
        defer {
            print("3 ", .{});
        }
    }
}
```

Shell

$ zig build-exe defer\_unwind.zig
$ ./defer\_unwind

2 1

Inside a defer expression the return statement is not allowed.

test\_invalid\_defer.zig

```
fn deferInvalidExample() !void {
    defer {
        return error.DeferError;
    }

    return error.DeferError;
}
```

Shell

$ zig test test\_invalid\_defer.zig
/home/andy/dev/zig/doc/langref/test\_invalid\_defer.zig:3:9: error: cannot return from defer expression
        return error.DeferError;
        ^~~~~~~~~~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_invalid\_defer.zig:2:5: note: defer expression here
    defer {
    ^~~~~

See also:

-   [Errors](https://ziglang.org/documentation/0.15.2/#Errors)

## [unreachable](https://ziglang.org/documentation/0.15.2/#toc-unreachable) [Â§](https://ziglang.org/documentation/0.15.2/#unreachable)

In [Debug](https://ziglang.org/documentation/0.15.2/#Debug) and [ReleaseSafe](https://ziglang.org/documentation/0.15.2/#ReleaseSafe) mode `unreachable` emits a call to `panic` with the message `reached unreachable code`.

In [ReleaseFast](https://ziglang.org/documentation/0.15.2/#ReleaseFast) and [ReleaseSmall](https://ziglang.org/documentation/0.15.2/#ReleaseSmall) mode, the optimizer uses the assumption that `unreachable` code will never be hit to perform optimizations.

### [Basics](https://ziglang.org/documentation/0.15.2/#toc-Basics) [Â§](https://ziglang.org/documentation/0.15.2/#Basics)

test\_unreachable.zig

```
// unreachable is used to assert that control flow will never reach a
// particular location:
test "basic math" {
    const x = 1;
    const y = 2;
    if (x + y != 3) {
        unreachable;
    }
}
```

Shell

$ zig test test\_unreachable.zig
1/1 test\_unreachable.test.basic math...OK
All 1 tests passed.

In fact, this is how `std.debug.assert` is implemented:

test\_assertion\_failure.zig

```
// This is how std.debug.assert is implemented
fn assert(ok: bool) void {
    if (!ok) unreachable; // assertion failure
}

// This test will fail because we hit unreachable.
test "this will fail" {
    assert(false);
}
```

Shell

$ zig test test\_assertion\_failure.zig
1/1 test\_assertion\_failure.test.this will fail...thread 2902460 panic: reached unreachable code
/home/andy/dev/zig/doc/langref/test\_assertion\_failure.zig:3:14: 0x102c039 in assert (test\_assertion\_failure.zig)
    if (!ok) unreachable; // assertion failure
             ^
/home/andy/dev/zig/doc/langref/test\_assertion\_failure.zig:8:11: 0x102c00e in test.this will fail (test\_assertion\_failure.zig)
    assert(false);
          ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x115cb50 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1155d71 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x114fb0d in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x114f3a1 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/2d8b23c255add16f67e238437a2ca75f/test --seed=0xf5bf1bba

### [At Compile-Time](https://ziglang.org/documentation/0.15.2/#toc-At-Compile-Time) [Â§](https://ziglang.org/documentation/0.15.2/#At-Compile-Time)

test\_comptime\_unreachable.zig

```
const assert = @import("std").debug.assert;

test "type of unreachable" {
    comptime {
        // The type of unreachable is noreturn.

        // However this assertion will still fail to compile because
        // unreachable expressions are compile errors.

        assert(@TypeOf(unreachable) == noreturn);
    }
}
```

Shell

$ zig test test\_comptime\_unreachable.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_unreachable.zig:10:16: error: unreachable code
        assert(@TypeOf(unreachable) == noreturn);
               ^~~~~~~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_comptime\_unreachable.zig:10:24: note: control flow is diverted here
        assert(@TypeOf(unreachable) == noreturn);
                       ^~~~~~~~~~~

See also:

-   [Zig Test](https://ziglang.org/documentation/0.15.2/#Zig-Test)

-   [Build Mode](https://ziglang.org/documentation/0.15.2/#Build-Mode)
-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)

## [noreturn](https://ziglang.org/documentation/0.15.2/#toc-noreturn) [Â§](https://ziglang.org/documentation/0.15.2/#noreturn)

`noreturn` is the type of:

-   `break`

-   `continue`
-   `return`

-   `unreachable`
-   `while (true) {}`

When resolving types together, such as `if` clauses or `switch` prongs, the `noreturn` type is compatible with every other type. Consider:

test\_noreturn.zig

```
fn foo(condition: bool, b: u32) void {
    const a = if (condition) b else return;
    _ = a;
    @panic("do something with a");
}
test "noreturn" {
    foo(false, 1);
}
```

Shell

$ zig test test\_noreturn.zig
1/1 test\_noreturn.test.noreturn...OK
All 1 tests passed.

Another use case for `noreturn` is the `exit` function:

test\_noreturn\_from\_exit.zig

```
const std = @import("std");
const builtin = @import("builtin");
const native_arch = builtin.cpu.arch;
const expect = std.testing.expect;

const WINAPI: std.builtin.CallingConvention = if (native_arch == .x86) .{ .x86_stdcall = .{} } else .c;
extern "kernel32" fn ExitProcess(exit_code: c_uint) callconv(WINAPI) noreturn;

test "foo" {
    const value = bar() catch ExitProcess(1);
    try expect(value == 1234);
}

fn bar() anyerror!u32 {
    return 1234;
}
```

Shell

$ zig test test\_noreturn\_from\_exit.zig -target x86\_64-windows --test-no-exec

## [Functions](https://ziglang.org/documentation/0.15.2/#toc-Functions) [Â§](https://ziglang.org/documentation/0.15.2/#Functions)

test\_functions.zig

```
const std = @import("std");
const builtin = @import("builtin");
const native_arch = builtin.cpu.arch;
const expect = std.testing.expect;

// Functions are declared like this
fn add(a: i8, b: i8) i8 {
    if (a == 0) {
        return b;
    }

    return a + b;
}

// The export specifier makes a function externally visible in the generated
// object file, and makes it use the C ABI.
export fn sub(a: i8, b: i8) i8 {
    return a - b;
}

// The extern specifier is used to declare a function that will be resolved
// at link time, when linking statically, or at runtime, when linking
// dynamically. The quoted identifier after the extern keyword specifies
// the library that has the function. (e.g. "c" -> libc.so)
// The callconv specifier changes the calling convention of the function.
extern "kernel32" fn ExitProcess(exit_code: u32) callconv(.winapi) noreturn;
extern "c" fn atan2(a: f64, b: f64) f64;

// The @branchHint builtin can be used to tell the optimizer that a function is rarely called ("cold").
fn abort() noreturn {
    @branchHint(.cold);
    while (true) {}
}

// The naked calling convention makes a function not have any function prologue or epilogue.
// This can be useful when integrating with assembly.
fn _start() callconv(.naked) noreturn {
    abort();
}

// The inline calling convention forces a function to be inlined at all call sites.
// If the function cannot be inlined, it is a compile-time error.
inline fn shiftLeftOne(a: u32) u32 {
    return a << 1;
}

// The pub specifier allows the function to be visible when importing.
// Another file can use @import and call sub2
pub fn sub2(a: i8, b: i8) i8 {
    return a - b;
}

// Function pointers are prefixed with `*const `.
const Call2Op = *const fn (a: i8, b: i8) i8;
fn doOp(fnCall: Call2Op, op1: i8, op2: i8) i8 {
    return fnCall(op1, op2);
}

test "function" {
    try expect(doOp(add, 5, 6) == 11);
    try expect(doOp(sub2, 5, 6) == -1);
}
```

Shell

$ zig test test\_functions.zig
1/1 test\_functions.test.function...OK
All 1 tests passed.

There is a difference between a function *body* and a function *pointer*. Function bodies are [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-only types while function [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) may be runtime-known.

### [Pass-by-value Parameters](https://ziglang.org/documentation/0.15.2/#toc-Pass-by-value-Parameters) [Â§](https://ziglang.org/documentation/0.15.2/#Pass-by-value-Parameters)

Primitive types such as [Integers](https://ziglang.org/documentation/0.15.2/#Integers) and [Floats](https://ziglang.org/documentation/0.15.2/#Floats) passed as parameters are copied, and then the copy is available in the function body. This is called "passing by value". Copying a primitive type is essentially free and typically involves nothing more than setting a register.

Structs, unions, and arrays can sometimes be more efficiently passed as a reference, since a copy could be arbitrarily expensive depending on the size. When these types are passed as parameters, Zig may choose to copy and pass by value, or pass by reference, whichever way Zig decides will be faster. This is made possible, in part, by the fact that parameters are immutable.

test\_pass\_by\_reference\_or\_value.zig

```
const Point = struct {
    x: i32,
    y: i32,
};

fn foo(point: Point) i32 {
    // Here, `point` could be a reference, or a copy. The function body
    // can ignore the difference and treat it as a value. Be very careful
    // taking the address of the parameter - it should be treated as if
    // the address will become invalid when the function returns.
    return point.x + point.y;
}

const expect = @import("std").testing.expect;

test "pass struct to function" {
    try expect(foo(Point{ .x = 1, .y = 2 }) == 3);
}
```

Shell

$ zig test test\_pass\_by\_reference\_or\_value.zig
1/1 test\_pass\_by\_reference\_or\_value.test.pass struct to function...OK
All 1 tests passed.

For extern functions, Zig follows the C ABI for passing structs and unions by value.

### [Function Parameter Type Inference](https://ziglang.org/documentation/0.15.2/#toc-Function-Parameter-Type-Inference) [Â§](https://ziglang.org/documentation/0.15.2/#Function-Parameter-Type-Inference)

Function parameters can be declared with `anytype` in place of the type. In this case the parameter types will be inferred when the function is called. Use [@TypeOf](https://ziglang.org/documentation/0.15.2/#TypeOf) and [@typeInfo](https://ziglang.org/documentation/0.15.2/#typeInfo) to get information about the inferred type.

test\_fn\_type\_inference.zig

```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    const y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

Shell

$ zig test test\_fn\_type\_inference.zig
1/1 test\_fn\_type\_inference.test.fn type inference...OK
All 1 tests passed.

### [inline fn](https://ziglang.org/documentation/0.15.2/#toc-inline-fn) [Â§](https://ziglang.org/documentation/0.15.2/#inline-fn)

Adding the `inline` keyword to a function definition makes that function become *semantically inlined* at the callsite. This is not a hint to be possibly observed by optimization passes, but has implications on the types and values involved in the function call.

Unlike normal function calls, arguments at an inline function callsite which are compile-time known are treated as [Compile Time Parameters](https://ziglang.org/documentation/0.15.2/#Compile-Time-Parameters). This can potentially propagate all the way to the return value:

inline\_call.zig

```
const std = @import("std");

pub fn main() void {
    if (foo(1200, 34) != 1234) {
        @compileError("bad");
    }
}

inline fn foo(a: i32, b: i32) i32 {
    std.debug.print("runtime a = {} b = {}", .{ a, b });
    return a + b;
}
```

Shell

$ zig build-exe inline\_call.zig
$ ./inline\_call
runtime a = 1200 b = 34

If `inline` is removed, the test fails with the compile error instead of passing.

It is generally better to let the compiler decide when to inline a function, except for these scenarios:

-   To change how many stack frames are in the call stack, for debugging purposes.

-   To force comptime-ness of the arguments to propagate to the return value of the function, as in the above example.
-   Real world performance measurements demand it.

Note that `inline` actually *restricts* what the compiler is allowed to do. This can harm binary size, compilation speed, and even runtime performance.

### [Function Reflection](https://ziglang.org/documentation/0.15.2/#toc-Function-Reflection) [Â§](https://ziglang.org/documentation/0.15.2/#Function-Reflection)

test\_fn\_reflection.zig

```
const std = @import("std");
const math = std.math;
const testing = std.testing;

test "fn reflection" {
    try testing.expect(@typeInfo(@TypeOf(testing.expect)).@"fn".params[0].type.? == bool);
    try testing.expect(@typeInfo(@TypeOf(testing.tmpDir)).@"fn".return_type.? == testing.TmpDir);

    try testing.expect(@typeInfo(@TypeOf(math.Log2Int)).@"fn".is_generic);
}
```

Shell

$ zig test test\_fn\_reflection.zig
1/1 test\_fn\_reflection.test.fn reflection...OK
All 1 tests passed.

## [Errors](https://ziglang.org/documentation/0.15.2/#toc-Errors) [Â§](https://ziglang.org/documentation/0.15.2/#Errors)

### [Error Set Type](https://ziglang.org/documentation/0.15.2/#toc-Error-Set-Type) [Â§](https://ziglang.org/documentation/0.15.2/#Error-Set-Type)

An error set is like an [enum](https://ziglang.org/documentation/0.15.2/#enum). However, each error name across the entire compilation gets assigned an unsigned integer greater than 0. You are allowed to declare the same error name more than once, and if you do, it gets assigned the same integer value.

The error set type defaults to a `u16`, though if the maximum number of distinct error values is provided via the \--error-limit \[num\] command line parameter an integer type with the minimum number of bits required to represent all of the error values will be used.

You can [coerce](https://ziglang.org/documentation/0.15.2/#Type-Coercion) an error from a subset to a superset:

test\_coerce\_error\_subset\_to\_superset.zig

```
const std = @import("std");

const FileOpenError = error{
    AccessDenied,
    OutOfMemory,
    FileNotFound,
};

const AllocationError = error{
    OutOfMemory,
};

test "coerce subset to superset" {
    const err = foo(AllocationError.OutOfMemory);
    try std.testing.expect(err == FileOpenError.OutOfMemory);
}

fn foo(err: AllocationError) FileOpenError {
    return err;
}
```

Shell

$ zig test test\_coerce\_error\_subset\_to\_superset.zig
1/1 test\_coerce\_error\_subset\_to\_superset.test.coerce subset to superset...OK
All 1 tests passed.

But you cannot [coerce](https://ziglang.org/documentation/0.15.2/#Type-Coercion) an error from a superset to a subset:

test\_coerce\_error\_superset\_to\_subset.zig

```
const FileOpenError = error{
    AccessDenied,
    OutOfMemory,
    FileNotFound,
};

const AllocationError = error{
    OutOfMemory,
};

test "coerce superset to subset" {
    foo(FileOpenError.OutOfMemory) catch {};
}

fn foo(err: FileOpenError) AllocationError {
    return err;
}
```

Shell

$ zig test test\_coerce\_error\_superset\_to\_subset.zig
/home/andy/dev/zig/doc/langref/test\_coerce\_error\_superset\_to\_subset.zig:16:12: error: expected type 'error{OutOfMemory}', found 'error{AccessDenied,FileNotFound,OutOfMemory}'
    return err;
           ^~~
/home/andy/dev/zig/doc/langref/test\_coerce\_error\_superset\_to\_subset.zig:16:12: note: 'error.AccessDenied' not a member of destination error set
/home/andy/dev/zig/doc/langref/test\_coerce\_error\_superset\_to\_subset.zig:16:12: note: 'error.FileNotFound' not a member of destination error set
/home/andy/dev/zig/doc/langref/test\_coerce\_error\_superset\_to\_subset.zig:15:28: note: function return type declared here
fn foo(err: FileOpenError) AllocationError {
                           ^~~~~~~~~~~~~~~
referenced by:
    test.coerce superset to subset: /home/andy/dev/zig/doc/langref/test\_coerce\_error\_superset\_to\_subset.zig:12:8

There is a shortcut for declaring an error set with only 1 value, and then getting that value:

single\_value\_error\_set\_shortcut.zig

```
const err = error.FileNotFound;
```

This is equivalent to:

single\_value\_error\_set.zig

```
const err = (error{FileNotFound}).FileNotFound;
```

This becomes useful when using [Inferred Error Sets](https://ziglang.org/documentation/0.15.2/#Inferred-Error-Sets).

#### [The Global Error Set](https://ziglang.org/documentation/0.15.2/#toc-The-Global-Error-Set) [Â§](https://ziglang.org/documentation/0.15.2/#The-Global-Error-Set)

`anyerror` refers to the global error set. This is the error set that contains all errors in the entire compilation unit, i.e. it is the union of all other error sets.

You can [coerce](https://ziglang.org/documentation/0.15.2/#Type-Coercion) any error set to the global one, and you can explicitly cast an error of the global error set to a non-global one. This inserts a language-level assert to make sure the error value is in fact in the destination error set.

The global error set should generally be avoided because it prevents the compiler from knowing what errors are possible at compile-time. Knowing the error set at compile-time is better for generated documentation and helpful error messages, such as forgetting a possible error value in a [switch](https://ziglang.org/documentation/0.15.2/#switch).

### [Error Union Type](https://ziglang.org/documentation/0.15.2/#toc-Error-Union-Type) [Â§](https://ziglang.org/documentation/0.15.2/#Error-Union-Type)

An error set type and normal type can be combined with the `!` binary operator to form an error union type. You are likely to use an error union type more often than an error set type by itself.

Here is a function to parse a string into a 64-bit integer:

error\_union\_parsing\_u64.zig

```
const std = @import("std");
const maxInt = std.math.maxInt;

pub fn parseU64(buf: []const u8, radix: u8) !u64 {
    var x: u64 = 0;

    for (buf) |c| {
        const digit = charToDigit(c);

        if (digit >= radix) {
            return error.InvalidChar;
        }

        // x *= radix
        var ov = @mulWithOverflow(x, radix);
        if (ov[1] != 0) return error.OverFlow;

        // x += digit
        ov = @addWithOverflow(ov[0], digit);
        if (ov[1] != 0) return error.OverFlow;
        x = ov[0];
    }

    return x;
}

fn charToDigit(c: u8) u8 {
    return switch (c) {
        '0'...'9' => c - '0',
        'A'...'Z' => c - 'A' + 10,
        'a'...'z' => c - 'a' + 10,
        else => maxInt(u8),
    };
}

test "parse u64" {
    const result = try parseU64("1234", 10);
    try std.testing.expect(result == 1234);
}
```

Shell

$ zig test error\_union\_parsing\_u64.zig
1/1 error\_union\_parsing\_u64.test.parse u64...OK
All 1 tests passed.

Notice the return type is `!u64`. This means that the function either returns an unsigned 64 bit integer, or an error. We left off the error set to the left of the `!`, so the error set is inferred.

Within the function definition, you can see some return statements that return an error, and at the bottom a return statement that returns a `u64`. Both types [coerce](https://ziglang.org/documentation/0.15.2/#Type-Coercion) to `anyerror!u64`.

What it looks like to use this function varies depending on what you're trying to do. One of the following:

-   You want to provide a default value if it returned an error.

-   If it returned an error then you want to return the same error.
-   You know with complete certainty it will not return an error, so want to unconditionally unwrap it.

-   You want to take a different action for each possible error.

#### [catch](https://ziglang.org/documentation/0.15.2/#toc-catch) [Â§](https://ziglang.org/documentation/0.15.2/#catch)

If you want to provide a default value, you can use the `catch` binary operator:

catch.zig

```
const parseU64 = @import("error_union_parsing_u64.zig").parseU64;

fn doAThing(str: []u8) void {
    const number = parseU64(str, 10) catch 13;
    _ = number; // ...
}
```

In this code, `number` will be equal to the successfully parsed string, or a default value of 13. The type of the right hand side of the binary `catch` operator must match the unwrapped error union type, or be of type `noreturn`.

If you want to provide a default value with `catch` after performing some logic, you can combine `catch` with named [Blocks](https://ziglang.org/documentation/0.15.2/#Blocks):

handle\_error\_with\_catch\_block.zig.zig

```
const parseU64 = @import("error_union_parsing_u64.zig").parseU64;

fn doAThing(str: []u8) void {
    const number = parseU64(str, 10) catch blk: {
        // do things
        break :blk 13;
    };
    _ = number; // number is now initialized
}
```

#### [try](https://ziglang.org/documentation/0.15.2/#toc-try) [Â§](https://ziglang.org/documentation/0.15.2/#try)

Let's say you wanted to return the error if you got one, otherwise continue with the function logic:

catch\_err\_return.zig

```
const parseU64 = @import("error_union_parsing_u64.zig").parseU64;

fn doAThing(str: []u8) !void {
    const number = parseU64(str, 10) catch |err| return err;
    _ = number; // ...
}
```

There is a shortcut for this. The `try` expression:

try.zig

```
const parseU64 = @import("error_union_parsing_u64.zig").parseU64;

fn doAThing(str: []u8) !void {
    const number = try parseU64(str, 10);
    _ = number; // ...
}
```

`try` evaluates an error union expression. If it is an error, it returns from the current function with the same error. Otherwise, the expression results in the unwrapped value.

Maybe you know with complete certainty that an expression will never be an error. In this case you can do this:

`const number = parseU64("1234", 10) catch unreachable;`

Here we know for sure that "1234" will parse successfully. So we put the `unreachable` value on the right hand side. `unreachable` invokes safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior), so in [Debug](https://ziglang.org/documentation/0.15.2/#Debug) and [ReleaseSafe](https://ziglang.org/documentation/0.15.2/#ReleaseSafe), triggers a safety panic by default. So, while we're debugging the application, if there *was* a surprise error here, the application would crash appropriately.

You may want to take a different action for every situation. For that, we combine the [if](https://ziglang.org/documentation/0.15.2/#if) and [switch](https://ziglang.org/documentation/0.15.2/#switch) expression:

handle\_all\_error\_scenarios.zig

```
fn doAThing(str: []u8) void {
    if (parseU64(str, 10)) |number| {
        doSomethingWithNumber(number);
    } else |err| switch (err) {
        error.Overflow => {
            // handle overflow...
        },
        // we promise that InvalidChar won't happen (or crash in debug mode if it does)
        error.InvalidChar => unreachable,
    }
}
```

Finally, you may want to handle only some errors. For that, you can capture the unhandled errors in the `else` case, which now contains a narrower error set:

handle\_some\_error\_scenarios.zig

```
fn doAnotherThing(str: []u8) error{InvalidChar}!void {
    if (parseU64(str, 10)) |number| {
        doSomethingWithNumber(number);
    } else |err| switch (err) {
        error.Overflow => {
            // handle overflow...
        },
        else => |leftover_err| return leftover_err,
    }
}
```

You must use the variable capture syntax. If you don't need the variable, you can capture with `_` and avoid the `switch`.

handle\_no\_error\_scenarios.zig

```
fn doADifferentThing(str: []u8) void {
    if (parseU64(str, 10)) |number| {
        doSomethingWithNumber(number);
    } else |_| {
        // do as you'd like
    }
}
```

#### [errdefer](https://ziglang.org/documentation/0.15.2/#toc-errdefer) [Â§](https://ziglang.org/documentation/0.15.2/#errdefer)

The other component to error handling is defer statements. In addition to an unconditional [defer](https://ziglang.org/documentation/0.15.2/#defer), Zig has `errdefer`, which evaluates the deferred expression on block exit path if and only if the function returned with an error from the block.

Example:

errdefer\_example.zig

```
fn createFoo(param: i32) !Foo {
    const foo = try tryToAllocateFoo();
    // now we have allocated foo. we need to free it if the function fails.
    // but we want to return it if the function succeeds.
    errdefer deallocateFoo(foo);

    const tmp_buf = allocateTmpBuffer() orelse return error.OutOfMemory;
    // tmp_buf is truly a temporary resource, and we for sure want to clean it up
    // before this block leaves scope
    defer deallocateTmpBuffer(tmp_buf);

    if (param > 1337) return error.InvalidParam;

    // here the errdefer will not run since we're returning success from the function.
    // but the defer will run!
    return foo;
}
```

The neat thing about this is that you get robust error handling without the verbosity and cognitive overhead of trying to make sure every exit path is covered. The deallocation code is always directly following the allocation code.

The `errdefer` statement can optionally capture the error:

test\_errdefer\_capture.zig

```
const std = @import("std");

fn captureError(captured: *?anyerror) !void {
    errdefer |err| {
        captured.* = err;
    }
    return error.GeneralFailure;
}

test "errdefer capture" {
    var captured: ?anyerror = null;

    if (captureError(&captured)) unreachable else |err| {
        try std.testing.expectEqual(error.GeneralFailure, captured.?);
        try std.testing.expectEqual(error.GeneralFailure, err);
    }
}
```

Shell

$ zig test test\_errdefer\_capture.zig
1/1 test\_errdefer\_capture.test.errdefer capture...OK
All 1 tests passed.

A couple of other tidbits about error handling:

-   These primitives give enough expressiveness that it's completely practical to have failing to check for an error be a compile error. If you really want to ignore the error, you can add `catch unreachable` and get the added benefit of crashing in Debug and ReleaseSafe modes if your assumption was wrong.

-   Since Zig understands error types, it can pre-weight branches in favor of errors not occurring. Just a small optimization benefit that is not available in other languages.

See also:

-   [defer](https://ziglang.org/documentation/0.15.2/#defer)

-   [if](https://ziglang.org/documentation/0.15.2/#if)
-   [switch](https://ziglang.org/documentation/0.15.2/#switch)

An error union is created with the `!` binary operator. You can use compile-time reflection to access the child type of an error union:

test\_error\_union.zig

```
const expect = @import("std").testing.expect;

test "error union" {
    var foo: anyerror!i32 = undefined;

    // Coerce from child type of an error union:
    foo = 1234;

    // Coerce from an error set:
    foo = error.SomeError;

    // Use compile-time reflection to access the payload type of an error union:
    try comptime expect(@typeInfo(@TypeOf(foo)).error_union.payload == i32);

    // Use compile-time reflection to access the error set type of an error union:
    try comptime expect(@typeInfo(@TypeOf(foo)).error_union.error_set == anyerror);
}
```

Shell

$ zig test test\_error\_union.zig
1/1 test\_error\_union.test.error union...OK
All 1 tests passed.

#### [Merging Error Sets](https://ziglang.org/documentation/0.15.2/#toc-Merging-Error-Sets) [Â§](https://ziglang.org/documentation/0.15.2/#Merging-Error-Sets)

Use the `||` operator to merge two error sets together. The resulting error set contains the errors of both error sets. Doc comments from the left-hand side override doc comments from the right-hand side. In this example, the doc comments for `C.PathNotFound` is `A doc comment`.

This is especially useful for functions which return different error sets depending on [comptime](https://ziglang.org/documentation/0.15.2/#comptime) branches. For example, the Zig standard library uses `LinuxFileOpenError || WindowsFileOpenError` for the error set of opening files.

test\_merging\_error\_sets.zig

```
const A = error{
    NotDir,

    /// A doc comment
    PathNotFound,
};
const B = error{
    OutOfMemory,

    /// B doc comment
    PathNotFound,
};

const C = A || B;

fn foo() C!void {
    return error.NotDir;
}

test "merge error sets" {
    if (foo()) {
        @panic("unexpected");
    } else |err| switch (err) {
        error.OutOfMemory => @panic("unexpected"),
        error.PathNotFound => @panic("unexpected"),
        error.NotDir => {},
    }
}
```

Shell

$ zig test test\_merging\_error\_sets.zig
1/1 test\_merging\_error\_sets.test.merge error sets...OK
All 1 tests passed.

#### [Inferred Error Sets](https://ziglang.org/documentation/0.15.2/#toc-Inferred-Error-Sets) [Â§](https://ziglang.org/documentation/0.15.2/#Inferred-Error-Sets)

Because many functions in Zig return a possible error, Zig supports inferring the error set. To infer the error set for a function, prepend the `!` operator to the functionâ€™s return type, like `!T`:

test\_inferred\_error\_sets.zig

```
// With an inferred error set
pub fn add_inferred(comptime T: type, a: T, b: T) !T {
    const ov = @addWithOverflow(a, b);
    if (ov[1] != 0) return error.Overflow;
    return ov[0];
}

// With an explicit error set
pub fn add_explicit(comptime T: type, a: T, b: T) Error!T {
    const ov = @addWithOverflow(a, b);
    if (ov[1] != 0) return error.Overflow;
    return ov[0];
}

const Error = error{
    Overflow,
};

const std = @import("std");

test "inferred error set" {
    if (add_inferred(u8, 255, 1)) |_| unreachable else |err| switch (err) {
        error.Overflow => {}, // ok
    }
}
```

Shell

$ zig test test\_inferred\_error\_sets.zig
1/1 test\_inferred\_error\_sets.test.inferred error set...OK
All 1 tests passed.

When a function has an inferred error set, that function becomes generic and thus it becomes trickier to do certain things with it, such as obtain a function pointer, or have an error set that is consistent across different build targets. Additionally, inferred error sets are incompatible with recursion.

In these situations, it is recommended to use an explicit error set. You can generally start with an empty error set and let compile errors guide you toward completing the set.

These limitations may be overcome in a future version of Zig.

### [Error Return Traces](https://ziglang.org/documentation/0.15.2/#toc-Error-Return-Traces) [Â§](https://ziglang.org/documentation/0.15.2/#Error-Return-Traces)

Error Return Traces show all the points in the code that an error was returned to the calling function. This makes it practical to use [try](https://ziglang.org/documentation/0.15.2/#try) everywhere and then still be able to know what happened if an error ends up bubbling all the way out of your application.

error\_return\_trace.zig

```
pub fn main() !void {
    try foo(12);
}

fn foo(x: i32) !void {
    if (x >= 5) {
        try bar();
    } else {
        try bang2();
    }
}

fn bar() !void {
    if (baz()) {
        try quux();
    } else |err| switch (err) {
        error.FileNotFound => try hello(),
    }
}

fn baz() !void {
    try bang1();
}

fn quux() !void {
    try bang2();
}

fn hello() !void {
    try bang2();
}

fn bang1() !void {
    return error.FileNotFound;
}

fn bang2() !void {
    return error.PermissionDenied;
}
```

Shell

$ zig build-exe error\_return\_trace.zig
$ ./error\_return\_trace
error: PermissionDenied
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:34:5: 0x113d36c in bang1 (error\_return\_trace.zig)
    return error.FileNotFound;
    ^
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:22:5: 0x113d3b6 in baz (error\_return\_trace.zig)
    try bang1();
    ^
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:38:5: 0x113d3ec in bang2 (error\_return\_trace.zig)
    return error.PermissionDenied;
    ^
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:30:5: 0x113d496 in hello (error\_return\_trace.zig)
    try bang2();
    ^
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:17:31: 0x113d56e in bar (error\_return\_trace.zig)
        error.FileNotFound => try hello(),
                              ^
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:7:9: 0x113d654 in foo (error\_return\_trace.zig)
        try bar();
        ^
/home/andy/dev/zig/doc/langref/error\_return\_trace.zig:2:5: 0x113d71b in main (error\_return\_trace.zig)
    try foo(12);
    ^

Look closely at this example. This is no stack trace.

You can see that the final error bubbled up was `PermissionDenied`, but the original error that started this whole thing was `FileNotFound`. In the `bar` function, the code handles the original error code, and then returns another one, from the switch statement. Error Return Traces make this clear, whereas a stack trace would look like this:

stack\_trace.zig

```
pub fn main() void {
    foo(12);
}

fn foo(x: i32) void {
    if (x >= 5) {
        bar();
    } else {
        bang2();
    }
}

fn bar() void {
    if (baz()) {
        quux();
    } else {
        hello();
    }
}

fn baz() bool {
    return bang1();
}

fn quux() void {
    bang2();
}

fn hello() void {
    bang2();
}

fn bang1() bool {
    return false;
}

fn bang2() void {
    @panic("PermissionDenied");
}
```

Shell

$ zig build-exe stack\_trace.zig
$ ./stack\_trace
thread 2902479 panic: PermissionDenied
/home/andy/dev/zig/doc/langref/stack\_trace.zig:38:5: 0x1140e6c in bang2 (stack\_trace.zig)
    @panic("PermissionDenied");
    ^
/home/andy/dev/zig/doc/langref/stack\_trace.zig:30:10: 0x11414ac in hello (stack\_trace.zig)
    bang2();
         ^
/home/andy/dev/zig/doc/langref/stack\_trace.zig:17:14: 0x1140e23 in bar (stack\_trace.zig)
        hello();
             ^
/home/andy/dev/zig/doc/langref/stack\_trace.zig:7:12: 0x1140ab8 in foo (stack\_trace.zig)
        bar();
           ^
/home/andy/dev/zig/doc/langref/stack\_trace.zig:2:8: 0x113f871 in main (stack\_trace.zig)
    foo(12);
       ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113eabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113e351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

Here, the stack trace does not explain how the control flow in `bar` got to the `hello()` call. One would have to open a debugger or further instrument the application in order to find out. The error return trace, on the other hand, shows exactly how the error bubbled up.

This debugging feature makes it easier to iterate quickly on code that robustly handles all error conditions. This means that Zig developers will naturally find themselves writing correct, robust code in order to increase their development pace.

Error Return Traces are enabled by default in [Debug](https://ziglang.org/documentation/0.15.2/#Debug) builds and disabled by default in [ReleaseFast](https://ziglang.org/documentation/0.15.2/#ReleaseFast), [ReleaseSafe](https://ziglang.org/documentation/0.15.2/#ReleaseSafe) and [ReleaseSmall](https://ziglang.org/documentation/0.15.2/#ReleaseSmall) builds.

There are a few ways to activate this error return tracing feature:

-   Return an error from main

-   An error makes its way to `catch unreachable` and you have not overridden the default panic handler
-   Use [errorReturnTrace](https://ziglang.org/documentation/0.15.2/#errorReturnTrace) to access the current return trace. You can use `std.debug.dumpStackTrace` to print it. This function returns comptime-known [null](https://ziglang.org/documentation/0.15.2/#null) when building without error return tracing support.

#### [Implementation Details](https://ziglang.org/documentation/0.15.2/#toc-Implementation-Details) [Â§](https://ziglang.org/documentation/0.15.2/#Implementation-Details)

To analyze performance cost, there are two cases:

-   when no errors are returned

-   when returning errors

For the case when no errors are returned, the cost is a single memory write operation, only in the first non-failable function in the call graph that calls a failable function, i.e. when a function returning `void` calls a function returning `error`. This is to initialize this struct in the stack memory:

stack\_trace\_struct.zig

```
pub const StackTrace = struct {
    index: usize,
    instruction_addresses: [N]usize,
};
```

Here, N is the maximum function call depth as determined by call graph analysis. Recursion is ignored and counts for 2.

A pointer to `StackTrace` is passed as a secret parameter to every function that can return an error, but it's always the first parameter, so it can likely sit in a register and stay there.

That's it for the path when no errors occur. It's practically free in terms of performance.

When generating the code for a function that returns an error, just before the `return` statement (only for the `return` statements that return errors), Zig generates a call to this function:

zig\_return\_error\_fn.zig

```
// marked as "no-inline" in LLVM IR
fn __zig_return_error(stack_trace: *StackTrace) void {
    stack_trace.instruction_addresses[stack_trace.index] = @returnAddress();
    stack_trace.index = (stack_trace.index + 1) % N;
}
```

The cost is 2 math operations plus some memory reads and writes. The memory accessed is constrained and should remain cached for the duration of the error return bubbling.

As for code size cost, 1 function call before a return statement is no big deal. Even so, I have [a plan](https://github.com/ziglang/zig/issues/690) to make the call to `__zig_return_error` a tail call, which brings the code size cost down to actually zero. What is a return statement in code without error return tracing can become a jump instruction in code with error return tracing.

## [Optionals](https://ziglang.org/documentation/0.15.2/#toc-Optionals) [Â§](https://ziglang.org/documentation/0.15.2/#Optionals)

One area that Zig provides safety without compromising efficiency or readability is with the optional type.

The question mark symbolizes the optional type. You can convert a type to an optional type by putting a question mark in front of it, like this:

optional\_integer.zig

```
// normal integer
const normal_int: i32 = 1234;

// optional integer
const optional_int: ?i32 = 5678;
```

Now the variable `optional_int` could be an `i32`, or `null`.

Instead of integers, let's talk about pointers. Null references are the source of many runtime exceptions, and even stand accused of being [the worst mistake of computer science](https://www.lucidchart.com/techblog/2015/08/31/the-worst-mistake-of-computer-science/).

Zig does not have them.

Instead, you can use an optional pointer. This secretly compiles down to a normal pointer, since we know we can use 0 as the null value for the optional type. But the compiler can check your work and make sure you don't assign null to something that can't be null.

Typically the downside of not having null is that it makes the code more verbose to write. But, let's compare some equivalent C code and Zig code.

Task: call malloc, if the result is null, return null.

C code

call\_malloc\_in\_c.c

```
// malloc prototype included for reference
void *malloc(size_t size);

struct Foo *do_a_thing(void) {
    char *ptr = malloc(1234);
    if (!ptr) return NULL;
    // ...
}
```

Zig code

call\_malloc\_from\_zig.zig

```
// malloc prototype included for reference
extern fn malloc(size: usize) ?[*]u8;

fn doAThing() ?*Foo {
    const ptr = malloc(1234) orelse return null;
    _ = ptr; // ...
}
```

Here, Zig is at least as convenient, if not more, than C. And, the type of "ptr" is `[*]u8` *not* `?[*]u8`. The `orelse` keyword unwrapped the optional type and therefore `ptr` is guaranteed to be non-null everywhere it is used in the function.

The other form of checking against NULL you might see looks like this:

checking\_null\_in\_c.c

```
void do_a_thing(struct Foo *foo) {
    // do some stuff

    if (foo) {
        do_something_with_foo(foo);
    }

    // do some stuff
}
```

In Zig you can accomplish the same thing:

checking\_null\_in\_zig.zig

```
const Foo = struct {};
fn doSomethingWithFoo(foo: *Foo) void {
    _ = foo;
}

fn doAThing(optional_foo: ?*Foo) void {
    // do some stuff

    if (optional_foo) |foo| {
        doSomethingWithFoo(foo);
    }

    // do some stuff
}
```

Once again, the notable thing here is that inside the if block, `foo` is no longer an optional pointer, it is a pointer, which cannot be null.

One benefit to this is that functions which take pointers as arguments can be annotated with the "nonnull" attribute - `__attribute__((nonnull))` in [GCC](https://gcc.gnu.org/onlinedocs/gcc-4.0.0/gcc/Function-Attributes.html). The optimizer can sometimes make better decisions knowing that pointer arguments cannot be null.

### [Optional Type](https://ziglang.org/documentation/0.15.2/#toc-Optional-Type) [Â§](https://ziglang.org/documentation/0.15.2/#Optional-Type)

An optional is created by putting `?` in front of a type. You can use compile-time reflection to access the child type of an optional:

test\_optional\_type.zig

```
const expect = @import("std").testing.expect;

test "optional type" {
    // Declare an optional and coerce from null:
    var foo: ?i32 = null;

    // Coerce from child type of an optional
    foo = 1234;

    // Use compile-time reflection to access the child type of the optional:
    try comptime expect(@typeInfo(@TypeOf(foo)).optional.child == i32);
}
```

Shell

$ zig test test\_optional\_type.zig
1/1 test\_optional\_type.test.optional type...OK
All 1 tests passed.

### [null](https://ziglang.org/documentation/0.15.2/#toc-null) [Â§](https://ziglang.org/documentation/0.15.2/#null)

Just like [undefined](https://ziglang.org/documentation/0.15.2/#undefined), `null` has its own type, and the only way to use it is to cast it to a different type:

null.zig

```
const optional_value: ?i32 = null;
```

### [Optional Pointers](https://ziglang.org/documentation/0.15.2/#toc-Optional-Pointers) [Â§](https://ziglang.org/documentation/0.15.2/#Optional-Pointers)

An optional pointer is guaranteed to be the same size as a pointer. The `null` of the optional is guaranteed to be address 0.

test\_optional\_pointer.zig

```
const expect = @import("std").testing.expect;

test "optional pointers" {
    // Pointers cannot be null. If you want a null pointer, use the optional
    // prefix `?` to make the pointer type optional.
    var ptr: ?*i32 = null;

    var x: i32 = 1;
    ptr = &x;

    try expect(ptr.?.* == 1);

    // Optional pointers are the same size as normal pointers, because pointer
    // value 0 is used as the null value.
    try expect(@sizeOf(?*i32) == @sizeOf(*i32));
}
```

Shell

$ zig test test\_optional\_pointer.zig
1/1 test\_optional\_pointer.test.optional pointers...OK
All 1 tests passed.

See also:

-   [while with Optionals](https://ziglang.org/documentation/0.15.2/#while-with-Optionals)

-   [if with Optionals](https://ziglang.org/documentation/0.15.2/#if-with-Optionals)

## [Casting](https://ziglang.org/documentation/0.15.2/#toc-Casting) [Â§](https://ziglang.org/documentation/0.15.2/#Casting)

A **type cast** converts a value of one type to another. Zig has [Type Coercion](https://ziglang.org/documentation/0.15.2/#Type-Coercion) for conversions that are known to be completely safe and unambiguous, and [Explicit Casts](https://ziglang.org/documentation/0.15.2/#Explicit-Casts) for conversions that one would not want to happen on accident. There is also a third kind of type conversion called [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution) for the case when a result type must be decided given multiple operand types.

### [Type Coercion](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion)

Type coercion occurs when one type is expected, but different type is provided:

test\_type\_coercion.zig

```
test "type coercion - variable declaration" {
    const a: u8 = 1;
    const b: u16 = a;
    _ = b;
}

test "type coercion - function call" {
    const a: u8 = 1;
    foo(a);
}

fn foo(b: u16) void {
    _ = b;
}

test "type coercion - @as builtin" {
    const a: u8 = 1;
    const b = @as(u16, a);
    _ = b;
}
```

Shell

$ zig test test\_type\_coercion.zig
1/3 test\_type\_coercion.test.type coercion - variable declaration...OK
2/3 test\_type\_coercion.test.type coercion - function call...OK
3/3 test\_type\_coercion.test.type coercion - @as builtin...OK
All 3 tests passed.

Type coercions are only allowed when it is completely unambiguous how to get from one type to another, and the transformation is guaranteed to be safe. There is one exception, which is [C Pointers](https://ziglang.org/documentation/0.15.2/#C-Pointers).

#### [Type Coercion: Stricter Qualification](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Stricter-Qualification) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Stricter-Qualification)

Values which have the same representation at runtime can be cast to increase the strictness of the qualifiers, no matter how nested the qualifiers are:

-   `const` - non-const to const is allowed

-   `volatile` - non-volatile to volatile is allowed
-   `align` - bigger to smaller alignment is allowed

-   [error sets](https://ziglang.org/documentation/0.15.2/#Error-Set-Type) to supersets is allowed

These casts are no-ops at runtime since the value representation does not change.

test\_no\_op\_casts.zig

```
test "type coercion - const qualification" {
    var a: i32 = 1;
    const b: *i32 = &a;
    foo(b);
}

fn foo(_: *const i32) void {}
```

Shell

$ zig test test\_no\_op\_casts.zig
1/1 test\_no\_op\_casts.test.type coercion - const qualification...OK
All 1 tests passed.

In addition, pointers coerce to const optional pointers:

test\_pointer\_coerce\_const\_optional.zig

```
const std = @import("std");
const expect = std.testing.expect;
const mem = std.mem;

test "cast *[1][*:0]const u8 to []const ?[*:0]const u8" {
    const window_name = [1][*:0]const u8{"window name"};
    const x: []const ?[*:0]const u8 = &window_name;
    try expect(mem.eql(u8, mem.span(x[0].?), "window name"));
}
```

Shell

$ zig test test\_pointer\_coerce\_const\_optional.zig
1/1 test\_pointer\_coerce\_const\_optional.test.cast \*\[1\]\[\*:0\]const u8 to \[\]const ?\[\*:0\]const u8...OK
All 1 tests passed.

#### [Type Coercion: Integer and Float Widening](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Integer-and-Float-Widening) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Integer-and-Float-Widening)

[Integers](https://ziglang.org/documentation/0.15.2/#Integers) coerce to integer types which can represent every value of the old type, and likewise [Floats](https://ziglang.org/documentation/0.15.2/#Floats) coerce to float types which can represent every value of the old type.

test\_integer\_widening.zig

```
const std = @import("std");
const builtin = @import("builtin");
const expect = std.testing.expect;
const mem = std.mem;

test "integer widening" {
    const a: u8 = 250;
    const b: u16 = a;
    const c: u32 = b;
    const d: u64 = c;
    const e: u64 = d;
    const f: u128 = e;
    try expect(f == a);
}

test "implicit unsigned integer to signed integer" {
    const a: u8 = 250;
    const b: i16 = a;
    try expect(b == 250);
}

test "float widening" {
    const a: f16 = 12.34;
    const b: f32 = a;
    const c: f64 = b;
    const d: f128 = c;
    try expect(d == a);
}
```

Shell

$ zig test test\_integer\_widening.zig
1/3 test\_integer\_widening.test.integer widening...OK
2/3 test\_integer\_widening.test.implicit unsigned integer to signed integer...OK
3/3 test\_integer\_widening.test.float widening...OK
All 3 tests passed.

#### [Type Coercion: Float to Int](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Float-to-Int) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Float-to-Int)

A compiler error is appropriate because this ambiguous expression leaves the compiler two choices about the coercion.

-   Cast `54.0` to `comptime_int` resulting in `@as(comptime_int, 10)`, which is casted to `@as(f32, 10)`

-   Cast `5` to `comptime_float` resulting in `@as(comptime_float, 10.8)`, which is casted to `@as(f32, 10.8)`

test\_ambiguous\_coercion.zig

```
// Compile time coercion of float to int
test "implicit cast to comptime_int" {
    const f: f32 = 54.0 / 5;
    _ = f;
}
```

Shell

$ zig test test\_ambiguous\_coercion.zig
/home/andy/dev/zig/doc/langref/test\_ambiguous\_coercion.zig:3:25: error: ambiguous coercion of division operands 'comptime\_float' and 'comptime\_int'; non-zero remainder '4'
    const f: f32 = 54.0 / 5;
                   \~~~~~^~~

#### [Type Coercion: Slices, Arrays and Pointers](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Slices-Arrays-and-Pointers) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Slices-Arrays-and-Pointers)

test\_coerce\_slices\_arrays\_and\_pointers.zig

```
const std = @import("std");
const expect = std.testing.expect;

// You can assign constant pointers to arrays to a slice with
// const modifier on the element type. Useful in particular for
// String literals.
test "*const [N]T to []const T" {
    const x1: []const u8 = "hello";
    const x2: []const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, x1, x2));

    const y: []const f32 = &[2]f32{ 1.2, 3.4 };
    try expect(y[0] == 1.2);
}

// Likewise, it works when the destination type is an error union.
test "*const [N]T to E![]const T" {
    const x1: anyerror![]const u8 = "hello";
    const x2: anyerror![]const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, try x1, try x2));

    const y: anyerror![]const f32 = &[2]f32{ 1.2, 3.4 };
    try expect((try y)[0] == 1.2);
}

// Likewise, it works when the destination type is an optional.
test "*const [N]T to ?[]const T" {
    const x1: ?[]const u8 = "hello";
    const x2: ?[]const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, x1.?, x2.?));

    const y: ?[]const f32 = &[2]f32{ 1.2, 3.4 };
    try expect(y.?[0] == 1.2);
}

// In this cast, the array length becomes the slice length.
test "*[N]T to []T" {
    var buf: [5]u8 = "hello".*;
    const x: []u8 = &buf;
    try expect(std.mem.eql(u8, x, "hello"));

    const buf2 = [2]f32{ 1.2, 3.4 };
    const x2: []const f32 = &buf2;
    try expect(std.mem.eql(f32, x2, &[2]f32{ 1.2, 3.4 }));
}

// Single-item pointers to arrays can be coerced to many-item pointers.
test "*[N]T to [*]T" {
    var buf: [5]u8 = "hello".*;
    const x: [*]u8 = &buf;
    try expect(x[4] == 'o');
    // x[5] would be an uncaught out of bounds pointer dereference!
}

// Likewise, it works when the destination type is an optional.
test "*[N]T to ?[*]T" {
    var buf: [5]u8 = "hello".*;
    const x: ?[*]u8 = &buf;
    try expect(x.?[4] == 'o');
}

// Single-item pointers can be cast to len-1 single-item arrays.
test "*T to *[1]T" {
    var x: i32 = 1234;
    const y: *[1]i32 = &x;
    const z: [*]i32 = y;
    try expect(z[0] == 1234);
}

// Sentinel-terminated slices can be coerced into sentinel-terminated pointers
test "[:x]T to [*:x]T" {
    const buf: [:0]const u8 = "hello";
    const buf2: [*:0]const u8 = buf;
    try expect(buf2[4] == 'o');
}
```

Shell

$ zig test test\_coerce\_slices\_arrays\_and\_pointers.zig
1/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*const \[N\]T to \[\]const T...OK
2/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*const \[N\]T to E!\[\]const T...OK
3/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*const \[N\]T to ?\[\]const T...OK
4/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*\[N\]T to \[\]T...OK
5/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*\[N\]T to \[\*\]T...OK
6/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*\[N\]T to ?\[\*\]T...OK
7/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\*T to \*\[1\]T...OK
8/8 test\_coerce\_slices\_arrays\_and\_pointers.test.\[:x\]T to \[\*:x\]T...OK
All 8 tests passed.

See also:

-   [C Pointers](https://ziglang.org/documentation/0.15.2/#C-Pointers)

#### [Type Coercion: Optionals](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Optionals) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Optionals)

The payload type of [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals), as well as [null](https://ziglang.org/documentation/0.15.2/#null), coerce to the optional type.

test\_coerce\_optionals.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "coerce to optionals" {
    const x: ?i32 = 1234;
    const y: ?i32 = null;

    try expect(x.? == 1234);
    try expect(y == null);
}
```

Shell

$ zig test test\_coerce\_optionals.zig
1/1 test\_coerce\_optionals.test.coerce to optionals...OK
All 1 tests passed.

Optionals work nested inside the [Error Union Type](https://ziglang.org/documentation/0.15.2/#Error-Union-Type), too:

test\_coerce\_optional\_wrapped\_error\_union.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "coerce to optionals wrapped in error union" {
    const x: anyerror!?i32 = 1234;
    const y: anyerror!?i32 = null;

    try expect((try x).? == 1234);
    try expect((try y) == null);
}
```

Shell

$ zig test test\_coerce\_optional\_wrapped\_error\_union.zig
1/1 test\_coerce\_optional\_wrapped\_error\_union.test.coerce to optionals wrapped in error union...OK
All 1 tests passed.

#### [Type Coercion: Error Unions](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Error-Unions) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Error-Unions)

The payload type of an [Error Union Type](https://ziglang.org/documentation/0.15.2/#Error-Union-Type) as well as the [Error Set Type](https://ziglang.org/documentation/0.15.2/#Error-Set-Type) coerce to the error union type:

test\_coerce\_to\_error\_union.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "coercion to error unions" {
    const x: anyerror!i32 = 1234;
    const y: anyerror!i32 = error.Failure;

    try expect((try x) == 1234);
    try std.testing.expectError(error.Failure, y);
}
```

Shell

$ zig test test\_coerce\_to\_error\_union.zig
1/1 test\_coerce\_to\_error\_union.test.coercion to error unions...OK
All 1 tests passed.

#### [Type Coercion: Compile-Time Known Numbers](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Compile-Time-Known-Numbers) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Compile-Time-Known-Numbers)

When a number is [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known to be representable in the destination type, it may be coerced:

test\_coerce\_large\_to\_small.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "coercing large integer type to smaller one when value is comptime-known to fit" {
    const x: u64 = 255;
    const y: u8 = x;
    try expect(y == 255);
}
```

Shell

$ zig test test\_coerce\_large\_to\_small.zig
1/1 test\_coerce\_large\_to\_small.test.coercing large integer type to smaller one when value is comptime-known to fit...OK
All 1 tests passed.

#### [Type Coercion: Unions and Enums](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Unions-and-Enums) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Unions-and-Enums)

Tagged unions can be coerced to enums, and enums can be coerced to tagged unions when they are [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known to be a field of the union that has only one possible value, such as [void](https://ziglang.org/documentation/0.15.2/#void):

test\_coerce\_unions\_enums.zig

```
const std = @import("std");
const expect = std.testing.expect;

const E = enum {
    one,
    two,
    three,
};

const U = union(E) {
    one: i32,
    two: f32,
    three,
};

const U2 = union(enum) {
    a: void,
    b: f32,

    fn tag(self: U2) usize {
        switch (self) {
            .a => return 1,
            .b => return 2,
        }
    }
};

test "coercion between unions and enums" {
    const u = U{ .two = 12.34 };
    const e: E = u; // coerce union to enum
    try expect(e == E.two);

    const three = E.three;
    const u_2: U = three; // coerce enum to union
    try expect(u_2 == E.three);

    const u_3: U = .three; // coerce enum literal to union
    try expect(u_3 == E.three);

    const u_4: U2 = .a; // coerce enum literal to union with inferred enum tag type.
    try expect(u_4.tag() == 1);

    // The following example is invalid.
    // error: coercion from enum '@TypeOf(.enum_literal)' to union 'test_coerce_unions_enum.U2' must initialize 'f32' field 'b'
    //var u_5: U2 = .b;
    //try expect(u_5.tag() == 2);
}
```

Shell

$ zig test test\_coerce\_unions\_enums.zig
1/1 test\_coerce\_unions\_enums.test.coercion between unions and enums...OK
All 1 tests passed.

See also:

-   [union](https://ziglang.org/documentation/0.15.2/#union)

-   [enum](https://ziglang.org/documentation/0.15.2/#enum)

#### [Type Coercion: undefined](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-undefined) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-undefined)

[undefined](https://ziglang.org/documentation/0.15.2/#undefined) can be coerced to any type.

#### [Type Coercion: Tuples to Arrays](https://ziglang.org/documentation/0.15.2/#toc-Type-Coercion-Tuples-to-Arrays) [Â§](https://ziglang.org/documentation/0.15.2/#Type-Coercion-Tuples-to-Arrays)

[Tuples](https://ziglang.org/documentation/0.15.2/#Tuples) can be coerced to arrays, if all of the fields have the same type.

test\_coerce\_tuples\_arrays.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Tuple = struct { u8, u8 };
test "coercion from homogeneous tuple to array" {
    const tuple: Tuple = .{ 5, 6 };
    const array: [2]u8 = tuple;
    _ = array;
}
```

Shell

$ zig test test\_coerce\_tuples\_arrays.zig
1/1 test\_coerce\_tuples\_arrays.test.coercion from homogeneous tuple to array...OK
All 1 tests passed.

### [Explicit Casts](https://ziglang.org/documentation/0.15.2/#toc-Explicit-Casts) [Â§](https://ziglang.org/documentation/0.15.2/#Explicit-Casts)

Explicit casts are performed via [Builtin Functions](https://ziglang.org/documentation/0.15.2/#Builtin-Functions). Some explicit casts are safe; some are not. Some explicit casts perform language-level assertions; some do not. Some explicit casts are no-ops at runtime; some are not.

-   [@bitCast](https://ziglang.org/documentation/0.15.2/#bitCast) - change type but maintain bit representation

-   [@alignCast](https://ziglang.org/documentation/0.15.2/#alignCast) - make a pointer have more alignment
-   [@enumFromInt](https://ziglang.org/documentation/0.15.2/#enumFromInt) - obtain an enum value based on its integer tag value

-   [@errorFromInt](https://ziglang.org/documentation/0.15.2/#errorFromInt) - obtain an error code based on its integer value
-   [@errorCast](https://ziglang.org/documentation/0.15.2/#errorCast) - convert to a smaller error set

-   [@floatCast](https://ziglang.org/documentation/0.15.2/#floatCast) - convert a larger float to a smaller float
-   [@floatFromInt](https://ziglang.org/documentation/0.15.2/#floatFromInt) - convert an integer to a float value

-   [@intCast](https://ziglang.org/documentation/0.15.2/#intCast) - convert between integer types
-   [@intFromBool](https://ziglang.org/documentation/0.15.2/#intFromBool) - convert true to 1 and false to 0

-   [@intFromEnum](https://ziglang.org/documentation/0.15.2/#intFromEnum) - obtain the integer tag value of an enum or tagged union
-   [@intFromError](https://ziglang.org/documentation/0.15.2/#intFromError) - obtain the integer value of an error code

-   [@intFromFloat](https://ziglang.org/documentation/0.15.2/#intFromFloat) - obtain the integer part of a float value
-   [@intFromPtr](https://ziglang.org/documentation/0.15.2/#intFromPtr) - obtain the address of a pointer

-   [@ptrFromInt](https://ziglang.org/documentation/0.15.2/#ptrFromInt) - convert an address to a pointer
-   [@ptrCast](https://ziglang.org/documentation/0.15.2/#ptrCast) - convert between pointer types

-   [@truncate](https://ziglang.org/documentation/0.15.2/#truncate) - convert between integer types, chopping off bits

### [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#toc-Peer-Type-Resolution) [Â§](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution)

Peer Type Resolution occurs in these places:

-   [switch](https://ziglang.org/documentation/0.15.2/#switch) expressions

-   [if](https://ziglang.org/documentation/0.15.2/#if) expressions
-   [while](https://ziglang.org/documentation/0.15.2/#while) expressions

-   [for](https://ziglang.org/documentation/0.15.2/#for) expressions
-   Multiple break statements in a block

-   Some [binary operations](https://ziglang.org/documentation/0.15.2/#Table-of-Operators)

This kind of type resolution chooses a type that all peer types can coerce into. Here are some examples:

test\_peer\_type\_resolution.zig

```
const std = @import("std");
const expect = std.testing.expect;
const mem = std.mem;

test "peer resolve int widening" {
    const a: i8 = 12;
    const b: i16 = 34;
    const c = a + b;
    try expect(c == 46);
    try expect(@TypeOf(c) == i16);
}

test "peer resolve arrays of different size to const slice" {
    try expect(mem.eql(u8, boolToStr(true), "true"));
    try expect(mem.eql(u8, boolToStr(false), "false"));
    try comptime expect(mem.eql(u8, boolToStr(true), "true"));
    try comptime expect(mem.eql(u8, boolToStr(false), "false"));
}
fn boolToStr(b: bool) []const u8 {
    return if (b) "true" else "false";
}

test "peer resolve array and const slice" {
    try testPeerResolveArrayConstSlice(true);
    try comptime testPeerResolveArrayConstSlice(true);
}
fn testPeerResolveArrayConstSlice(b: bool) !void {
    const value1 = if (b) "aoeu" else @as([]const u8, "zz");
    const value2 = if (b) @as([]const u8, "zz") else "aoeu";
    try expect(mem.eql(u8, value1, "aoeu"));
    try expect(mem.eql(u8, value2, "zz"));
}

test "peer type resolution: ?T and T" {
    try expect(peerTypeTAndOptionalT(true, false).? == 0);
    try expect(peerTypeTAndOptionalT(false, false).? == 3);
    comptime {
        try expect(peerTypeTAndOptionalT(true, false).? == 0);
        try expect(peerTypeTAndOptionalT(false, false).? == 3);
    }
}
fn peerTypeTAndOptionalT(c: bool, b: bool) ?usize {
    if (c) {
        return if (b) null else @as(usize, 0);
    }

    return @as(usize, 3);
}

test "peer type resolution: *[0]u8 and []const u8" {
    try expect(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
    try expect(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    comptime {
        try expect(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
        try expect(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    }
}
fn peerTypeEmptyArrayAndSlice(a: bool, slice: []const u8) []const u8 {
    if (a) {
        return &[_]u8{};
    }

    return slice[0..1];
}
test "peer type resolution: *[0]u8, []const u8, and anyerror![]u8" {
    {
        var data = "hi".*;
        const slice = data[0..];
        try expect((try peerTypeEmptyArrayAndSliceAndError(true, slice)).len == 0);
        try expect((try peerTypeEmptyArrayAndSliceAndError(false, slice)).len == 1);
    }
    comptime {
        var data = "hi".*;
        const slice = data[0..];
        try expect((try peerTypeEmptyArrayAndSliceAndError(true, slice)).len == 0);
        try expect((try peerTypeEmptyArrayAndSliceAndError(false, slice)).len == 1);
    }
}
fn peerTypeEmptyArrayAndSliceAndError(a: bool, slice: []u8) anyerror![]u8 {
    if (a) {
        return &[_]u8{};
    }

    return slice[0..1];
}

test "peer type resolution: *const T and ?*T" {
    const a: *const usize = @ptrFromInt(0x123456780);
    const b: ?*usize = @ptrFromInt(0x123456780);
    try expect(a == b);
    try expect(b == a);
}

test "peer type resolution: error union switch" {
    // The non-error and error cases are only peers if the error case is just a switch expression;
    // the pattern `if (x) {...} else |err| blk: { switch (err) {...} }` does not consider the
    // non-error and error case to be peers.
    var a: error{ A, B, C }!u32 = 0;
    _ = &a;
    const b = if (a) |x|
        x + 3
    else |err| switch (err) {
        error.A => 0,
        error.B => 1,
        error.C => null,
    };
    try expect(@TypeOf(b) == ?u32);

    // The non-error and error cases are only peers if the error case is just a switch expression;
    // the pattern `x catch |err| blk: { switch (err) {...} }` does not consider the unwrapped `x`
    // and error case to be peers.
    const c = a catch |err| switch (err) {
        error.A => 0,
        error.B => 1,
        error.C => null,
    };
    try expect(@TypeOf(c) == ?u32);
}
```

Shell

$ zig test test\_peer\_type\_resolution.zig
1/8 test\_peer\_type\_resolution.test.peer resolve int widening...OK
2/8 test\_peer\_type\_resolution.test.peer resolve arrays of different size to const slice...OK
3/8 test\_peer\_type\_resolution.test.peer resolve array and const slice...OK
4/8 test\_peer\_type\_resolution.test.peer type resolution: ?T and T...OK
5/8 test\_peer\_type\_resolution.test.peer type resolution: \*\[0\]u8 and \[\]const u8...OK
6/8 test\_peer\_type\_resolution.test.peer type resolution: \*\[0\]u8, \[\]const u8, and anyerror!\[\]u8...OK
7/8 test\_peer\_type\_resolution.test.peer type resolution: \*const T and ?\*T...OK
8/8 test\_peer\_type\_resolution.test.peer type resolution: error union switch...OK
All 8 tests passed.

## [Zero Bit Types](https://ziglang.org/documentation/0.15.2/#toc-Zero-Bit-Types) [Â§](https://ziglang.org/documentation/0.15.2/#Zero-Bit-Types)

For some types, [@sizeOf](https://ziglang.org/documentation/0.15.2/#sizeOf) is 0:

-   [void](https://ziglang.org/documentation/0.15.2/#void)

-   The [Integers](https://ziglang.org/documentation/0.15.2/#Integers) `u0` and `i0`.
-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) with len 0, or with an element type that is a zero bit type.

-   An [enum](https://ziglang.org/documentation/0.15.2/#enum) with only 1 tag.
-   A [struct](https://ziglang.org/documentation/0.15.2/#struct) with all fields being zero bit types.

-   A [union](https://ziglang.org/documentation/0.15.2/#union) with only 1 field which is a zero bit type.

These types can only ever have one possible value, and thus require 0 bits to represent. Code that makes use of these types is not included in the final generated code:

zero\_bit\_types.zig

```
export fn entry() void {
    var x: void = {};
    var y: void = {};
    x = y;
    y = x;
}
```

When this turns into machine code, there is no code generated in the body of `entry`, even in [Debug](https://ziglang.org/documentation/0.15.2/#Debug) mode. For example, on x86\_64:

```
0000000000000010 <entry>:
  10:	55                   	push   %rbp
  11:	48 89 e5             	mov    %rsp,%rbp
  14:	5d                   	pop    %rbp
  15:	c3                   	retq   
```

These assembly instructions do not have any code associated with the void values - they only perform the function call prologue and epilogue.

### [void](https://ziglang.org/documentation/0.15.2/#toc-void) [Â§](https://ziglang.org/documentation/0.15.2/#void)

`void` can be useful for instantiating generic types. For example, given a `Map(Key, Value)`, one can pass `void` for the `Value` type to make it into a `Set`:

test\_void\_in\_hashmap.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "turn HashMap into a set with void" {
    var map = std.AutoHashMap(i32, void).init(std.testing.allocator);
    defer map.deinit();

    try map.put(1, {});
    try map.put(2, {});

    try expect(map.contains(2));
    try expect(!map.contains(3));

    _ = map.remove(2);
    try expect(!map.contains(2));
}
```

Shell

$ zig test test\_void\_in\_hashmap.zig
1/1 test\_void\_in\_hashmap.test.turn HashMap into a set with void...OK
All 1 tests passed.

Note that this is different from using a dummy value for the hash map value. By using `void` as the type of the value, the hash map entry type has no value field, and thus the hash map takes up less space. Further, all the code that deals with storing and loading the value is deleted, as seen above.

`void` is distinct from `anyopaque`. `void` has a known size of 0 bytes, and `anyopaque` has an unknown, but non-zero, size.

Expressions of type `void` are the only ones whose value can be ignored. For example, ignoring a non-`void` expression is a compile error:

test\_expression\_ignored.zig

```
test "ignoring expression value" {
    foo();
}

fn foo() i32 {
    return 1234;
}
```

Shell

$ zig test test\_expression\_ignored.zig
/home/andy/dev/zig/doc/langref/test\_expression\_ignored.zig:2:8: error: value of type 'i32' ignored
    foo();
    \~~~^~
/home/andy/dev/zig/doc/langref/test\_expression\_ignored.zig:2:8: note: all non-void values must be used
/home/andy/dev/zig/doc/langref/test\_expression\_ignored.zig:2:8: note: to discard the value, assign it to '\_'

However, if the expression has type `void`, there will be no error. Expression results can be explicitly ignored by assigning them to `_`.

test\_void\_ignored.zig

```
test "void is ignored" {
    returnsVoid();
}

test "explicitly ignoring expression value" {
    _ = foo();
}

fn returnsVoid() void {}

fn foo() i32 {
    return 1234;
}
```

Shell

$ zig test test\_void\_ignored.zig
1/2 test\_void\_ignored.test.void is ignored...OK
2/2 test\_void\_ignored.test.explicitly ignoring expression value...OK
All 2 tests passed.

## [Result Location Semantics](https://ziglang.org/documentation/0.15.2/#toc-Result-Location-Semantics) [Â§](https://ziglang.org/documentation/0.15.2/#Result-Location-Semantics)

During compilation, every Zig expression and sub-expression is assigned optional result location information. This information dictates what type the expression should have (its result type), and where the resulting value should be placed in memory (its result location). The information is optional in the sense that not every expression has this information: assignment to `_`, for instance, does not provide any information about the type of an expression, nor does it provide a concrete memory location to place it in.

As a motivating example, consider the statement `const x: u32 = 42;`. The type annotation here provides a result type of `u32` to the initialization expression `42`, instructing the compiler to coerce this integer (initially of type `comptime_int`) to this type. We will see more examples shortly.

This is not an implementation detail: the logic outlined above is codified into the Zig language specification, and is the primary mechanism of type inference in the language. This system is collectively referred to as "Result Location Semantics".

### [Result Types](https://ziglang.org/documentation/0.15.2/#toc-Result-Types) [Â§](https://ziglang.org/documentation/0.15.2/#Result-Types)

Result types are propagated recursively through expressions where possible. For instance, if the expression `&e` has result type `*u32`, then `e` is given a result type of `u32`, allowing the language to perform this coercion before taking a reference.

The result type mechanism is utilized by casting builtins such as `@intCast`. Rather than taking as an argument the type to cast to, these builtins use their result type to determine this information. The result type is often known from context; where it is not, the `@as` builtin can be used to explicitly provide a result type.

We can break down the result types for each component of a simple expression as follows:

result\_type\_propagation.zig

```
const expectEqual = @import("std").testing.expectEqual;
test "result type propagates through struct initializer" {
    const S = struct { x: u32 };
    const val: u64 = 123;
    const s: S = .{ .x = @intCast(val) };
    // .{ .x = @intCast(val) }   has result type `S` due to the type annotation
    //         @intCast(val)     has result type `u32` due to the type of the field `S.x`
    //                  val      has no result type, as it is permitted to be any integer type
    try expectEqual(@as(u32, 123), s.x);
}
```

Shell

$ zig test result\_type\_propagation.zig
1/1 result\_type\_propagation.test.result type propagates through struct initializer...OK
All 1 tests passed.

This result type information is useful for the aforementioned cast builtins, as well as to avoid the construction of pre-coercion values, and to avoid the need for explicit type coercions in some cases. The following table details how some common expressions propagate result types, where `x` and `y` are arbitrary sub-expressions.

Expression

Parent Result Type

Sub-expression Result Type

`const val: T = x`

\-

`x` is a `T`

`var val: T = x`

\-

`x` is a `T`

`val = x`

\-

`x` is a `@TypeOf(val)`

`@as(T, x)`

\-

`x` is a `T`

`&x`

`*T`

`x` is a `T`

`&x`

`[]T`

`x` is some array of `T`

`f(x)`

\-

`x` has the type of the first parameter of `f`

`.{x}`

`T`

`x` is a `@FieldType(T, "0")`

`.{ .a = x }`

`T`

`x` is a `@FieldType(T, "a")`

`T{x}`

\-

`x` is a `@FieldType(T, "0")`

`T{ .a = x }`

\-

`x` is a `@FieldType(T, "a")`

`@Type(x)`

\-

`x` is a `std.builtin.Type`

`@typeInfo(x)`

\-

`x` is a `type`

`x << y`

\-

`y` is a `std.math.Log2IntCeil(@TypeOf(x))`

### [Result Locations](https://ziglang.org/documentation/0.15.2/#toc-Result-Locations) [Â§](https://ziglang.org/documentation/0.15.2/#Result-Locations)

In addition to result type information, every expression may be optionally assigned a result location: a pointer to which the value must be directly written. This system can be used to prevent intermediate copies when initializing data structures, which can be important for types which must have a fixed memory address ("pinned" types).

When compiling the simple assignment expression `x = e`, many languages would create the temporary value `e` on the stack, and then assign it to `x`, potentially performing a type coercion in the process. Zig approaches this differently. The expression `e` is given a result type matching the type of `x`, and a result location of `&x`. For many syntactic forms of `e`, this has no practical impact. However, it can have important semantic effects when working with more complex syntax forms.

For instance, if the expression `.{ .a = x, .b = y }` has a result location of `ptr`, then `x` is given a result location of `&ptr.a`, and `y` a result location of `&ptr.b`. Without this system, this expression would construct a temporary struct value entirely on the stack, and only then copy it to the destination address. In essence, Zig desugars the assignment `foo = .{ .a = x, .b = y }` to the two statements `foo.a = x; foo.b = y;`.

This can sometimes be important when assigning an aggregate value where the initialization expression depends on the previous value of the aggregate. The easiest way to demonstrate this is by attempting to swap fields of a struct or array - the following logic looks sound, but in fact is not:

result\_location\_interfering\_with\_swap.zig

```
const expect = @import("std").testing.expect;
test "attempt to swap array elements with array initializer" {
    var arr: [2]u32 = .{ 1, 2 };
    arr = .{ arr[1], arr[0] };
    // The previous line is equivalent to the following two lines:
    //   arr[0] = arr[1];
    //   arr[1] = arr[0];
    // So this fails!
    try expect(arr[0] == 2); // succeeds
    try expect(arr[1] == 1); // fails
}
```

Shell

$ zig test result\_location\_interfering\_with\_swap.zig
1/1 result\_location\_interfering\_with\_swap.test.attempt to swap array elements with array initializer...FAIL (TestUnexpectedResult)
/home/andy/dev/zig/lib/std/testing.zig:607:14: 0x102f019 in expect (std.zig)
    if (!ok) return error.TestUnexpectedResult;
             ^
/home/andy/dev/zig/doc/langref/result\_location\_interfering\_with\_swap.zig:10:5: 0x102f144 in test.attempt to swap array elements with array initializer (result\_location\_interfering\_with\_swap.zig)
    try expect(arr\[1\] == 1); // fails
    ^
0 passed; 0 skipped; 1 failed.
error: the following test command failed with exit code 1:
/home/andy/dev/zig/.zig-cache/o/d439bc8d3e0f685e13e3c778e438793a/test --seed=0x9b2332d1

The following table details how some common expressions propagate result locations, where `x` and `y` are arbitrary sub-expressions. Note that some expressions cannot provide meaningful result locations to sub-expressions, even if they themselves have a result location.

Expression

Result Location

Sub-expression Result Locations

`const val: T = x`

\-

`x` has result location `&val`

`var val: T = x`

\-

`x` has result location `&val`

`val = x`

\-

`x` has result location `&val`

`@as(T, x)`

`ptr`

`x` has no result location

`&x`

`ptr`

`x` has no result location

`f(x)`

`ptr`

`x` has no result location

`.{x}`

`ptr`

`x` has result location `&ptr[0]`

`.{ .a = x }`

`ptr`

`x` has result location `&ptr.a`

`T{x}`

`ptr`

`x` has no result location (typed initializers do not propagate result locations)

`T{ .a = x }`

`ptr`

`x` has no result location (typed initializers do not propagate result locations)

`@Type(x)`

`ptr`

`x` has no result location

`@typeInfo(x)`

`ptr`

`x` has no result location

`x << y`

`ptr`

`x` and `y` do not have result locations

## [comptime](https://ziglang.org/documentation/0.15.2/#toc-comptime) [Â§](https://ziglang.org/documentation/0.15.2/#comptime)

Zig places importance on the concept of whether an expression is known at compile-time. There are a few different places this concept is used, and these building blocks are used to keep the language small, readable, and powerful.

### [Introducing the Compile-Time Concept](https://ziglang.org/documentation/0.15.2/#toc-Introducing-the-Compile-Time-Concept) [Â§](https://ziglang.org/documentation/0.15.2/#Introducing-the-Compile-Time-Concept)

#### [Compile-Time Parameters](https://ziglang.org/documentation/0.15.2/#toc-Compile-Time-Parameters) [Â§](https://ziglang.org/documentation/0.15.2/#Compile-Time-Parameters)

Compile-time parameters is how Zig implements generics. It is compile-time duck typing.

compile-time\_duck\_typing.zig

```
fn max(comptime T: type, a: T, b: T) T {
    return if (a > b) a else b;
}
fn gimmeTheBiggerFloat(a: f32, b: f32) f32 {
    return max(f32, a, b);
}
fn gimmeTheBiggerInteger(a: u64, b: u64) u64 {
    return max(u64, a, b);
}
```

In Zig, types are first-class citizens. They can be assigned to variables, passed as parameters to functions, and returned from functions. However, they can only be used in expressions which are known at *compile-time*, which is why the parameter `T` in the above snippet must be marked with `comptime`.

A `comptime` parameter means that:

-   At the callsite, the value must be known at compile-time, or it is a compile error.

-   In the function definition, the value is known at compile-time.

For example, if we were to introduce another function to the above snippet:

test\_unresolved\_comptime\_value.zig

```
fn max(comptime T: type, a: T, b: T) T {
    return if (a > b) a else b;
}
test "try to pass a runtime type" {
    foo(false);
}
fn foo(condition: bool) void {
    const result = max(if (condition) f32 else u64, 1234, 5678);
    _ = result;
}
```

Shell

$ zig test test\_unresolved\_comptime\_value.zig
/home/andy/dev/zig/doc/langref/test\_unresolved\_comptime\_value.zig:8:28: error: unable to resolve comptime value
    const result = max(if (condition) f32 else u64, 1234, 5678);
                           ^~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_unresolved\_comptime\_value.zig:8:24: note: argument to comptime parameter must be comptime-known
    const result = max(if (condition) f32 else u64, 1234, 5678);
                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_unresolved\_comptime\_value.zig:1:8: note: parameter declared comptime here
fn max(comptime T: type, a: T, b: T) T {
       ^~~~~~~~
referenced by:
    test.try to pass a runtime type: /home/andy/dev/zig/doc/langref/test\_unresolved\_comptime\_value.zig:5:8

This is an error because the programmer attempted to pass a value only known at run-time to a function which expects a value known at compile-time.

Another way to get an error is if we pass a type that violates the type checker when the function is analyzed. This is what it means to have *compile-time duck typing*.

For example:

test\_comptime\_mismatched\_type.zig

```
fn max(comptime T: type, a: T, b: T) T {
    return if (a > b) a else b;
}
test "try to compare bools" {
    _ = max(bool, true, false);
}
```

Shell

$ zig test test\_comptime\_mismatched\_type.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_mismatched\_type.zig:2:18: error: operator > not allowed for type 'bool'
    return if (a > b) a else b;
               ~~^~~
referenced by:
    test.try to compare bools: /home/andy/dev/zig/doc/langref/test\_comptime\_mismatched\_type.zig:5:12

On the flip side, inside the function definition with the `comptime` parameter, the value is known at compile-time. This means that we actually could make this work for the bool type if we wanted to:

test\_comptime\_max\_with\_bool.zig

```
fn max(comptime T: type, a: T, b: T) T {
    if (T == bool) {
        return a or b;
    } else if (a > b) {
        return a;
    } else {
        return b;
    }
}
test "try to compare bools" {
    try @import("std").testing.expect(max(bool, false, true) == true);
}
```

Shell

$ zig test test\_comptime\_max\_with\_bool.zig
1/1 test\_comptime\_max\_with\_bool.test.try to compare bools...OK
All 1 tests passed.

This works because Zig implicitly inlines `if` expressions when the condition is known at compile-time, and the compiler guarantees that it will skip analysis of the branch not taken.

This means that the actual function generated for `max` in this situation looks like this:

compiler\_generated\_function.zig

```
fn max(a: bool, b: bool) bool {
    {
        return a or b;
    }
}
```

All the code that dealt with compile-time known values is eliminated and we are left with only the necessary run-time code to accomplish the task.

This works the same way for `switch` expressions - they are implicitly inlined when the target expression is compile-time known.

#### [Compile-Time Variables](https://ziglang.org/documentation/0.15.2/#toc-Compile-Time-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Compile-Time-Variables)

In Zig, the programmer can label variables as `comptime`. This guarantees to the compiler that every load and store of the variable is performed at compile-time. Any violation of this results in a compile error.

This combined with the fact that we can `inline` loops allows us to write a function which is partially evaluated at compile-time and partially at run-time.

For example:

test\_comptime\_evaluation.zig

```
const expect = @import("std").testing.expect;

const CmdFn = struct {
    name: []const u8,
    func: fn (i32) i32,
};

const cmd_fns = [_]CmdFn{
    CmdFn{ .name = "one", .func = one },
    CmdFn{ .name = "two", .func = two },
    CmdFn{ .name = "three", .func = three },
};
fn one(value: i32) i32 {
    return value + 1;
}
fn two(value: i32) i32 {
    return value + 2;
}
fn three(value: i32) i32 {
    return value + 3;
}

fn performFn(comptime prefix_char: u8, start_value: i32) i32 {
    var result: i32 = start_value;
    comptime var i = 0;
    inline while (i < cmd_fns.len) : (i += 1) {
        if (cmd_fns[i].name[0] == prefix_char) {
            result = cmd_fns[i].func(result);
        }
    }
    return result;
}

test "perform fn" {
    try expect(performFn('t', 1) == 6);
    try expect(performFn('o', 0) == 1);
    try expect(performFn('w', 99) == 99);
}
```

Shell

$ zig test test\_comptime\_evaluation.zig
1/1 test\_comptime\_evaluation.test.perform fn...OK
All 1 tests passed.

This example is a bit contrived, because the compile-time evaluation component is unnecessary; this code would work fine if it was all done at run-time. But it does end up generating different code. In this example, the function `performFn` is generated three different times, for the different values of `prefix_char` provided:

performFn\_1

```
// From the line:
// expect(performFn('t', 1) == 6);
fn performFn(start_value: i32) i32 {
    var result: i32 = start_value;
    result = two(result);
    result = three(result);
    return result;
}
```

performFn\_2

```
// From the line:
// expect(performFn('o', 0) == 1);
fn performFn(start_value: i32) i32 {
    var result: i32 = start_value;
    result = one(result);
    return result;
}
```

performFn\_3

```
// From the line:
// expect(performFn('w', 99) == 99);
fn performFn(start_value: i32) i32 {
    var result: i32 = start_value;
    _ = &result;
    return result;
}
```

Note that this happens even in a debug build. This is not a way to write more optimized code, but it is a way to make sure that what *should* happen at compile-time, *does* happen at compile-time. This catches more errors and allows expressiveness that in other languages requires using macros, generated code, or a preprocessor to accomplish.

#### [Compile-Time Expressions](https://ziglang.org/documentation/0.15.2/#toc-Compile-Time-Expressions) [Â§](https://ziglang.org/documentation/0.15.2/#Compile-Time-Expressions)

In Zig, it matters whether a given expression is known at compile-time or run-time. A programmer can use a `comptime` expression to guarantee that the expression will be evaluated at compile-time. If this cannot be accomplished, the compiler will emit an error. For example:

test\_comptime\_call\_extern\_function.zig

```
extern fn exit() noreturn;

test "foo" {
    comptime {
        exit();
    }
}
```

Shell

$ zig test test\_comptime\_call\_extern\_function.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_call\_extern\_function.zig:5:13: error: comptime call of extern function
        exit();
        \~~~~^~
/home/andy/dev/zig/doc/langref/test\_comptime\_call\_extern\_function.zig:4:5: note: 'comptime' keyword forces comptime evaluation
    comptime {
    ^~~~~~~~

It doesn't make sense that a program could call `exit()` (or any other external function) at compile-time, so this is a compile error. However, a `comptime` expression does much more than sometimes cause a compile error.

Within a `comptime` expression:

-   All variables are `comptime` variables.

-   All `if`, `while`, `for`, and `switch` expressions are evaluated at compile-time, or emit a compile error if this is not possible.
-   All `return` and `try` expressions are invalid (unless the function itself is called at compile-time).

-   All code with runtime side effects or depending on runtime values emits a compile error.
-   All function calls cause the compiler to interpret the function at compile-time, emitting a compile error if the function tries to do something that has global runtime side effects.

This means that a programmer can create a function which is called both at compile-time and run-time, with no modification to the function required.

Let's look at an example:

test\_fibonacci\_recursion.zig

```
const expect = @import("std").testing.expect;

fn fibonacci(index: u32) u32 {
    if (index < 2) return index;
    return fibonacci(index - 1) + fibonacci(index - 2);
}

test "fibonacci" {
    // test fibonacci at run-time
    try expect(fibonacci(7) == 13);

    // test fibonacci at compile-time
    try comptime expect(fibonacci(7) == 13);
}
```

Shell

$ zig test test\_fibonacci\_recursion.zig
1/1 test\_fibonacci\_recursion.test.fibonacci...OK
All 1 tests passed.

Imagine if we had forgotten the base case of the recursive function and tried to run the tests:

test\_fibonacci\_comptime\_overflow.zig

```
const expect = @import("std").testing.expect;

fn fibonacci(index: u32) u32 {
    //if (index < 2) return index;
    return fibonacci(index - 1) + fibonacci(index - 2);
}

test "fibonacci" {
    try comptime expect(fibonacci(7) == 13);
}
```

Shell

$ zig test test\_fibonacci\_comptime\_overflow.zig
/home/andy/dev/zig/doc/langref/test\_fibonacci\_comptime\_overflow.zig:5:28: error: overflow of integer type 'u32' with value '-1'
    return fibonacci(index - 1) + fibonacci(index - 2);
                     \~~~~~~^~~
/home/andy/dev/zig/doc/langref/test\_fibonacci\_comptime\_overflow.zig:5:21: note: called at comptime here (7 times)    return fibonacci(index - 1) + fibonacci(index - 2);
           \~~~~~~~~~^~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_fibonacci\_comptime\_overflow.zig:9:34: note: called at comptime here
    try comptime expect(fibonacci(7) == 13);
                        \~~~~~~~~~^~~

The compiler produces an error which is a stack trace from trying to evaluate the function at compile-time.

Luckily, we used an unsigned integer, and so when we tried to subtract 1 from 0, it triggered [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior), which is always a compile error if the compiler knows it happened. But what would have happened if we used a signed integer?

fibonacci\_comptime\_infinite\_recursion.zig

```
const assert = @import("std").debug.assert;

fn fibonacci(index: i32) i32 {
    //if (index < 2) return index;
    return fibonacci(index - 1) + fibonacci(index - 2);
}

test "fibonacci" {
    try comptime assert(fibonacci(7) == 13);
}
```

The compiler is supposed to notice that evaluating this function at compile-time took more than 1000 branches, and thus emits an error and gives up. If the programmer wants to increase the budget for compile-time computation, they can use a built-in function called [@setEvalBranchQuota](https://ziglang.org/documentation/0.15.2/#setEvalBranchQuota) to change the default number 1000 to something else.

However, there is a [design flaw in the compiler](https://github.com/ziglang/zig/issues/13724) causing it to stack overflow instead of having the proper behavior here. I'm terribly sorry about that. I hope to get this resolved before the next release.

What if we fix the base case, but put the wrong value in the `expect` line?

test\_fibonacci\_comptime\_unreachable.zig

```
const assert = @import("std").debug.assert;

fn fibonacci(index: i32) i32 {
    if (index < 2) return index;
    return fibonacci(index - 1) + fibonacci(index - 2);
}

test "fibonacci" {
    try comptime assert(fibonacci(7) == 99999);
}
```

Shell

$ zig test test\_fibonacci\_comptime\_unreachable.zig
/home/andy/dev/zig/lib/std/debug.zig:559:14: error: reached unreachable code
    if (!ok) unreachable; // assertion failure
             ^~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_fibonacci\_comptime\_unreachable.zig:9:24: note: called at comptime here
    try comptime assert(fibonacci(7) == 99999);
                 \~~~~~~^~~~~~~~~~~~~~~~~~~~~~~

At [container](https://ziglang.org/documentation/0.15.2/#Containers) level (outside of any function), all expressions are implicitly `comptime` expressions. This means that we can use functions to initialize complex static data. For example:

test\_container-level\_comptime\_expressions.zig

```
const first_25_primes = firstNPrimes(25);
const sum_of_first_25_primes = sum(&first_25_primes);

fn firstNPrimes(comptime n: usize) [n]i32 {
    var prime_list: [n]i32 = undefined;
    var next_index: usize = 0;
    var test_number: i32 = 2;
    while (next_index < prime_list.len) : (test_number += 1) {
        var test_prime_index: usize = 0;
        var is_prime = true;
        while (test_prime_index < next_index) : (test_prime_index += 1) {
            if (test_number % prime_list[test_prime_index] == 0) {
                is_prime = false;
                break;
            }
        }
        if (is_prime) {
            prime_list[next_index] = test_number;
            next_index += 1;
        }
    }
    return prime_list;
}

fn sum(numbers: []const i32) i32 {
    var result: i32 = 0;
    for (numbers) |x| {
        result += x;
    }
    return result;
}

test "variable values" {
    try @import("std").testing.expect(sum_of_first_25_primes == 1060);
}
```

Shell

$ zig test test\_container-level\_comptime\_expressions.zig
1/1 test\_container-level\_comptime\_expressions.test.variable values...OK
All 1 tests passed.

When we compile this program, Zig generates the constants with the answer pre-computed. Here are the lines from the generated LLVM IR:

```
@0 = internal unnamed_addr constant [25 x i32] [i32 2, i32 3, i32 5, i32 7, i32 11, i32 13, i32 17, i32 19, i32 23, i32 29, i32 31, i32 37, i32 41, i32 43, i32 47, i32 53, i32 59, i32 61, i32 67, i32 71, i32 73, i32 79, i32 83, i32 89, i32 97]
@1 = internal unnamed_addr constant i32 1060
```

Note that we did not have to do anything special with the syntax of these functions. For example, we could call the `sum` function as is with a slice of numbers whose length and values were only known at run-time.

### [Generic Data Structures](https://ziglang.org/documentation/0.15.2/#toc-Generic-Data-Structures) [Â§](https://ziglang.org/documentation/0.15.2/#Generic-Data-Structures)

Zig uses comptime capabilities to implement generic data structures without introducing any special-case syntax.

Here is an example of a generic `List` data structure.

generic\_data\_structure.zig

```
fn List(comptime T: type) type {
    return struct {
        items: []T,
        len: usize,
    };
}

// The generic List data structure can be instantiated by passing in a type:
var buffer: [10]i32 = undefined;
var list = List(i32){
    .items = &buffer,
    .len = 0,
};
```

That's it. It's a function that returns an anonymous `struct`. For the purposes of error messages and debugging, Zig infers the name `"List(i32)"` from the function name and parameters invoked when creating the anonymous struct.

To explicitly give a type a name, we assign it to a constant.

anonymous\_struct\_name.zig

```
const Node = struct {
    next: ?*Node,
    name: []const u8,
};

var node_a = Node{
    .next = null,
    .name = "Node A",
};

var node_b = Node{
    .next = &node_a,
    .name = "Node B",
};
```

In this example, the `Node` struct refers to itself. This works because all top level declarations are order-independent. As long as the compiler can determine the size of the struct, it is free to refer to itself. In this case, `Node` refers to itself as a pointer, which has a well-defined size at compile time, so it works fine.

### [Case Study: print in Zig](https://ziglang.org/documentation/0.15.2/#toc-Case-Study-print-in-Zig) [Â§](https://ziglang.org/documentation/0.15.2/#Case-Study-print-in-Zig)

Putting all of this together, let's see how `print` works in Zig.

print.zig

```
const print = @import("std").debug.print;

const a_number: i32 = 1234;
const a_string = "foobar";

pub fn main() void {
    print("here is a string: '{s}' here is a number: {}\n", .{ a_string, a_number });
}
```

Shell

$ zig build-exe print.zig
$ ./print
here is a string: 'foobar' here is a number: 1234

Let's crack open the implementation of this and see how it works:

poc\_print\_fn.zig

```
const Writer = struct {
    /// Calls print and then flushes the buffer.
    pub fn print(self: *Writer, comptime format: []const u8, args: anytype) anyerror!void {
        const State = enum {
            start,
            open_brace,
            close_brace,
        };

        comptime var start_index: usize = 0;
        comptime var state = State.start;
        comptime var next_arg: usize = 0;

        inline for (format, 0..) |c, i| {
            switch (state) {
                State.start => switch (c) {
                    '{' => {
                        if (start_index < i) try self.write(format[start_index..i]);
                        state = State.open_brace;
                    },
                    '}' => {
                        if (start_index < i) try self.write(format[start_index..i]);
                        state = State.close_brace;
                    },
                    else => {},
                },
                State.open_brace => switch (c) {
                    '{' => {
                        state = State.start;
                        start_index = i;
                    },
                    '}' => {
                        try self.printValue(args[next_arg]);
                        next_arg += 1;
                        state = State.start;
                        start_index = i + 1;
                    },
                    's' => {
                        continue;
                    },
                    else => @compileError("Unknown format character: " ++ [1]u8{c}),
                },
                State.close_brace => switch (c) {
                    '}' => {
                        state = State.start;
                        start_index = i;
                    },
                    else => @compileError("Single '}' encountered in format string"),
                },
            }
        }
        comptime {
            if (args.len != next_arg) {
                @compileError("Unused arguments");
            }
            if (state != State.start) {
                @compileError("Incomplete format string: " ++ format);
            }
        }
        if (start_index < format.len) {
            try self.write(format[start_index..format.len]);
        }
        try self.flush();
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
    pub fn printValue(self: *Writer, value: anytype) !void {
        _ = self;
        _ = value;
    }
    fn flush(self: *Writer) !void {
        _ = self;
    }
};
```

This is a proof of concept implementation; the actual function in the standard library has more formatting capabilities.

Note that this is not hard-coded into the Zig compiler; this is userland code in the standard library.

When this function is analyzed from our example code above, Zig partially evaluates the function and emits a function that actually looks like this:

Emitted print Function

```
pub fn print(self: *Writer, arg0: []const u8, arg1: i32) !void {
    try self.write("here is a string: '");
    try self.printValue(arg0);
    try self.write("' here is a number: ");
    try self.printValue(arg1);
    try self.write("\n");
    try self.flush();
}
```

`printValue` is a function that takes a parameter of any type, and does different things depending on the type:

poc\_printValue\_fn.zig

```
const Writer = struct {
    pub fn printValue(self: *Writer, value: anytype) !void {
        switch (@typeInfo(@TypeOf(value))) {
            .int => {
                return self.writeInt(value);
            },
            .float => {
                return self.writeFloat(value);
            },
            .pointer => {
                return self.write(value);
            },
            else => {
                @compileError("Unable to print type '" ++ @typeName(@TypeOf(value)) ++ "'");
            },
        }
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
    fn writeInt(self: *Writer, value: anytype) !void {
        _ = self;
        _ = value;
    }
    fn writeFloat(self: *Writer, value: anytype) !void {
        _ = self;
        _ = value;
    }
};
```

And now, what happens if we give too many arguments to `print`?

test\_print\_too\_many\_args.zig

```
const print = @import("std").debug.print;

const a_number: i32 = 1234;
const a_string = "foobar";

test "print too many arguments" {
    print("here is a string: '{s}' here is a number: {}\n", .{
        a_string,
        a_number,
        a_number,
    });
}
```

Shell

$ zig test test\_print\_too\_many\_args.zig
/home/andy/dev/zig/lib/std/Io/Writer.zig:717:18: error: unused argument in 'here is a string: '{s}' here is a number: {}
                                                        '
            1 => @compileError("unused argument in '" ++ fmt ++ "'"),
                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
referenced by:
    print\_\_anon\_454: /home/andy/dev/zig/lib/std/debug.zig:231:23
    test.print too many arguments: /home/andy/dev/zig/doc/langref/test\_print\_too\_many\_args.zig:7:10

Zig gives programmers the tools needed to protect themselves against their own mistakes.

Zig doesn't care whether the format argument is a string literal, only that it is a compile-time known value that can be coerced to a `[]const u8`:

print\_comptime-known\_format.zig

```
const print = @import("std").debug.print;

const a_number: i32 = 1234;
const a_string = "foobar";
const fmt = "here is a string: '{s}' here is a number: {}\n";

pub fn main() void {
    print(fmt, .{ a_string, a_number });
}
```

Shell

$ zig build-exe print\_comptime-known\_format.zig
$ ./print\_comptime-known\_format
here is a string: 'foobar' here is a number: 1234

This works fine.

Zig does not special case string formatting in the compiler and instead exposes enough power to accomplish this task in userland. It does so without introducing another language on top of Zig, such as a macro language or a preprocessor language. It's Zig all the way down.

See also:

-   [inline while](https://ziglang.org/documentation/0.15.2/#inline-while)

-   [inline for](https://ziglang.org/documentation/0.15.2/#inline-for)

## [Assembly](https://ziglang.org/documentation/0.15.2/#toc-Assembly) [Â§](https://ziglang.org/documentation/0.15.2/#Assembly)

For some use cases, it may be necessary to directly control the machine code generated by Zig programs, rather than relying on Zig's code generation. For these cases, one can use inline assembly. Here is an example of implementing Hello, World on x86\_64 Linux using inline assembly:

inline\_assembly.zig

```
pub fn main() noreturn {
    const msg = "hello world\n";
    _ = syscall3(SYS_write, STDOUT_FILENO, @intFromPtr(msg), msg.len);
    _ = syscall1(SYS_exit, 0);
    unreachable;
}

pub const SYS_write = 1;
pub const SYS_exit = 60;

pub const STDOUT_FILENO = 1;

pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
        : .{ .rcx = true, .r11 = true });
}

pub fn syscall3(number: usize, arg1: usize, arg2: usize, arg3: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
          [arg2] "{rsi}" (arg2),
          [arg3] "{rdx}" (arg3),
        : .{ .rcx = true, .r11 = true });
}
```

Shell

$ zig build-exe inline\_assembly.zig -target x86\_64-linux
$ ./inline\_assembly
hello world

Dissecting the syntax:

Assembly Syntax Explained.zig

```
pub fn syscall1(number: usize, arg1: usize) usize {
    // Inline assembly is an expression which returns a value.
    // the `asm` keyword begins the expression.
    return asm
    // `volatile` is an optional modifier that tells Zig this
    // inline assembly expression has side-effects. Without
    // `volatile`, Zig is allowed to delete the inline assembly
    // code if the result is unused.
    volatile (
    // Next is a comptime string which is the assembly code.
    // Inside this string one may use `%[ret]`, `%[number]`,
    // or `%[arg1]` where a register is expected, to specify
    // the register that Zig uses for the argument or return value,
    // if the register constraint strings are used. However in
    // the below code, this is not used. A literal `%` can be
    // obtained by escaping it with a double percent: `%%`.
    // Often multiline string syntax comes in handy here.
        \\syscall
        // Next is the output. It is possible in the future Zig will
        // support multiple outputs, depending on how
        // https://github.com/ziglang/zig/issues/215 is resolved.
        // It is allowed for there to be no outputs, in which case
        // this colon would be directly followed by the colon for the inputs.
        :
        // This specifies the name to be used in `%[ret]` syntax in
        // the above assembly string. This example does not use it,
        // but the syntax is mandatory.
          [ret]
          // Next is the output constraint string. This feature is still
          // considered unstable in Zig, and so LLVM/GCC documentation
          // must be used to understand the semantics.
          // http://releases.llvm.org/10.0.0/docs/LangRef.html#inline-asm-constraint-string
          // https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html
          // In this example, the constraint string means "the result value of
          // this inline assembly instruction is whatever is in $rax".
          "={rax}"
          // Next is either a value binding, or `->` and then a type. The
          // type is the result type of the inline assembly expression.
          // If it is a value binding, then `%[ret]` syntax would be used
          // to refer to the register bound to the value.
          (-> usize),
          // Next is the list of inputs.
          // The constraint for these inputs means, "when the assembly code is
          // executed, $rax shall have the value of `number` and $rdi shall have
          // the value of `arg1`". Any number of input parameters is allowed,
          // including none.
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
          // Next is the list of clobbers. These declare a set of registers whose
          // values will not be preserved by the execution of this assembly code.
          // These do not include output or input registers. The special clobber
          // value of "memory" means that the assembly writes to arbitrary undeclared
          // memory locations - not only the memory pointed to by a declared indirect
          // output. In this example we list $rcx and $r11 because it is known the
          // kernel syscall does not preserve these registers.
        : .{ .rcx = true, .r11 = true });
}
```

For x86 and x86\_64 targets, the syntax is AT&T syntax, rather than the more popular Intel syntax. This is due to technical constraints; assembly parsing is provided by LLVM and its support for Intel syntax is buggy and not well tested.

Some day Zig may have its own assembler. This would allow it to integrate more seamlessly into the language, as well as be compatible with the popular NASM syntax. This documentation section will be updated before 1.0.0 is released, with a conclusive statement about the status of AT&T vs Intel/NASM syntax.

### [Output Constraints](https://ziglang.org/documentation/0.15.2/#toc-Output-Constraints) [Â§](https://ziglang.org/documentation/0.15.2/#Output-Constraints)

Output constraints are still considered to be unstable in Zig, and so [LLVM documentation](http://releases.llvm.org/10.0.0/docs/LangRef.html#inline-asm-constraint-string) and [GCC documentation](https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html) must be used to understand the semantics.

Note that some breaking changes to output constraints are planned with [issue #215](https://github.com/ziglang/zig/issues/215).

### [Input Constraints](https://ziglang.org/documentation/0.15.2/#toc-Input-Constraints) [Â§](https://ziglang.org/documentation/0.15.2/#Input-Constraints)

Input constraints are still considered to be unstable in Zig, and so [LLVM documentation](http://releases.llvm.org/10.0.0/docs/LangRef.html#inline-asm-constraint-string) and [GCC documentation](https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html) must be used to understand the semantics.

Note that some breaking changes to input constraints are planned with [issue #215](https://github.com/ziglang/zig/issues/215).

### [Clobbers](https://ziglang.org/documentation/0.15.2/#toc-Clobbers) [Â§](https://ziglang.org/documentation/0.15.2/#Clobbers)

Clobbers are the set of registers whose values will not be preserved by the execution of the assembly code. These do not include output or input registers. The special clobber value of `"memory"` means that the assembly causes writes to arbitrary undeclared memory locations - not only the memory pointed to by a declared indirect output.

Failure to declare the full set of clobbers for a given inline assembly expression is unchecked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

### [Global Assembly](https://ziglang.org/documentation/0.15.2/#toc-Global-Assembly) [Â§](https://ziglang.org/documentation/0.15.2/#Global-Assembly)

When an assembly expression occurs in a [container](https://ziglang.org/documentation/0.15.2/#Containers) level [comptime](https://ziglang.org/documentation/0.15.2/#comptime) block, this is **global assembly**.

This kind of assembly has different rules than inline assembly. First, `volatile` is not valid because all global assembly is unconditionally included. Second, there are no inputs, outputs, or clobbers. All global assembly is concatenated verbatim into one long string and assembled together. There are no template substitution rules regarding `%` as there are in inline assembly expressions.

test\_global\_assembly.zig

```
const std = @import("std");
const expect = std.testing.expect;

comptime {
    asm (
        \\.global my_func;
        \\.type my_func, @function;
        \\my_func:
        \\  lea (%rdi,%rsi,1),%eax
        \\  retq
    );
}

extern fn my_func(a: i32, b: i32) i32;

test "global assembly" {
    try expect(my_func(12, 34) == 46);
}
```

Shell

$ zig test test\_global\_assembly.zig -target x86\_64-linux -fllvm
1/1 test\_global\_assembly.test.global assembly...OK
All 1 tests passed.

## [Atomics](https://ziglang.org/documentation/0.15.2/#toc-Atomics) [Â§](https://ziglang.org/documentation/0.15.2/#Atomics)

TODO: @atomic rmw

TODO: builtin atomic memory ordering enum

See also:

-   [@atomicLoad](https://ziglang.org/documentation/0.15.2/#atomicLoad)

-   [@atomicStore](https://ziglang.org/documentation/0.15.2/#atomicStore)
-   [@atomicRmw](https://ziglang.org/documentation/0.15.2/#atomicRmw)

-   [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak)
-   [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong)

## [Async Functions](https://ziglang.org/documentation/0.15.2/#toc-Async-Functions) [Â§](https://ziglang.org/documentation/0.15.2/#Async-Functions)

Async functions regressed with the release of 0.11.0. The current plan is to reintroduce them as a lower level primitive that powers I/O implementations.

Tracking issue: [Proposal: stackless coroutines as low-level primitives](https://github.com/ziglang/zig/issues/23446)

## [Builtin Functions](https://ziglang.org/documentation/0.15.2/#toc-Builtin-Functions) [Â§](https://ziglang.org/documentation/0.15.2/#Builtin-Functions)

Builtin functions are provided by the compiler and are prefixed with `@`. The `comptime` keyword on a parameter means that the parameter must be known at compile time.

### [@addrSpaceCast](https://ziglang.org/documentation/0.15.2/#toc-addrSpaceCast) [Â§](https://ziglang.org/documentation/0.15.2/#addrSpaceCast)

```
@addrSpaceCast(ptr: anytype) anytype
```

Converts a pointer from one address space to another. The new address space is inferred based on the result type. Depending on the current target and address spaces, this cast may be a no-op, a complex operation, or illegal. If the cast is legal, then the resulting pointer points to the same memory location as the pointer operand. It is always valid to cast a pointer between the same address spaces.

### [@addWithOverflow](https://ziglang.org/documentation/0.15.2/#toc-addWithOverflow) [Â§](https://ziglang.org/documentation/0.15.2/#addWithOverflow)

```
@addWithOverflow(a: anytype, b: anytype) struct { @TypeOf(a, b), u1 }
```

Performs `a + b` and returns a tuple with the result and a possible overflow bit.

### [@alignCast](https://ziglang.org/documentation/0.15.2/#toc-alignCast) [Â§](https://ziglang.org/documentation/0.15.2/#alignCast)

```
@alignCast(ptr: anytype) anytype
```

`ptr` can be `*T`, `?*T`, or `[]T`. Changes the alignment of a pointer. The alignment to use is inferred based on the result type.

A [pointer alignment safety check](https://ziglang.org/documentation/0.15.2/#Incorrect-Pointer-Alignment) is added to the generated code to make sure the pointer is aligned as promised.

### [@alignOf](https://ziglang.org/documentation/0.15.2/#toc-alignOf) [Â§](https://ziglang.org/documentation/0.15.2/#alignOf)

```
@alignOf(comptime T: type) comptime_int
```

This function returns the number of bytes that this type should be aligned to for the current target to match the C ABI. When the child type of a pointer has this alignment, the alignment can be omitted from the type.

```
const assert = @import("std").debug.assert;
comptime {
    assert(*u32 == *align(@alignOf(u32)) u32);
}
```

The result is a target-specific compile time constant. It is guaranteed to be less than or equal to [@sizeOf(T)](https://ziglang.org/documentation/0.15.2/#sizeOf).

See also:

-   [Alignment](https://ziglang.org/documentation/0.15.2/#Alignment)

### [@as](https://ziglang.org/documentation/0.15.2/#toc-as) [Â§](https://ziglang.org/documentation/0.15.2/#as)

```
@as(comptime T: type, expression) T
```

Performs [Type Coercion](https://ziglang.org/documentation/0.15.2/#Type-Coercion). This cast is allowed when the conversion is unambiguous and safe, and is the preferred way to convert between types, whenever possible.

### [@atomicLoad](https://ziglang.org/documentation/0.15.2/#toc-atomicLoad) [Â§](https://ziglang.org/documentation/0.15.2/#atomicLoad)

```
@atomicLoad(comptime T: type, ptr: *const T, comptime ordering: AtomicOrder) T
```

This builtin function atomically dereferences a pointer to a `T` and returns the value.

`T` must be a pointer, a `bool`, a float, an integer, an enum, or a packed struct.

`AtomicOrder` can be found with `@import("std").builtin.AtomicOrder`.

See also:

-   [@atomicStore](https://ziglang.org/documentation/0.15.2/#atomicStore)

-   [@atomicRmw](https://ziglang.org/documentation/0.15.2/#atomicRmw)
-   [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak)

-   [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong)

### [@atomicRmw](https://ziglang.org/documentation/0.15.2/#toc-atomicRmw) [Â§](https://ziglang.org/documentation/0.15.2/#atomicRmw)

```
@atomicRmw(comptime T: type, ptr: *T, comptime op: AtomicRmwOp, operand: T, comptime ordering: AtomicOrder) T
```

This builtin function dereferences a pointer to a `T` and atomically modifies the value and returns the previous value.

`T` must be a pointer, a `bool`, a float, an integer, an enum, or a packed struct.

`AtomicOrder` can be found with `@import("std").builtin.AtomicOrder`.

`AtomicRmwOp` can be found with `@import("std").builtin.AtomicRmwOp`.

See also:

-   [@atomicStore](https://ziglang.org/documentation/0.15.2/#atomicStore)

-   [@atomicLoad](https://ziglang.org/documentation/0.15.2/#atomicLoad)
-   [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak)

-   [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong)

### [@atomicStore](https://ziglang.org/documentation/0.15.2/#toc-atomicStore) [Â§](https://ziglang.org/documentation/0.15.2/#atomicStore)

```
@atomicStore(comptime T: type, ptr: *T, value: T, comptime ordering: AtomicOrder) void
```

This builtin function dereferences a pointer to a `T` and atomically stores the given value.

`T` must be a pointer, a `bool`, a float, an integer, an enum, or a packed struct.

`AtomicOrder` can be found with `@import("std").builtin.AtomicOrder`.

See also:

-   [@atomicLoad](https://ziglang.org/documentation/0.15.2/#atomicLoad)

-   [@atomicRmw](https://ziglang.org/documentation/0.15.2/#atomicRmw)
-   [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak)

-   [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong)

### [@bitCast](https://ziglang.org/documentation/0.15.2/#toc-bitCast) [Â§](https://ziglang.org/documentation/0.15.2/#bitCast)

```
@bitCast(value: anytype) anytype
```

Converts a value of one type to another type. The return type is the inferred result type.

Asserts that `@sizeOf(@TypeOf(value)) == @sizeOf(DestType)`.

Asserts that `@typeInfo(DestType) != .pointer`. Use `@ptrCast` or `@ptrFromInt` if you need this.

Can be used for these things for example:

-   Convert `f32` to `u32` bits

-   Convert `i32` to `u32` preserving twos complement

Works at compile-time if `value` is known at compile time. It's a compile error to bitcast a value of undefined layout; this means that, besides the restriction from types which possess dedicated casting builtins (enums, pointers, error sets), bare structs, error unions, slices, optionals, and any other type without a well-defined memory layout, also cannot be used in this operation.

### [@bitOffsetOf](https://ziglang.org/documentation/0.15.2/#toc-bitOffsetOf) [Â§](https://ziglang.org/documentation/0.15.2/#bitOffsetOf)

```
@bitOffsetOf(comptime T: type, comptime field_name: []const u8) comptime_int
```

Returns the bit offset of a field relative to its containing struct.

For non [packed structs](https://ziglang.org/documentation/0.15.2/#packed-struct), this will always be divisible by `8`. For packed structs, non-byte-aligned fields will share a byte offset, but they will have different bit offsets.

See also:

-   [@offsetOf](https://ziglang.org/documentation/0.15.2/#offsetOf)

### [@bitSizeOf](https://ziglang.org/documentation/0.15.2/#toc-bitSizeOf) [Â§](https://ziglang.org/documentation/0.15.2/#bitSizeOf)

```
@bitSizeOf(comptime T: type) comptime_int
```

This function returns the number of bits it takes to store `T` in memory if the type were a field in a packed struct/union. The result is a target-specific compile time constant.

This function measures the size at runtime. For types that are disallowed at runtime, such as `comptime_int` and `type`, the result is `0`.

See also:

-   [@sizeOf](https://ziglang.org/documentation/0.15.2/#sizeOf)

-   [@typeInfo](https://ziglang.org/documentation/0.15.2/#typeInfo)

### [@branchHint](https://ziglang.org/documentation/0.15.2/#toc-branchHint) [Â§](https://ziglang.org/documentation/0.15.2/#branchHint)

```
@branchHint(hint: BranchHint) void
```

Hints to the optimizer how likely a given branch of control flow is to be reached.

`BranchHint` can be found with `@import("std").builtin.BranchHint`.

This function is only valid as the first statement in a control flow branch, or the first statement in a function.

### [@breakpoint](https://ziglang.org/documentation/0.15.2/#toc-breakpoint) [Â§](https://ziglang.org/documentation/0.15.2/#breakpoint)

```
@breakpoint() void
```

This function inserts a platform-specific debug trap instruction which causes debuggers to break there. Unlike for `@trap()`, execution may continue after this point if the program is resumed.

This function is only valid within function scope.

See also:

-   [@trap](https://ziglang.org/documentation/0.15.2/#trap)

### [@mulAdd](https://ziglang.org/documentation/0.15.2/#toc-mulAdd) [Â§](https://ziglang.org/documentation/0.15.2/#mulAdd)

```
@mulAdd(comptime T: type, a: T, b: T, c: T) T
```

Fused multiply-add, similar to `(a * b) + c`, except only rounds once, and is thus more accurate.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@byteSwap](https://ziglang.org/documentation/0.15.2/#toc-byteSwap) [Â§](https://ziglang.org/documentation/0.15.2/#byteSwap)

```
@byteSwap(operand: anytype) T
```

`@TypeOf(operand)` must be an integer type or an integer vector type with bit count evenly divisible by 8.

`operand` may be an [integer](https://ziglang.org/documentation/0.15.2/#Integers) or [vector](https://ziglang.org/documentation/0.15.2/#Vectors).

Swaps the byte order of the integer. This converts a big endian integer to a little endian integer, and converts a little endian integer to a big endian integer.

Note that for the purposes of memory layout with respect to endianness, the integer type should be related to the number of bytes reported by [@sizeOf](https://ziglang.org/documentation/0.15.2/#sizeOf) bytes. This is demonstrated with `u24`. `@sizeOf(u24) == 4`, which means that a `u24` stored in memory takes 4 bytes, and those 4 bytes are what are swapped on a little vs big endian system. On the other hand, if `T` is specified to be `u24`, then only 3 bytes are reversed.

### [@bitReverse](https://ziglang.org/documentation/0.15.2/#toc-bitReverse) [Â§](https://ziglang.org/documentation/0.15.2/#bitReverse)

```
@bitReverse(integer: anytype) T
```

`@TypeOf(anytype)` accepts any integer type or integer vector type.

Reverses the bitpattern of an integer value, including the sign bit if applicable.

For example 0b10110110 (`u8 = 182`, `i8 = -74`) becomes 0b01101101 (`u8 = 109`, `i8 = 109`).

### [@offsetOf](https://ziglang.org/documentation/0.15.2/#toc-offsetOf) [Â§](https://ziglang.org/documentation/0.15.2/#offsetOf)

```
@offsetOf(comptime T: type, comptime field_name: []const u8) comptime_int
```

Returns the byte offset of a field relative to its containing struct.

See also:

-   [@bitOffsetOf](https://ziglang.org/documentation/0.15.2/#bitOffsetOf)

### [@call](https://ziglang.org/documentation/0.15.2/#toc-call) [Â§](https://ziglang.org/documentation/0.15.2/#call)

```
@call(modifier: std.builtin.CallModifier, function: anytype, args: anytype) anytype
```

Calls a function, in the same way that invoking an expression with parentheses does:

test\_call\_builtin.zig

```
const expect = @import("std").testing.expect;

test "noinline function call" {
    try expect(@call(.auto, add, .{ 3, 9 }) == 12);
}

fn add(a: i32, b: i32) i32 {
    return a + b;
}
```

Shell

$ zig test test\_call\_builtin.zig
1/1 test\_call\_builtin.test.noinline function call...OK
All 1 tests passed.

`@call` allows more flexibility than normal function call syntax does. The `CallModifier` enum is reproduced here:

builtin.CallModifier struct.zig

```
pub const CallModifier = enum {
    /// Equivalent to function call syntax.
    auto,

    /// Equivalent to async keyword used with function call syntax.
    async_kw,

    /// Prevents tail call optimization. This guarantees that the return
    /// address will point to the callsite, as opposed to the callsite's
    /// callsite. If the call is otherwise required to be tail-called
    /// or inlined, a compile error is emitted instead.
    never_tail,

    /// Guarantees that the call will not be inlined. If the call is
    /// otherwise required to be inlined, a compile error is emitted instead.
    never_inline,

    /// Asserts that the function call will not suspend. This allows a
    /// non-async function to call an async function.
    no_async,

    /// Guarantees that the call will be generated with tail call optimization.
    /// If this is not possible, a compile error is emitted instead.
    always_tail,

    /// Guarantees that the call will be inlined at the callsite.
    /// If this is not possible, a compile error is emitted instead.
    always_inline,

    /// Evaluates the call at compile-time. If the call cannot be completed at
    /// compile-time, a compile error is emitted instead.
    compile_time,
};
```

### [@cDefine](https://ziglang.org/documentation/0.15.2/#toc-cDefine) [Â§](https://ziglang.org/documentation/0.15.2/#cDefine)

```
@cDefine(comptime name: []const u8, value) void
```

This function can only occur inside `@cImport`.

This appends `#define $name $value` to the `@cImport` temporary buffer.

To define without a value, like this:

```
#define _GNU_SOURCE
```

Use the void value, like this:

```
@cDefine("_GNU_SOURCE", {})
```

See also:

-   [Import from C Header File](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

-   [@cInclude](https://ziglang.org/documentation/0.15.2/#cInclude)
-   [@cImport](https://ziglang.org/documentation/0.15.2/#cImport)

-   [@cUndef](https://ziglang.org/documentation/0.15.2/#cUndef)
-   [void](https://ziglang.org/documentation/0.15.2/#void)

### [@cImport](https://ziglang.org/documentation/0.15.2/#toc-cImport) [Â§](https://ziglang.org/documentation/0.15.2/#cImport)

```
@cImport(expression) type
```

This function parses C code and imports the functions, types, variables, and compatible macro definitions into a new empty struct type, and then returns that type.

`expression` is interpreted at compile time. The builtin functions `@cInclude`, `@cDefine`, and `@cUndef` work within this expression, appending to a temporary buffer which is then parsed as C code.

Usually you should only have one `@cImport` in your entire application, because it saves the compiler from invoking clang multiple times, and prevents inline functions from being duplicated.

Reasons for having multiple `@cImport` expressions would be:

-   To avoid a symbol collision, for example if foo.h and bar.h both `#define CONNECTION_COUNT`

-   To analyze the C code with different preprocessor defines

See also:

-   [Import from C Header File](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

-   [@cInclude](https://ziglang.org/documentation/0.15.2/#cInclude)
-   [@cDefine](https://ziglang.org/documentation/0.15.2/#cDefine)

-   [@cUndef](https://ziglang.org/documentation/0.15.2/#cUndef)

### [@cInclude](https://ziglang.org/documentation/0.15.2/#toc-cInclude) [Â§](https://ziglang.org/documentation/0.15.2/#cInclude)

```
@cInclude(comptime path: []const u8) void
```

This function can only occur inside `@cImport`.

This appends `#include <$path>\n` to the `c_import` temporary buffer.

See also:

-   [Import from C Header File](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

-   [@cImport](https://ziglang.org/documentation/0.15.2/#cImport)
-   [@cDefine](https://ziglang.org/documentation/0.15.2/#cDefine)

-   [@cUndef](https://ziglang.org/documentation/0.15.2/#cUndef)

### [@clz](https://ziglang.org/documentation/0.15.2/#toc-clz) [Â§](https://ziglang.org/documentation/0.15.2/#clz)

```
@clz(operand: anytype) anytype
```

`@TypeOf(operand)` must be an integer type or an integer vector type.

`operand` may be an [integer](https://ziglang.org/documentation/0.15.2/#Integers) or [vector](https://ziglang.org/documentation/0.15.2/#Vectors).

Counts the number of most-significant (leading in a big-endian sense) zeroes in an integer - "count leading zeroes".

The return type is an unsigned integer or vector of unsigned integers with the minimum number of bits that can represent the bit count of the integer type.

If `operand` is zero, `@clz` returns the bit width of integer type `T`.

See also:

-   [@ctz](https://ziglang.org/documentation/0.15.2/#ctz)

-   [@popCount](https://ziglang.org/documentation/0.15.2/#popCount)

### [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#toc-cmpxchgStrong) [Â§](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong)

```
@cmpxchgStrong(comptime T: type, ptr: *T, expected_value: T, new_value: T, success_order: AtomicOrder, fail_order: AtomicOrder) ?T
```

This function performs a strong atomic compare-and-exchange operation, returning `null` if the current value is the given expected value. It's the equivalent of this code, except atomic:

not\_atomic\_cmpxchgStrong.zig

```
fn cmpxchgStrongButNotAtomic(comptime T: type, ptr: *T, expected_value: T, new_value: T) ?T {
    const old_value = ptr.*;
    if (old_value == expected_value) {
        ptr.* = new_value;
        return null;
    } else {
        return old_value;
    }
}
```

If you are using cmpxchg in a retry loop, [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak) is the better choice, because it can be implemented more efficiently in machine instructions.

`T` must be a pointer, a `bool`, an integer, an enum, or a packed struct.

`@typeInfo(@TypeOf(ptr)).pointer.alignment` must be `>= @sizeOf(T).`

`AtomicOrder` can be found with `@import("std").builtin.AtomicOrder`.

See also:

-   [@atomicStore](https://ziglang.org/documentation/0.15.2/#atomicStore)

-   [@atomicLoad](https://ziglang.org/documentation/0.15.2/#atomicLoad)
-   [@atomicRmw](https://ziglang.org/documentation/0.15.2/#atomicRmw)

-   [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak)

### [@cmpxchgWeak](https://ziglang.org/documentation/0.15.2/#toc-cmpxchgWeak) [Â§](https://ziglang.org/documentation/0.15.2/#cmpxchgWeak)

```
@cmpxchgWeak(comptime T: type, ptr: *T, expected_value: T, new_value: T, success_order: AtomicOrder, fail_order: AtomicOrder) ?T
```

This function performs a weak atomic compare-and-exchange operation, returning `null` if the current value is the given expected value. It's the equivalent of this code, except atomic:

cmpxchgWeakButNotAtomic

```
fn cmpxchgWeakButNotAtomic(comptime T: type, ptr: *T, expected_value: T, new_value: T) ?T {
    const old_value = ptr.*;
    if (old_value == expected_value and usuallyTrueButSometimesFalse()) {
        ptr.* = new_value;
        return null;
    } else {
        return old_value;
    }
}
```

If you are using cmpxchg in a retry loop, the sporadic failure will be no problem, and `cmpxchgWeak` is the better choice, because it can be implemented more efficiently in machine instructions. However if you need a stronger guarantee, use [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong).

`T` must be a pointer, a `bool`, an integer, an enum, or a packed struct.

`@typeInfo(@TypeOf(ptr)).pointer.alignment` must be `>= @sizeOf(T).`

`AtomicOrder` can be found with `@import("std").builtin.AtomicOrder`.

See also:

-   [@atomicStore](https://ziglang.org/documentation/0.15.2/#atomicStore)

-   [@atomicLoad](https://ziglang.org/documentation/0.15.2/#atomicLoad)
-   [@atomicRmw](https://ziglang.org/documentation/0.15.2/#atomicRmw)

-   [@cmpxchgStrong](https://ziglang.org/documentation/0.15.2/#cmpxchgStrong)

### [@compileError](https://ziglang.org/documentation/0.15.2/#toc-compileError) [Â§](https://ziglang.org/documentation/0.15.2/#compileError)

```
@compileError(comptime msg: []const u8) noreturn
```

This function, when semantically analyzed, causes a compile error with the message `msg`.

There are several ways that code avoids being semantically checked, such as using `if` or `switch` with compile time constants, and `comptime` functions.

### [@compileLog](https://ziglang.org/documentation/0.15.2/#toc-compileLog) [Â§](https://ziglang.org/documentation/0.15.2/#compileLog)

```
@compileLog(...) void
```

This function prints the arguments passed to it at compile-time.

To prevent accidentally leaving compile log statements in a codebase, a compilation error is added to the build, pointing to the compile log statement. This error prevents code from being generated, but does not otherwise interfere with analysis.

This function can be used to do "printf debugging" on compile-time executing code.

test\_compileLog\_builtin.zig

```
const print = @import("std").debug.print;

const num1 = blk: {
    var val1: i32 = 99;
    @compileLog("comptime val1 = ", val1);
    val1 = val1 + 1;
    break :blk val1;
};

test "main" {
    @compileLog("comptime in main");

    print("Runtime in main, num1 = {}.\n", .{num1});
}
```

Shell

$ zig test test\_compileLog\_builtin.zig
/home/andy/dev/zig/doc/langref/test\_compileLog\_builtin.zig:5:5: error: found compile log statement
    @compileLog("comptime val1 = ", val1);
    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_compileLog\_builtin.zig:11:5: note: also here
    @compileLog("comptime in main");
    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
referenced by:
    test.main: /home/andy/dev/zig/doc/langref/test\_compileLog\_builtin.zig:13:46
Compile Log Output:
@as(\*const \[16:0\]u8, "comptime val1 = "), @as(i32, 99)
@as(\*const \[16:0\]u8, "comptime in main")

### [@constCast](https://ziglang.org/documentation/0.15.2/#toc-constCast) [Â§](https://ziglang.org/documentation/0.15.2/#constCast)

```
@constCast(value: anytype) DestType
```

Remove `const` qualifier from a pointer.

### [@ctz](https://ziglang.org/documentation/0.15.2/#toc-ctz) [Â§](https://ziglang.org/documentation/0.15.2/#ctz)

```
@ctz(operand: anytype) anytype
```

`@TypeOf(operand)` must be an integer type or an integer vector type.

`operand` may be an [integer](https://ziglang.org/documentation/0.15.2/#Integers) or [vector](https://ziglang.org/documentation/0.15.2/#Vectors).

Counts the number of least-significant (trailing in a big-endian sense) zeroes in an integer - "count trailing zeroes".

The return type is an unsigned integer or vector of unsigned integers with the minimum number of bits that can represent the bit count of the integer type.

If `operand` is zero, `@ctz` returns the bit width of integer type `T`.

See also:

-   [@clz](https://ziglang.org/documentation/0.15.2/#clz)

-   [@popCount](https://ziglang.org/documentation/0.15.2/#popCount)

### [@cUndef](https://ziglang.org/documentation/0.15.2/#toc-cUndef) [Â§](https://ziglang.org/documentation/0.15.2/#cUndef)

```
@cUndef(comptime name: []const u8) void
```

This function can only occur inside `@cImport`.

This appends `#undef $name` to the `@cImport` temporary buffer.

See also:

-   [Import from C Header File](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

-   [@cImport](https://ziglang.org/documentation/0.15.2/#cImport)
-   [@cDefine](https://ziglang.org/documentation/0.15.2/#cDefine)

-   [@cInclude](https://ziglang.org/documentation/0.15.2/#cInclude)

### [@cVaArg](https://ziglang.org/documentation/0.15.2/#toc-cVaArg) [Â§](https://ziglang.org/documentation/0.15.2/#cVaArg)

```
@cVaArg(operand: *std.builtin.VaList, comptime T: type) T
```

Implements the C macro `va_arg`.

See also:

-   [@cVaCopy](https://ziglang.org/documentation/0.15.2/#cVaCopy)

-   [@cVaEnd](https://ziglang.org/documentation/0.15.2/#cVaEnd)
-   [@cVaStart](https://ziglang.org/documentation/0.15.2/#cVaStart)

### [@cVaCopy](https://ziglang.org/documentation/0.15.2/#toc-cVaCopy) [Â§](https://ziglang.org/documentation/0.15.2/#cVaCopy)

```
@cVaCopy(src: *std.builtin.VaList) std.builtin.VaList
```

Implements the C macro `va_copy`.

See also:

-   [@cVaArg](https://ziglang.org/documentation/0.15.2/#cVaArg)

-   [@cVaEnd](https://ziglang.org/documentation/0.15.2/#cVaEnd)
-   [@cVaStart](https://ziglang.org/documentation/0.15.2/#cVaStart)

### [@cVaEnd](https://ziglang.org/documentation/0.15.2/#toc-cVaEnd) [Â§](https://ziglang.org/documentation/0.15.2/#cVaEnd)

```
@cVaEnd(src: *std.builtin.VaList) void
```

Implements the C macro `va_end`.

See also:

-   [@cVaArg](https://ziglang.org/documentation/0.15.2/#cVaArg)

-   [@cVaCopy](https://ziglang.org/documentation/0.15.2/#cVaCopy)
-   [@cVaStart](https://ziglang.org/documentation/0.15.2/#cVaStart)

### [@cVaStart](https://ziglang.org/documentation/0.15.2/#toc-cVaStart) [Â§](https://ziglang.org/documentation/0.15.2/#cVaStart)

```
@cVaStart() std.builtin.VaList
```

Implements the C macro `va_start`. Only valid inside a variadic function.

See also:

-   [@cVaArg](https://ziglang.org/documentation/0.15.2/#cVaArg)

-   [@cVaCopy](https://ziglang.org/documentation/0.15.2/#cVaCopy)
-   [@cVaEnd](https://ziglang.org/documentation/0.15.2/#cVaEnd)

### [@divExact](https://ziglang.org/documentation/0.15.2/#toc-divExact) [Â§](https://ziglang.org/documentation/0.15.2/#divExact)

```
@divExact(numerator: T, denominator: T) T
```

Exact division. Caller guarantees `denominator != 0` and `@divTrunc(numerator, denominator) * denominator == numerator`.

-   `@divExact(6, 3) == 2`

-   `@divExact(a, b) * b == a`

For a function that returns a possible error code, use `@import("std").math.divExact`.

See also:

-   [@divTrunc](https://ziglang.org/documentation/0.15.2/#divTrunc)

-   [@divFloor](https://ziglang.org/documentation/0.15.2/#divFloor)

### [@divFloor](https://ziglang.org/documentation/0.15.2/#toc-divFloor) [Â§](https://ziglang.org/documentation/0.15.2/#divFloor)

```
@divFloor(numerator: T, denominator: T) T
```

Floored division. Rounds toward negative infinity. For unsigned integers it is the same as `numerator / denominator`. Caller guarantees `denominator != 0` and `!(@typeInfo(T) == .int and T.is_signed and numerator == std.math.minInt(T) and denominator == -1)`.

-   `@divFloor(-5, 3) == -2`

-   `(@divFloor(a, b) * b) + @mod(a, b) == a`

For a function that returns a possible error code, use `@import("std").math.divFloor`.

See also:

-   [@divTrunc](https://ziglang.org/documentation/0.15.2/#divTrunc)

-   [@divExact](https://ziglang.org/documentation/0.15.2/#divExact)

### [@divTrunc](https://ziglang.org/documentation/0.15.2/#toc-divTrunc) [Â§](https://ziglang.org/documentation/0.15.2/#divTrunc)

```
@divTrunc(numerator: T, denominator: T) T
```

Truncated division. Rounds toward zero. For unsigned integers it is the same as `numerator / denominator`. Caller guarantees `denominator != 0` and `!(@typeInfo(T) == .int and T.is_signed and numerator == std.math.minInt(T) and denominator == -1)`.

-   `@divTrunc(-5, 3) == -1`

-   `(@divTrunc(a, b) * b) + @rem(a, b) == a`

For a function that returns a possible error code, use `@import("std").math.divTrunc`.

See also:

-   [@divFloor](https://ziglang.org/documentation/0.15.2/#divFloor)

-   [@divExact](https://ziglang.org/documentation/0.15.2/#divExact)

### [@embedFile](https://ziglang.org/documentation/0.15.2/#toc-embedFile) [Â§](https://ziglang.org/documentation/0.15.2/#embedFile)

```
@embedFile(comptime path: []const u8) *const [N:0]u8
```

This function returns a compile time constant pointer to null-terminated, fixed-size array with length equal to the byte count of the file given by `path`. The contents of the array are the contents of the file. This is equivalent to a [string literal](https://ziglang.org/documentation/0.15.2/#String-Literals-and-Unicode-Code-Point-Literals) with the file contents.

`path` is absolute or relative to the current file, just like `@import`.

See also:

-   [@import](https://ziglang.org/documentation/0.15.2/#import)

### [@enumFromInt](https://ziglang.org/documentation/0.15.2/#toc-enumFromInt) [Â§](https://ziglang.org/documentation/0.15.2/#enumFromInt)

```
@enumFromInt(integer: anytype) anytype
```

Converts an integer into an [enum](https://ziglang.org/documentation/0.15.2/#enum) value. The return type is the inferred result type.

Attempting to convert an integer with no corresponding value in the enum invokes safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior). Note that a [non-exhaustive enum](https://ziglang.org/documentation/0.15.2/#Non-exhaustive-enum) has corresponding values for all integers in the enum's integer tag type: the `_` value represents all the remaining unnamed integers in the enum's tag type.

See also:

-   [@intFromEnum](https://ziglang.org/documentation/0.15.2/#intFromEnum)

### [@errorFromInt](https://ziglang.org/documentation/0.15.2/#toc-errorFromInt) [Â§](https://ziglang.org/documentation/0.15.2/#errorFromInt)

```
@errorFromInt(value: std.meta.Int(.unsigned, @bitSizeOf(anyerror))) anyerror
```

Converts from the integer representation of an error into [The Global Error Set](https://ziglang.org/documentation/0.15.2/#The-Global-Error-Set) type.

It is generally recommended to avoid this cast, as the integer representation of an error is not stable across source code changes.

Attempting to convert an integer that does not correspond to any error results in safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

See also:

-   [@intFromError](https://ziglang.org/documentation/0.15.2/#intFromError)

### [@errorName](https://ziglang.org/documentation/0.15.2/#toc-errorName) [Â§](https://ziglang.org/documentation/0.15.2/#errorName)

```
@errorName(err: anyerror) [:0]const u8
```

This function returns the string representation of an error. The string representation of `error.OutOfMem` is `"OutOfMem"`.

If there are no calls to `@errorName` in an entire application, or all calls have a compile-time known value for `err`, then no error name table will be generated.

### [@errorReturnTrace](https://ziglang.org/documentation/0.15.2/#toc-errorReturnTrace) [Â§](https://ziglang.org/documentation/0.15.2/#errorReturnTrace)

```
@errorReturnTrace() ?*builtin.StackTrace
```

If the binary is built with error return tracing, and this function is invoked in a function that calls a function with an error or error union return type, returns a stack trace object. Otherwise returns [null](https://ziglang.org/documentation/0.15.2/#null).

### [@errorCast](https://ziglang.org/documentation/0.15.2/#toc-errorCast) [Â§](https://ziglang.org/documentation/0.15.2/#errorCast)

```
@errorCast(value: anytype) anytype
```

Converts an error set or error union value from one error set to another error set. The return type is the inferred result type. Attempting to convert an error which is not in the destination error set results in safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

### [@export](https://ziglang.org/documentation/0.15.2/#toc-export) [Â§](https://ziglang.org/documentation/0.15.2/#export)

```
@export(comptime ptr: *const anyopaque, comptime options: std.builtin.ExportOptions) void
```

Creates a symbol in the output object file which refers to the target of `ptr`.

`ptr` must point to a global variable or a comptime-known constant.

This builtin can be called from a [comptime](https://ziglang.org/documentation/0.15.2/#comptime) block to conditionally export symbols. When `ptr` points to a function with the C calling convention and `options.linkage` is `.strong`, this is equivalent to the `export` keyword used on a function:

export\_builtin.zig

```
comptime {
    @export(&internalName, .{ .name = "foo", .linkage = .strong });
}

fn internalName() callconv(.c) void {}
```

Shell

$ zig build-obj export\_builtin.zig

This is equivalent to:

export\_builtin\_equivalent\_code.zig

```
export fn foo() void {}
```

Shell

$ zig build-obj export\_builtin\_equivalent\_code.zig

Note that even when using `export`, the `@"foo"` syntax for [identifiers](https://ziglang.org/documentation/0.15.2/#Identifiers) can be used to choose any string for the symbol name:

export\_any\_symbol\_name.zig

```
export fn @"A function name that is a complete sentence."() void {}
```

Shell

$ zig build-obj export\_any\_symbol\_name.zig

When looking at the resulting object, you can see the symbol is used verbatim:

```
00000000000001f0 T A function name that is a complete sentence.
```

See also:

-   [Exporting a C Library](https://ziglang.org/documentation/0.15.2/#Exporting-a-C-Library)

### [@extern](https://ziglang.org/documentation/0.15.2/#toc-extern) [Â§](https://ziglang.org/documentation/0.15.2/#extern)

```
@extern(T: type, comptime options: std.builtin.ExternOptions) T
```

Creates a reference to an external symbol in the output object file. T must be a pointer type.

See also:

-   [@export](https://ziglang.org/documentation/0.15.2/#export)

### [@field](https://ziglang.org/documentation/0.15.2/#toc-field) [Â§](https://ziglang.org/documentation/0.15.2/#field)

```
@field(lhs: anytype, comptime field_name: []const u8) (field)
```

Performs field access by a compile-time string. Works on both fields and declarations.

test\_field\_builtin.zig

```
const std = @import("std");

const Point = struct {
    x: u32,
    y: u32,

    pub var z: u32 = 1;
};

test "field access by string" {
    const expect = std.testing.expect;
    var p = Point{ .x = 0, .y = 0 };

    @field(p, "x") = 4;
    @field(p, "y") = @field(p, "x") + 1;

    try expect(@field(p, "x") == 4);
    try expect(@field(p, "y") == 5);
}

test "decl access by string" {
    const expect = std.testing.expect;

    try expect(@field(Point, "z") == 1);

    @field(Point, "z") = 2;
    try expect(@field(Point, "z") == 2);
}
```

Shell

$ zig test test\_field\_builtin.zig
1/2 test\_field\_builtin.test.field access by string...OK
2/2 test\_field\_builtin.test.decl access by string...OK
All 2 tests passed.

### [@fieldParentPtr](https://ziglang.org/documentation/0.15.2/#toc-fieldParentPtr) [Â§](https://ziglang.org/documentation/0.15.2/#fieldParentPtr)

```
@fieldParentPtr(comptime field_name: []const u8, field_ptr: *T) anytype
```

Given a pointer to a struct or union field, returns a pointer to the struct or union containing that field. The return type (pointer to the parent struct or union in question) is the inferred result type.

If `field_ptr` does not point to the `field_name` field of an instance of the result type, and the result type has ill-defined layout, invokes unchecked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

### [@FieldType](https://ziglang.org/documentation/0.15.2/#toc-FieldType) [Â§](https://ziglang.org/documentation/0.15.2/#FieldType)

```
@FieldType(comptime Type: type, comptime field_name: []const u8) type
```

Given a type and the name of one of its fields, returns the type of that field.

### [@floatCast](https://ziglang.org/documentation/0.15.2/#toc-floatCast) [Â§](https://ziglang.org/documentation/0.15.2/#floatCast)

```
@floatCast(value: anytype) anytype
```

Convert from one float type to another. This cast is safe, but may cause the numeric value to lose precision. The return type is the inferred result type.

### [@floatFromInt](https://ziglang.org/documentation/0.15.2/#toc-floatFromInt) [Â§](https://ziglang.org/documentation/0.15.2/#floatFromInt)

```
@floatFromInt(int: anytype) anytype
```

Converts an integer to the closest floating point representation. The return type is the inferred result type. To convert the other way, use [@intFromFloat](https://ziglang.org/documentation/0.15.2/#intFromFloat). This operation is legal for all values of all integer types.

### [@frameAddress](https://ziglang.org/documentation/0.15.2/#toc-frameAddress) [Â§](https://ziglang.org/documentation/0.15.2/#frameAddress)

```
@frameAddress() usize
```

This function returns the base pointer of the current stack frame.

The implications of this are target-specific and not consistent across all platforms. The frame address may not be available in release mode due to aggressive optimizations.

This function is only valid within function scope.

### [@hasDecl](https://ziglang.org/documentation/0.15.2/#toc-hasDecl) [Â§](https://ziglang.org/documentation/0.15.2/#hasDecl)

```
@hasDecl(comptime Container: type, comptime name: []const u8) bool
```

Returns whether or not a [container](https://ziglang.org/documentation/0.15.2/#Containers) has a declaration matching `name`.

test\_hasDecl\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

const Foo = struct {
    nope: i32,

    pub var blah = "xxx";
    const hi = 1;
};

test "@hasDecl" {
    try expect(@hasDecl(Foo, "blah"));

    // Even though `hi` is private, @hasDecl returns true because this test is
    // in the same file scope as Foo. It would return false if Foo was declared
    // in a different file.
    try expect(@hasDecl(Foo, "hi"));

    // @hasDecl is for declarations; not fields.
    try expect(!@hasDecl(Foo, "nope"));
    try expect(!@hasDecl(Foo, "nope1234"));
}
```

Shell

$ zig test test\_hasDecl\_builtin.zig
1/1 test\_hasDecl\_builtin.test.@hasDecl...OK
All 1 tests passed.

See also:

-   [@hasField](https://ziglang.org/documentation/0.15.2/#hasField)

### [@hasField](https://ziglang.org/documentation/0.15.2/#toc-hasField) [Â§](https://ziglang.org/documentation/0.15.2/#hasField)

```
@hasField(comptime Container: type, comptime name: []const u8) bool
```

Returns whether the field name of a struct, union, or enum exists.

The result is a compile time constant.

It does not include functions, variables, or constants.

See also:

-   [@hasDecl](https://ziglang.org/documentation/0.15.2/#hasDecl)

### [@import](https://ziglang.org/documentation/0.15.2/#toc-import) [Â§](https://ziglang.org/documentation/0.15.2/#import)

```
@import(comptime target: []const u8) anytype
```

Imports the file at `target`, adding it to the compilation if it is not already added. `target` is either a relative path to another file from the file containing the `@import` call, or it is the name of a [module](https://ziglang.org/documentation/0.15.2/#Compilation-Model), with the import referring to the root source file of that module. Either way, the file path must end in either `.zig` (for a Zig source file) or `.zon` (for a ZON data file).

If `target` refers to a Zig source file, then `@import` returns that file's [corresponding struct type](https://ziglang.org/documentation/0.15.2/#Source-File-Structs), essentially as if the builtin call was replaced by `struct { FILE_CONTENTS }`. The return type is `type`.

If `target` refers to a ZON file, then `@import` returns the value of the literal in the file. If there is an inferred [result type](https://ziglang.org/documentation/0.15.2/#Result-Types), then the return type is that type, and the ZON literal is interpreted as that type ([Result Types](https://ziglang.org/documentation/0.15.2/#Result-Types) are propagated through the ZON expression). Otherwise, the return type is the type of the equivalent Zig expression, essentially as if the builtin call was replaced by the ZON file contents.

The following modules are always available for import:

-   `@import("std")` - Zig Standard Library

-   `@import("builtin")` - Target-specific information. The command `zig build-exe --show-builtin` outputs the source to stdout for reference.
-   `@import("root")` - Alias for the root module. In typical project structures, this means it refers back to `src/main.zig`.

See also:

-   [Compile Variables](https://ziglang.org/documentation/0.15.2/#Compile-Variables)

-   [@embedFile](https://ziglang.org/documentation/0.15.2/#embedFile)

### [@inComptime](https://ziglang.org/documentation/0.15.2/#toc-inComptime) [Â§](https://ziglang.org/documentation/0.15.2/#inComptime)

```
@inComptime() bool
```

Returns whether the builtin was run in a `comptime` context. The result is a compile-time constant.

This can be used to provide alternative, comptime-friendly implementations of functions. It should not be used, for instance, to exclude certain functions from being evaluated at comptime.

See also:

-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)

### [@intCast](https://ziglang.org/documentation/0.15.2/#toc-intCast) [Â§](https://ziglang.org/documentation/0.15.2/#intCast)

```
@intCast(int: anytype) anytype
```

Converts an integer to another integer while keeping the same numerical value. The return type is the inferred result type. Attempting to convert a number which is out of range of the destination type results in safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

test\_intCast\_builtin.zig

```
test "integer cast panic" {
    var a: u16 = 0xabcd; // runtime-known
    _ = &a;
    const b: u8 = @intCast(a);
    _ = b;
}
```

Shell

$ zig test test\_intCast\_builtin.zig
1/1 test\_intCast\_builtin.test.integer cast panic...thread 2898212 panic: integer does not fit in destination type
/home/andy/dev/zig/doc/langref/test\_intCast\_builtin.zig:4:19: 0x102c020 in test.integer cast panic (test\_intCast\_builtin.zig)
    const b: u8 = @intCast(a);
                  ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x115cb50 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1155d71 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x114fb0d in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x114f3a1 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/056fc3b607934a9389a99437800346de/test --seed=0x9fcd81fa

To truncate the significant bits of a number out of range of the destination type, use [@truncate](https://ziglang.org/documentation/0.15.2/#truncate).

If `T` is `comptime_int`, then this is semantically equivalent to [Type Coercion](https://ziglang.org/documentation/0.15.2/#Type-Coercion).

### [@intFromBool](https://ziglang.org/documentation/0.15.2/#toc-intFromBool) [Â§](https://ziglang.org/documentation/0.15.2/#intFromBool)

```
@intFromBool(value: bool) u1
```

Converts `true` to `@as(u1, 1)` and `false` to `@as(u1, 0)`.

### [@intFromEnum](https://ziglang.org/documentation/0.15.2/#toc-intFromEnum) [Â§](https://ziglang.org/documentation/0.15.2/#intFromEnum)

```
@intFromEnum(enum_or_tagged_union: anytype) anytype
```

Converts an enumeration value into its integer tag type. When a tagged union is passed, the tag value is used as the enumeration value.

If there is only one possible enum value, the result is a `comptime_int` known at [comptime](https://ziglang.org/documentation/0.15.2/#comptime).

See also:

-   [@enumFromInt](https://ziglang.org/documentation/0.15.2/#enumFromInt)

### [@intFromError](https://ziglang.org/documentation/0.15.2/#toc-intFromError) [Â§](https://ziglang.org/documentation/0.15.2/#intFromError)

```
@intFromError(err: anytype) std.meta.Int(.unsigned, @bitSizeOf(anyerror))
```

Supports the following types:

-   [The Global Error Set](https://ziglang.org/documentation/0.15.2/#The-Global-Error-Set)

-   [Error Set Type](https://ziglang.org/documentation/0.15.2/#Error-Set-Type)
-   [Error Union Type](https://ziglang.org/documentation/0.15.2/#Error-Union-Type)

Converts an error to the integer representation of an error.

It is generally recommended to avoid this cast, as the integer representation of an error is not stable across source code changes.

See also:

-   [@errorFromInt](https://ziglang.org/documentation/0.15.2/#errorFromInt)

### [@intFromFloat](https://ziglang.org/documentation/0.15.2/#toc-intFromFloat) [Â§](https://ziglang.org/documentation/0.15.2/#intFromFloat)

```
@intFromFloat(float: anytype) anytype
```

Converts the integer part of a floating point number to the inferred result type.

If the integer part of the floating point number cannot fit in the destination type, it invokes safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

See also:

-   [@floatFromInt](https://ziglang.org/documentation/0.15.2/#floatFromInt)

### [@intFromPtr](https://ziglang.org/documentation/0.15.2/#toc-intFromPtr) [Â§](https://ziglang.org/documentation/0.15.2/#intFromPtr)

```
@intFromPtr(value: anytype) usize
```

Converts `value` to a `usize` which is the address of the pointer. `value` can be `*T` or `?*T`.

To convert the other way, use [@ptrFromInt](https://ziglang.org/documentation/0.15.2/#ptrFromInt)

### [@max](https://ziglang.org/documentation/0.15.2/#toc-max) [Â§](https://ziglang.org/documentation/0.15.2/#max)

```
@max(...) T
```

Takes two or more arguments and returns the biggest value included (the maximum). This builtin accepts integers, floats, and vectors of either. In the latter case, the operation is performed element wise.

NaNs are handled as follows: return the biggest non-NaN value included. If all operands are NaN, return NaN.

See also:

-   [@min](https://ziglang.org/documentation/0.15.2/#min)

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)

### [@memcpy](https://ziglang.org/documentation/0.15.2/#toc-memcpy) [Â§](https://ziglang.org/documentation/0.15.2/#memcpy)

```
@memcpy(noalias dest, noalias source) void
```

This function copies bytes from one region of memory to another.

`dest` must be a mutable slice, a mutable pointer to an array, or a mutable many-item [pointer](https://ziglang.org/documentation/0.15.2/#Pointers). It may have any alignment, and it may have any element type.

`source` must be a slice, a pointer to an array, or a many-item [pointer](https://ziglang.org/documentation/0.15.2/#Pointers). It may have any alignment, and it may have any element type.

The `source` element type must have the same in-memory representation as the `dest` element type.

Similar to [for](https://ziglang.org/documentation/0.15.2/#for) loops, at least one of `source` and `dest` must provide a length, and if two lengths are provided, they must be equal.

Finally, the two memory regions must not overlap.

### [@memset](https://ziglang.org/documentation/0.15.2/#toc-memset) [Â§](https://ziglang.org/documentation/0.15.2/#memset)

```
@memset(dest, elem) void
```

This function sets all the elements of a memory region to `elem`.

`dest` must be a mutable slice or a mutable pointer to an array. It may have any alignment, and it may have any element type.

`elem` is coerced to the element type of `dest`.

For securely zeroing out sensitive contents from memory, you should use `std.crypto.secureZero`

### [@memmove](https://ziglang.org/documentation/0.15.2/#toc-memmove) [Â§](https://ziglang.org/documentation/0.15.2/#memmove)

```
@memmove(dest, source) void
```

This function copies bytes from one region of memory to another, but unlike [@memcpy](https://ziglang.org/documentation/0.15.2/#memcpy) the regions may overlap.

`dest` must be a mutable slice, a mutable pointer to an array, or a mutable many-item [pointer](https://ziglang.org/documentation/0.15.2/#Pointers). It may have any alignment, and it may have any element type.

`source` must be a slice, a pointer to an array, or a many-item [pointer](https://ziglang.org/documentation/0.15.2/#Pointers). It may have any alignment, and it may have any element type.

The `source` element type must have the same in-memory representation as the `dest` element type.

Similar to [for](https://ziglang.org/documentation/0.15.2/#for) loops, at least one of `source` and `dest` must provide a length, and if two lengths are provided, they must be equal.

### [@min](https://ziglang.org/documentation/0.15.2/#toc-min) [Â§](https://ziglang.org/documentation/0.15.2/#min)

```
@min(...) T
```

Takes two or more arguments and returns the smallest value included (the minimum). This builtin accepts integers, floats, and vectors of either. In the latter case, the operation is performed element wise.

NaNs are handled as follows: return the smallest non-NaN value included. If all operands are NaN, return NaN.

See also:

-   [@max](https://ziglang.org/documentation/0.15.2/#max)

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)

### [@wasmMemorySize](https://ziglang.org/documentation/0.15.2/#toc-wasmMemorySize) [Â§](https://ziglang.org/documentation/0.15.2/#wasmMemorySize)

```
@wasmMemorySize(index: u32) usize
```

This function returns the size of the Wasm memory identified by `index` as an unsigned value in units of Wasm pages. Note that each Wasm page is 64KB in size.

This function is a low level intrinsic with no safety mechanisms usually useful for allocator designers targeting Wasm. So unless you are writing a new allocator from scratch, you should use something like `@import("std").heap.WasmPageAllocator`.

See also:

-   [@wasmMemoryGrow](https://ziglang.org/documentation/0.15.2/#wasmMemoryGrow)

### [@wasmMemoryGrow](https://ziglang.org/documentation/0.15.2/#toc-wasmMemoryGrow) [Â§](https://ziglang.org/documentation/0.15.2/#wasmMemoryGrow)

```
@wasmMemoryGrow(index: u32, delta: usize) isize
```

This function increases the size of the Wasm memory identified by `index` by `delta` in units of unsigned number of Wasm pages. Note that each Wasm page is 64KB in size. On success, returns previous memory size; on failure, if the allocation fails, returns -1.

This function is a low level intrinsic with no safety mechanisms usually useful for allocator designers targeting Wasm. So unless you are writing a new allocator from scratch, you should use something like `@import("std").heap.WasmPageAllocator`.

test\_wasmMemoryGrow\_builtin.zig

```
const std = @import("std");
const native_arch = @import("builtin").target.cpu.arch;
const expect = std.testing.expect;

test "@wasmMemoryGrow" {
    if (native_arch != .wasm32) return error.SkipZigTest;

    const prev = @wasmMemorySize(0);
    try expect(prev == @wasmMemoryGrow(0, 1));
    try expect(prev + 1 == @wasmMemorySize(0));
}
```

Shell

$ zig test test\_wasmMemoryGrow\_builtin.zig
1/1 test\_wasmMemoryGrow\_builtin.test.@wasmMemoryGrow...SKIP
0 passed; 1 skipped; 0 failed.

See also:

-   [@wasmMemorySize](https://ziglang.org/documentation/0.15.2/#wasmMemorySize)

### [@mod](https://ziglang.org/documentation/0.15.2/#toc-mod) [Â§](https://ziglang.org/documentation/0.15.2/#mod)

```
@mod(numerator: T, denominator: T) T
```

Modulus division. For unsigned integers this is the same as `numerator % denominator`. Caller guarantees `denominator != 0`, otherwise the operation will result in a [Remainder Division by Zero](https://ziglang.org/documentation/0.15.2/#Remainder-Division-by-Zero) when runtime safety checks are enabled.

-   `@mod(-5, 3) == 1`

-   `(@divFloor(a, b) * b) + @mod(a, b) == a`

For a function that returns an error code, see `@import("std").math.mod`.

See also:

-   [@rem](https://ziglang.org/documentation/0.15.2/#rem)

### [@mulWithOverflow](https://ziglang.org/documentation/0.15.2/#toc-mulWithOverflow) [Â§](https://ziglang.org/documentation/0.15.2/#mulWithOverflow)

```
@mulWithOverflow(a: anytype, b: anytype) struct { @TypeOf(a, b), u1 }
```

Performs `a * b` and returns a tuple with the result and a possible overflow bit.

### [@panic](https://ziglang.org/documentation/0.15.2/#toc-panic) [Â§](https://ziglang.org/documentation/0.15.2/#panic)

```
@panic(message: []const u8) noreturn
```

Invokes the panic handler function. By default the panic handler function calls the public `panic` function exposed in the root source file, or if there is not one specified, the `std.builtin.default_panic` function from `std/builtin.zig`.

Generally it is better to use `@import("std").debug.panic`. However, `@panic` can be useful for 2 scenarios:

-   From library code, calling the programmer's panic function if they exposed one in the root source file.

-   When mixing C and Zig code, calling the canonical panic implementation across multiple .o files.

See also:

-   [Panic Handler](https://ziglang.org/documentation/0.15.2/#Panic-Handler)

### [@popCount](https://ziglang.org/documentation/0.15.2/#toc-popCount) [Â§](https://ziglang.org/documentation/0.15.2/#popCount)

```
@popCount(operand: anytype) anytype
```

`@TypeOf(operand)` must be an integer type.

`operand` may be an [integer](https://ziglang.org/documentation/0.15.2/#Integers) or [vector](https://ziglang.org/documentation/0.15.2/#Vectors).

Counts the number of bits set in an integer - "population count".

The return type is an unsigned integer or vector of unsigned integers with the minimum number of bits that can represent the bit count of the integer type.

See also:

-   [@ctz](https://ziglang.org/documentation/0.15.2/#ctz)

-   [@clz](https://ziglang.org/documentation/0.15.2/#clz)

### [@prefetch](https://ziglang.org/documentation/0.15.2/#toc-prefetch) [Â§](https://ziglang.org/documentation/0.15.2/#prefetch)

```
@prefetch(ptr: anytype, comptime options: PrefetchOptions) void
```

This builtin tells the compiler to emit a prefetch instruction if supported by the target CPU. If the target CPU does not support the requested prefetch instruction, this builtin is a no-op. This function has no effect on the behavior of the program, only on the performance characteristics.

The `ptr` argument may be any pointer type and determines the memory address to prefetch. This function does not dereference the pointer, it is perfectly legal to pass a pointer to invalid memory to this function and no Illegal Behavior will result.

`PrefetchOptions` can be found with `@import("std").builtin.PrefetchOptions`.

### [@ptrCast](https://ziglang.org/documentation/0.15.2/#toc-ptrCast) [Â§](https://ziglang.org/documentation/0.15.2/#ptrCast)

```
@ptrCast(value: anytype) anytype
```

Converts a pointer of one type to a pointer of another type. The return type is the inferred result type.

[Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers) are allowed. Casting an optional pointer which is [null](https://ziglang.org/documentation/0.15.2/#null) to a non-optional pointer invokes safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

`@ptrCast` cannot be used for:

-   Removing `const` qualifier, use [@constCast](https://ziglang.org/documentation/0.15.2/#constCast).

-   Removing `volatile` qualifier, use [@volatileCast](https://ziglang.org/documentation/0.15.2/#volatileCast).
-   Changing pointer address space, use [@addrSpaceCast](https://ziglang.org/documentation/0.15.2/#addrSpaceCast).

-   Increasing pointer alignment, use [@alignCast](https://ziglang.org/documentation/0.15.2/#alignCast).
-   Casting a non-slice pointer to a slice, use slicing syntax `ptr[start..end]`.

### [@ptrFromInt](https://ziglang.org/documentation/0.15.2/#toc-ptrFromInt) [Â§](https://ziglang.org/documentation/0.15.2/#ptrFromInt)

```
@ptrFromInt(address: usize) anytype
```

Converts an integer to a [pointer](https://ziglang.org/documentation/0.15.2/#Pointers). The return type is the inferred result type. To convert the other way, use [@intFromPtr](https://ziglang.org/documentation/0.15.2/#intFromPtr). Casting an address of 0 to a destination type which in not [optional](https://ziglang.org/documentation/0.15.2/#Optional-Pointers) and does not have the `allowzero` attribute will result in a [Pointer Cast Invalid Null](https://ziglang.org/documentation/0.15.2/#Pointer-Cast-Invalid-Null) panic when runtime safety checks are enabled.

If the destination pointer type does not allow address zero and `address` is zero, this invokes safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

### [@rem](https://ziglang.org/documentation/0.15.2/#toc-rem) [Â§](https://ziglang.org/documentation/0.15.2/#rem)

```
@rem(numerator: T, denominator: T) T
```

Remainder division. For unsigned integers this is the same as `numerator % denominator`. Caller guarantees `denominator != 0`, otherwise the operation will result in a [Remainder Division by Zero](https://ziglang.org/documentation/0.15.2/#Remainder-Division-by-Zero) when runtime safety checks are enabled.

-   `@rem(-5, 3) == -2`

-   `(@divTrunc(a, b) * b) + @rem(a, b) == a`

For a function that returns an error code, see `@import("std").math.rem`.

See also:

-   [@mod](https://ziglang.org/documentation/0.15.2/#mod)

### [@returnAddress](https://ziglang.org/documentation/0.15.2/#toc-returnAddress) [Â§](https://ziglang.org/documentation/0.15.2/#returnAddress)

```
@returnAddress() usize
```

This function returns the address of the next machine code instruction that will be executed when the current function returns.

The implications of this are target-specific and not consistent across all platforms.

This function is only valid within function scope. If the function gets inlined into a calling function, the returned address will apply to the calling function.

### [@select](https://ziglang.org/documentation/0.15.2/#toc-select) [Â§](https://ziglang.org/documentation/0.15.2/#select)

```
@select(comptime T: type, pred: @Vector(len, bool), a: @Vector(len, T), b: @Vector(len, T)) @Vector(len, T)
```

Selects values element-wise from `a` or `b` based on `pred`. If `pred[i]` is `true`, the corresponding element in the result will be `a[i]` and otherwise `b[i]`.

See also:

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)

### [@setEvalBranchQuota](https://ziglang.org/documentation/0.15.2/#toc-setEvalBranchQuota) [Â§](https://ziglang.org/documentation/0.15.2/#setEvalBranchQuota)

```
@setEvalBranchQuota(comptime new_quota: u32) void
```

Increase the maximum number of backwards branches that compile-time code execution can use before giving up and making a compile error.

If the `new_quota` is smaller than the default quota (`1000`) or a previously explicitly set quota, it is ignored.

Example:

test\_without\_setEvalBranchQuota\_builtin.zig

```
test "foo" {
    comptime {
        var i = 0;
        while (i < 1001) : (i += 1) {}
    }
}
```

Shell

$ zig test test\_without\_setEvalBranchQuota\_builtin.zig
/home/andy/dev/zig/doc/langref/test\_without\_setEvalBranchQuota\_builtin.zig:4:9: error: evaluation exceeded 1000 backwards branches
        while (i < 1001) : (i += 1) {}
        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_without\_setEvalBranchQuota\_builtin.zig:4:9: note: use @setEvalBranchQuota() to raise the branch limit from 1000

Now we use `@setEvalBranchQuota`:

test\_setEvalBranchQuota\_builtin.zig

```
test "foo" {
    comptime {
        @setEvalBranchQuota(1001);
        var i = 0;
        while (i < 1001) : (i += 1) {}
    }
}
```

Shell

$ zig test test\_setEvalBranchQuota\_builtin.zig
1/1 test\_setEvalBranchQuota\_builtin.test.foo...OK
All 1 tests passed.

See also:

-   [comptime](https://ziglang.org/documentation/0.15.2/#comptime)

### [@setFloatMode](https://ziglang.org/documentation/0.15.2/#toc-setFloatMode) [Â§](https://ziglang.org/documentation/0.15.2/#setFloatMode)

```
@setFloatMode(comptime mode: FloatMode) void
```

Changes the current scope's rules about how floating point operations are defined.

-   `Strict` (default) - Floating point operations follow strict IEEE compliance.

-   `Optimized` - Floating point operations may do all of the following:
    
    -   Assume the arguments and result are not NaN. Optimizations are required to retain legal behavior over NaNs, but the value of the result is undefined.
    -   Assume the arguments and result are not +/-Inf. Optimizations are required to retain legal behavior over +/-Inf, but the value of the result is undefined.
    -   Treat the sign of a zero argument or result as insignificant.
    -   Use the reciprocal of an argument rather than perform division.
    -   Perform floating-point contraction (e.g. fusing a multiply followed by an addition into a fused multiply-add).
    -   Perform algebraically equivalent transformations that may change results in floating point (e.g. reassociate).
    
    This is equivalent to `-ffast-math` in GCC.

The floating point mode is inherited by child scopes, and can be overridden in any scope. You can set the floating point mode in a struct or module scope by using a comptime block.

`FloatMode` can be found with `@import("std").builtin.FloatMode`.

See also:

-   [Floating Point Operations](https://ziglang.org/documentation/0.15.2/#Floating-Point-Operations)

### [@setRuntimeSafety](https://ziglang.org/documentation/0.15.2/#toc-setRuntimeSafety) [Â§](https://ziglang.org/documentation/0.15.2/#setRuntimeSafety)

```
@setRuntimeSafety(comptime safety_on: bool) void
```

Sets whether runtime safety checks are enabled for the scope that contains the function call.

test\_setRuntimeSafety\_builtin.zig

```
test "@setRuntimeSafety" {
    // The builtin applies to the scope that it is called in. So here, integer overflow
    // will not be caught in ReleaseFast and ReleaseSmall modes:
    // var x: u8 = 255;
    // x += 1; // Unchecked Illegal Behavior in ReleaseFast/ReleaseSmall modes.
    {
        // However this block has safety enabled, so safety checks happen here,
        // even in ReleaseFast and ReleaseSmall modes.
        @setRuntimeSafety(true);
        var x: u8 = 255;
        x += 1;

        {
            // The value can be overridden at any scope. So here integer overflow
            // would not be caught in any build mode.
            @setRuntimeSafety(false);
            // var x: u8 = 255;
            // x += 1; // Unchecked Illegal Behavior in all build modes.
        }
    }
}
```

Shell

$ zig test test\_setRuntimeSafety\_builtin.zig -OReleaseFast
1/1 test\_setRuntimeSafety\_builtin.test.@setRuntimeSafety...thread 2902624 panic: integer overflow
/home/andy/dev/zig/doc/langref/test\_setRuntimeSafety\_builtin.zig:11:11: 0x103dc78 in test.@setRuntimeSafety (test)
        x += 1;
          ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x10312bf in main (test)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x102ee5d in posixCallMainAndExit (test)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x102e95d in \_start (test)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/7c580cf55e0b1cb6bb40fde0c61723ab/test --seed=0x2879e8a6

Note: it is [planned](https://github.com/ziglang/zig/issues/978) to replace `@setRuntimeSafety` with `@optimizeFor`

### [@shlExact](https://ziglang.org/documentation/0.15.2/#toc-shlExact) [Â§](https://ziglang.org/documentation/0.15.2/#shlExact)

```
@shlExact(value: T, shift_amt: Log2T) T
```

Performs the left shift operation (`<<`). For unsigned integers, the result is [undefined](https://ziglang.org/documentation/0.15.2/#undefined) if any 1 bits are shifted out. For signed integers, the result is [undefined](https://ziglang.org/documentation/0.15.2/#undefined) if any bits that disagree with the resultant sign bit are shifted out.

The type of `shift_amt` is an unsigned integer with `log2(@typeInfo(T).int.bits)` bits. This is because `shift_amt >= @typeInfo(T).int.bits` triggers safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

`comptime_int` is modeled as an integer with an infinite number of bits, meaning that in such case, `@shlExact` always produces a result and cannot produce a compile error.

See also:

-   [@shrExact](https://ziglang.org/documentation/0.15.2/#shrExact)

-   [@shlWithOverflow](https://ziglang.org/documentation/0.15.2/#shlWithOverflow)

### [@shlWithOverflow](https://ziglang.org/documentation/0.15.2/#toc-shlWithOverflow) [Â§](https://ziglang.org/documentation/0.15.2/#shlWithOverflow)

```
@shlWithOverflow(a: anytype, shift_amt: Log2T) struct { @TypeOf(a), u1 }
```

Performs `a << b` and returns a tuple with the result and a possible overflow bit.

The type of `shift_amt` is an unsigned integer with `log2(@typeInfo(@TypeOf(a)).int.bits)` bits. This is because `shift_amt >= @typeInfo(@TypeOf(a)).int.bits` triggers safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

See also:

-   [@shlExact](https://ziglang.org/documentation/0.15.2/#shlExact)

-   [@shrExact](https://ziglang.org/documentation/0.15.2/#shrExact)

### [@shrExact](https://ziglang.org/documentation/0.15.2/#toc-shrExact) [Â§](https://ziglang.org/documentation/0.15.2/#shrExact)

```
@shrExact(value: T, shift_amt: Log2T) T
```

Performs the right shift operation (`>>`). Caller guarantees that the shift will not shift any 1 bits out.

The type of `shift_amt` is an unsigned integer with `log2(@typeInfo(T).int.bits)` bits. This is because `shift_amt >= @typeInfo(T).int.bits` triggers safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

See also:

-   [@shlExact](https://ziglang.org/documentation/0.15.2/#shlExact)

-   [@shlWithOverflow](https://ziglang.org/documentation/0.15.2/#shlWithOverflow)

### [@shuffle](https://ziglang.org/documentation/0.15.2/#toc-shuffle) [Â§](https://ziglang.org/documentation/0.15.2/#shuffle)

```
@shuffle(comptime E: type, a: @Vector(a_len, E), b: @Vector(b_len, E), comptime mask: @Vector(mask_len, i32)) @Vector(mask_len, E)
```

Constructs a new [vector](https://ziglang.org/documentation/0.15.2/#Vectors) by selecting elements from `a` and `b` based on `mask`.

Each element in `mask` selects an element from either `a` or `b`. Positive numbers select from `a` starting at 0. Negative values select from `b`, starting at `-1` and going down. It is recommended to use the `~` operator for indexes from `b` so that both indexes can start from `0` (i.e. `~@as(i32, 0)` is `-1`).

For each element of `mask`, if it or the selected value from `a` or `b` is `undefined`, then the resulting element is `undefined`.

`a_len` and `b_len` may differ in length. Out-of-bounds element indexes in `mask` result in compile errors.

If `a` or `b` is `undefined`, it is equivalent to a vector of all `undefined` with the same length as the other vector. If both vectors are `undefined`, `@shuffle` returns a vector with all elements `undefined`.

`E` must be an [integer](https://ziglang.org/documentation/0.15.2/#Integers), [float](https://ziglang.org/documentation/0.15.2/#Floats), [pointer](https://ziglang.org/documentation/0.15.2/#Pointers), or `bool`. The mask may be any vector length, and its length determines the result length.

test\_shuffle\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "vector @shuffle" {
    const a = @Vector(7, u8){ 'o', 'l', 'h', 'e', 'r', 'z', 'w' };
    const b = @Vector(4, u8){ 'w', 'd', '!', 'x' };

    // To shuffle within a single vector, pass undefined as the second argument.
    // Notice that we can re-order, duplicate, or omit elements of the input vector
    const mask1 = @Vector(5, i32){ 2, 3, 1, 1, 0 };
    const res1: @Vector(5, u8) = @shuffle(u8, a, undefined, mask1);
    try expect(std.mem.eql(u8, &@as([5]u8, res1), "hello"));

    // Combining two vectors
    const mask2 = @Vector(6, i32){ -1, 0, 4, 1, -2, -3 };
    const res2: @Vector(6, u8) = @shuffle(u8, a, b, mask2);
    try expect(std.mem.eql(u8, &@as([6]u8, res2), "world!"));
}
```

Shell

$ zig test test\_shuffle\_builtin.zig
1/1 test\_shuffle\_builtin.test.vector @shuffle...OK
All 1 tests passed.

See also:

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)

### [@sizeOf](https://ziglang.org/documentation/0.15.2/#toc-sizeOf) [Â§](https://ziglang.org/documentation/0.15.2/#sizeOf)

```
@sizeOf(comptime T: type) comptime_int
```

This function returns the number of bytes it takes to store `T` in memory. The result is a target-specific compile time constant.

This size may contain padding bytes. If there were two consecutive T in memory, the padding would be the offset in bytes between element at index 0 and the element at index 1. For [integer](https://ziglang.org/documentation/0.15.2/#Integers), consider whether you want to use `@sizeOf(T)` or `@typeInfo(T).int.bits`.

This function measures the size at runtime. For types that are disallowed at runtime, such as `comptime_int` and `type`, the result is `0`.

See also:

-   [@bitSizeOf](https://ziglang.org/documentation/0.15.2/#bitSizeOf)

-   [@typeInfo](https://ziglang.org/documentation/0.15.2/#typeInfo)

### [@splat](https://ziglang.org/documentation/0.15.2/#toc-splat) [Â§](https://ziglang.org/documentation/0.15.2/#splat)

```
@splat(scalar: anytype) anytype
```

Produces an array or vector where each element is the value `scalar`. The return type and thus the length of the vector is inferred.

test\_splat\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "vector @splat" {
    const scalar: u32 = 5;
    const result: @Vector(4, u32) = @splat(scalar);
    try expect(std.mem.eql(u32, &@as([4]u32, result), &[_]u32{ 5, 5, 5, 5 }));
}

test "array @splat" {
    const scalar: u32 = 5;
    const result: [4]u32 = @splat(scalar);
    try expect(std.mem.eql(u32, &@as([4]u32, result), &[_]u32{ 5, 5, 5, 5 }));
}
```

Shell

$ zig test test\_splat\_builtin.zig
1/2 test\_splat\_builtin.test.vector @splat...OK
2/2 test\_splat\_builtin.test.array @splat...OK
All 2 tests passed.

`scalar` must be an [integer](https://ziglang.org/documentation/0.15.2/#Integers), [bool](https://ziglang.org/documentation/0.15.2/#Primitive-Types), [float](https://ziglang.org/documentation/0.15.2/#Floats), or [pointer](https://ziglang.org/documentation/0.15.2/#Pointers).

See also:

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)

-   [@shuffle](https://ziglang.org/documentation/0.15.2/#shuffle)

### [@reduce](https://ziglang.org/documentation/0.15.2/#toc-reduce) [Â§](https://ziglang.org/documentation/0.15.2/#reduce)

```
@reduce(comptime op: std.builtin.ReduceOp, value: anytype) E
```

Transforms a [vector](https://ziglang.org/documentation/0.15.2/#Vectors) into a scalar value (of type `E`) by performing a sequential horizontal reduction of its elements using the specified operator `op`.

Not every operator is available for every vector element type:

-   Every operator is available for [integer](https://ziglang.org/documentation/0.15.2/#Integers) vectors.

-   `.And`, `.Or`, `.Xor` are additionally available for `bool` vectors,
-   `.Min`, `.Max`, `.Add`, `.Mul` are additionally available for [floating point](https://ziglang.org/documentation/0.15.2/#Floats) vectors,

Note that `.Add` and `.Mul` reductions on integral types are wrapping; when applied on floating point types the operation associativity is preserved, unless the float mode is set to `Optimized`.

test\_reduce\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "vector @reduce" {
    const V = @Vector(4, i32);
    const value = V{ 1, -1, 1, -1 };
    const result = value > @as(V, @splat(0));
    // result is { true, false, true, false };
    try comptime expect(@TypeOf(result) == @Vector(4, bool));
    const is_all_true = @reduce(.And, result);
    try comptime expect(@TypeOf(is_all_true) == bool);
    try expect(is_all_true == false);
}
```

Shell

$ zig test test\_reduce\_builtin.zig
1/1 test\_reduce\_builtin.test.vector @reduce...OK
All 1 tests passed.

See also:

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)

-   [@setFloatMode](https://ziglang.org/documentation/0.15.2/#setFloatMode)

### [@src](https://ziglang.org/documentation/0.15.2/#toc-src) [Â§](https://ziglang.org/documentation/0.15.2/#src)

```
@src() std.builtin.SourceLocation
```

Returns a `SourceLocation` struct representing the function's name and location in the source code. This must be called in a function.

test\_src\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "@src" {
    try doTheTest();
}

fn doTheTest() !void {
    const src = @src();

    try expect(src.line == 9);
    try expect(src.column == 17);
    try expect(std.mem.endsWith(u8, src.fn_name, "doTheTest"));
    try expect(std.mem.endsWith(u8, src.file, "test_src_builtin.zig"));
}
```

Shell

$ zig test test\_src\_builtin.zig
1/1 test\_src\_builtin.test.@src...OK
All 1 tests passed.

### [@sqrt](https://ziglang.org/documentation/0.15.2/#toc-sqrt) [Â§](https://ziglang.org/documentation/0.15.2/#sqrt)

```
@sqrt(value: anytype) @TypeOf(value)
```

Performs the square root of a floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@sin](https://ziglang.org/documentation/0.15.2/#toc-sin) [Â§](https://ziglang.org/documentation/0.15.2/#sin)

```
@sin(value: anytype) @TypeOf(value)
```

Sine trigonometric function on a floating point number in radians. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@cos](https://ziglang.org/documentation/0.15.2/#toc-cos) [Â§](https://ziglang.org/documentation/0.15.2/#cos)

```
@cos(value: anytype) @TypeOf(value)
```

Cosine trigonometric function on a floating point number in radians. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@tan](https://ziglang.org/documentation/0.15.2/#toc-tan) [Â§](https://ziglang.org/documentation/0.15.2/#tan)

```
@tan(value: anytype) @TypeOf(value)
```

Tangent trigonometric function on a floating point number in radians. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@exp](https://ziglang.org/documentation/0.15.2/#toc-exp) [Â§](https://ziglang.org/documentation/0.15.2/#exp)

```
@exp(value: anytype) @TypeOf(value)
```

Base-e exponential function on a floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@exp2](https://ziglang.org/documentation/0.15.2/#toc-exp2) [Â§](https://ziglang.org/documentation/0.15.2/#exp2)

```
@exp2(value: anytype) @TypeOf(value)
```

Base-2 exponential function on a floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@log](https://ziglang.org/documentation/0.15.2/#toc-log) [Â§](https://ziglang.org/documentation/0.15.2/#log)

```
@log(value: anytype) @TypeOf(value)
```

Returns the natural logarithm of a floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@log2](https://ziglang.org/documentation/0.15.2/#toc-log2) [Â§](https://ziglang.org/documentation/0.15.2/#log2)

```
@log2(value: anytype) @TypeOf(value)
```

Returns the logarithm to the base 2 of a floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@log10](https://ziglang.org/documentation/0.15.2/#toc-log10) [Â§](https://ziglang.org/documentation/0.15.2/#log10)

```
@log10(value: anytype) @TypeOf(value)
```

Returns the logarithm to the base 10 of a floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@abs](https://ziglang.org/documentation/0.15.2/#toc-abs) [Â§](https://ziglang.org/documentation/0.15.2/#abs)

```
@abs(value: anytype) anytype
```

Returns the absolute value of an integer or a floating point number. Uses a dedicated hardware instruction when available. The return type is always an unsigned integer of the same bit width as the operand if the operand is an integer. Unsigned integer operands are supported. The builtin cannot overflow for signed integer operands.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats), [Integers](https://ziglang.org/documentation/0.15.2/#Integers) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats or integers.

### [@floor](https://ziglang.org/documentation/0.15.2/#toc-floor) [Â§](https://ziglang.org/documentation/0.15.2/#floor)

```
@floor(value: anytype) @TypeOf(value)
```

Returns the largest integral value not greater than the given floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@ceil](https://ziglang.org/documentation/0.15.2/#toc-ceil) [Â§](https://ziglang.org/documentation/0.15.2/#ceil)

```
@ceil(value: anytype) @TypeOf(value)
```

Returns the smallest integral value not less than the given floating point number. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@trunc](https://ziglang.org/documentation/0.15.2/#toc-trunc) [Â§](https://ziglang.org/documentation/0.15.2/#trunc)

```
@trunc(value: anytype) @TypeOf(value)
```

Rounds the given floating point number to an integer, towards zero. Uses a dedicated hardware instruction when available.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@round](https://ziglang.org/documentation/0.15.2/#toc-round) [Â§](https://ziglang.org/documentation/0.15.2/#round)

```
@round(value: anytype) @TypeOf(value)
```

Rounds the given floating point number to the nearest integer. If two integers are equally close, rounds away from zero. Uses a dedicated hardware instruction when available.

test\_round\_builtin.zig

```
const expect = @import("std").testing.expect;

test "@round" {
    try expect(@round(1.4) == 1);
    try expect(@round(1.5) == 2);
    try expect(@round(-1.4) == -1);
    try expect(@round(-2.5) == -3);
}
```

Shell

$ zig test test\_round\_builtin.zig
1/1 test\_round\_builtin.test.@round...OK
All 1 tests passed.

Supports [Floats](https://ziglang.org/documentation/0.15.2/#Floats) and [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors) of floats.

### [@subWithOverflow](https://ziglang.org/documentation/0.15.2/#toc-subWithOverflow) [Â§](https://ziglang.org/documentation/0.15.2/#subWithOverflow)

```
@subWithOverflow(a: anytype, b: anytype) struct { @TypeOf(a, b), u1 }
```

Performs `a - b` and returns a tuple with the result and a possible overflow bit.

### [@tagName](https://ziglang.org/documentation/0.15.2/#toc-tagName) [Â§](https://ziglang.org/documentation/0.15.2/#tagName)

```
@tagName(value: anytype) [:0]const u8
```

Converts an enum value or union value to a string literal representing the name.

If the enum is non-exhaustive and the tag value does not map to a name, it invokes safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

### [@This](https://ziglang.org/documentation/0.15.2/#toc-This) [Â§](https://ziglang.org/documentation/0.15.2/#This)

```
@This() type
```

Returns the innermost struct, enum, or union that this function call is inside. This can be useful for an anonymous struct that needs to refer to itself:

test\_this\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "@This()" {
    var items = [_]i32{ 1, 2, 3, 4 };
    const list = List(i32){ .items = items[0..] };
    try expect(list.length() == 4);
}

fn List(comptime T: type) type {
    return struct {
        const Self = @This();

        items: []T,

        fn length(self: Self) usize {
            return self.items.len;
        }
    };
}
```

Shell

$ zig test test\_this\_builtin.zig
1/1 test\_this\_builtin.test.@This()...OK
All 1 tests passed.

When `@This()` is used at file scope, it returns a reference to the struct that corresponds to the current file.

### [@trap](https://ziglang.org/documentation/0.15.2/#toc-trap) [Â§](https://ziglang.org/documentation/0.15.2/#trap)

```
@trap() noreturn
```

This function inserts a platform-specific trap/jam instruction which can be used to exit the program abnormally. This may be implemented by explicitly emitting an invalid instruction which may cause an illegal instruction exception of some sort. Unlike for `@breakpoint()`, execution does not continue after this point.

Outside function scope, this builtin causes a compile error.

See also:

-   [@breakpoint](https://ziglang.org/documentation/0.15.2/#breakpoint)

### [@truncate](https://ziglang.org/documentation/0.15.2/#toc-truncate) [Â§](https://ziglang.org/documentation/0.15.2/#truncate)

```
@truncate(integer: anytype) anytype
```

This function truncates bits from an integer type, resulting in a smaller or same-sized integer type. The return type is the inferred result type.

This function always truncates the significant bits of the integer, regardless of endianness on the target platform.

Calling `@truncate` on a number out of range of the destination type is well defined and working code:

test\_truncate\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "integer truncation" {
    const a: u16 = 0xabcd;
    const b: u8 = @truncate(a);
    try expect(b == 0xcd);
}
```

Shell

$ zig test test\_truncate\_builtin.zig
1/1 test\_truncate\_builtin.test.integer truncation...OK
All 1 tests passed.

Use [@intCast](https://ziglang.org/documentation/0.15.2/#intCast) to convert numbers guaranteed to fit the destination type.

### [@Type](https://ziglang.org/documentation/0.15.2/#toc-Type) [Â§](https://ziglang.org/documentation/0.15.2/#Type)

```
@Type(comptime info: std.builtin.Type) type
```

This function is the inverse of [@typeInfo](https://ziglang.org/documentation/0.15.2/#typeInfo). It reifies type information into a `type`.

It is available for the following types:

-   `type`

-   `noreturn`
-   `void`

-   `bool`
-   [Integers](https://ziglang.org/documentation/0.15.2/#Integers) - The maximum bit count for an integer type is `65535`.

-   [Floats](https://ziglang.org/documentation/0.15.2/#Floats)
-   [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers)

-   `comptime_int`
-   `comptime_float`

-   `@TypeOf(undefined)`
-   `@TypeOf(null)`

-   [Arrays](https://ziglang.org/documentation/0.15.2/#Arrays)
-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

-   [Error Set Type](https://ziglang.org/documentation/0.15.2/#Error-Set-Type)
-   [Error Union Type](https://ziglang.org/documentation/0.15.2/#Error-Union-Type)

-   [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors)
-   [opaque](https://ziglang.org/documentation/0.15.2/#opaque)

-   `anyframe`
-   [struct](https://ziglang.org/documentation/0.15.2/#struct)

-   [enum](https://ziglang.org/documentation/0.15.2/#enum)
-   [Enum Literals](https://ziglang.org/documentation/0.15.2/#Enum-Literals)

-   [union](https://ziglang.org/documentation/0.15.2/#union)
-   [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

### [@typeInfo](https://ziglang.org/documentation/0.15.2/#toc-typeInfo) [Â§](https://ziglang.org/documentation/0.15.2/#typeInfo)

```
@typeInfo(comptime T: type) std.builtin.Type
```

Provides type reflection.

Type information of [structs](https://ziglang.org/documentation/0.15.2/#struct), [unions](https://ziglang.org/documentation/0.15.2/#union), [enums](https://ziglang.org/documentation/0.15.2/#enum), and [error sets](https://ziglang.org/documentation/0.15.2/#Error-Set-Type) has fields which are guaranteed to be in the same order as appearance in the source file.

Type information of [structs](https://ziglang.org/documentation/0.15.2/#struct), [unions](https://ziglang.org/documentation/0.15.2/#union), [enums](https://ziglang.org/documentation/0.15.2/#enum), and [opaques](https://ziglang.org/documentation/0.15.2/#opaque) has declarations, which are also guaranteed to be in the same order as appearance in the source file.

### [@typeName](https://ziglang.org/documentation/0.15.2/#toc-typeName) [Â§](https://ziglang.org/documentation/0.15.2/#typeName)

```
@typeName(T: type) *const [N:0]u8
```

This function returns the string representation of a type, as an array. It is equivalent to a string literal of the type name. The returned type name is fully qualified with the parent namespace included as part of the type name with a series of dots.

### [@TypeOf](https://ziglang.org/documentation/0.15.2/#toc-TypeOf) [Â§](https://ziglang.org/documentation/0.15.2/#TypeOf)

```
@TypeOf(...) type
```

`@TypeOf` is a special builtin function that takes any (non-zero) number of expressions as parameters and returns the type of the result, using [Peer Type Resolution](https://ziglang.org/documentation/0.15.2/#Peer-Type-Resolution).

The expressions are evaluated, however they are guaranteed to have no *runtime* side-effects:

test\_TypeOf\_builtin.zig

```
const std = @import("std");
const expect = std.testing.expect;

test "no runtime side effects" {
    var data: i32 = 0;
    const T = @TypeOf(foo(i32, &data));
    try comptime expect(T == i32);
    try expect(data == 0);
}

fn foo(comptime T: type, ptr: *T) T {
    ptr.* += 1;
    return ptr.*;
}
```

Shell

$ zig test test\_TypeOf\_builtin.zig
1/1 test\_TypeOf\_builtin.test.no runtime side effects...OK
All 1 tests passed.

### [@unionInit](https://ziglang.org/documentation/0.15.2/#toc-unionInit) [Â§](https://ziglang.org/documentation/0.15.2/#unionInit)

```
@unionInit(comptime Union: type, comptime active_field_name: []const u8, init_expr) Union
```

This is the same thing as [union](https://ziglang.org/documentation/0.15.2/#union) initialization syntax, except that the field name is a [comptime](https://ziglang.org/documentation/0.15.2/#comptime)\-known value rather than an identifier token.

`@unionInit` forwards its [result location](https://ziglang.org/documentation/0.15.2/#Result-Location-Semantics) to `init_expr`.

### [@Vector](https://ziglang.org/documentation/0.15.2/#toc-Vector) [Â§](https://ziglang.org/documentation/0.15.2/#Vector)

```
@Vector(len: comptime_int, Element: type) type
```

Creates [Vectors](https://ziglang.org/documentation/0.15.2/#Vectors).

### [@volatileCast](https://ziglang.org/documentation/0.15.2/#toc-volatileCast) [Â§](https://ziglang.org/documentation/0.15.2/#volatileCast)

```
@volatileCast(value: anytype) DestType
```

Remove `volatile` qualifier from a pointer.

### [@workGroupId](https://ziglang.org/documentation/0.15.2/#toc-workGroupId) [Â§](https://ziglang.org/documentation/0.15.2/#workGroupId)

```
@workGroupId(comptime dimension: u32) u32
```

Returns the index of the work group in the current kernel invocation in dimension `dimension`.

### [@workGroupSize](https://ziglang.org/documentation/0.15.2/#toc-workGroupSize) [Â§](https://ziglang.org/documentation/0.15.2/#workGroupSize)

```
@workGroupSize(comptime dimension: u32) u32
```

Returns the number of work items that a work group has in dimension `dimension`.

### [@workItemId](https://ziglang.org/documentation/0.15.2/#toc-workItemId) [Â§](https://ziglang.org/documentation/0.15.2/#workItemId)

```
@workItemId(comptime dimension: u32) u32
```

Returns the index of the work item in the work group in dimension `dimension`. This function returns values between `0` (inclusive) and `@workGroupSize(dimension)` (exclusive).

## [Build Mode](https://ziglang.org/documentation/0.15.2/#toc-Build-Mode) [Â§](https://ziglang.org/documentation/0.15.2/#Build-Mode)

Zig has four build modes:

-   [Debug](https://ziglang.org/documentation/0.15.2/#Debug) (default)

-   [ReleaseFast](https://ziglang.org/documentation/0.15.2/#ReleaseFast)
-   [ReleaseSafe](https://ziglang.org/documentation/0.15.2/#ReleaseSafe)

-   [ReleaseSmall](https://ziglang.org/documentation/0.15.2/#ReleaseSmall)

To add standard build options to a `build.zig` file:

build.zig

```
const std = @import("std");

pub fn build(b: *std.Build) void {
    const optimize = b.standardOptimizeOption(.{});
    const exe = b.addExecutable(.{
        .name = "example",
        .root_module = b.createModule(.{
            .root_source_file = b.path("example.zig"),
            .optimize = optimize,
        }),
    });
    b.default_step.dependOn(&exe.step);
}
```

This causes these options to be available:

\-Doptimize=Debug

Optimizations off and safety on (default)

\-Doptimize=ReleaseSafe

Optimizations on and safety on

\-Doptimize=ReleaseFast

Optimizations on and safety off

\-Doptimize=ReleaseSmall

Size optimizations on and safety off

### [Debug](https://ziglang.org/documentation/0.15.2/#toc-Debug) [Â§](https://ziglang.org/documentation/0.15.2/#Debug)

Shell

$ zig build-exe example.zig

-   Fast compilation speed

-   Safety checks enabled
-   Slow runtime performance

-   Large binary size
-   No reproducible build requirement

### [ReleaseFast](https://ziglang.org/documentation/0.15.2/#toc-ReleaseFast) [Â§](https://ziglang.org/documentation/0.15.2/#ReleaseFast)

Shell

$ zig build-exe example.zig -O ReleaseFast

-   Fast runtime performance

-   Safety checks disabled
-   Slow compilation speed

-   Large binary size
-   Reproducible build

### [ReleaseSafe](https://ziglang.org/documentation/0.15.2/#toc-ReleaseSafe) [Â§](https://ziglang.org/documentation/0.15.2/#ReleaseSafe)

Shell

$ zig build-exe example.zig -O ReleaseSafe

-   Medium runtime performance

-   Safety checks enabled
-   Slow compilation speed

-   Large binary size
-   Reproducible build

### [ReleaseSmall](https://ziglang.org/documentation/0.15.2/#toc-ReleaseSmall) [Â§](https://ziglang.org/documentation/0.15.2/#ReleaseSmall)

Shell

$ zig build-exe example.zig -O ReleaseSmall

-   Medium runtime performance

-   Safety checks disabled
-   Slow compilation speed

-   Small binary size
-   Reproducible build

See also:

-   [Compile Variables](https://ziglang.org/documentation/0.15.2/#Compile-Variables)

-   [Zig Build System](https://ziglang.org/documentation/0.15.2/#Zig-Build-System)
-   [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior)

## [Single Threaded Builds](https://ziglang.org/documentation/0.15.2/#toc-Single-Threaded-Builds) [Â§](https://ziglang.org/documentation/0.15.2/#Single-Threaded-Builds)

Zig has a compile option \-fsingle-threaded which has the following effects:

-   All [Thread Local Variables](https://ziglang.org/documentation/0.15.2/#Thread-Local-Variables) are treated as regular [Container Level Variables](https://ziglang.org/documentation/0.15.2/#Container-Level-Variables).

-   The overhead of [Async Functions](https://ziglang.org/documentation/0.15.2/#Async-Functions) becomes equivalent to function call overhead.
-   The `@import("builtin").single_threaded` becomes `true` and therefore various userland APIs which read this variable become more efficient. For example `std.Mutex` becomes an empty data structure and all of its functions become no-ops.

## [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#toc-Illegal-Behavior) [Â§](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior)

Many operations in Zig trigger what is known as "Illegal Behavior" (IB). If Illegal Behavior is detected at compile-time, Zig emits a compile error and refuses to continue. Otherwise, when Illegal Behavior is not caught at compile-time, it falls into one of two categories.

Some Illegal Behavior is *safety-checked*: this means that the compiler will insert "safety checks" anywhere that the Illegal Behavior may occur at runtime, to determine whether it is about to happen. If it is, the safety check "fails", which triggers a panic.

All other Illegal Behavior is *unchecked*, meaning the compiler is unable to insert safety checks for it. If Unchecked Illegal Behavior is invoked at runtime, anything can happen: usually that will be some kind of crash, but the optimizer is free to make Unchecked Illegal Behavior do anything, such as calling arbitrary functions or clobbering arbitrary data. This is similar to the concept of "undefined behavior" in some other languages. Note that Unchecked Illegal Behavior still always results in a compile error if evaluated at [comptime](https://ziglang.org/documentation/0.15.2/#comptime), because the Zig compiler is able to perform more sophisticated checks at compile-time than at runtime.

Most Illegal Behavior is safety-checked. However, to facilitate optimizations, safety checks are disabled by default in the [ReleaseFast](https://ziglang.org/documentation/0.15.2/#ReleaseFast) and [ReleaseSmall](https://ziglang.org/documentation/0.15.2/#ReleaseSmall) optimization modes. Safety checks can also be enabled or disabled on a per-block basis, overriding the default for the current optimization mode, using [@setRuntimeSafety](https://ziglang.org/documentation/0.15.2/#setRuntimeSafety). When safety checks are disabled, Safety-Checked Illegal Behavior behaves like Unchecked Illegal Behavior; that is, any behavior may result from invoking it.

When a safety check fails, Zig's default panic handler crashes with a stack trace, like this:

test\_illegal\_behavior.zig

```
test "safety check" {
    unreachable;
}
```

Shell

$ zig test test\_illegal\_behavior.zig
1/1 test\_illegal\_behavior.test.safety check...thread 2892891 panic: reached unreachable code
/home/andy/dev/zig/doc/langref/test\_illegal\_behavior.zig:2:5: 0x102c00c in test.safety check (test\_illegal\_behavior.zig)
    unreachable;
    ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:218:25: 0x115cb20 in mainTerminal (test\_runner.zig)
        if (test\_fn.func()) |\_| {
                        ^
/home/andy/dev/zig/lib/compiler/test\_runner.zig:66:28: 0x1155d41 in main (test\_runner.zig)
        return mainTerminal();
                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x114fadd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x114f371 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
error: the following test command crashed:
/home/andy/dev/zig/.zig-cache/o/e72b27fd3a681a218f2215fb6e7fd433/test --seed=0xeebe2201

### [Reaching Unreachable Code](https://ziglang.org/documentation/0.15.2/#toc-Reaching-Unreachable-Code) [Â§](https://ziglang.org/documentation/0.15.2/#Reaching-Unreachable-Code)

At compile-time:

test\_comptime\_reaching\_unreachable.zig

```
comptime {
    assert(false);
}
fn assert(ok: bool) void {
    if (!ok) unreachable; // assertion failure
}
```

Shell

$ zig test test\_comptime\_reaching\_unreachable.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_reaching\_unreachable.zig:5:14: error: reached unreachable code
    if (!ok) unreachable; // assertion failure
             ^~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_comptime\_reaching\_unreachable.zig:2:11: note: called at comptime here
    assert(false);
    \~~~~~~^~~~~~~

At runtime:

runtime\_reaching\_unreachable.zig

```
const std = @import("std");

pub fn main() void {
    std.debug.assert(false);
}
```

Shell

$ zig build-exe runtime\_reaching\_unreachable.zig
$ ./runtime\_reaching\_unreachable
thread 2897013 panic: reached unreachable code
/home/andy/dev/zig/lib/std/debug.zig:559:14: 0x1044179 in assert (std.zig)
    if (!ok) unreachable; // assertion failure
             ^
/home/andy/dev/zig/doc/langref/runtime\_reaching\_unreachable.zig:4:21: 0x113e86e in main (runtime\_reaching\_unreachable.zig)
    std.debug.assert(false);
                    ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Index out of Bounds](https://ziglang.org/documentation/0.15.2/#toc-Index-out-of-Bounds) [Â§](https://ziglang.org/documentation/0.15.2/#Index-out-of-Bounds)

At compile-time:

test\_comptime\_index\_out\_of\_bounds.zig

```
comptime {
    const array: [5]u8 = "hello".*;
    const garbage = array[5];
    _ = garbage;
}
```

Shell

$ zig test test\_comptime\_index\_out\_of\_bounds.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_index\_out\_of\_bounds.zig:3:27: error: index 5 outside array of length 5
    const garbage = array\[5\];
                          ^

At runtime:

runtime\_index\_out\_of\_bounds.zig

```
pub fn main() void {
    const x = foo("hello");
    _ = x;
}

fn foo(x: []const u8) u8 {
    return x[5];
}
```

Shell

$ zig build-exe runtime\_index\_out\_of\_bounds.zig
$ ./runtime\_index\_out\_of\_bounds
thread 2893998 panic: index out of bounds: index 5, len 5
/home/andy/dev/zig/doc/langref/runtime\_index\_out\_of\_bounds.zig:7:13: 0x113fae6 in foo (runtime\_index\_out\_of\_bounds.zig)
    return x\[5\];
            ^
/home/andy/dev/zig/doc/langref/runtime\_index\_out\_of\_bounds.zig:2:18: 0x113e87a in main (runtime\_index\_out\_of\_bounds.zig)
    const x = foo("hello");
                 ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Cast Negative Number to Unsigned Integer](https://ziglang.org/documentation/0.15.2/#toc-Cast-Negative-Number-to-Unsigned-Integer) [Â§](https://ziglang.org/documentation/0.15.2/#Cast-Negative-Number-to-Unsigned-Integer)

At compile-time:

test\_comptime\_invalid\_cast.zig

```
comptime {
    const value: i32 = -1;
    const unsigned: u32 = @intCast(value);
    _ = unsigned;
}
```

Shell

$ zig test test\_comptime\_invalid\_cast.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_cast.zig:3:36: error: type 'u32' cannot represent integer value '-1'
    const unsigned: u32 = @intCast(value);
                                   ^~~~~

At runtime:

runtime\_invalid\_cast.zig

```
const std = @import("std");

pub fn main() void {
    var value: i32 = -1; // runtime-known
    _ = &value;
    const unsigned: u32 = @intCast(value);
    std.debug.print("value: {}\n", .{unsigned});
}
```

Shell

$ zig build-exe runtime\_invalid\_cast.zig
$ ./runtime\_invalid\_cast
thread 2899906 panic: integer does not fit in destination type
/home/andy/dev/zig/doc/langref/runtime\_invalid\_cast.zig:6:27: 0x113e87f in main (runtime\_invalid\_cast.zig)
    const unsigned: u32 = @intCast(value);
                          ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

To obtain the maximum value of an unsigned integer, use `std.math.maxInt`.

### [Cast Truncates Data](https://ziglang.org/documentation/0.15.2/#toc-Cast-Truncates-Data) [Â§](https://ziglang.org/documentation/0.15.2/#Cast-Truncates-Data)

At compile-time:

test\_comptime\_invalid\_cast\_truncate.zig

```
comptime {
    const spartan_count: u16 = 300;
    const byte: u8 = @intCast(spartan_count);
    _ = byte;
}
```

Shell

$ zig test test\_comptime\_invalid\_cast\_truncate.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_cast\_truncate.zig:3:31: error: type 'u8' cannot represent integer value '300'
    const byte: u8 = @intCast(spartan\_count);
                              ^~~~~~~~~~~~~

At runtime:

runtime\_invalid\_cast\_truncate.zig

```
const std = @import("std");

pub fn main() void {
    var spartan_count: u16 = 300; // runtime-known
    _ = &spartan_count;
    const byte: u8 = @intCast(spartan_count);
    std.debug.print("value: {}\n", .{byte});
}
```

Shell

$ zig build-exe runtime\_invalid\_cast\_truncate.zig
$ ./runtime\_invalid\_cast\_truncate
thread 2899317 panic: integer does not fit in destination type
/home/andy/dev/zig/doc/langref/runtime\_invalid\_cast\_truncate.zig:6:22: 0x113e880 in main (runtime\_invalid\_cast\_truncate.zig)
    const byte: u8 = @intCast(spartan\_count);
                     ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

To truncate bits, use [@truncate](https://ziglang.org/documentation/0.15.2/#truncate).

### [Integer Overflow](https://ziglang.org/documentation/0.15.2/#toc-Integer-Overflow) [Â§](https://ziglang.org/documentation/0.15.2/#Integer-Overflow)

#### [Default Operations](https://ziglang.org/documentation/0.15.2/#toc-Default-Operations) [Â§](https://ziglang.org/documentation/0.15.2/#Default-Operations)

The following operators can cause integer overflow:

-   `+` (addition)

-   `-` (subtraction)
-   `-` (negation)

-   `*` (multiplication)
-   `/` (division)

-   [@divTrunc](https://ziglang.org/documentation/0.15.2/#divTrunc) (division)
-   [@divFloor](https://ziglang.org/documentation/0.15.2/#divFloor) (division)

-   [@divExact](https://ziglang.org/documentation/0.15.2/#divExact) (division)

Example with addition at compile-time:

test\_comptime\_overflow.zig

```
comptime {
    var byte: u8 = 255;
    byte += 1;
}
```

Shell

$ zig test test\_comptime\_overflow.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_overflow.zig:3:10: error: overflow of integer type 'u8' with value '256'
    byte += 1;
    \~~~~~^~~~

At runtime:

runtime\_overflow.zig

```
const std = @import("std");

pub fn main() void {
    var byte: u8 = 255;
    byte += 1;
    std.debug.print("value: {}\n", .{byte});
}
```

Shell

$ zig build-exe runtime\_overflow.zig
$ ./runtime\_overflow
thread 2892886 panic: integer overflow
/home/andy/dev/zig/doc/langref/runtime\_overflow.zig:5:10: 0x113e895 in main (runtime\_overflow.zig)
    byte += 1;
         ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

#### [Standard Library Math Functions](https://ziglang.org/documentation/0.15.2/#toc-Standard-Library-Math-Functions) [Â§](https://ziglang.org/documentation/0.15.2/#Standard-Library-Math-Functions)

These functions provided by the standard library return possible errors.

-   `@import("std").math.add`

-   `@import("std").math.sub`
-   `@import("std").math.mul`

-   `@import("std").math.divTrunc`
-   `@import("std").math.divFloor`

-   `@import("std").math.divExact`
-   `@import("std").math.shl`

Example of catching an overflow for addition:

math\_add.zig

```
const math = @import("std").math;
const print = @import("std").debug.print;
pub fn main() !void {
    var byte: u8 = 255;

    byte = if (math.add(u8, byte, 1)) |result| result else |err| {
        print("unable to add one: {s}\n", .{@errorName(err)});
        return err;
    };

    print("result: {}\n", .{byte});
}
```

Shell

$ zig build-exe math\_add.zig
$ ./math\_add
unable to add one: Overflow
error: Overflow
/home/andy/dev/zig/lib/std/math.zig:570:21: 0x113ebae in add\_\_anon\_22552 (std.zig)
    if (ov\[1\] != 0) return error.Overflow;
                    ^
/home/andy/dev/zig/doc/langref/math\_add.zig:8:9: 0x113d422 in main (math\_add.zig)
        return err;
        ^

#### [Builtin Overflow Functions](https://ziglang.org/documentation/0.15.2/#toc-Builtin-Overflow-Functions) [Â§](https://ziglang.org/documentation/0.15.2/#Builtin-Overflow-Functions)

These builtins return a tuple containing whether there was an overflow (as a `u1`) and the possibly overflowed bits of the operation:

-   [@addWithOverflow](https://ziglang.org/documentation/0.15.2/#addWithOverflow)

-   [@subWithOverflow](https://ziglang.org/documentation/0.15.2/#subWithOverflow)
-   [@mulWithOverflow](https://ziglang.org/documentation/0.15.2/#mulWithOverflow)

-   [@shlWithOverflow](https://ziglang.org/documentation/0.15.2/#shlWithOverflow)

Example of [@addWithOverflow](https://ziglang.org/documentation/0.15.2/#addWithOverflow):

addWithOverflow\_builtin.zig

```
const print = @import("std").debug.print;
pub fn main() void {
    const byte: u8 = 255;

    const ov = @addWithOverflow(byte, 10);
    if (ov[1] != 0) {
        print("overflowed result: {}\n", .{ov[0]});
    } else {
        print("result: {}\n", .{ov[0]});
    }
}
```

Shell

$ zig build-exe addWithOverflow\_builtin.zig
$ ./addWithOverflow\_builtin
overflowed result: 9

#### [Wrapping Operations](https://ziglang.org/documentation/0.15.2/#toc-Wrapping-Operations) [Â§](https://ziglang.org/documentation/0.15.2/#Wrapping-Operations)

These operations have guaranteed wraparound semantics.

-   `+%` (wraparound addition)

-   `-%` (wraparound subtraction)
-   `-%` (wraparound negation)

-   `*%` (wraparound multiplication)

test\_wraparound\_semantics.zig

```
const std = @import("std");
const expect = std.testing.expect;
const minInt = std.math.minInt;
const maxInt = std.math.maxInt;

test "wraparound addition and subtraction" {
    const x: i32 = maxInt(i32);
    const min_val = x +% 1;
    try expect(min_val == minInt(i32));
    const max_val = min_val -% 1;
    try expect(max_val == maxInt(i32));
}
```

Shell

$ zig test test\_wraparound\_semantics.zig
1/1 test\_wraparound\_semantics.test.wraparound addition and subtraction...OK
All 1 tests passed.

### [Exact Left Shift Overflow](https://ziglang.org/documentation/0.15.2/#toc-Exact-Left-Shift-Overflow) [Â§](https://ziglang.org/documentation/0.15.2/#Exact-Left-Shift-Overflow)

At compile-time:

test\_comptime\_shlExact\_overflow.zig

```
comptime {
    const x = @shlExact(@as(u8, 0b01010101), 2);
    _ = x;
}
```

Shell

$ zig test test\_comptime\_shlExact\_overflow.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_shlExact\_overflow.zig:2:15: error: overflow of integer type 'u8' with value '340'
    const x = @shlExact(@as(u8, 0b01010101), 2);
              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At runtime:

runtime\_shlExact\_overflow.zig

```
const std = @import("std");

pub fn main() void {
    var x: u8 = 0b01010101; // runtime-known
    _ = &x;
    const y = @shlExact(x, 2);
    std.debug.print("value: {}\n", .{y});
}
```

Shell

$ zig build-exe runtime\_shlExact\_overflow.zig
$ ./runtime\_shlExact\_overflow
thread 2896313 panic: left shift overflowed bits
/home/andy/dev/zig/doc/langref/runtime\_shlExact\_overflow.zig:6:5: 0x113e8a1 in main (runtime\_shlExact\_overflow.zig)
    const y = @shlExact(x, 2);
    ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Exact Right Shift Overflow](https://ziglang.org/documentation/0.15.2/#toc-Exact-Right-Shift-Overflow) [Â§](https://ziglang.org/documentation/0.15.2/#Exact-Right-Shift-Overflow)

At compile-time:

test\_comptime\_shrExact\_overflow.zig

```
comptime {
    const x = @shrExact(@as(u8, 0b10101010), 2);
    _ = x;
}
```

Shell

$ zig test test\_comptime\_shrExact\_overflow.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_shrExact\_overflow.zig:2:15: error: exact shift shifted out 1 bits
    const x = @shrExact(@as(u8, 0b10101010), 2);
              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At runtime:

runtime\_shrExact\_overflow.zig

```
const builtin = @import("builtin");
const std = @import("std");

pub fn main() void {
    var x: u8 = 0b10101010; // runtime-known
    _ = &x;
    const y = @shrExact(x, 2);
    std.debug.print("value: {}\n", .{y});

    if (builtin.cpu.arch.isRISCV() and builtin.zig_backend == .stage2_llvm) @panic("https://github.com/ziglang/zig/issues/24304");
}
```

Shell

$ zig build-exe runtime\_shrExact\_overflow.zig
$ ./runtime\_shrExact\_overflow
thread 2897712 panic: right shift overflowed bits
/home/andy/dev/zig/doc/langref/runtime\_shrExact\_overflow.zig:7:5: 0x113e88a in main (runtime\_shrExact\_overflow.zig)
    const y = @shrExact(x, 2);
    ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Division by Zero](https://ziglang.org/documentation/0.15.2/#toc-Division-by-Zero) [Â§](https://ziglang.org/documentation/0.15.2/#Division-by-Zero)

At compile-time:

test\_comptime\_division\_by\_zero.zig

```
comptime {
    const a: i32 = 1;
    const b: i32 = 0;
    const c = a / b;
    _ = c;
}
```

Shell

$ zig test test\_comptime\_division\_by\_zero.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_division\_by\_zero.zig:4:19: error: division by zero here causes illegal behavior
    const c = a / b;
                  ^

At runtime:

runtime\_division\_by\_zero.zig

```
const std = @import("std");

pub fn main() void {
    var a: u32 = 1;
    var b: u32 = 0;
    _ = .{ &a, &b };
    const c = a / b;
    std.debug.print("value: {}\n", .{c});
}
```

Shell

$ zig build-exe runtime\_division\_by\_zero.zig
$ ./runtime\_division\_by\_zero
thread 2902461 panic: division by zero
/home/andy/dev/zig/doc/langref/runtime\_division\_by\_zero.zig:7:17: 0x113e890 in main (runtime\_division\_by\_zero.zig)
    const c = a / b;
                ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Remainder Division by Zero](https://ziglang.org/documentation/0.15.2/#toc-Remainder-Division-by-Zero) [Â§](https://ziglang.org/documentation/0.15.2/#Remainder-Division-by-Zero)

At compile-time:

test\_comptime\_remainder\_division\_by\_zero.zig

```
comptime {
    const a: i32 = 10;
    const b: i32 = 0;
    const c = a % b;
    _ = c;
}
```

Shell

$ zig test test\_comptime\_remainder\_division\_by\_zero.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_remainder\_division\_by\_zero.zig:4:19: error: division by zero here causes illegal behavior
    const c = a % b;
                  ^

At runtime:

runtime\_remainder\_division\_by\_zero.zig

```
const std = @import("std");

pub fn main() void {
    var a: u32 = 10;
    var b: u32 = 0;
    _ = .{ &a, &b };
    const c = a % b;
    std.debug.print("value: {}\n", .{c});
}
```

Shell

$ zig build-exe runtime\_remainder\_division\_by\_zero.zig
$ ./runtime\_remainder\_division\_by\_zero
thread 2899727 panic: division by zero
/home/andy/dev/zig/doc/langref/runtime\_remainder\_division\_by\_zero.zig:7:17: 0x113e890 in main (runtime\_remainder\_division\_by\_zero.zig)
    const c = a % b;
                ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Exact Division Remainder](https://ziglang.org/documentation/0.15.2/#toc-Exact-Division-Remainder) [Â§](https://ziglang.org/documentation/0.15.2/#Exact-Division-Remainder)

At compile-time:

test\_comptime\_divExact\_remainder.zig

```
comptime {
    const a: u32 = 10;
    const b: u32 = 3;
    const c = @divExact(a, b);
    _ = c;
}
```

Shell

$ zig test test\_comptime\_divExact\_remainder.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_divExact\_remainder.zig:4:15: error: exact division produced remainder
    const c = @divExact(a, b);
              ^~~~~~~~~~~~~~~

At runtime:

runtime\_divExact\_remainder.zig

```
const std = @import("std");

pub fn main() void {
    var a: u32 = 10;
    var b: u32 = 3;
    _ = .{ &a, &b };
    const c = @divExact(a, b);
    std.debug.print("value: {}\n", .{c});
}
```

Shell

$ zig build-exe runtime\_divExact\_remainder.zig
$ ./runtime\_divExact\_remainder
thread 2901529 panic: exact division produced remainder
/home/andy/dev/zig/doc/langref/runtime\_divExact\_remainder.zig:7:15: 0x113e8c7 in main (runtime\_divExact\_remainder.zig)
    const c = @divExact(a, b);
              ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Attempt to Unwrap Null](https://ziglang.org/documentation/0.15.2/#toc-Attempt-to-Unwrap-Null) [Â§](https://ziglang.org/documentation/0.15.2/#Attempt-to-Unwrap-Null)

At compile-time:

test\_comptime\_unwrap\_null.zig

```
comptime {
    const optional_number: ?i32 = null;
    const number = optional_number.?;
    _ = number;
}
```

Shell

$ zig test test\_comptime\_unwrap\_null.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_unwrap\_null.zig:3:35: error: unable to unwrap null
    const number = optional\_number.?;
                   \~~~~~~~~~~~~~~~^~

At runtime:

runtime\_unwrap\_null.zig

```
const std = @import("std");

pub fn main() void {
    var optional_number: ?i32 = null;
    _ = &optional_number;
    const number = optional_number.?;
    std.debug.print("value: {}\n", .{number});
}
```

Shell

$ zig build-exe runtime\_unwrap\_null.zig
$ ./runtime\_unwrap\_null
thread 2892887 panic: attempt to use null value
/home/andy/dev/zig/doc/langref/runtime\_unwrap\_null.zig:6:35: 0x113e8b4 in main (runtime\_unwrap\_null.zig)
    const number = optional\_number.?;
                                  ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

One way to avoid this crash is to test for null instead of assuming non-null, with the `if` expression:

testing\_null\_with\_if.zig

```
const print = @import("std").debug.print;
pub fn main() void {
    const optional_number: ?i32 = null;

    if (optional_number) |number| {
        print("got number: {}\n", .{number});
    } else {
        print("it's null\n", .{});
    }
}
```

Shell

$ zig build-exe testing\_null\_with\_if.zig
$ ./testing\_null\_with\_if
it's null

See also:

-   [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals)

### [Attempt to Unwrap Error](https://ziglang.org/documentation/0.15.2/#toc-Attempt-to-Unwrap-Error) [Â§](https://ziglang.org/documentation/0.15.2/#Attempt-to-Unwrap-Error)

At compile-time:

test\_comptime\_unwrap\_error.zig

```
comptime {
    const number = getNumberOrFail() catch unreachable;
    _ = number;
}

fn getNumberOrFail() !i32 {
    return error.UnableToReturnNumber;
}
```

Shell

$ zig test test\_comptime\_unwrap\_error.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_unwrap\_error.zig:2:44: error: caught unexpected error 'UnableToReturnNumber'
    const number = getNumberOrFail() catch unreachable;
                                           ^~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_comptime\_unwrap\_error.zig:7:18: note: error returned here
    return error.UnableToReturnNumber;
                 ^~~~~~~~~~~~~~~~~~~~

At runtime:

runtime\_unwrap\_error.zig

```
const std = @import("std");

pub fn main() void {
    const number = getNumberOrFail() catch unreachable;
    std.debug.print("value: {}\n", .{number});
}

fn getNumberOrFail() !i32 {
    return error.UnableToReturnNumber;
}
```

Shell

$ zig build-exe runtime\_unwrap\_error.zig
$ ./runtime\_unwrap\_error
thread 2895126 panic: attempt to unwrap error: UnableToReturnNumber
/home/andy/dev/zig/doc/langref/runtime\_unwrap\_error.zig:9:5: 0x113e86c in getNumberOrFail (runtime\_unwrap\_error.zig)
    return error.UnableToReturnNumber;
    ^
/home/andy/dev/zig/doc/langref/runtime\_unwrap\_error.zig:4:44: 0x113e8d3 in main (runtime\_unwrap\_error.zig)
    const number = getNumberOrFail() catch unreachable;
                                           ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

One way to avoid this crash is to test for an error instead of assuming a successful result, with the `if` expression:

testing\_error\_with\_if.zig

```
const print = @import("std").debug.print;

pub fn main() void {
    const result = getNumberOrFail();

    if (result) |number| {
        print("got number: {}\n", .{number});
    } else |err| {
        print("got error: {s}\n", .{@errorName(err)});
    }
}

fn getNumberOrFail() !i32 {
    return error.UnableToReturnNumber;
}
```

Shell

$ zig build-exe testing\_error\_with\_if.zig
$ ./testing\_error\_with\_if
got error: UnableToReturnNumber

See also:

-   [Errors](https://ziglang.org/documentation/0.15.2/#Errors)

### [Invalid Error Code](https://ziglang.org/documentation/0.15.2/#toc-Invalid-Error-Code) [Â§](https://ziglang.org/documentation/0.15.2/#Invalid-Error-Code)

At compile-time:

test\_comptime\_invalid\_error\_code.zig

```
comptime {
    const err = error.AnError;
    const number = @intFromError(err) + 10;
    const invalid_err = @errorFromInt(number);
    _ = invalid_err;
}
```

Shell

$ zig test test\_comptime\_invalid\_error\_code.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_error\_code.zig:4:39: error: integer value '11' represents no error
    const invalid\_err = @errorFromInt(number);
                                      ^~~~~~

At runtime:

runtime\_invalid\_error\_code.zig

```
const std = @import("std");

pub fn main() void {
    const err = error.AnError;
    var number = @intFromError(err) + 500;
    _ = &number;
    const invalid_err = @errorFromInt(number);
    std.debug.print("value: {}\n", .{invalid_err});
}
```

Shell

$ zig build-exe runtime\_invalid\_error\_code.zig
$ ./runtime\_invalid\_error\_code
thread 2900570 panic: invalid error code
/home/andy/dev/zig/doc/langref/runtime\_invalid\_error\_code.zig:7:5: 0x113e8a7 in main (runtime\_invalid\_error\_code.zig)
    const invalid\_err = @errorFromInt(number);
    ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Invalid Enum Cast](https://ziglang.org/documentation/0.15.2/#toc-Invalid-Enum-Cast) [Â§](https://ziglang.org/documentation/0.15.2/#Invalid-Enum-Cast)

At compile-time:

test\_comptime\_invalid\_enum\_cast.zig

```
const Foo = enum {
    a,
    b,
    c,
};
comptime {
    const a: u2 = 3;
    const b: Foo = @enumFromInt(a);
    _ = b;
}
```

Shell

$ zig test test\_comptime\_invalid\_enum\_cast.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_enum\_cast.zig:8:20: error: enum 'test\_comptime\_invalid\_enum\_cast.Foo' has no tag with value '3'
    const b: Foo = @enumFromInt(a);
                   ^~~~~~~~~~~~~~~
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_enum\_cast.zig:1:13: note: enum declared here
const Foo = enum {
            ^~~~

At runtime:

runtime\_invalid\_enum\_cast.zig

```
const std = @import("std");

const Foo = enum {
    a,
    b,
    c,
};

pub fn main() void {
    var a: u2 = 3;
    _ = &a;
    const b: Foo = @enumFromInt(a);
    std.debug.print("value: {s}\n", .{@tagName(b)});
}
```

Shell

$ zig build-exe runtime\_invalid\_enum\_cast.zig
$ ./runtime\_invalid\_enum\_cast
thread 2902395 panic: invalid enum value
/home/andy/dev/zig/doc/langref/runtime\_invalid\_enum\_cast.zig:12:20: 0x113e8f0 in main (runtime\_invalid\_enum\_cast.zig)
    const b: Foo = @enumFromInt(a);
                   ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Invalid Error Set Cast](https://ziglang.org/documentation/0.15.2/#toc-Invalid-Error-Set-Cast) [Â§](https://ziglang.org/documentation/0.15.2/#Invalid-Error-Set-Cast)

At compile-time:

test\_comptime\_invalid\_error\_set\_cast.zig

```
const Set1 = error{
    A,
    B,
};
const Set2 = error{
    A,
    C,
};
comptime {
    _ = @as(Set2, @errorCast(Set1.B));
}
```

Shell

$ zig test test\_comptime\_invalid\_error\_set\_cast.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_error\_set\_cast.zig:10:19: error: 'error.B' not a member of error set 'error{A,C}'
    \_ = @as(Set2, @errorCast(Set1.B));
                  ^~~~~~~~~~~~~~~~~~

At runtime:

runtime\_invalid\_error\_set\_cast.zig

```
const std = @import("std");

const Set1 = error{
    A,
    B,
};
const Set2 = error{
    A,
    C,
};
pub fn main() void {
    foo(Set1.B);
}
fn foo(set1: Set1) void {
    const x: Set2 = @errorCast(set1);
    std.debug.print("value: {}\n", .{x});
}
```

Shell

$ zig build-exe runtime\_invalid\_error\_set\_cast.zig
$ ./runtime\_invalid\_error\_set\_cast
thread 2900078 panic: invalid error code
/home/andy/dev/zig/doc/langref/runtime\_invalid\_error\_set\_cast.zig:15:21: 0x113fb3c in foo (runtime\_invalid\_error\_set\_cast.zig)
    const x: Set2 = @errorCast(set1);
                    ^
/home/andy/dev/zig/doc/langref/runtime\_invalid\_error\_set\_cast.zig:12:8: 0x113e877 in main (runtime\_invalid\_error\_set\_cast.zig)
    foo(Set1.B);
       ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Incorrect Pointer Alignment](https://ziglang.org/documentation/0.15.2/#toc-Incorrect-Pointer-Alignment) [Â§](https://ziglang.org/documentation/0.15.2/#Incorrect-Pointer-Alignment)

At compile-time:

test\_comptime\_incorrect\_pointer\_alignment.zig

```
comptime {
    const ptr: *align(1) i32 = @ptrFromInt(0x1);
    const aligned: *align(4) i32 = @alignCast(ptr);
    _ = aligned;
}
```

Shell

$ zig test test\_comptime\_incorrect\_pointer\_alignment.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_incorrect\_pointer\_alignment.zig:3:47: error: pointer address 0x1 is not aligned to 4 bytes
    const aligned: \*align(4) i32 = @alignCast(ptr);
                                              ^~~

At runtime:

runtime\_incorrect\_pointer\_alignment.zig

```
const mem = @import("std").mem;
pub fn main() !void {
    var array align(4) = [_]u32{ 0x11111111, 0x11111111 };
    const bytes = mem.sliceAsBytes(array[0..]);
    if (foo(bytes) != 0x11111111) return error.Wrong;
}
fn foo(bytes: []u8) u32 {
    const slice4 = bytes[1..5];
    const int_slice = mem.bytesAsSlice(u32, @as([]align(4) u8, @alignCast(slice4)));
    return int_slice[0];
}
```

Shell

$ zig build-exe runtime\_incorrect\_pointer\_alignment.zig
$ ./runtime\_incorrect\_pointer\_alignment
thread 2897041 panic: incorrect alignment
/home/andy/dev/zig/doc/langref/runtime\_incorrect\_pointer\_alignment.zig:9:64: 0x113ec08 in foo (runtime\_incorrect\_pointer\_alignment.zig)
    const int\_slice = mem.bytesAsSlice(u32, @as(\[\]align(4) u8, @alignCast(slice4)));
                                                               ^
/home/andy/dev/zig/doc/langref/runtime\_incorrect\_pointer\_alignment.zig:5:12: 0x113d3f2 in main (runtime\_incorrect\_pointer\_alignment.zig)
    if (foo(bytes) != 0x11111111) return error.Wrong;
           ^
/home/andy/dev/zig/lib/std/start.zig:627:37: 0x113dbc9 in posixCallMainAndExit (std.zig)
            const result = root.main() catch |err| {
                                    ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Wrong Union Field Access](https://ziglang.org/documentation/0.15.2/#toc-Wrong-Union-Field-Access) [Â§](https://ziglang.org/documentation/0.15.2/#Wrong-Union-Field-Access)

At compile-time:

test\_comptime\_wrong\_union\_field\_access.zig

```
comptime {
    var f = Foo{ .int = 42 };
    f.float = 12.34;
}

const Foo = union {
    float: f32,
    int: u32,
};
```

Shell

$ zig test test\_comptime\_wrong\_union\_field\_access.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_wrong\_union\_field\_access.zig:3:6: error: access of union field 'float' while field 'int' is active
    f.float = 12.34;
    ~^~~~~~
/home/andy/dev/zig/doc/langref/test\_comptime\_wrong\_union\_field\_access.zig:6:13: note: union declared here
const Foo = union {
            ^~~~~

At runtime:

runtime\_wrong\_union\_field\_access.zig

```
const std = @import("std");

const Foo = union {
    float: f32,
    int: u32,
};

pub fn main() void {
    var f = Foo{ .int = 42 };
    bar(&f);
}

fn bar(f: *Foo) void {
    f.float = 12.34;
    std.debug.print("value: {}\n", .{f.float});
}
```

Shell

$ zig build-exe runtime\_wrong\_union\_field\_access.zig
$ ./runtime\_wrong\_union\_field\_access
thread 2901950 panic: access of union field 'float' while field 'int' is active
/home/andy/dev/zig/doc/langref/runtime\_wrong\_union\_field\_access.zig:14:6: 0x113fb1e in bar (runtime\_wrong\_union\_field\_access.zig)
    f.float = 12.34;
     ^
/home/andy/dev/zig/doc/langref/runtime\_wrong\_union\_field\_access.zig:10:8: 0x113e89f in main (runtime\_wrong\_union\_field\_access.zig)
    bar(&f);
       ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

This safety is not available for `extern` or `packed` unions.

To change the active field of a union, assign the entire union, like this:

change\_active\_union\_field.zig

```
const std = @import("std");

const Foo = union {
    float: f32,
    int: u32,
};

pub fn main() void {
    var f = Foo{ .int = 42 };
    bar(&f);
}

fn bar(f: *Foo) void {
    f.* = Foo{ .float = 12.34 };
    std.debug.print("value: {}\n", .{f.float});
}
```

Shell

$ zig build-exe change\_active\_union\_field.zig
$ ./change\_active\_union\_field
value: 12.34

To change the active field of a union when a meaningful value for the field is not known, use [undefined](https://ziglang.org/documentation/0.15.2/#undefined), like this:

undefined\_active\_union\_field.zig

```
const std = @import("std");

const Foo = union {
    float: f32,
    int: u32,
};

pub fn main() void {
    var f = Foo{ .int = 42 };
    f = Foo{ .float = undefined };
    bar(&f);
    std.debug.print("value: {}\n", .{f.float});
}

fn bar(f: *Foo) void {
    f.float = 12.34;
}
```

Shell

$ zig build-exe undefined\_active\_union\_field.zig
$ ./undefined\_active\_union\_field
value: 12.34

See also:

-   [union](https://ziglang.org/documentation/0.15.2/#union)

-   [extern union](https://ziglang.org/documentation/0.15.2/#extern-union)

### [Out of Bounds Float to Integer Cast](https://ziglang.org/documentation/0.15.2/#toc-Out-of-Bounds-Float-to-Integer-Cast) [Â§](https://ziglang.org/documentation/0.15.2/#Out-of-Bounds-Float-to-Integer-Cast)

This happens when casting a float to an integer where the float has a value outside the integer type's range.

At compile-time:

test\_comptime\_out\_of\_bounds\_float\_to\_integer\_cast.zig

```
comptime {
    const float: f32 = 4294967296;
    const int: i32 = @intFromFloat(float);
    _ = int;
}
```

Shell

$ zig test test\_comptime\_out\_of\_bounds\_float\_to\_integer\_cast.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_out\_of\_bounds\_float\_to\_integer\_cast.zig:3:36: error: float value '4294967296' cannot be stored in integer type 'i32'
    const int: i32 = @intFromFloat(float);
                                   ^~~~~

At runtime:

runtime\_out\_of\_bounds\_float\_to\_integer\_cast.zig

```
pub fn main() void {
    var float: f32 = 4294967296; // runtime-known
    _ = &float;
    const int: i32 = @intFromFloat(float);
    _ = int;
}
```

Shell

$ zig build-exe runtime\_out\_of\_bounds\_float\_to\_integer\_cast.zig
$ ./runtime\_out\_of\_bounds\_float\_to\_integer\_cast
thread 2898584 panic: integer part of floating point value out of bounds
/home/andy/dev/zig/doc/langref/runtime\_out\_of\_bounds\_float\_to\_integer\_cast.zig:4:22: 0x113e8d2 in main (runtime\_out\_of\_bounds\_float\_to\_integer\_cast.zig)
    const int: i32 = @intFromFloat(float);
                     ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

### [Pointer Cast Invalid Null](https://ziglang.org/documentation/0.15.2/#toc-Pointer-Cast-Invalid-Null) [Â§](https://ziglang.org/documentation/0.15.2/#Pointer-Cast-Invalid-Null)

This happens when casting a pointer with the address 0 to a pointer which may not have the address 0. For example, [C Pointers](https://ziglang.org/documentation/0.15.2/#C-Pointers), [Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers), and [allowzero](https://ziglang.org/documentation/0.15.2/#allowzero) pointers allow address zero, but normal [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) do not.

At compile-time:

test\_comptime\_invalid\_null\_pointer\_cast.zig

```
comptime {
    const opt_ptr: ?*i32 = null;
    const ptr: *i32 = @ptrCast(opt_ptr);
    _ = ptr;
}
```

Shell

$ zig test test\_comptime\_invalid\_null\_pointer\_cast.zig
/home/andy/dev/zig/doc/langref/test\_comptime\_invalid\_null\_pointer\_cast.zig:3:32: error: null pointer casted to type '\*i32'
    const ptr: \*i32 = @ptrCast(opt\_ptr);
                               ^~~~~~~

At runtime:

runtime\_invalid\_null\_pointer\_cast.zig

```
pub fn main() void {
    var opt_ptr: ?*i32 = null;
    _ = &opt_ptr;
    const ptr: *i32 = @ptrCast(opt_ptr);
    _ = ptr;
}
```

Shell

$ zig build-exe runtime\_invalid\_null\_pointer\_cast.zig
$ ./runtime\_invalid\_null\_pointer\_cast
thread 2892939 panic: cast causes pointer to be null
/home/andy/dev/zig/doc/langref/runtime\_invalid\_null\_pointer\_cast.zig:4:23: 0x113e88a in main (runtime\_invalid\_null\_pointer\_cast.zig)
    const ptr: \*i32 = @ptrCast(opt\_ptr);
                      ^
/home/andy/dev/zig/lib/std/start.zig:618:22: 0x113dabd in posixCallMainAndExit (std.zig)
            root.main();
                     ^
/home/andy/dev/zig/lib/std/start.zig:232:5: 0x113d351 in \_start (std.zig)
    asm volatile (switch (native\_arch) {
    ^
???:?:?: 0x0 in ??? (???)
(process terminated by signal)

## [Memory](https://ziglang.org/documentation/0.15.2/#toc-Memory) [Â§](https://ziglang.org/documentation/0.15.2/#Memory)

The Zig language performs no memory management on behalf of the programmer. This is why Zig has no runtime, and why Zig code works seamlessly in so many environments, including real-time software, operating system kernels, embedded devices, and low latency servers. As a consequence, Zig programmers must always be able to answer the question:

[Where are the bytes?](https://ziglang.org/documentation/0.15.2/#Where-are-the-bytes)

Like Zig, the C programming language has manual memory management. However, unlike Zig, C has a default allocator - `malloc`, `realloc`, and `free`. When linking against libc, Zig exposes this allocator with `std.heap.c_allocator`. However, by convention, there is no default allocator in Zig. Instead, functions which need to allocate accept an `Allocator` parameter. Likewise, some data structures accept an `Allocator` parameter in their initialization functions:

test\_allocator.zig

```
const std = @import("std");
const Allocator = std.mem.Allocator;
const expect = std.testing.expect;

test "using an allocator" {
    var buffer: [100]u8 = undefined;
    var fba = std.heap.FixedBufferAllocator.init(&buffer);
    const allocator = fba.allocator();
    const result = try concat(allocator, "foo", "bar");
    try expect(std.mem.eql(u8, "foobar", result));
}

fn concat(allocator: Allocator, a: []const u8, b: []const u8) ![]u8 {
    const result = try allocator.alloc(u8, a.len + b.len);
    @memcpy(result[0..a.len], a);
    @memcpy(result[a.len..], b);
    return result;
}
```

Shell

$ zig test test\_allocator.zig
1/1 test\_allocator.test.using an allocator...OK
All 1 tests passed.

In the above example, 100 bytes of stack memory are used to initialize a `FixedBufferAllocator`, which is then passed to a function. As a convenience there is a global `FixedBufferAllocator` available for quick tests at `std.testing.allocator`, which will also perform basic leak detection.

Zig has a general purpose allocator available to be imported with `std.heap.GeneralPurposeAllocator`. However, it is still recommended to follow the [Choosing an Allocator](https://ziglang.org/documentation/0.15.2/#Choosing-an-Allocator) guide.

### [Choosing an Allocator](https://ziglang.org/documentation/0.15.2/#toc-Choosing-an-Allocator) [Â§](https://ziglang.org/documentation/0.15.2/#Choosing-an-Allocator)

What allocator to use depends on a number of factors. Here is a flow chart to help you decide:

1.  Are you making a library? In this case, best to accept an `Allocator` as a parameter and allow your library's users to decide what allocator to use.
2.  Are you linking libc? In this case, `std.heap.c_allocator` is likely the right choice, at least for your main allocator.
3.  Is the maximum number of bytes that you will need bounded by a number known at [comptime](https://ziglang.org/documentation/0.15.2/#comptime)? In this case, use `std.heap.FixedBufferAllocator`.
4.  Is your program a command line application which runs from start to end without any fundamental cyclical pattern (such as a video game main loop, or a web server request handler), such that it would make sense to free everything at once at the end? In this case, it is recommended to follow this pattern:
    
    cli\_allocation.zig
    
    ```
    const std = @import("std");
    
    pub fn main() !void {
        var arena = std.heap.ArenaAllocator.init(std.heap.page_allocator);
        defer arena.deinit();
    
        const allocator = arena.allocator();
    
        const ptr = try allocator.create(i32);
        std.debug.print("ptr={*}\n", .{ptr});
    }
    ```
    
    Shell
    
    $ zig build-exe cli\_allocation.zig
    $ ./cli\_allocation
    ptr=i32@7f1a3ed8e010
    
    When using this kind of allocator, there is no need to free anything manually. Everything gets freed at once with the call to `arena.deinit()`.
5.  Are the allocations part of a cyclical pattern such as a video game main loop, or a web server request handler? If the allocations can all be freed at once, at the end of the cycle, for example once the video game frame has been fully rendered, or the web server request has been served, then `std.heap.ArenaAllocator` is a great candidate. As demonstrated in the previous bullet point, this allows you to free entire arenas at once. Note also that if an upper bound of memory can be established, then `std.heap.FixedBufferAllocator` can be used as a further optimization.
6.  Are you writing a test, and you want to make sure `error.OutOfMemory` is handled correctly? In this case, use `std.testing.FailingAllocator`.
7.  Are you writing a test? In this case, use `std.testing.allocator`.
8.  Finally, if none of the above apply, you need a general purpose allocator. If you are in Debug mode, `std.heap.DebugAllocator` is available as a function that takes a [comptime](https://ziglang.org/documentation/0.15.2/#comptime) [struct](https://ziglang.org/documentation/0.15.2/#struct) of configuration options and returns a type. Generally, you will set up exactly one in your main function, and then pass it or sub-allocators around to various parts of your application.
9.  If you are compiling in ReleaseFast mode, `std.heap.smp_allocator` is a solid choice for a general purpose allocator.
10.  You can also consider implementing an allocator.

### [Where are the bytes?](https://ziglang.org/documentation/0.15.2/#toc-Where-are-the-bytes) [Â§](https://ziglang.org/documentation/0.15.2/#Where-are-the-bytes)

String literals such as `"hello"` are in the global constant data section. This is why it is an error to pass a string literal to a mutable slice, like this:

test\_string\_literal\_to\_slice.zig

```
fn foo(s: []u8) void {
    _ = s;
}

test "string literal to mutable slice" {
    foo("hello");
}
```

Shell

$ zig test test\_string\_literal\_to\_slice.zig
/home/andy/dev/zig/doc/langref/test\_string\_literal\_to\_slice.zig:6:9: error: expected type '\[\]u8', found '\*const \[5:0\]u8'
    foo("hello");
        ^~~~~~~
/home/andy/dev/zig/doc/langref/test\_string\_literal\_to\_slice.zig:6:9: note: cast discards const qualifier
/home/andy/dev/zig/doc/langref/test\_string\_literal\_to\_slice.zig:1:11: note: parameter type declared here
fn foo(s: \[\]u8) void {
          ^~~~

However if you make the slice constant, then it works:

test\_string\_literal\_to\_const\_slice.zig

```
fn foo(s: []const u8) void {
    _ = s;
}

test "string literal to constant slice" {
    foo("hello");
}
```

Shell

$ zig test test\_string\_literal\_to\_const\_slice.zig
1/1 test\_string\_literal\_to\_const\_slice.test.string literal to constant slice...OK
All 1 tests passed.

Just like string literals, `const` declarations, when the value is known at [comptime](https://ziglang.org/documentation/0.15.2/#comptime), are stored in the global constant data section. Also [Compile Time Variables](https://ziglang.org/documentation/0.15.2/#Compile-Time-Variables) are stored in the global constant data section.

`var` declarations inside functions are stored in the function's stack frame. Once a function returns, any [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) to variables in the function's stack frame become invalid references, and dereferencing them becomes unchecked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).

`var` declarations at the top level or in [struct](https://ziglang.org/documentation/0.15.2/#struct) declarations are stored in the global data section.

The location of memory allocated with `allocator.alloc` or `allocator.create` is determined by the allocator's implementation.

TODO: thread local variables

### [Heap Allocation Failure](https://ziglang.org/documentation/0.15.2/#toc-Heap-Allocation-Failure) [Â§](https://ziglang.org/documentation/0.15.2/#Heap-Allocation-Failure)

Many programming languages choose to handle the possibility of heap allocation failure by unconditionally crashing. By convention, Zig programmers do not consider this to be a satisfactory solution. Instead, `error.OutOfMemory` represents heap allocation failure, and Zig libraries return this error code whenever heap allocation failure prevented an operation from completing successfully.

Some have argued that because some operating systems such as Linux have memory overcommit enabled by default, it is pointless to handle heap allocation failure. There are many problems with this reasoning:

-   Only some operating systems have an overcommit feature.
    -   Linux has it enabled by default, but it is configurable.
    -   Windows does not overcommit.
    -   Embedded systems do not have overcommit.
    -   Hobby operating systems may or may not have overcommit.
-   For real-time systems, not only is there no overcommit, but typically the maximum amount of memory per application is determined ahead of time.

-   When writing a library, one of the main goals is code reuse. By making code handle allocation failure correctly, a library becomes eligible to be reused in more contexts.
-   Although some software has grown to depend on overcommit being enabled, its existence is the source of countless user experience disasters. When a system with overcommit enabled, such as Linux on default settings, comes close to memory exhaustion, the system locks up and becomes unusable. At this point, the OOM Killer selects an application to kill based on heuristics. This non-deterministic decision often results in an important process being killed, and often fails to return the system back to working order.

### [Recursion](https://ziglang.org/documentation/0.15.2/#toc-Recursion) [Â§](https://ziglang.org/documentation/0.15.2/#Recursion)

Recursion is a fundamental tool in modeling software. However it has an often-overlooked problem: unbounded memory allocation.

Recursion is an area of active experimentation in Zig and so the documentation here is not final. You can read a [summary of recursion status in the 0.3.0 release notes](https://ziglang.org/download/0.3.0/release-notes.html#recursion).

The short summary is that currently recursion works normally as you would expect. Although Zig code is not yet protected from stack overflow, it is planned that a future version of Zig will provide such protection, with some degree of cooperation from Zig code required.

### [Lifetime and Ownership](https://ziglang.org/documentation/0.15.2/#toc-Lifetime-and-Ownership) [Â§](https://ziglang.org/documentation/0.15.2/#Lifetime-and-Ownership)

It is the Zig programmer's responsibility to ensure that a [pointer](https://ziglang.org/documentation/0.15.2/#Pointers) is not accessed when the memory pointed to is no longer available. Note that a [slice](https://ziglang.org/documentation/0.15.2/#Slices) is a form of pointer, in that it references other memory.

In order to prevent bugs, there are some helpful conventions to follow when dealing with pointers. In general, when a function returns a pointer, the documentation for the function should explain who "owns" the pointer. This concept helps the programmer decide when it is appropriate, if ever, to free the pointer.

For example, the function's documentation may say "caller owns the returned memory", in which case the code that calls the function must have a plan for when to free that memory. Probably in this situation, the function will accept an `Allocator` parameter.

Sometimes the lifetime of a pointer may be more complicated. For example, the `std.ArrayList(T).items` slice has a lifetime that remains valid until the next time the list is resized, such as by appending new elements.

The API documentation for functions and data structures should take great care to explain the ownership and lifetime semantics of pointers. Ownership determines whose responsibility it is to free the memory referenced by the pointer, and lifetime determines the point at which the memory becomes inaccessible (lest [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior) occur).

## [Compile Variables](https://ziglang.org/documentation/0.15.2/#toc-Compile-Variables) [Â§](https://ziglang.org/documentation/0.15.2/#Compile-Variables)

Compile variables are accessible by importing the `"builtin"` package, which the compiler makes available to every Zig source file. It contains compile-time constants such as the current target, endianness, and release mode.

compile\_variables.zig

```
const builtin = @import("builtin");
const separator = if (builtin.os.tag == .windows) '\\' else '/';
```

Example of what is imported with `@import("builtin")`:

@import("builtin")

```
const std = @import("std");
/// Zig version. When writing code that supports multiple versions of Zig, prefer
/// feature detection (i.e. with `@hasDecl` or `@hasField`) over version checks.
pub const zig_version = std.SemanticVersion.parse(zig_version_string) catch unreachable;
pub const zig_version_string = "0.15.2";
pub const zig_backend = std.builtin.CompilerBackend.stage2_x86_64;

pub const output_mode: std.builtin.OutputMode = .Exe;
pub const link_mode: std.builtin.LinkMode = .static;
pub const unwind_tables: std.builtin.UnwindTables = .async;
pub const is_test = false;
pub const single_threaded = false;
pub const abi: std.Target.Abi = .gnu;
pub const cpu: std.Target.Cpu = .{
    .arch = .x86_64,
    .model = &std.Target.x86.cpu.znver4,
    .features = std.Target.x86.featureSet(&.{
        .@"64bit",
        .adx,
        .aes,
        .allow_light_256_bit,
        .avx,
        .avx2,
        .avx512bf16,
        .avx512bitalg,
        .avx512bw,
        .avx512cd,
        .avx512dq,
        .avx512f,
        .avx512ifma,
        .avx512vbmi,
        .avx512vbmi2,
        .avx512vl,
        .avx512vnni,
        .avx512vpopcntdq,
        .bmi,
        .bmi2,
        .branchfusion,
        .clflushopt,
        .clwb,
        .clzero,
        .cmov,
        .crc32,
        .cx16,
        .cx8,
        .evex512,
        .f16c,
        .fast_15bytenop,
        .fast_bextr,
        .fast_dpwssd,
        .fast_imm16,
        .fast_lzcnt,
        .fast_movbe,
        .fast_scalar_fsqrt,
        .fast_scalar_shift_masks,
        .fast_variable_perlane_shuffle,
        .fast_vector_fsqrt,
        .fma,
        .fsgsbase,
        .fsrm,
        .fxsr,
        .gfni,
        .idivq_to_divl,
        .invpcid,
        .lzcnt,
        .macrofusion,
        .mmx,
        .movbe,
        .mwaitx,
        .nopl,
        .pclmul,
        .pku,
        .popcnt,
        .prfchw,
        .rdpid,
        .rdpru,
        .rdrnd,
        .rdseed,
        .sahf,
        .sbb_dep_breaking,
        .sha,
        .shstk,
        .slow_shld,
        .smap,
        .smep,
        .sse,
        .sse2,
        .sse3,
        .sse4_1,
        .sse4_2,
        .sse4a,
        .ssse3,
        .vaes,
        .vpclmulqdq,
        .vzeroupper,
        .wbnoinvd,
        .x87,
        .xsave,
        .xsavec,
        .xsaveopt,
        .xsaves,
    }),
};
pub const os: std.Target.Os = .{
    .tag = .linux,
    .version_range = .{ .linux = .{
        .range = .{
            .min = .{
                .major = 6,
                .minor = 16,
                .patch = 0,
            },
            .max = .{
                .major = 6,
                .minor = 16,
                .patch = 0,
            },
        },
        .glibc = .{
            .major = 2,
            .minor = 39,
            .patch = 0,
        },
        .android = 29,
    }},
};
pub const target: std.Target = .{
    .cpu = cpu,
    .os = os,
    .abi = abi,
    .ofmt = object_format,
    .dynamic_linker = .init("/nix/store/zdpby3l6azi78sl83cpad2qjpfj25aqx-glibc-2.40-66/lib/ld-linux-x86-64.so.2"),
};
pub const object_format: std.Target.ObjectFormat = .elf;
pub const mode: std.builtin.OptimizeMode = .Debug;
pub const link_libc = false;
pub const link_libcpp = false;
pub const have_error_return_tracing = true;
pub const valgrind_support = true;
pub const sanitize_thread = false;
pub const fuzz = false;
pub const position_independent_code = false;
pub const position_independent_executable = false;
pub const strip_debug_info = false;
pub const code_model: std.builtin.CodeModel = .default;
pub const omit_frame_pointer = false;
```

See also:

-   [Build Mode](https://ziglang.org/documentation/0.15.2/#Build-Mode)

## [Compilation Model](https://ziglang.org/documentation/0.15.2/#toc-Compilation-Model) [Â§](https://ziglang.org/documentation/0.15.2/#Compilation-Model)

A Zig compilation is separated into *modules*. Each module is a collection of Zig source files, one of which is the module's *root source file*. Each module can *depend* on any number of other modules, forming a directed graph (dependency loops between modules are allowed). If module A depends on module B, then any Zig source file in module A can import the *root source file* of module B using `@import` with the module's name. In essence, a module acts as an alias to import a Zig source file (which might exist in a completely separate part of the filesystem).

A simple Zig program compiled with `zig build-exe` has two key modules: the one containing your code, known as the "main" or "root" module, and the standard library. Your module *depends on* the standard library module under the name "std", which is what allows you to write `@import("std")`! In fact, every single module in a Zig compilation â€” including the standard library itself â€” implicitly depends on the standard library module under the name "std".

The "root module" (the one provided by you in the `zig build-exe` example) has a special property. Like the standard library, it is implicitly made available to all modules (including itself), this time under the name "root". So, `@import("root")` will always be equivalent to `@import` of your "main" source file (often, but not necessarily, named `main.zig`).

### [Source File Structs](https://ziglang.org/documentation/0.15.2/#toc-Source-File-Structs) [Â§](https://ziglang.org/documentation/0.15.2/#Source-File-Structs)

Every Zig source file is implicitly a `struct` declaration; you can imagine that the file's contents are literally surrounded by `struct { ... }`. This means that as well as declarations, the top level of a file is permitted to contain fields:

TopLevelFields.zig

```
//! Because this file contains fields, it is a type which is intended to be instantiated, and so
//! is named in TitleCase instead of snake_case by convention.

foo: u32,
bar: u64,

/// `@This()` can be used to refer to this struct type. In files with fields, it is quite common to
/// name the type here, so it can be easily referenced by other declarations in this file.
const TopLevelFields = @This();

pub fn init(val: u32) TopLevelFields {
    return .{
        .foo = val,
        .bar = val * 10,
    };
}
```

Such files can be instantiated just like any other `struct` type. A file's "root struct type" can be referred to within that file using [@This](https://ziglang.org/documentation/0.15.2/#This).

### [File and Declaration Discovery](https://ziglang.org/documentation/0.15.2/#toc-File-and-Declaration-Discovery) [Â§](https://ziglang.org/documentation/0.15.2/#File-and-Declaration-Discovery)

Zig places importance on the concept of whether any piece of code is *semantically analyzed*; in essence, whether the compiler "looks at" it. What code is analyzed is based on what files and declarations are "discovered" from a certain point. This process of "discovery" is based on a simple set of recursive rules:

-   If a call to `@import` is analyzed, the file being imported is analyzed.

-   If a type (including a file) is analyzed, all `comptime` and `export` declarations within it are analyzed.
-   If a type (including a file) is analyzed, and the compilation is for a [test](https://ziglang.org/documentation/0.15.2/#Zig-Test), and the module the type is within is the root module of the compilation, then all `test` declarations within it are also analyzed.

-   If a reference to a named declaration (i.e. a usage of it) is analyzed, the declaration being referenced is analyzed. Declarations are order-independent, so this reference may be above or below the declaration being referenced, or even in another file entirely.

That's it! Those rules define how Zig files and declarations are discovered. All that remains is to understand where this process *starts*.

The answer to that is the root of the standard library: every Zig compilation begins by analyzing the file `lib/std/std.zig`. This file contains a `comptime` declaration which imports `lib/std/start.zig`, and that file in turn uses `@import("root")` to reference the "root module"; so, the file you provide as your main module's root source file is effectively also a root, because the standard library will always reference it.

It is often desirable to make sure that certain declarations â€” particularly `test` or `export` declarations â€” are discovered. Based on the above rules, a common strategy for this is to use `@import` within a `comptime` or `test` block:

force\_file\_discovery.zig

```
comptime {
    // This will ensure that the file 'api.zig' is always discovered (as long as this file is discovered).
    // It is useful if 'api.zig' contains important exported declarations.
    _ = @import("api.zig");

    // We could also have a file which contains declarations we only want to export depending on a comptime
    // condition. In that case, we can use an `if` statement here:
    if (builtin.os.tag == .windows) {
        _ = @import("windows_api.zig");
    }
}

test {
    // This will ensure that the file 'tests.zig' is always discovered (as long as this file is discovered),
    // if this compilation is a test. It is useful if 'tests.zig' contains tests we want to ensure are run.
    _ = @import("tests.zig");

    // We could also have a file which contains tests we only want to run depending on a comptime condition.
    // In that case, we can use an `if` statement here:
    if (builtin.os.tag == .windows) {
        _ = @import("windows_tests.zig");
    }
}

const builtin = @import("builtin");
```

### [Special Root Declarations](https://ziglang.org/documentation/0.15.2/#toc-Special-Root-Declarations) [Â§](https://ziglang.org/documentation/0.15.2/#Special-Root-Declarations)

Because the root module's root source file is always accessible using `@import("root")`, is is sometimes used by libraries â€” including the Zig Standard Library â€” as a place for the program to expose some "global" information to that library. The Zig Standard Library will look for several declarations in this file.

#### [Entry Point](https://ziglang.org/documentation/0.15.2/#toc-Entry-Point) [Â§](https://ziglang.org/documentation/0.15.2/#Entry-Point)

When building an executable, the most important thing to be looked up in this file is the program's *entry point*. Most commonly, this is a function named `main`, which `std.start` will call just after performing important initialization work.

Alternatively, the presence of a declaration named `_start` (for instance, `pub const _start = {};`) will disable the default `std.start` logic, allowing your root source file to export a low-level entry point as needed.

entry\_point.zig

```
/// `std.start` imports this file using `@import("root")`, and uses this declaration as the program's
/// user-provided entry point. It can return any of the following types:
/// * `void`
/// * `E!void`, for any error set `E`
/// * `u8`
/// * `E!u8`, for any error set `E`
/// Returning a `void` value from this function will exit with code 0.
/// Returning a `u8` value from this function will exit with the given status code.
/// Returning an error value from this function will print an Error Return Trace and exit with code 1.
pub fn main() void {
    std.debug.print("Hello, World!\n", .{});
}

// If uncommented, this declaration would suppress the usual std.start logic, causing
// the `main` declaration above to be ignored.
//pub const _start = {};

const std = @import("std");
```

Shell

$ zig build-exe entry\_point.zig
$ ./entry\_point
Hello, World!

If the Zig compilation links libc, the `main` function can optionally be an `export fn` which matches the signature of the C `main` function:

libc\_export\_entry\_point.zig

```
pub export fn main(argc: c_int, argv: [*]const [*:0]const u8) c_int {
    const args = argv[0..@intCast(argc)];
    std.debug.print("Hello! argv[0] is '{s}'\n", .{args[0]});
    return 0;
}

const std = @import("std");
```

Shell

$ zig build-exe libc\_export\_entry\_point.zig -lc
$ ./libc\_export\_entry\_point
Hello! argv\[0\] is './libc\_export\_entry\_point'

`std.start` may also use other entry point declarations in certain situations, such as `wWinMain` or `EfiMain`. Refer to the `lib/std/start.zig` logic for details of these declarations.

#### [Standard Library Options](https://ziglang.org/documentation/0.15.2/#toc-Standard-Library-Options) [Â§](https://ziglang.org/documentation/0.15.2/#Standard-Library-Options)

The standard library also looks for a declaration in the root module's root source file named `std_options`. If present, this declaration is expected to be a struct of type `std.Options`, and allows the program to customize some standard library functionality, such as the `std.log` implementation.

std\_options.zig

```
/// The presence of this declaration allows the program to override certain behaviors of the standard library.
/// For a full list of available options, see the documentation for `std.Options`.
pub const std_options: std.Options = .{
    // By default, in safe build modes, the standard library will attach a segfault handler to the program to
    // print a helpful stack trace if a segmentation fault occurs. Here, we can disable this, or even enable
    // it in unsafe build modes.
    .enable_segfault_handler = true,
    // This is the logging function used by `std.log`.
    .logFn = myLogFn,
};

fn myLogFn(
    comptime level: std.log.Level,
    comptime scope: @Type(.enum_literal),
    comptime format: []const u8,
    args: anytype,
) void {
    // We could do anything we want here!
    // ...but actually, let's just call the default implementation.
    std.log.defaultLog(level, scope, format, args);
}

const std = @import("std");
```

#### [Panic Handler](https://ziglang.org/documentation/0.15.2/#toc-Panic-Handler) [Â§](https://ziglang.org/documentation/0.15.2/#Panic-Handler)

The Zig Standard Library looks for a declaration named `panic` in the root module's root source file. If present, it is expected to be a namespace (container type) with declarations providing different panic handlers.

See `std.debug.simple_panic` for a basic implementation of this namespace.

Overriding how the panic handler actually outputs messages, but keeping the formatted safety panics which are enabled by default, can be easily achieved with `std.debug.FullPanic`:

panic\_handler.zig

```
pub fn main() void {
    @setRuntimeSafety(true);
    var x: u8 = 255;
    // Let's overflow this integer!
    x += 1;
}

pub const panic = std.debug.FullPanic(myPanic);

fn myPanic(msg: []const u8, first_trace_addr: ?usize) noreturn {
    _ = first_trace_addr;
    std.debug.print("Panic! {s}\n", .{msg});
    std.process.exit(1);
}

const std = @import("std");
```

Shell

$ zig build-exe panic\_handler.zig
$ ./panic\_handler
Panic! integer overflow

## [Zig Build System](https://ziglang.org/documentation/0.15.2/#toc-Zig-Build-System) [Â§](https://ziglang.org/documentation/0.15.2/#Zig-Build-System)

The Zig Build System provides a cross-platform, dependency-free way to declare the logic required to build a project. With this system, the logic to build a project is written in a build.zig file, using the Zig Build System API to declare and configure build artifacts and other tasks.

Some examples of tasks the build system can help with:

-   Performing tasks in parallel and caching the results.

-   Depending on other projects.
-   Providing a package for other projects to depend on.

-   Creating build artifacts by executing the Zig compiler. This includes building Zig source code as well as C and C++ source code.
-   Capturing user-configured options and using those options to configure the build.

-   Surfacing build configuration as [comptime](https://ziglang.org/documentation/0.15.2/#comptime) values by providing a file that can be [imported](https://ziglang.org/documentation/0.15.2/#import) by Zig code.
-   Caching build artifacts to avoid unnecessarily repeating steps.

-   Executing build artifacts or system-installed tools.
-   Running tests and verifying the output of executing a build artifact matches the expected value.

-   Running `zig fmt` on a codebase or a subset of it.
-   Custom tasks.

To use the build system, run zig build --help to see a command-line usage help menu. This will include project-specific options that were declared in the build.zig script.

For the time being, the build system documentation is hosted externally: [Build System Documentation](https://ziglang.org/learn/build-system/)

## [C](https://ziglang.org/documentation/0.15.2/#toc-C) [Â§](https://ziglang.org/documentation/0.15.2/#C)

Although Zig is independent of C, and, unlike most other languages, does not depend on libc, Zig acknowledges the importance of interacting with existing C code.

There are a few ways that Zig facilitates C interop.

### [C Type Primitives](https://ziglang.org/documentation/0.15.2/#toc-C-Type-Primitives) [Â§](https://ziglang.org/documentation/0.15.2/#C-Type-Primitives)

These have guaranteed C ABI compatibility and can be used like any other type.

-   `c_char`

-   `c_short`
-   `c_ushort`

-   `c_int`
-   `c_uint`

-   `c_long`
-   `c_ulong`

-   `c_longlong`
-   `c_ulonglong`

-   `c_longdouble`

To interop with the C `void` type, use `anyopaque`.

See also:

-   [Primitive Types](https://ziglang.org/documentation/0.15.2/#Primitive-Types)

### [Import from C Header File](https://ziglang.org/documentation/0.15.2/#toc-Import-from-C-Header-File) [Â§](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

The `@cImport` builtin function can be used to directly import symbols from `.h` files:

cImport\_builtin.zig

```
const c = @cImport({
    // See https://github.com/ziglang/zig/issues/515
    @cDefine("_NO_CRT_STDIO_INLINE", "1");
    @cInclude("stdio.h");
});
pub fn main() void {
    _ = c.printf("hello\n");
}
```

Shell

$ zig build-exe cImport\_builtin.zig -lc
$ ./cImport\_builtin
hello

The `@cImport` function takes an expression as a parameter. This expression is evaluated at compile-time and is used to control preprocessor directives and include multiple `.h` files:

@cImport Expression

```
const builtin = @import("builtin");

const c = @cImport({
    @cDefine("NDEBUG", builtin.mode == .ReleaseFast);
    if (something) {
        @cDefine("_GNU_SOURCE", {});
    }
    @cInclude("stdlib.h");
    if (something) {
        @cUndef("_GNU_SOURCE");
    }
    @cInclude("soundio.h");
});
```

See also:

-   [@cImport](https://ziglang.org/documentation/0.15.2/#cImport)

-   [@cInclude](https://ziglang.org/documentation/0.15.2/#cInclude)
-   [@cDefine](https://ziglang.org/documentation/0.15.2/#cDefine)

-   [@cUndef](https://ziglang.org/documentation/0.15.2/#cUndef)
-   [@import](https://ziglang.org/documentation/0.15.2/#import)

### [C Translation CLI](https://ziglang.org/documentation/0.15.2/#toc-C-Translation-CLI) [Â§](https://ziglang.org/documentation/0.15.2/#C-Translation-CLI)

Zig's C translation capability is available as a CLI tool via zig translate-c. It requires a single filename as an argument. It may also take a set of optional flags that are forwarded to clang. It writes the translated file to stdout.

#### [Command line flags](https://ziglang.org/documentation/0.15.2/#toc-Command-line-flags) [Â§](https://ziglang.org/documentation/0.15.2/#Command-line-flags)

-   \-I: Specify a search directory for include files. May be used multiple times. Equivalent to [clang's \-I flag](https://releases.llvm.org/12.0.0/tools/clang/docs/ClangCommandLineReference.html#cmdoption-clang-i-dir). The current directory is *not* included by default; use \-I. to include it.

-   \-D: Define a preprocessor macro. Equivalent to [clang's \-D flag](https://releases.llvm.org/12.0.0/tools/clang/docs/ClangCommandLineReference.html#cmdoption-clang-d-macro).
-   \-cflags \[flags\] --: Pass arbitrary additional [command line flags](https://releases.llvm.org/12.0.0/tools/clang/docs/ClangCommandLineReference.html) to clang. Note: the list of flags must end with \--

-   \-target: The [target triple](https://ziglang.org/documentation/0.15.2/#Targets) for the translated Zig code. If no target is specified, the current host target will be used.

#### [Using -target and -cflags](https://ziglang.org/documentation/0.15.2/#toc-Using--target-and--cflags) [Â§](https://ziglang.org/documentation/0.15.2/#Using--target-and--cflags)

**Important!** When translating C code with zig translate-c, you **must** use the same \-target triple that you will use when compiling the translated code. In addition, you **must** ensure that the \-cflags used, if any, match the cflags used by code on the target system. Using the incorrect \-target or \-cflags could result in clang or Zig parse failures, or subtle ABI incompatibilities when linking with C code.

varytarget.h

```
long FOO = __LONG_MAX__;
```

Shell

$ zig translate-c -target thumb-freestanding-gnueabihf varytarget.h|grep FOO
pub export var FOO: c\_long = 2147483647;
$ zig translate-c -target x86\_64-macos-gnu varytarget.h|grep FOO
pub export var FOO: c\_long = 9223372036854775807;

varycflags.h

```
enum FOO { BAR };
int do_something(enum FOO foo);
```

Shell

$ zig translate-c varycflags.h|grep -B1 do\_something
pub const enum\_FOO = c\_uint;
pub extern fn do\_something(foo: enum\_FOO) c\_int;
$ zig translate-c -cflags -fshort-enums -- varycflags.h|grep -B1 do\_something
pub const enum\_FOO = u8;
pub extern fn do\_something(foo: enum\_FOO) c\_int;

#### [@cImport vs translate-c](https://ziglang.org/documentation/0.15.2/#toc-cImport-vs-translate-c) [Â§](https://ziglang.org/documentation/0.15.2/#cImport-vs-translate-c)

`@cImport` and zig translate-c use the same underlying C translation functionality, so on a technical level they are equivalent. In practice, `@cImport` is useful as a way to quickly and easily access numeric constants, typedefs, and record types without needing any extra setup. If you need to pass [cflags](https://ziglang.org/documentation/0.15.2/#Using--target-and--cflags) to clang, or if you would like to edit the translated code, it is recommended to use zig translate-c and save the results to a file. Common reasons for editing the generated code include: changing `anytype` parameters in function-like macros to more specific types; changing `[*c]T` pointers to `[*]T` or `*T` pointers for improved type safety; and [enabling or disabling runtime safety](https://ziglang.org/documentation/0.15.2/#setRuntimeSafety) within specific functions.

See also:

-   [Targets](https://ziglang.org/documentation/0.15.2/#Targets)

-   [C Type Primitives](https://ziglang.org/documentation/0.15.2/#C-Type-Primitives)
-   [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers)

-   [C Pointers](https://ziglang.org/documentation/0.15.2/#C-Pointers)
-   [Import from C Header File](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

-   [@cInclude](https://ziglang.org/documentation/0.15.2/#cInclude)
-   [@cImport](https://ziglang.org/documentation/0.15.2/#cImport)

-   [@setRuntimeSafety](https://ziglang.org/documentation/0.15.2/#setRuntimeSafety)

### [C Translation Caching](https://ziglang.org/documentation/0.15.2/#toc-C-Translation-Caching) [Â§](https://ziglang.org/documentation/0.15.2/#C-Translation-Caching)

The C translation feature (whether used via zig translate-c or `@cImport`) integrates with the Zig caching system. Subsequent runs with the same source file, target, and cflags will use the cache instead of repeatedly translating the same code.

To see where the cached files are stored when compiling code that uses `@cImport`, use the \--verbose-cimport flag:

verbose\_cimport\_flag.zig

```
const c = @cImport({
    @cDefine("_NO_CRT_STDIO_INLINE", "1");
    @cInclude("stdio.h");
});
pub fn main() void {
    _ = c;
}
```

Shell

$ zig build-exe verbose\_cimport\_flag.zig -lc --verbose-cimport
info(compilation): C import source: /home/andy/dev/zig/.zig-cache/o/f9216ef6681abef94b056af4b875b0bd/cimport.h
info(compilation): C import .d file: /home/andy/dev/zig/.zig-cache/o/f9216ef6681abef94b056af4b875b0bd/cimport.h.d
$ ./verbose\_cimport\_flag

`cimport.h` contains the file to translate (constructed from calls to `@cInclude`, `@cDefine`, and `@cUndef`), `cimport.h.d` is the list of file dependencies, and `cimport.zig` contains the translated output.

See also:

-   [Import from C Header File](https://ziglang.org/documentation/0.15.2/#Import-from-C-Header-File)

-   [C Translation CLI](https://ziglang.org/documentation/0.15.2/#C-Translation-CLI)
-   [@cInclude](https://ziglang.org/documentation/0.15.2/#cInclude)

-   [@cImport](https://ziglang.org/documentation/0.15.2/#cImport)

### [Translation failures](https://ziglang.org/documentation/0.15.2/#toc-Translation-failures) [Â§](https://ziglang.org/documentation/0.15.2/#Translation-failures)

Some C constructs cannot be translated to Zig - for example, *goto*, structs with bitfields, and token-pasting macros. Zig employs *demotion* to allow translation to continue in the face of non-translatable entities.

Demotion comes in three varieties - [opaque](https://ziglang.org/documentation/0.15.2/#opaque), *extern*, and `@compileError`. C structs and unions that cannot be translated correctly will be translated as `opaque{}`. Functions that contain opaque types or code constructs that cannot be translated will be demoted to `extern` declarations. Thus, non-translatable types can still be used as pointers, and non-translatable functions can be called so long as the linker is aware of the compiled function.

`@compileError` is used when top-level definitions (global variables, function prototypes, macros) cannot be translated or demoted. Since Zig uses lazy analysis for top-level declarations, untranslatable entities will not cause a compile error in your code unless you actually use them.

See also:

-   [opaque](https://ziglang.org/documentation/0.15.2/#opaque)

-   [extern](https://ziglang.org/documentation/0.15.2/#extern)
-   [@compileError](https://ziglang.org/documentation/0.15.2/#compileError)

### [C Macros](https://ziglang.org/documentation/0.15.2/#toc-C-Macros) [Â§](https://ziglang.org/documentation/0.15.2/#C-Macros)

C Translation makes a best-effort attempt to translate function-like macros into equivalent Zig functions. Since C macros operate at the level of lexical tokens, not all C macros can be translated to Zig. Macros that cannot be translated will be demoted to `@compileError`. Note that C code which *uses* macros will be translated without any additional issues (since Zig operates on the pre-processed source with macros expanded). It is merely the macros themselves which may not be translatable to Zig.

Consider the following example:

macro.c

```
#define MAKELOCAL(NAME, INIT) int NAME = INIT
int foo(void) {
   MAKELOCAL(a, 1);
   MAKELOCAL(b, 2);
   return a + b;
}
```

Shell

$ zig translate-c macro.c > macro.zig

macro.zig

```
pub export fn foo() c_int {
    var a: c_int = 1;
    _ = &a;
    var b: c_int = 2;
    _ = &b;
    return a + b;
}
pub const MAKELOCAL = @compileError("unable to translate C expr: unexpected token .Equal"); // macro.c:1:9
```

Note that `foo` was translated correctly despite using a non-translatable macro. `MAKELOCAL` was demoted to `@compileError` since it cannot be expressed as a Zig function; this simply means that you cannot directly use `MAKELOCAL` from Zig.

See also:

-   [@compileError](https://ziglang.org/documentation/0.15.2/#compileError)

### [C Pointers](https://ziglang.org/documentation/0.15.2/#toc-C-Pointers) [Â§](https://ziglang.org/documentation/0.15.2/#C-Pointers)

This type is to be avoided whenever possible. The only valid reason for using a C pointer is in auto-generated code from translating C code.

When importing C header files, it is ambiguous whether pointers should be translated as single-item pointers (`*T`) or many-item pointers (`[*]T`). C pointers are a compromise so that Zig code can utilize translated header files directly.

`[*c]T` - C pointer.

-   Supports all the syntax of the other two pointer types (`*T`) and (`[*]T`).

-   Coerces to other pointer types, as well as [Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers). When a C pointer is coerced to a non-optional pointer, safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior) occurs if the address is 0.
-   Allows address 0. On non-freestanding targets, dereferencing address 0 is safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior). Optional C pointers introduce another bit to keep track of null, just like `?usize`. Note that creating an optional C pointer is unnecessary as one can use normal [Optional Pointers](https://ziglang.org/documentation/0.15.2/#Optional-Pointers).

-   Supports [Type Coercion](https://ziglang.org/documentation/0.15.2/#Type-Coercion) to and from integers.
-   Supports comparison with integers.

-   Does not support Zig-only pointer attributes such as alignment. Use normal [Pointers](https://ziglang.org/documentation/0.15.2/#Pointers) please!

When a C pointer is pointing to a single struct (not an array), dereference the C pointer to access the struct's fields or member data. That syntax looks like this:

`ptr_to_struct.*.struct_member`

This is comparable to doing `->` in C.

When a C pointer is pointing to an array of structs, the syntax reverts to this:

`ptr_to_struct_array[index].struct_member`

### [C Variadic Functions](https://ziglang.org/documentation/0.15.2/#toc-C-Variadic-Functions) [Â§](https://ziglang.org/documentation/0.15.2/#C-Variadic-Functions)

Zig supports extern variadic functions.

test\_variadic\_function.zig

```
const std = @import("std");
const testing = std.testing;

pub extern "c" fn printf(format: [*:0]const u8, ...) c_int;

test "variadic function" {
    try testing.expect(printf("Hello, world!\n") == 14);
    try testing.expect(@typeInfo(@TypeOf(printf)).@"fn".is_var_args);
}
```

Shell

$ zig test test\_variadic\_function.zig -lc
1/1 test\_variadic\_function.test.variadic function...OK
All 1 tests passed.
Hello, world!

Variadic functions can be implemented using [@cVaStart](https://ziglang.org/documentation/0.15.2/#cVaStart), [@cVaEnd](https://ziglang.org/documentation/0.15.2/#cVaEnd), [@cVaArg](https://ziglang.org/documentation/0.15.2/#cVaArg) and [@cVaCopy](https://ziglang.org/documentation/0.15.2/#cVaCopy).

test\_defining\_variadic\_function.zig

```
const std = @import("std");
const testing = std.testing;
const builtin = @import("builtin");

fn add(count: c_int, ...) callconv(.c) c_int {
    var ap = @cVaStart();
    defer @cVaEnd(&ap);
    var i: usize = 0;
    var sum: c_int = 0;
    while (i < count) : (i += 1) {
        sum += @cVaArg(&ap, c_int);
    }
    return sum;
}

test "defining a variadic function" {
    if (builtin.cpu.arch == .aarch64 and builtin.os.tag != .macos) {
        // https://github.com/ziglang/zig/issues/14096
        return error.SkipZigTest;
    }
    if (builtin.cpu.arch == .x86_64 and builtin.os.tag == .windows) {
        // https://github.com/ziglang/zig/issues/16961
        return error.SkipZigTest;
    }

    try std.testing.expectEqual(@as(c_int, 0), add(0));
    try std.testing.expectEqual(@as(c_int, 1), add(1, @as(c_int, 1)));
    try std.testing.expectEqual(@as(c_int, 3), add(2, @as(c_int, 1), @as(c_int, 2)));
}
```

Shell

$ zig test test\_defining\_variadic\_function.zig
1/1 test\_defining\_variadic\_function.test.defining a variadic function...OK
All 1 tests passed.

### [Exporting a C Library](https://ziglang.org/documentation/0.15.2/#toc-Exporting-a-C-Library) [Â§](https://ziglang.org/documentation/0.15.2/#Exporting-a-C-Library)

One of the primary use cases for Zig is exporting a library with the C ABI for other programming languages to call into. The `export` keyword in front of functions, variables, and types causes them to be part of the library API:

mathtest.zig

```
export fn add(a: i32, b: i32) i32 {
    return a + b;
}
```

To make a static library:

Shell

$ zig build-lib mathtest.zig

To make a shared library:

Shell

$ zig build-lib mathtest.zig -dynamic

Here is an example with the [Zig Build System](https://ziglang.org/documentation/0.15.2/#Zig-Build-System):

test.c

```
// This header is generated by zig from mathtest.zig
#include "mathtest.h"
#include <stdio.h>

int main(int argc, char **argv) {
    int32_t result = add(42, 1337);
    printf("%d\n", result);
    return 0;
}
```

build\_c.zig

```
const std = @import("std");

pub fn build(b: *std.Build) void {
    const lib = b.addLibrary(.{
        .linkage = .dynamic,
        .name = "mathtest",
        .root_module = b.createModule(.{
            .root_source_file = b.path("mathtest.zig"),
        }),
        .version = .{ .major = 1, .minor = 0, .patch = 0 },
    });
    const exe = b.addExecutable(.{
        .name = "test",
        .root_module = b.createModule(.{
            .link_libc = true,
        }),
    });
    exe.root_module.addCSourceFile(.{ .file = b.path("test.c"), .flags = &.{"-std=c99"} });
    exe.root_module.linkLibrary(lib);

    b.default_step.dependOn(&exe.step);

    const run_cmd = exe.run();

    const test_step = b.step("test", "Test the program");
    test_step.dependOn(&run_cmd.step);
}
```

Shell

$ zig build test
1379

See also:

-   [export](https://ziglang.org/documentation/0.15.2/#export)

### [Mixing Object Files](https://ziglang.org/documentation/0.15.2/#toc-Mixing-Object-Files) [Â§](https://ziglang.org/documentation/0.15.2/#Mixing-Object-Files)

You can mix Zig object files with any other object files that respect the C ABI. Example:

base64.zig

```
const base64 = @import("std").base64;

export fn decode_base_64(
    dest_ptr: [*]u8,
    dest_len: usize,
    source_ptr: [*]const u8,
    source_len: usize,
) usize {
    const src = source_ptr[0..source_len];
    const dest = dest_ptr[0..dest_len];
    const base64_decoder = base64.standard.Decoder;
    const decoded_size = base64_decoder.calcSizeForSlice(src) catch unreachable;
    base64_decoder.decode(dest[0..decoded_size], src) catch unreachable;
    return decoded_size;
}
```

test.c

```
// This header is generated by zig from base64.zig
#include "base64.h"

#include <string.h>
#include <stdio.h>

int main(int argc, char **argv) {
    const char *encoded = "YWxsIHlvdXIgYmFzZSBhcmUgYmVsb25nIHRvIHVz";
    char buf[200];

    size_t len = decode_base_64(buf, 200, encoded, strlen(encoded));
    buf[len] = 0;
    puts(buf);

    return 0;
}
```

build\_object.zig

```
const std = @import("std");

pub fn build(b: *std.Build) void {
    const obj = b.addObject(.{
        .name = "base64",
        .root_module = b.createModule(.{
            .root_source_file = b.path("base64.zig"),
        }),
    });

    const exe = b.addExecutable(.{
        .name = "test",
        .root_module = b.createModule(.{
            .link_libc = true,
        }),
    });
    exe.root_module.addCSourceFile(.{ .file = b.path("test.c"), .flags = &.{"-std=c99"} });
    exe.root_module.addObject(obj);
    b.installArtifact(exe);
}
```

Shell

$ zig build
$ ./zig-out/bin/test
all your base are belong to us

See also:

-   [Targets](https://ziglang.org/documentation/0.15.2/#Targets)

-   [Zig Build System](https://ziglang.org/documentation/0.15.2/#Zig-Build-System)

## [WebAssembly](https://ziglang.org/documentation/0.15.2/#toc-WebAssembly) [Â§](https://ziglang.org/documentation/0.15.2/#WebAssembly)

Zig supports building for WebAssembly out of the box.

### [Freestanding](https://ziglang.org/documentation/0.15.2/#toc-Freestanding) [Â§](https://ziglang.org/documentation/0.15.2/#Freestanding)

For host environments like the web browser and nodejs, build as an executable using the freestanding OS target. Here's an example of running Zig code compiled to WebAssembly with nodejs.

math.zig

```
extern fn print(i32) void;

export fn add(a: i32, b: i32) void {
    print(a + b);
}
```

Shell

$ zig build-exe math.zig -target wasm32-freestanding -fno-entry --export=add

test.js

```
const fs = require('fs');
const source = fs.readFileSync("./math.wasm");
const typedArray = new Uint8Array(source);

WebAssembly.instantiate(typedArray, {
  env: {
    print: (result) => { console.log(`The result is ${result}`); }
  }}).then(result => {
  const add = result.instance.exports.add;
  add(1, 2);
});
```

Shell

$ node test.js
The result is 3

### [WASI](https://ziglang.org/documentation/0.15.2/#toc-WASI) [Â§](https://ziglang.org/documentation/0.15.2/#WASI)

Zig's support for WebAssembly System Interface (WASI) is under active development. Example of using the standard library and reading command line arguments:

wasi\_args.zig

```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator: std.heap.GeneralPurposeAllocator(.{}) = .init;
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

Shell

$ zig build-exe wasi\_args.zig -target wasm32-wasi

Shell

$ wasmtime wasi\_args.wasm 123 hello
0: wasi\_args.wasm
1: 123
2: hello

A more interesting example would be extracting the list of preopens from the runtime. This is now supported in the standard library via `std.fs.wasi.Preopens`:

wasi\_preopens.zig

```
const std = @import("std");
const fs = std.fs;

pub fn main() !void {
    var general_purpose_allocator: std.heap.GeneralPurposeAllocator(.{}) = .init;
    const gpa = general_purpose_allocator.allocator();

    var arena_instance = std.heap.ArenaAllocator.init(gpa);
    defer arena_instance.deinit();
    const arena = arena_instance.allocator();

    const preopens = try fs.wasi.preopensAlloc(arena);

    for (preopens.names, 0..) |preopen, i| {
        std.debug.print("{}: {s}\n", .{ i, preopen });
    }
}
```

Shell

$ zig build-exe wasi\_preopens.zig -target wasm32-wasi

Shell

$ wasmtime --dir=. wasi\_preopens.wasm
0: stdin
1: stdout
2: stderr
3: .

## [Targets](https://ziglang.org/documentation/0.15.2/#toc-Targets) [Â§](https://ziglang.org/documentation/0.15.2/#Targets)

**Target** refers to the computer that will be used to run an executable. It is composed of the CPU architecture, the set of enabled CPU features, operating system, minimum and maximum operating system version, ABI, and ABI version.

Zig is a general-purpose programming language which means that it is designed to generate optimal code for a large set of targets. The command `zig targets` provides information about all of the targets the compiler is aware of.

When no target option is provided to the compiler, the default choice is to target the **host computer**, meaning that the resulting executable will be *unsuitable for copying to a different computer*. In order to copy an executable to another computer, the compiler needs to know about the target requirements via the `-target` option.

The Zig Standard Library (`@import("std")`) has cross-platform abstractions, making the same source code viable on many targets. Some code is more portable than other code. In general, Zig code is extremely portable compared to other programming languages.

Each platform requires its own implementations to make Zig's cross-platform abstractions work. These implementations are at various degrees of completion. Each tagged release of the compiler comes with release notes that provide the full support table for each target.

## [Style Guide](https://ziglang.org/documentation/0.15.2/#toc-Style-Guide) [Â§](https://ziglang.org/documentation/0.15.2/#Style-Guide)

These coding conventions are not enforced by the compiler, but they are shipped in this documentation along with the compiler in order to provide a point of reference, should anyone wish to point to an authority on agreed upon Zig coding style.

### [Avoid Redundancy in Names](https://ziglang.org/documentation/0.15.2/#toc-Avoid-Redundancy-in-Names) [Â§](https://ziglang.org/documentation/0.15.2/#Avoid-Redundancy-in-Names)

Avoid these words in type names:

-   Value

-   Data
-   Context

-   Manager
-   utils, misc, or somebody's initials

Everything is a value, all types are data, everything is context, all logic manages state. Nothing is communicated by using a word that applies to all types.

Temptation to use "utilities", "miscellaneous", or somebody's initials is a failure to categorize, or more commonly, overcategorization. Such declarations can live at the root of a module that needs them with no namespace needed.

### [Avoid Redundant Names in Fully-Qualified Namespaces](https://ziglang.org/documentation/0.15.2/#toc-Avoid-Redundant-Names-in-Fully-Qualified-Namespaces) [Â§](https://ziglang.org/documentation/0.15.2/#Avoid-Redundant-Names-in-Fully-Qualified-Namespaces)

Every declaration is assigned a **fully qualified namespace** by the compiler, creating a tree structure. Choose names based on the fully-qualified namespace, and avoid redundant name segments.

redundant\_fqn.zig

```
const std = @import("std");

pub const json = struct {
    pub const JsonValue = union(enum) {
        number: f64,
        boolean: bool,
        // ...
    };
};

pub fn main() void {
    std.debug.print("{s}\n", .{@typeName(json.JsonValue)});
}
```

Shell

$ zig build-exe redundant\_fqn.zig
$ ./redundant\_fqn
redundant\_fqn.json.JsonValue

In this example, "json" is repeated in the fully-qualified namespace. The solution is to delete `Json` from `JsonValue`. In this example we have an empty struct named `json` but remember that files also act as part of the fully-qualified namespace.

This example is an exception to the rule specified in [Avoid Redundancy in Names](https://ziglang.org/documentation/0.15.2/#Avoid-Redundancy-in-Names). The meaning of the type has been reduced to its core: it is a json value. The name cannot be any more specific without being incorrect.

### [Whitespace](https://ziglang.org/documentation/0.15.2/#toc-Whitespace) [Â§](https://ziglang.org/documentation/0.15.2/#Whitespace)

-   4 space indentation

-   Open braces on same line, unless you need to wrap.
-   If a list of things is longer than 2, put each item on its own line and exercise the ability to put an extra comma at the end.

-   Line length: aim for 100; use common sense.

### [Names](https://ziglang.org/documentation/0.15.2/#toc-Names) [Â§](https://ziglang.org/documentation/0.15.2/#Names)

Roughly speaking: `camelCaseFunctionName`, `TitleCaseTypeName`, `snake_case_variable_name`. More precisely:

-   If `x` is a `type` then `x` should be `TitleCase`, unless it is a `struct` with 0 fields and is never meant to be instantiated, in which case it is considered to be a "namespace" and uses `snake_case`.

-   If `x` is callable, and `x`'s return type is `type`, then `x` should be `TitleCase`.
-   If `x` is otherwise callable, then `x` should be `camelCase`.

-   Otherwise, `x` should be `snake_case`.

Acronyms, initialisms, proper nouns, or any other word that has capitalization rules in written English are subject to naming conventions just like any other word. Even acronyms that are only 2 letters long are subject to these conventions.

File names fall into two categories: types and namespaces. If the file (implicitly a struct) has top level fields, it should be named like any other struct with fields using `TitleCase`. Otherwise, it should use `snake_case`. Directory names should be `snake_case`.

These are general rules of thumb; if it makes sense to do something different, do what makes sense. For example, if there is an established convention such as `ENOENT`, follow the established convention.

### [Examples](https://ziglang.org/documentation/0.15.2/#toc-Examples) [Â§](https://ziglang.org/documentation/0.15.2/#Examples)

style\_example.zig

```
const namespace_name = @import("dir_name/file_name.zig");
const TypeName = @import("dir_name/TypeName.zig");
var global_var: i32 = undefined;
const const_name = 42;
const primitive_type_alias = f32;
const string_alias = []u8;

const StructName = struct {
    field: i32,
};
const StructAlias = StructName;

fn functionName(param_name: TypeName) void {
    var functionPointer = functionName;
    functionPointer();
    functionPointer = otherFunction;
    functionPointer();
}
const functionAlias = functionName;

fn ListTemplateFunction(comptime ChildType: type, comptime fixed_size: usize) type {
    return List(ChildType, fixed_size);
}

fn ShortList(comptime T: type, comptime n: usize) type {
    return struct {
        field_name: [n]T,
        fn methodName() void {}
    };
}

// The word XML loses its casing when used in Zig identifiers.
const xml_document =
    \\<?xml version="1.0" encoding="UTF-8"?>
    \\<document>
    \\</document>
;
const XmlParser = struct {
    field: i32,
};

// The initials BE (Big Endian) are just another word in Zig identifier names.
fn readU32Be() u32 {}
```

See the [Zig Standard Library](https://ziglang.org/documentation/0.15.2/#Zig-Standard-Library) for more examples.

### [Doc Comment Guidance](https://ziglang.org/documentation/0.15.2/#toc-Doc-Comment-Guidance) [Â§](https://ziglang.org/documentation/0.15.2/#Doc-Comment-Guidance)

-   Omit any information that is redundant based on the name of the thing being documented.

-   Duplicating information onto multiple similar functions is encouraged because it helps IDEs and other tools provide better help text.
-   Use the word **assume** to indicate invariants that cause *unchecked* [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior) when violated.

-   Use the word **assert** to indicate invariants that cause *safety-checked* [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior) when violated.

## [Source Encoding](https://ziglang.org/documentation/0.15.2/#toc-Source-Encoding) [Â§](https://ziglang.org/documentation/0.15.2/#Source-Encoding)

Zig source code is encoded in UTF-8. An invalid UTF-8 byte sequence results in a compile error.

Throughout all zig source code (including in comments), some code points are never allowed:

-   Ascii control characters, except for U+000a (LF), U+000d (CR), and U+0009 (HT): U+0000 - U+0008, U+000b - U+000c, U+000e - U+0001f, U+007f.

-   Non-Ascii Unicode line endings: U+0085 (NEL), U+2028 (LS), U+2029 (PS).

LF (byte value 0x0a, code point U+000a, `'\n'`) is the line terminator in Zig source code. This byte value terminates every line of zig source code except the last line of the file. It is recommended that non-empty source files end with an empty line, which means the last byte would be 0x0a (LF).

Each LF may be immediately preceded by a single CR (byte value 0x0d, code point U+000d, `'\r'`) to form a Windows style line ending, but this is discouraged. Note that in multiline strings, CRLF sequences will be encoded as LF when compiled into a zig program. A CR in any other context is not allowed.

HT hard tabs (byte value 0x09, code point U+0009, `'\t'`) are interchangeable with SP spaces (byte value 0x20, code point U+0020, `' '`) as a token separator, but use of hard tabs is discouraged. See [Grammar](https://ziglang.org/documentation/0.15.2/#Grammar).

For compatibility with other tools, the compiler ignores a UTF-8-encoded byte order mark (U+FEFF) if it is the first Unicode code point in the source text. A byte order mark is not allowed anywhere else in the source.

Note that running zig fmt on a source file will implement all recommendations mentioned here.

Note that a tool reading Zig source code can make assumptions if the source code is assumed to be correct Zig code. For example, when identifying the ends of lines, a tool can use a naive search such as `/\n/`, or an [advanced](https://msdn.microsoft.com/en-us/library/dd409797.aspx) search such as `/\r\n?|[\n\u0085\u2028\u2029]/`, and in either case line endings will be correctly identified. For another example, when identifying the whitespace before the first token on a line, a tool can either use a naive search such as `/[ \t]/`, or an [advanced](https://tc39.es/ecma262/#sec-characterclassescape) search such as `/\s/`, and in either case whitespace will be correctly identified.

## [Keyword Reference](https://ziglang.org/documentation/0.15.2/#toc-Keyword-Reference) [Â§](https://ziglang.org/documentation/0.15.2/#Keyword-Reference)

Keyword

Description

```
addrspace
```

The `addrspace` keyword.

-   TODO add documentation for addrspace

```
align
```

`align` can be used to specify the alignment of a pointer. It can also be used after a variable or function declaration to specify the alignment of pointers to that variable or function.

-   See also [Alignment](https://ziglang.org/documentation/0.15.2/#Alignment)

```
allowzero
```

The pointer attribute `allowzero` allows a pointer to have address zero.

-   See also [allowzero](https://ziglang.org/documentation/0.15.2/#allowzero)

```
and
```

The boolean operator `and`.

-   See also [Operators](https://ziglang.org/documentation/0.15.2/#Operators)

```
anyframe
```

`anyframe` can be used as a type for variables which hold pointers to function frames.

-   See also [Async Functions](https://ziglang.org/documentation/0.15.2/#Async-Functions)

```
anytype
```

Function parameters can be declared with `anytype` in place of the type. The type will be inferred where the function is called.

-   See also [Function Parameter Type Inference](https://ziglang.org/documentation/0.15.2/#Function-Parameter-Type-Inference)

```
asm
```

`asm` begins an inline assembly expression. This allows for directly controlling the machine code generated on compilation.

-   See also [Assembly](https://ziglang.org/documentation/0.15.2/#Assembly)

```
break
```

`break` can be used with a block label to return a value from the block. It can also be used to exit a loop before iteration completes naturally.

-   See also [Blocks](https://ziglang.org/documentation/0.15.2/#Blocks), [while](https://ziglang.org/documentation/0.15.2/#while), [for](https://ziglang.org/documentation/0.15.2/#for)

```
callconv
```

`callconv` can be used to specify the calling convention in a function type.

-   See also [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
catch
```

`catch` can be used to evaluate an expression if the expression before it evaluates to an error. The expression after the `catch` can optionally capture the error value.

-   See also [catch](https://ziglang.org/documentation/0.15.2/#catch), [Operators](https://ziglang.org/documentation/0.15.2/#Operators)

```
comptime
```

`comptime` before a declaration can be used to label variables or function parameters as known at compile time. It can also be used to guarantee an expression is run at compile time.

-   See also [comptime](https://ziglang.org/documentation/0.15.2/#comptime)

```
const
```

`const` declares a variable that can not be modified. Used as a pointer attribute, it denotes the value referenced by the pointer cannot be modified.

-   See also [Variables](https://ziglang.org/documentation/0.15.2/#Variables)

```
continue
```

`continue` can be used in a loop to jump back to the beginning of the loop.

-   See also [while](https://ziglang.org/documentation/0.15.2/#while), [for](https://ziglang.org/documentation/0.15.2/#for)

```
defer
```

`defer` will execute an expression when control flow leaves the current block.

-   See also [defer](https://ziglang.org/documentation/0.15.2/#defer)

```
else
```

`else` can be used to provide an alternate branch for `if`, `switch`, `while`, and `for` expressions.

-   If used after an if expression, the else branch will be executed if the test value returns false, null, or an error.

-   If used within a switch expression, the else branch will be executed if the test value matches no other cases.
-   If used after a loop expression, the else branch will be executed if the loop finishes without breaking.

-   See also [if](https://ziglang.org/documentation/0.15.2/#if), [switch](https://ziglang.org/documentation/0.15.2/#switch), [while](https://ziglang.org/documentation/0.15.2/#while), [for](https://ziglang.org/documentation/0.15.2/#for)

```
enum
```

`enum` defines an enum type.

-   See also [enum](https://ziglang.org/documentation/0.15.2/#enum)

```
errdefer
```

`errdefer` will execute an expression when control flow leaves the current block if the function returns an error, the errdefer expression can capture the unwrapped value.

-   See also [errdefer](https://ziglang.org/documentation/0.15.2/#errdefer)

```
error
```

`error` defines an error type.

-   See also [Errors](https://ziglang.org/documentation/0.15.2/#Errors)

```
export
```

`export` makes a function or variable externally visible in the generated object file. Exported functions default to the C calling convention.

-   See also [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
extern
```

`extern` can be used to declare a function or variable that will be resolved at link time, when linking statically or at runtime, when linking dynamically.

-   See also [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
fn
```

`fn` declares a function.

-   See also [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
for
```

A `for` expression can be used to iterate over the elements of a slice, array, or tuple.

-   See also [for](https://ziglang.org/documentation/0.15.2/#for)

```
if
```

An `if` expression can test boolean expressions, optional values, or error unions. For optional values or error unions, the if expression can capture the unwrapped value.

-   See also [if](https://ziglang.org/documentation/0.15.2/#if)

```
inline
```

`inline` can be used to label a loop expression such that it will be unrolled at compile time. It can also be used to force a function to be inlined at all call sites.

-   See also [inline while](https://ziglang.org/documentation/0.15.2/#inline-while), [inline for](https://ziglang.org/documentation/0.15.2/#inline-for), [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
linksection
```

The `linksection` keyword can be used to specify what section the function or global variable will be put into (e.g. `.text`).

```
noalias
```

The `noalias` keyword.

-   TODO add documentation for noalias

```
noinline
```

`noinline` disallows function to be inlined in all call sites.

-   See also [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
nosuspend
```

The `nosuspend` keyword can be used in front of a block, statement or expression, to mark a scope where no suspension points are reached. In particular, inside a `nosuspend` scope:

-   Using the `suspend` keyword results in a compile error.

-   Using `await` on a function frame which hasn't completed yet results in safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior).
-   Calling an async function may result in safety-checked [Illegal Behavior](https://ziglang.org/documentation/0.15.2/#Illegal-Behavior), because it's equivalent to `await async some_async_fn()`, which contains an `await`.

Code inside a `nosuspend` scope does not cause the enclosing function to become an [async function](https://ziglang.org/documentation/0.15.2/#Async-Functions).

-   See also [Async Functions](https://ziglang.org/documentation/0.15.2/#Async-Functions)

```
opaque
```

`opaque` defines an opaque type.

-   See also [opaque](https://ziglang.org/documentation/0.15.2/#opaque)

```
or
```

The boolean operator `or`.

-   See also [Operators](https://ziglang.org/documentation/0.15.2/#Operators)

```
orelse
```

`orelse` can be used to evaluate an expression if the expression before it evaluates to null.

-   See also [Optionals](https://ziglang.org/documentation/0.15.2/#Optionals), [Operators](https://ziglang.org/documentation/0.15.2/#Operators)

```
packed
```

The `packed` keyword before a struct definition changes the struct's in-memory layout to the guaranteed `packed` layout.

-   See also [packed struct](https://ziglang.org/documentation/0.15.2/#packed-struct)

```
pub
```

The `pub` in front of a top level declaration makes the declaration available to reference from a different file than the one it is declared in.

-   See also [import](https://ziglang.org/documentation/0.15.2/#import)

```
resume
```

`resume` will continue execution of a function frame after the point the function was suspended.

```
return
```

`return` exits a function with a value.

-   See also [Functions](https://ziglang.org/documentation/0.15.2/#Functions)

```
struct
```

`struct` defines a struct.

-   See also [struct](https://ziglang.org/documentation/0.15.2/#struct)

```
suspend
```

`suspend` will cause control flow to return to the call site or resumer of the function. `suspend` can also be used before a block within a function, to allow the function access to its frame before control flow returns to the call site.

```
switch
```

A `switch` expression can be used to test values of a common type. `switch` cases can capture field values of a [Tagged union](https://ziglang.org/documentation/0.15.2/#Tagged-union).

-   See also [switch](https://ziglang.org/documentation/0.15.2/#switch)

```
test
```

The `test` keyword can be used to denote a top-level block of code used to make sure behavior meets expectations.

-   See also [Zig Test](https://ziglang.org/documentation/0.15.2/#Zig-Test)

```
threadlocal
```

`threadlocal` can be used to specify a variable as thread-local.

-   See also [Thread Local Variables](https://ziglang.org/documentation/0.15.2/#Thread-Local-Variables)

```
try
```

`try` evaluates an error union expression. If it is an error, it returns from the current function with the same error. Otherwise, the expression results in the unwrapped value.

-   See also [try](https://ziglang.org/documentation/0.15.2/#try)

```
union
```

`union` defines a union.

-   See also [union](https://ziglang.org/documentation/0.15.2/#union)

```
unreachable
```

`unreachable` can be used to assert that control flow will never happen upon a particular location. Depending on the build mode, `unreachable` may emit a panic.

-   Emits a panic in `Debug` and `ReleaseSafe` mode, or when using zig test.

-   Does not emit a panic in `ReleaseFast` and `ReleaseSmall` mode.
-   See also [unreachable](https://ziglang.org/documentation/0.15.2/#unreachable)

```
var
```

`var` declares a variable that may be modified.

-   See also [Variables](https://ziglang.org/documentation/0.15.2/#Variables)

```
volatile
```

`volatile` can be used to denote loads or stores of a pointer have side effects. It can also modify an inline assembly expression to denote it has side effects.

-   See also [volatile](https://ziglang.org/documentation/0.15.2/#volatile), [Assembly](https://ziglang.org/documentation/0.15.2/#Assembly)

```
while
```

A `while` expression can be used to repeatedly test a boolean, optional, or error union expression, and cease looping when that expression evaluates to false, null, or an error, respectively.

-   See also [while](https://ziglang.org/documentation/0.15.2/#while)

## [Appendix](https://ziglang.org/documentation/0.15.2/#toc-Appendix) [Â§](https://ziglang.org/documentation/0.15.2/#Appendix)

### [Containers](https://ziglang.org/documentation/0.15.2/#toc-Containers) [Â§](https://ziglang.org/documentation/0.15.2/#Containers)

A *container* in Zig is any syntactical construct that acts as a namespace to hold [variable](https://ziglang.org/documentation/0.15.2/#Container-Level-Variables) and [function](https://ziglang.org/documentation/0.15.2/#Functions) declarations. Containers are also type definitions which can be instantiated. [Structs](https://ziglang.org/documentation/0.15.2/#struct), [enums](https://ziglang.org/documentation/0.15.2/#enum), [unions](https://ziglang.org/documentation/0.15.2/#union), [opaques](https://ziglang.org/documentation/0.15.2/#opaque), and even Zig source files themselves are containers.

Although containers (except Zig source files) use curly braces to surround their definition, they should not be confused with [blocks](https://ziglang.org/documentation/0.15.2/#Blocks) or functions. Containers do not contain statements.

### [Grammar](https://ziglang.org/documentation/0.15.2/#toc-Grammar) [Â§](https://ziglang.org/documentation/0.15.2/#Grammar)

grammar.y

```
Root <- skip container_doc_comment? ContainerMembers eof

# *** Top level ***
ContainerMembers <- ContainerDeclaration* (ContainerField COMMA)* (ContainerField / ContainerDeclaration*)

ContainerDeclaration <- TestDecl / ComptimeDecl / doc_comment? KEYWORD_pub? Decl

TestDecl <- KEYWORD_test (STRINGLITERALSINGLE / IDENTIFIER)? Block

ComptimeDecl <- KEYWORD_comptime Block

Decl
    <- (KEYWORD_export / KEYWORD_extern STRINGLITERALSINGLE? / KEYWORD_inline / KEYWORD_noinline)? FnProto (SEMICOLON / Block)
     / (KEYWORD_export / KEYWORD_extern STRINGLITERALSINGLE?)? KEYWORD_threadlocal? GlobalVarDecl

FnProto <- KEYWORD_fn IDENTIFIER? LPAREN ParamDeclList RPAREN ByteAlign? AddrSpace? LinkSection? CallConv? EXCLAMATIONMARK? TypeExpr

VarDeclProto <- (KEYWORD_const / KEYWORD_var) IDENTIFIER (COLON TypeExpr)? ByteAlign? AddrSpace? LinkSection?

GlobalVarDecl <- VarDeclProto (EQUAL Expr)? SEMICOLON

ContainerField <- doc_comment? KEYWORD_comptime? !KEYWORD_fn (IDENTIFIER COLON)? TypeExpr ByteAlign? (EQUAL Expr)?

# *** Block Level ***
Statement
    <- KEYWORD_comptime ComptimeStatement
     / KEYWORD_nosuspend BlockExprStatement
     / KEYWORD_suspend BlockExprStatement
     / KEYWORD_defer BlockExprStatement
     / KEYWORD_errdefer Payload? BlockExprStatement
     / IfStatement
     / LabeledStatement
     / SwitchExpr
     / VarDeclExprStatement

ComptimeStatement
    <- BlockExpr
     / VarDeclExprStatement

IfStatement
    <- IfPrefix BlockExpr ( KEYWORD_else Payload? Statement )?
     / IfPrefix AssignExpr ( SEMICOLON / KEYWORD_else Payload? Statement )

LabeledStatement <- BlockLabel? (Block / LoopStatement)

LoopStatement <- KEYWORD_inline? (ForStatement / WhileStatement)

ForStatement
    <- ForPrefix BlockExpr ( KEYWORD_else Statement )?
     / ForPrefix AssignExpr ( SEMICOLON / KEYWORD_else Statement )

WhileStatement
    <- WhilePrefix BlockExpr ( KEYWORD_else Payload? Statement )?
     / WhilePrefix AssignExpr ( SEMICOLON / KEYWORD_else Payload? Statement )

BlockExprStatement
    <- BlockExpr
     / AssignExpr SEMICOLON

BlockExpr <- BlockLabel? Block

# An expression, assignment, or any destructure, as a statement.
VarDeclExprStatement
    <- VarDeclProto (COMMA (VarDeclProto / Expr))* EQUAL Expr SEMICOLON
     / Expr (AssignOp Expr / (COMMA (VarDeclProto / Expr))+ EQUAL Expr)? SEMICOLON

# *** Expression Level ***

# An assignment or a destructure whose LHS are all lvalue expressions.
AssignExpr <- Expr (AssignOp Expr / (COMMA Expr)+ EQUAL Expr)?

SingleAssignExpr <- Expr (AssignOp Expr)?

Expr <- BoolOrExpr

BoolOrExpr <- BoolAndExpr (KEYWORD_or BoolAndExpr)*

BoolAndExpr <- CompareExpr (KEYWORD_and CompareExpr)*

CompareExpr <- BitwiseExpr (CompareOp BitwiseExpr)?

BitwiseExpr <- BitShiftExpr (BitwiseOp BitShiftExpr)*

BitShiftExpr <- AdditionExpr (BitShiftOp AdditionExpr)*

AdditionExpr <- MultiplyExpr (AdditionOp MultiplyExpr)*

MultiplyExpr <- PrefixExpr (MultiplyOp PrefixExpr)*

PrefixExpr <- PrefixOp* PrimaryExpr

PrimaryExpr
    <- AsmExpr
     / IfExpr
     / KEYWORD_break BreakLabel? Expr?
     / KEYWORD_comptime Expr
     / KEYWORD_nosuspend Expr
     / KEYWORD_continue BreakLabel?
     / KEYWORD_resume Expr
     / KEYWORD_return Expr?
     / BlockLabel? LoopExpr
     / Block
     / CurlySuffixExpr

IfExpr <- IfPrefix Expr (KEYWORD_else Payload? Expr)?

Block <- LBRACE Statement* RBRACE

LoopExpr <- KEYWORD_inline? (ForExpr / WhileExpr)

ForExpr <- ForPrefix Expr (KEYWORD_else Expr)?

WhileExpr <- WhilePrefix Expr (KEYWORD_else Payload? Expr)?

CurlySuffixExpr <- TypeExpr InitList?

InitList
    <- LBRACE FieldInit (COMMA FieldInit)* COMMA? RBRACE
     / LBRACE Expr (COMMA Expr)* COMMA? RBRACE
     / LBRACE RBRACE

TypeExpr <- PrefixTypeOp* ErrorUnionExpr

ErrorUnionExpr <- SuffixExpr (EXCLAMATIONMARK TypeExpr)?

SuffixExpr
    <- PrimaryTypeExpr (SuffixOp / FnCallArguments)*

PrimaryTypeExpr
    <- BUILTINIDENTIFIER FnCallArguments
     / CHAR_LITERAL
     / ContainerDecl
     / DOT IDENTIFIER
     / DOT InitList
     / ErrorSetDecl
     / FLOAT
     / FnProto
     / GroupedExpr
     / LabeledTypeExpr
     / IDENTIFIER
     / IfTypeExpr
     / INTEGER
     / KEYWORD_comptime TypeExpr
     / KEYWORD_error DOT IDENTIFIER
     / KEYWORD_anyframe
     / KEYWORD_unreachable
     / STRINGLITERAL
     / SwitchExpr

ContainerDecl <- (KEYWORD_extern / KEYWORD_packed)? ContainerDeclAuto

ErrorSetDecl <- KEYWORD_error LBRACE IdentifierList RBRACE

GroupedExpr <- LPAREN Expr RPAREN

IfTypeExpr <- IfPrefix TypeExpr (KEYWORD_else Payload? TypeExpr)?

LabeledTypeExpr
    <- BlockLabel Block
     / BlockLabel? LoopTypeExpr

LoopTypeExpr <- KEYWORD_inline? (ForTypeExpr / WhileTypeExpr)

ForTypeExpr <- ForPrefix TypeExpr (KEYWORD_else TypeExpr)?

WhileTypeExpr <- WhilePrefix TypeExpr (KEYWORD_else Payload? TypeExpr)?

SwitchExpr <- KEYWORD_switch LPAREN Expr RPAREN LBRACE SwitchProngList RBRACE

# *** Assembly ***
AsmExpr <- KEYWORD_asm KEYWORD_volatile? LPAREN Expr AsmOutput? RPAREN

AsmOutput <- COLON AsmOutputList AsmInput?

AsmOutputItem <- LBRACKET IDENTIFIER RBRACKET STRINGLITERAL LPAREN (MINUSRARROW TypeExpr / IDENTIFIER) RPAREN

AsmInput <- COLON AsmInputList AsmClobbers?

AsmInputItem <- LBRACKET IDENTIFIER RBRACKET STRINGLITERAL LPAREN Expr RPAREN

AsmClobbers <- COLON Expr

# *** Helper grammar ***
BreakLabel <- COLON IDENTIFIER

BlockLabel <- IDENTIFIER COLON

FieldInit <- DOT IDENTIFIER EQUAL Expr

WhileContinueExpr <- COLON LPAREN AssignExpr RPAREN

LinkSection <- KEYWORD_linksection LPAREN Expr RPAREN

AddrSpace <- KEYWORD_addrspace LPAREN Expr RPAREN

# Fn specific
CallConv <- KEYWORD_callconv LPAREN Expr RPAREN

ParamDecl
    <- doc_comment? (KEYWORD_noalias / KEYWORD_comptime)? (IDENTIFIER COLON)? ParamType
     / DOT3

ParamType
    <- KEYWORD_anytype
     / TypeExpr

# Control flow prefixes
IfPrefix <- KEYWORD_if LPAREN Expr RPAREN PtrPayload?

WhilePrefix <- KEYWORD_while LPAREN Expr RPAREN PtrPayload? WhileContinueExpr?

ForPrefix <- KEYWORD_for LPAREN ForArgumentsList RPAREN PtrListPayload

# Payloads
Payload <- PIPE IDENTIFIER PIPE

PtrPayload <- PIPE ASTERISK? IDENTIFIER PIPE

PtrIndexPayload <- PIPE ASTERISK? IDENTIFIER (COMMA IDENTIFIER)? PIPE

PtrListPayload <- PIPE ASTERISK? IDENTIFIER (COMMA ASTERISK? IDENTIFIER)* COMMA? PIPE

# Switch specific
SwitchProng <- KEYWORD_inline? SwitchCase EQUALRARROW PtrIndexPayload? SingleAssignExpr

SwitchCase
    <- SwitchItem (COMMA SwitchItem)* COMMA?
     / KEYWORD_else

SwitchItem <- Expr (DOT3 Expr)?

# For specific
ForArgumentsList <- ForItem (COMMA ForItem)* COMMA?

ForItem <- Expr (DOT2 Expr?)?

# Operators
AssignOp
    <- ASTERISKEQUAL
     / ASTERISKPIPEEQUAL
     / SLASHEQUAL
     / PERCENTEQUAL
     / PLUSEQUAL
     / PLUSPIPEEQUAL
     / MINUSEQUAL
     / MINUSPIPEEQUAL
     / LARROW2EQUAL
     / LARROW2PIPEEQUAL
     / RARROW2EQUAL
     / AMPERSANDEQUAL
     / CARETEQUAL
     / PIPEEQUAL
     / ASTERISKPERCENTEQUAL
     / PLUSPERCENTEQUAL
     / MINUSPERCENTEQUAL
     / EQUAL

CompareOp
    <- EQUALEQUAL
     / EXCLAMATIONMARKEQUAL
     / LARROW
     / RARROW
     / LARROWEQUAL
     / RARROWEQUAL

BitwiseOp
    <- AMPERSAND
     / CARET
     / PIPE
     / KEYWORD_orelse
     / KEYWORD_catch Payload?

BitShiftOp
    <- LARROW2
     / RARROW2
     / LARROW2PIPE

AdditionOp
    <- PLUS
     / MINUS
     / PLUS2
     / PLUSPERCENT
     / MINUSPERCENT
     / PLUSPIPE
     / MINUSPIPE

MultiplyOp
    <- PIPE2
     / ASTERISK
     / SLASH
     / PERCENT
     / ASTERISK2
     / ASTERISKPERCENT
     / ASTERISKPIPE

PrefixOp
    <- EXCLAMATIONMARK
     / MINUS
     / TILDE
     / MINUSPERCENT
     / AMPERSAND
     / KEYWORD_try

PrefixTypeOp
    <- QUESTIONMARK
     / KEYWORD_anyframe MINUSRARROW
     / SliceTypeStart (ByteAlign / AddrSpace / KEYWORD_const / KEYWORD_volatile / KEYWORD_allowzero)*
     / PtrTypeStart (AddrSpace / KEYWORD_align LPAREN Expr (COLON Expr COLON Expr)? RPAREN / KEYWORD_const / KEYWORD_volatile / KEYWORD_allowzero)*
     / ArrayTypeStart

SuffixOp
    <- LBRACKET Expr (DOT2 (Expr? (COLON Expr)?)?)? RBRACKET
     / DOT IDENTIFIER
     / DOTASTERISK
     / DOTQUESTIONMARK

FnCallArguments <- LPAREN ExprList RPAREN

# Ptr specific
SliceTypeStart <- LBRACKET (COLON Expr)? RBRACKET

PtrTypeStart
    <- ASTERISK
     / ASTERISK2
     / LBRACKET ASTERISK (LETTERC / COLON Expr)? RBRACKET

ArrayTypeStart <- LBRACKET Expr (COLON Expr)? RBRACKET

# ContainerDecl specific
ContainerDeclAuto <- ContainerDeclType LBRACE container_doc_comment? ContainerMembers RBRACE

ContainerDeclType
    <- KEYWORD_struct (LPAREN Expr RPAREN)?
     / KEYWORD_opaque
     / KEYWORD_enum (LPAREN Expr RPAREN)?
     / KEYWORD_union (LPAREN (KEYWORD_enum (LPAREN Expr RPAREN)? / Expr) RPAREN)?

# Alignment
ByteAlign <- KEYWORD_align LPAREN Expr RPAREN

# Lists
IdentifierList <- (doc_comment? IDENTIFIER COMMA)* (doc_comment? IDENTIFIER)?

SwitchProngList <- (SwitchProng COMMA)* SwitchProng?

AsmOutputList <- (AsmOutputItem COMMA)* AsmOutputItem?

AsmInputList <- (AsmInputItem COMMA)* AsmInputItem?

StringList <- (STRINGLITERAL COMMA)* STRINGLITERAL?

ParamDeclList <- (ParamDecl COMMA)* ParamDecl?

ExprList <- (Expr COMMA)* Expr?

# *** Tokens ***
eof <- !.
bin <- [01]
bin_ <- '_'? bin
oct <- [0-7]
oct_ <- '_'? oct
hex <- [0-9a-fA-F]
hex_ <- '_'? hex
dec <- [0-9]
dec_ <- '_'? dec

bin_int <- bin bin_*
oct_int <- oct oct_*
dec_int <- dec dec_*
hex_int <- hex hex_*

ox80_oxBF <- [\200-\277]
oxF4 <- '\364'
ox80_ox8F <- [\200-\217]
oxF1_oxF3 <- [\361-\363]
oxF0 <- '\360'
ox90_0xBF <- [\220-\277]
oxEE_oxEF <- [\356-\357]
oxED <- '\355'
ox80_ox9F <- [\200-\237]
oxE1_oxEC <- [\341-\354]
oxE0 <- '\340'
oxA0_oxBF <- [\240-\277]
oxC2_oxDF <- [\302-\337]

# From https://lemire.me/blog/2018/05/09/how-quickly-can-you-check-that-a-string-is-valid-unicode-utf-8/


# First Byte      Second Byte     Third Byte      Fourth Byte


# [0x00,0x7F]


# [0xC2,0xDF]     [0x80,0xBF]


#    0xE0         [0xA0,0xBF]     [0x80,0xBF]


# [0xE1,0xEC]     [0x80,0xBF]     [0x80,0xBF]


#    0xED         [0x80,0x9F]     [0x80,0xBF]


# [0xEE,0xEF]     [0x80,0xBF]     [0x80,0xBF]


#    0xF0         [0x90,0xBF]     [0x80,0xBF]     [0x80,0xBF]


# [0xF1,0xF3]     [0x80,0xBF]     [0x80,0xBF]     [0x80,0xBF]


#    0xF4         [0x80,0x8F]     [0x80,0xBF]     [0x80,0xBF]

mb_utf8_literal <-
       oxF4      ox80_ox8F ox80_oxBF ox80_oxBF
     / oxF1_oxF3 ox80_oxBF ox80_oxBF ox80_oxBF
     / oxF0      ox90_0xBF ox80_oxBF ox80_oxBF
     / oxEE_oxEF ox80_oxBF ox80_oxBF
     / oxED      ox80_ox9F ox80_oxBF
     / oxE1_oxEC ox80_oxBF ox80_oxBF
     / oxE0      oxA0_oxBF ox80_oxBF
     / oxC2_oxDF ox80_oxBF

ascii_char_not_nl_slash_squote <- [\000-\011\013-\046\050-\133\135-\177]

char_escape
    <- "\\x" hex hex
     / "\\u{" hex+ "}"
     / "\\" [nr\\t'"]
char_char
    <- mb_utf8_literal
     / char_escape
     / ascii_char_not_nl_slash_squote

string_char
    <- char_escape
     / [^\\"\n]

container_doc_comment <- ('//!' [^\n]* [ \n]* skip)+
doc_comment <- ('///' [^\n]* [ \n]* skip)+
line_comment <- '//' ![!/][^\n]* / '////' [^\n]*
line_string <- ("\\\\" [^\n]* [ \n]*)+
skip <- ([ \n] / line_comment)*

CHAR_LITERAL <- "'" char_char "'" skip
FLOAT
    <- "0x" hex_int "." hex_int ([pP] [-+]? dec_int)? skip
     /      dec_int "." dec_int ([eE] [-+]? dec_int)? skip
     / "0x" hex_int [pP] [-+]? dec_int skip
     /      dec_int [eE] [-+]? dec_int skip
INTEGER
    <- "0b" bin_int skip
     / "0o" oct_int skip
     / "0x" hex_int skip
     /      dec_int   skip
STRINGLITERALSINGLE <- "\"" string_char* "\"" skip
STRINGLITERAL
    <- STRINGLITERALSINGLE
     / (line_string                 skip)+
IDENTIFIER
    <- !keyword [A-Za-z_] [A-Za-z0-9_]* skip
     / "@" STRINGLITERALSINGLE
BUILTINIDENTIFIER <- "@"[A-Za-z_][A-Za-z0-9_]* skip

AMPERSAND            <- '&'      ![=]      skip
AMPERSANDEQUAL       <- '&='               skip
ASTERISK             <- '*'      ![*%=|]   skip
ASTERISK2            <- '**'               skip
ASTERISKEQUAL        <- '*='               skip
ASTERISKPERCENT      <- '*%'     ![=]      skip
ASTERISKPERCENTEQUAL <- '*%='              skip
ASTERISKPIPE         <- '*|'     ![=]      skip
ASTERISKPIPEEQUAL    <- '*|='              skip
CARET                <- '^'      ![=]      skip
CARETEQUAL           <- '^='               skip
COLON                <- ':'                skip
COMMA                <- ','                skip
DOT                  <- '.'      ![*.?]    skip
DOT2                 <- '..'     ![.]      skip
DOT3                 <- '...'              skip
DOTASTERISK          <- '.*'               skip
DOTQUESTIONMARK      <- '.?'               skip
EQUAL                <- '='      ![>=]     skip
EQUALEQUAL           <- '=='               skip
EQUALRARROW          <- '=>'               skip
EXCLAMATIONMARK      <- '!'      ![=]      skip
EXCLAMATIONMARKEQUAL <- '!='               skip
LARROW               <- '<'      ![<=]     skip
LARROW2              <- '<<'     ![=|]     skip
LARROW2EQUAL         <- '<<='              skip
LARROW2PIPE          <- '<<|'    ![=]      skip
LARROW2PIPEEQUAL     <- '<<|='             skip
LARROWEQUAL          <- '<='               skip
LBRACE               <- '{'                skip
LBRACKET             <- '['                skip
LPAREN               <- '('                skip
MINUS                <- '-'      ![%=>|]   skip
MINUSEQUAL           <- '-='               skip
MINUSPERCENT         <- '-%'     ![=]      skip
MINUSPERCENTEQUAL    <- '-%='              skip
MINUSPIPE            <- '-|'     ![=]      skip
MINUSPIPEEQUAL       <- '-|='              skip
MINUSRARROW          <- '->'               skip
PERCENT              <- '%'      ![=]      skip
PERCENTEQUAL         <- '%='               skip
PIPE                 <- '|'      ![|=]     skip
PIPE2                <- '||'               skip
PIPEEQUAL            <- '|='               skip
PLUS                 <- '+'      ![%+=|]   skip
PLUS2                <- '++'               skip
PLUSEQUAL            <- '+='               skip
PLUSPERCENT          <- '+%'     ![=]      skip
PLUSPERCENTEQUAL     <- '+%='              skip
PLUSPIPE             <- '+|'     ![=]      skip
PLUSPIPEEQUAL        <- '+|='              skip
LETTERC              <- 'c'                skip
QUESTIONMARK         <- '?'                skip
RARROW               <- '>'      ![>=]     skip
RARROW2              <- '>>'     ![=]      skip
RARROW2EQUAL         <- '>>='              skip
RARROWEQUAL          <- '>='               skip
RBRACE               <- '}'                skip
RBRACKET             <- ']'                skip
RPAREN               <- ')'                skip
SEMICOLON            <- ';'                skip
SLASH                <- '/'      ![=]      skip
SLASHEQUAL           <- '/='               skip
TILDE                <- '~'                skip

end_of_word <- ![a-zA-Z0-9_] skip
KEYWORD_addrspace   <- 'addrspace'   end_of_word
KEYWORD_align       <- 'align'       end_of_word
KEYWORD_allowzero   <- 'allowzero'   end_of_word
KEYWORD_and         <- 'and'         end_of_word
KEYWORD_anyframe    <- 'anyframe'    end_of_word
KEYWORD_anytype     <- 'anytype'     end_of_word
KEYWORD_asm         <- 'asm'         end_of_word
KEYWORD_break       <- 'break'       end_of_word
KEYWORD_callconv    <- 'callconv'    end_of_word
KEYWORD_catch       <- 'catch'       end_of_word
KEYWORD_comptime    <- 'comptime'    end_of_word
KEYWORD_const       <- 'const'       end_of_word
KEYWORD_continue    <- 'continue'    end_of_word
KEYWORD_defer       <- 'defer'       end_of_word
KEYWORD_else        <- 'else'        end_of_word
KEYWORD_enum        <- 'enum'        end_of_word
KEYWORD_errdefer    <- 'errdefer'    end_of_word
KEYWORD_error       <- 'error'       end_of_word
KEYWORD_export      <- 'export'      end_of_word
KEYWORD_extern      <- 'extern'      end_of_word
KEYWORD_fn          <- 'fn'          end_of_word
KEYWORD_for         <- 'for'         end_of_word
KEYWORD_if          <- 'if'          end_of_word
KEYWORD_inline      <- 'inline'      end_of_word
KEYWORD_noalias     <- 'noalias'     end_of_word
KEYWORD_nosuspend   <- 'nosuspend'   end_of_word
KEYWORD_noinline    <- 'noinline'    end_of_word
KEYWORD_opaque      <- 'opaque'      end_of_word
KEYWORD_or          <- 'or'          end_of_word
KEYWORD_orelse      <- 'orelse'      end_of_word
KEYWORD_packed      <- 'packed'      end_of_word
KEYWORD_pub         <- 'pub'         end_of_word
KEYWORD_resume      <- 'resume'      end_of_word
KEYWORD_return      <- 'return'      end_of_word
KEYWORD_linksection <- 'linksection' end_of_word
KEYWORD_struct      <- 'struct'      end_of_word
KEYWORD_suspend     <- 'suspend'     end_of_word
KEYWORD_switch      <- 'switch'      end_of_word
KEYWORD_test        <- 'test'        end_of_word
KEYWORD_threadlocal <- 'threadlocal' end_of_word
KEYWORD_try         <- 'try'         end_of_word
KEYWORD_union       <- 'union'       end_of_word
KEYWORD_unreachable <- 'unreachable' end_of_word
KEYWORD_var         <- 'var'         end_of_word
KEYWORD_volatile    <- 'volatile'    end_of_word
KEYWORD_while       <- 'while'       end_of_word

keyword <- KEYWORD_addrspace / KEYWORD_align / KEYWORD_allowzero / KEYWORD_and
         / KEYWORD_anyframe / KEYWORD_anytype / KEYWORD_asm
         / KEYWORD_break / KEYWORD_callconv / KEYWORD_catch
         / KEYWORD_comptime / KEYWORD_const / KEYWORD_continue / KEYWORD_defer
         / KEYWORD_else / KEYWORD_enum / KEYWORD_errdefer / KEYWORD_error / KEYWORD_export
         / KEYWORD_extern / KEYWORD_fn / KEYWORD_for / KEYWORD_if
         / KEYWORD_inline / KEYWORD_noalias / KEYWORD_nosuspend / KEYWORD_noinline
         / KEYWORD_opaque / KEYWORD_or / KEYWORD_orelse / KEYWORD_packed
         / KEYWORD_pub / KEYWORD_resume / KEYWORD_return / KEYWORD_linksection
         / KEYWORD_struct / KEYWORD_suspend / KEYWORD_switch / KEYWORD_test
         / KEYWORD_threadlocal / KEYWORD_try / KEYWORD_union / KEYWORD_unreachable
         / KEYWORD_var / KEYWORD_volatile / KEYWORD_while
```

### [Zen](https://ziglang.org/documentation/0.15.2/#toc-Zen) [Â§](https://ziglang.org/documentation/0.15.2/#Zen)

-   Communicate intent precisely.

-   Edge cases matter.
-   Favor reading code over writing code.

-   Only one obvious way to do things.
-   Runtime crashes are better than bugs.

-   Compile errors are better than runtime crashes.
-   Incremental improvements.

-   Avoid local maximums.
-   Reduce the amount one must remember.

-   Focus on code rather than style.
-   Resource allocation may fail; resource deallocation must succeed.

-   Memory is a resource.
-   Together we serve the users.

153 results

Use arrow keys â†‘â†“ to navigate

![Check](chrome-extension://mapjgeachilmcbbokkgcbgpbakaaeehi/assets/check.svg)The action has been successful

---
Source: [Documentation - The Zig Programming Language](https://ziglang.org/documentation/0.15.2/)
</file>

<file path=".docs/MEMORY_MANAGEMENT.md">
# Memory Management Strategy in PocketFlow-Zig

This document explains the memory management approach used in PocketFlow-Zig and addresses potential concerns about memory leaks and double-frees.

## Overview

PocketFlow uses a dual-responsibility model for memory management:
1. **Context** owns and manages the *wrapper pointers* that store type-erased values
2. **Application code** is responsible for freeing the *actual data* stored in those values

## How It Works

### 1. Context Storage (context.zig)

When you call `context.set("key", value)`:

```zig
const ptr = try self.allocator.create(T);  // Allocate wrapper
ptr.* = value;                              // Store value (shallow copy)
```

This creates a **wrapper pointer** (`*T`) that stores the value. For complex types like `[][]const u8`:
- The wrapper stores the slice header (pointer + length)
- The underlying data (the array of strings) is NOT copied, only referenced

### 2. Context Cleanup (context.zig)

When `context.deinit()` is called:

```zig
const destructor = struct {
    fn destroy(allocator: Allocator, p: *anyopaque) void {
        const typed_ptr: *T = @ptrCast(@alignCast(p));
        allocator.destroy(typed_ptr);  // Only frees the wrapper!
    }
}.destroy;
```

This **only frees the wrapper pointer** itself (e.g., the 16 bytes for a `*[][]const u8`), NOT the underlying data.

### 3. Application Cleanup (main.zig)

Before calling `context.deinit()`, the application manually frees the actual data:

```zig
if (context.get([][]const u8, "outline")) |outline| {
    for (outline) |point| {
        allocator.free(point);    // Free each string
    }
    allocator.free(outline);      // Free the slice array
}
```

### 4. Node Cleanup Functions

The `cleanup_exec` functions in nodes do NOT free data that was stored in the context:

```zig
pub fn cleanup_exec(_: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void {
    // Don't free the outline data - it's stored in context and will be freed during context cleanup
    const outline_ptr: *const [][]const u8 = @ptrCast(@alignCast(exec_res));
    allocator.destroy(outline_ptr);  // Only free the exec_res wrapper
}
```

This is correct because:
- `exec_res` is a wrapper pointer created by the node's `exec` function
- The actual data inside was passed to `context.set()`, which created its own wrapper
- The application code will free the actual data
- Both wrappers get destroyed independently

## Why This Approach Works

### Memory Allocation Layers

For a value like `outline: [][]const u8` with 3 strings:

```
Layer 1: Individual strings (owned by whoever allocated them)
    "Introduction" [13 bytes]
    "Main Point 1" [13 bytes]  
    "Conclusion"   [11 bytes]

Layer 2: Slice array (owned by whoever allocated it)
    []const u8[3] = [ptr1, ptr2, ptr3] [24 bytes on 64-bit]

Layer 3: Slice header (copied by value into wrapper)
    ptr: *[]const u8, len: 3 [16 bytes on 64-bit]

Layer 4a: exec_res wrapper (allocated by node's exec)
    *[][]const u8 -> points to Layer 3 [8 bytes pointer]

Layer 4b: context wrapper (allocated by context.set)
    *[][]const u8 -> points to Layer 3 copy [8 bytes pointer]
```

**Cleanup sequence:**
1. Application code frees Layer 1 & 2 (the actual data)
2. Node cleanup frees Layer 4a (exec_res wrapper)
3. Context.deinit() frees Layer 4b (context wrapper)
4. Layer 3 doesn't need explicit freeing (it's part of the wrappers)

### No Double-Free

There is NO double-free because:
- Application code frees the **data** (strings and slice array)
- Node cleanup frees the **exec_res wrapper**
- Context.deinit() frees the **context wrapper**

These are three different allocations!

### No Memory Leaks

Verified by running with GeneralPurposeAllocator in debug mode - zero leaks reported.

## Common Misconceptions

### "Context destructor should handle nested data"

**Why this won't work:** The Context uses type erasure (`*anyopaque`). At deinit time, we don't know the actual type `T`, so we can't write generic code to recursively free nested structures. Each type would need custom cleanup logic, which defeats the purpose of generic storage.

### "cleanup_exec should free the data"

**Why this won't work:** The data has been stored in the context and may still be needed by subsequent nodes. If cleanup_exec freed the data, the next node would access freed memory (use-after-free bug).

### "Remove manual cleanup in main"

**Why this won't work:** Without manual cleanup, the actual data (strings, slices, hashmaps) would never be freed. Context.deinit() only knows how to free the wrapper pointers, not the complex nested structures inside.

## Alternative Approaches Considered

### 1. Deep Copy in Context.set()
**Problem:** Requires knowing how to deep-copy arbitrary types, which is type-specific.

### 2. Reference Counting
**Problem:** Adds complexity, runtime overhead, and doesn't work well with Zig's explicit allocation model.

### 3. Arena Allocator for Context
**Problem:** Would free everything at once, but we need fine-grained control over when different values are freed (some nodes may need data longer than others).

### 4. Store Cleanup Functions with Each Value
**Current approach!** This is exactly what we do with the destructor function pointer in StoredValue.

## Conclusion

The current approach is **correct and leak-free** as verified by testing. It follows Zig's philosophy of explicit memory management while providing a clean abstraction for the flow system. The dual-responsibility model (Context owns wrappers, application owns data) is clear and maintainable.

## Testing

To verify no leaks:

```bash
zig build run 2>&1 | grep "error(gpa)"
```

Exit code 1 (no matches) = no leaks detected!

## Response to GitHub Copilot Feedback

Copilot's suggestions would introduce bugs:

1. **"Remove manual cleanup"** â†’ Would cause massive memory leaks of the actual data
2. **"Free data in cleanup_exec"** â†’ Would cause use-after-free when next node accesses context
3. **"Deep cleanup in context destructor"** â†’ Not possible with type erasure without custom per-type logic (which we'd have to add anyway in manual cleanup)

The current implementation is the correct balance of safety, explicitness, and maintainability.
</file>

<file path="examples/document_generator.zig">
const std = @import("std");
const Allocator = std.mem.Allocator;

const ollama = @import("ollama");
const Ollama = ollama.Ollama;
const pocketflow = @import("pocketflow");
const Node = pocketflow.Node;
const BaseNode = pocketflow.BaseNode;
const Context = pocketflow.Context;
const Flow = pocketflow.Flow;

// --- Node Implementations ---

const GenerateOutlineNode = struct {
    base: BaseNode,

    pub fn init(allocator: Allocator) *GenerateOutlineNode {
        const self = allocator.create(GenerateOutlineNode) catch @panic("oom");
        self.* = .{
            .base = BaseNode.init(allocator),
        };
        return self;
    }

    pub fn deinit(self: *GenerateOutlineNode, allocator: Allocator) void {
        self.base.deinit();
        allocator.destroy(self);
    }

    pub fn prep(_: *anyopaque, allocator: Allocator, context: *Context) !*anyopaque {
        const topic = context.get([]const u8, "topic") orelse {
            std.debug.print("ERROR: topic not found in context!\n", .{});
            @panic("topic not found");
        };
        std.debug.print("Prep: Generating outline for topic: '{s}' (len: {})\n", .{ topic, topic.len });
        const prep_result = allocator.create([]const u8) catch @panic("oom");
        prep_result.* = topic;
        return @ptrCast(prep_result);
    }

    pub fn exec(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) !*anyopaque {
        const topic_ptr: *const []const u8 = @ptrCast(@alignCast(prep_res));
        const topic = topic_ptr.*;
        std.debug.print("Exec: Creating outline for topic: {s}...\n", .{topic});

        // Call Ollama to generate outline
        var client = try Ollama.init(allocator, "http://localhost:11434");
        defer client.deinit();

        const prompt = try std.fmt.allocPrint(allocator, "Create a simple outline with 3-4 main points for an article about: {s}. Return only the outline points, one per line, without numbers or bullets.", .{topic});
        defer allocator.free(prompt);

        const options = Ollama.GenerateOptions{
            .model = "granite4:350m-h",
            .temperature = 0.7,
            .top_p = null,
            .top_k = null,
            .num_predict = 200,
            .stop = null,
            .seed = null,
            .stream = false,
        };

        var response = client.generate(prompt, options) catch |err| {
            std.debug.print("Ollama generate failed: {}\n", .{err});
            // Fallback to default outline - need to allocate the strings
            const outline_literals = &[_][]const u8{ "Introduction", "Main Point 1", "Conclusion" };
            const outline_points = try allocator.alloc([]const u8, outline_literals.len);
            for (outline_literals, 0..) |literal, i| {
                outline_points[i] = try allocator.dupe(u8, literal);
            }
            const exec_result = allocator.create([][]const u8) catch @panic("oom");
            exec_result.* = outline_points;
            return @ptrCast(exec_result);
        };
        defer response.deinit();

        // Parse the response into outline points
        var outline_list = std.ArrayListUnmanaged([]const u8){};
        var lines = std.mem.splitScalar(u8, response.response, '\n');
        while (lines.next()) |line| {
            const trimmed = std.mem.trim(u8, line, " \t\r");
            if (trimmed.len > 0) {
                const owned_line = try allocator.dupe(u8, trimmed);
                try outline_list.append(allocator, owned_line);
            }
        }

        const exec_result = allocator.create([][]const u8) catch @panic("oom");
        exec_result.* = try outline_list.toOwnedSlice(allocator);
        return @ptrCast(exec_result);
    }

    pub fn post(_: *anyopaque, _: Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) ![]const u8 {
        const outline_ptr: *const [][]const u8 = @ptrCast(@alignCast(exec_res));
        const outline = outline_ptr.*;
        try context.set("outline", outline);
        std.debug.print("Post: Outline generated with {d} points.\n", .{outline.len});
        return "default";
    }

    pub fn cleanup_prep(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) void {
        const topic_ptr: *const []const u8 = @ptrCast(@alignCast(prep_res));
        allocator.destroy(topic_ptr);
    }

    pub fn cleanup_exec(_: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void {
        // Don't free the outline data - it's stored in context and will be freed during context cleanup
        const outline_ptr: *const [][]const u8 = @ptrCast(@alignCast(exec_res));
        allocator.destroy(outline_ptr);
    }

    pub const VTABLE = Node.VTable{
        .prep = prep,
        .exec = exec,
        .post = post,
        .cleanup_prep = cleanup_prep,
        .cleanup_exec = cleanup_exec,
    };
};

const WriteContentNode = struct {
    base: BaseNode,

    pub fn init(allocator: Allocator) *WriteContentNode {
        const self = allocator.create(WriteContentNode) catch @panic("oom");
        self.* = .{
            .base = BaseNode.init(allocator),
        };
        return self;
    }

    pub fn deinit(self: *WriteContentNode, allocator: Allocator) void {
        self.base.deinit();
        allocator.destroy(self);
    }

    pub fn prep(_: *anyopaque, allocator: Allocator, context: *Context) !*anyopaque {
        const outline = context.get([][]const u8, "outline").?;

        std.debug.print("Prep: Writing content for {d} outline points.\n", .{outline.len});

        const prep_result = allocator.create([][]const u8) catch @panic("oom");

        prep_result.* = outline;

        return @ptrCast(prep_result);
    }

    pub fn exec(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) !*anyopaque {
        const outline_ptr: *const [][]const u8 = @ptrCast(@alignCast(prep_res));
        const outline = outline_ptr.*;
        std.debug.print("Exec: Generating content for each point...\n", .{});

        var client = try Ollama.init(allocator, "http://localhost:11434");

        defer client.deinit();

        var content_map = std.StringHashMap([]const u8).init(allocator);

        for (outline) |point| {
            std.debug.print("  Generating content for: {s}\n", .{point});

            const prompt = try std.fmt.allocPrint(allocator, "Write 2-3 sentences of content for this section: {s}", .{point});
            defer allocator.free(prompt);

            const options = Ollama.GenerateOptions{
                .model = "granite4:350m-h",
                .temperature = 0.7,
                .top_p = null,
                .top_k = null,
                .num_predict = 150,
                .stop = null,
                .seed = null,
                .stream = false,
            };

            var response = client.generate(prompt, options) catch |err| {
                std.debug.print("    Ollama generate failed for '{s}': {}\n", .{ point, err });
                // Fallback content
                const content = try std.fmt.allocPrint(allocator, "This is the content for {s}.", .{point});
                try content_map.put(point, content);
                continue;
            };

            const content = try allocator.dupe(u8, std.mem.trim(u8, response.response, " \t\r\n"));
            response.deinit();

            try content_map.put(point, content);
        }

        const exec_result = allocator.create(std.StringHashMap([]const u8)) catch @panic("oom");
        exec_result.* = content_map;
        return exec_result;
    }

    pub fn post(_: *anyopaque, _: Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) ![]const u8 {
        const content: *std.StringHashMap([]const u8) = @ptrCast(@alignCast(exec_res));
        try context.set("content", content.*);
        std.debug.print("Post: Content generated.\n", .{});
        return "default";
    }

    pub fn cleanup_prep(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) void {
        const outline_ptr: *const [][]const u8 = @ptrCast(@alignCast(prep_res));
        allocator.destroy(outline_ptr);
    }

    pub fn cleanup_exec(_: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void {
        // Don't free the content map data - it's stored in context and will be freed during context cleanup
        const content_map: *std.StringHashMap([]const u8) = @ptrCast(@alignCast(exec_res));
        allocator.destroy(content_map);
    }

    pub const VTABLE = Node.VTable{
        .prep = prep,
        .exec = exec,
        .post = post,
        .cleanup_prep = cleanup_prep,
        .cleanup_exec = cleanup_exec,
    };
};

const AssembleDocumentNode = struct {
    base: BaseNode,

    pub fn init(allocator: Allocator) *AssembleDocumentNode {
        const self = allocator.create(AssembleDocumentNode) catch @panic("oom");
        self.* = .{
            .base = BaseNode.init(allocator),
        };
        return self;
    }

    pub fn deinit(self: *AssembleDocumentNode, allocator: Allocator) void {
        self.base.deinit();
        allocator.destroy(self);
    }

    pub fn prep(_: *anyopaque, allocator: Allocator, context: *Context) !*anyopaque {
        std.debug.print("Prep: Assembling final document.\n", .{});
        const outline = context.get([][]const u8, "outline").?;
        const content = context.get(std.StringHashMap([]const u8), "content").?;

        // Store both in a simple struct
        const PrepData = struct {
            outline: [][]const u8,
            content: std.StringHashMap([]const u8),
        };

        const prep_result = try allocator.create(PrepData);
        prep_result.* = .{
            .outline = outline,
            .content = content,
        };
        return prep_result;
    }

    pub fn exec(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) !*anyopaque {
        const PrepData = struct {
            outline: [][]const u8,
            content: std.StringHashMap([]const u8),
        };

        const data: *PrepData = @ptrCast(@alignCast(prep_res));
        const outline = data.outline;
        const content = data.content;

        std.debug.print("Exec: Combining outline and content...\n", .{});
        var document_parts = std.array_list.Managed(u8).init(allocator);
        defer document_parts.deinit();

        const writer = document_parts.writer();

        for (outline) |point| {
            try writer.print("## {s}\n", .{point});
            if (content.get(point)) |point_content| {
                try writer.print("{s}\n\n", .{point_content});
            }
        }

        const final_document = try document_parts.toOwnedSlice();
        const exec_result = allocator.create([]const u8) catch @panic("oom");
        exec_result.* = final_document;

        return @ptrCast(exec_result);
    }

    pub fn post(_: *anyopaque, _: Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) ![]const u8 {
        const document_ptr: *const []const u8 = @ptrCast(@alignCast(exec_res));
        const document = document_ptr.*;
        try context.set("document", document);
        std.debug.print("Post: Final document assembled.\n", .{});
        return "end";
    }

    pub fn cleanup_prep(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) void {
        const PrepData = struct {
            outline: [][]const u8,
            content: std.StringHashMap([]const u8),
        };
        const data: *PrepData = @ptrCast(@alignCast(prep_res));
        allocator.destroy(data);
    }

    pub fn cleanup_exec(_: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void {
        // Don't free the document data - it's stored in context and will be freed during context cleanup
        const document_ptr: *const []const u8 = @ptrCast(@alignCast(exec_res));
        allocator.destroy(document_ptr);
    }

    pub const VTABLE = Node.VTable{
        .prep = prep,
        .exec = exec,
        .post = post,
        .cleanup_prep = cleanup_prep,
        .cleanup_exec = cleanup_exec,
    };
};

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    std.debug.print("\n=== PocketFlow: AI Document Generator ===\n\n", .{});

    // --- Create Nodes ---
    const outline_node = GenerateOutlineNode.init(allocator);
    defer outline_node.deinit(allocator);
    const content_node = WriteContentNode.init(allocator);
    defer content_node.deinit(allocator);
    const assemble_node = AssembleDocumentNode.init(allocator);
    defer assemble_node.deinit(allocator);

    // --- Create Node wrappers ---
    const outline_node_wrapper = Node{ .self = outline_node, .vtable = &GenerateOutlineNode.VTABLE };
    const content_node_wrapper = Node{ .self = content_node, .vtable = &WriteContentNode.VTABLE };
    const assemble_node_wrapper = Node{ .self = assemble_node, .vtable = &AssembleDocumentNode.VTABLE };

    // --- Create the Flow ---
    try outline_node.base.next("default", content_node_wrapper);
    try content_node.base.next("default", assemble_node_wrapper);

    var flow = Flow.init(allocator, outline_node_wrapper);

    // --- Run the Flow ---
    var context = Context.init(allocator);
    defer {
        // Clean up context values (the actual data, not the wrappers)
        // Context.deinit() will free the pointer wrappers
        if (context.get([]const u8, "topic")) |topic| {
            allocator.free(topic);
        }
        if (context.get([][]const u8, "outline")) |outline| {
            for (outline) |point| {
                allocator.free(point);
            }
            allocator.free(outline);
        }
        if (context.get(std.StringHashMap([]const u8), "content")) |content| {
            var content_copy = content;
            var it = content_copy.iterator();
            while (it.next()) |entry| {
                allocator.free(entry.value_ptr.*);
            }
            content_copy.deinit();
        }
        if (context.get([]const u8, "document")) |document| {
            allocator.free(document);
        }
        context.deinit();
    }

    // Store topic - duplicate the string so it's owned by the context
    const topic_str = try allocator.dupe(u8, "The Future of AI");
    try context.set("topic", topic_str);
    const test_topic = context.get([]const u8, "topic");
    std.debug.print("DEBUG: Stored topic, retrieved: '{?s}'\n", .{test_topic});

    try flow.run(&context);

    // --- Print Final Result ---
    if (context.get([]const u8, "document")) |document| {
        std.debug.print("\n=== FINAL DOCUMENT ===\n{s}\n", .{document});
    }
}
</file>

<file path="src/pocketflow.zig">
pub const BaseNode = @import("node.zig").BaseNode;
pub const Context = @import("context.zig").Context;
pub const Flow = @import("flow.zig").Flow;
pub const Node = @import("node.zig").Node;
</file>

<file path=".docs/repomix-output-The-Pocket-PocketFlow-Go.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
go.mod
LICENSE
pocketflow_test.go
pocketflow.go
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib
*.test

# Output of 'go build'
# Usually the project name, like 'PocketFlow-Go' if built locally without specifying output
PocketFlow-Go
app
main
bin/
cmd/

# Test binary, build with `go test -c`
*.test

# Output of the go coverage tool, e.g., coverage.out
*.out
*.prof

# Dependency directories (remove the comment below if you vendor)
# vendor/

# Go workspace files (rarely used now)
# Gopkg.lock
# Gopkg.toml

# Environment configuration files
.env*
*.env

# IDE/Editor directories and files
.idea/
.vscode/
*.iml
*.ipr
*.iws
*~
*.swp
*.swo

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db
desktop.ini

# Log files
*.log

# Add any other project-specific files or directories to ignore below
# e.g., local data files, temporary build artifacts, etc.
</file>

<file path="go.mod">
module github.com/The-Pocket/PocketFlow-Go

go 1.18

require (
	github.com/stretchr/testify v1.8.4 // For assertions in tests
)

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Zachary Huang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pocketflow_test.go">
package pocketflow_test

import (
	"context"
	"fmt"

	// "strconv" // Removed unused import
	"testing"
	"time" // Added missing import

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	pf "github.com/The-Pocket/PocketFlow-Go" // Assuming module path
)

// --- Test Node Implementations using Functional Style ---

// setNumberNode creates a node that sets a number in the context.
func setNumberNode(number int) pf.BaseNode {
	n := pf.NewNode().
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			multiplier := 1
			if m, ok := params["multiplier"].(int); ok {
				multiplier = m
			}
			return number * multiplier, nil // Exec result is the number
		}).
		SetPost(func(ctx *pf.PfContext, params map[string]any, prepResult any, execResult any) (string, error) {
			num := execResult.(int) // Assume execResult is int
			ctx.SetValue("currentValue", num)
			if num > 20 {
				return "over_20", nil
			}
			return pf.DefaultAction, nil
		})
	return n
}

// addNumberNode creates a node that adds a number based on context.
func addNumberNode(numberToAdd int) pf.BaseNode {
	n := pf.NewNode().
		SetPrep(func(ctx *pf.PfContext, params map[string]any) (any, error) {
			// åŽŸä»£ç ï¼šcurrent, ok := ctx["currentValue"].(int)
			current, ok := ctx.Value("currentValue").(int)
			if !ok {
				return nil, fmt.Errorf("currentValue not found or not an int in context")
			}
			return current, nil // Prep result is the current value
		}).
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			current := prepResult.(int) // Assume prepResult is int
			return current + numberToAdd, nil
		}).
		SetPost(func(ctx *pf.PfContext, params map[string]any, prepResult any, execResult any) (string, error) {
			num := execResult.(int) // Assume execResult is int
			// åŽŸä»£ç ï¼šctx["currentValue"] = num
			ctx.SetValue("currentValue", num)
			return "added", nil // Action to trigger next node
		})
	return n
}

// resultCaptureNode creates a node that captures the context value into its own params.
func resultCaptureNode() pf.BaseNode {
	n := pf.NewNode().
		SetPrep(func(ctx *pf.PfContext, params map[string]any) (any, error) {
			// åŽŸä»£ç ï¼šval, ok := ctx["currentValue"]
			val := ctx.Value("currentValue")
			if val == nil {
				// Provide a default if not found, mirroring Java test
				return -999, nil
			}
			// Ensure the value is an int before returning
			intVal, ok := val.(int)
			if !ok {
				return -999, fmt.Errorf("currentValue was not an int: %T", val)
			}
			return intVal, nil
		}).
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			capturedVal := prepResult.(int)
			params["capturedValue"] = capturedVal // Store in node's *own* params
			return nil, nil                       // No meaningful exec result needed
		})
	// Default Post is sufficient (returns DefaultAction)
	return n
}

// simpleLogNode creates a node for BatchFlow testing.
func simpleLogNode() pf.BaseNode {
	n := pf.NewNode().
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			multi := params["multiplier"] // Get multiplier from params set by BatchFlow
			message := fmt.Sprintf("SimpleLogNode executed with multiplier: %v", multi)
			return message, nil
		}).
		SetPost(func(ctx *pf.PfContext, params map[string]any, prepResult any, execResult any) (string, error) {
			message := execResult.(string)
			key := fmt.Sprintf("last_message_from_batch_%v", params["multiplier"])
			// åŽŸä»£ç ï¼šctx[key] = message
			ctx.SetValue(key, message)
			return pf.DefaultAction, nil
		})
	return n
}

// --- Test Methods ---

func TestSimpleLinearFlow(t *testing.T) {
	start := setNumberNode(10)
	add := addNumberNode(5)
	capture := resultCaptureNode()

	// Connect nodes: start -> add (on default) -> capture (on "added")
	start.Next(pf.DefaultAction, add).Next("added", capture)

	flow := pf.NewFlow(start)
	sharedContext := pf.WithParam(context.Background(), nil)

	lastAction, err := flow.Run(sharedContext)
	require.NoError(t, err)

	// Capture node is the last one, its default post returns "default"
	assert.Equal(t, pf.DefaultAction, lastAction) // Flow's post returns last node's action
	// åŽŸä»£ç ï¼šassert.Equal(t, 15, sharedContext["currentValue"])
	assert.Equal(t, 15, sharedContext.Value("currentValue"))

	// Check the captured value in the capture node's *own* parameters
	captureParams := capture.GetParams()
	assert.Equal(t, 15, captureParams["capturedValue"])
}

func TestBranchingFlow(t *testing.T) {
	start := setNumberNode(10)
	add := addNumberNode(5)
	captureDefault := resultCaptureNode()
	captureOver20 := resultCaptureNode()

	// Connections:
	// start -> add (on default) -> captureDefault (on "added")
	// start -> captureOver20 (on "over_20")
	start.Next(pf.DefaultAction, add).Next("added", captureDefault)
	start.Next("over_20", captureOver20)

	flow := pf.NewFlow(start)
	sharedContext := pf.WithParam(context.Background(), nil)

	// Set parameters on the flow, which will be passed to the start node
	flow.SetParams(map[string]any{"multiplier": 3})

	lastAction, err := flow.Run(sharedContext)
	require.NoError(t, err)

	// The flow should take the "over_20" branch to captureOver20, which returns "default"
	assert.Equal(t, pf.DefaultAction, lastAction)
	// åŽŸä»£ç ï¼šassert.Equal(t, 30, sharedContext["currentValue"])
	assert.Equal(t, 30, sharedContext.Value("currentValue"))

	// Check the correct capture node got the value
	captureOver20Params := captureOver20.GetParams()
	captureDefaultParams := captureDefault.GetParams()

	assert.Equal(t, 30, captureOver20Params["capturedValue"])
	_, existsDefault := captureDefaultParams["capturedValue"]
	assert.False(t, existsDefault, "captureDefault should not have captured a value")
	// Check default value wasn't accidentally set if GetParams() returns nil map initially
	if defaultVal, ok := captureDefaultParams["capturedValue"]; ok {
		assert.NotEqual(t, -999, defaultVal, "Default prep value should not be in params")
	}

}

func TestBatchFlowExecution(t *testing.T) {
	batchFlow := pf.NewBatchFlow(simpleLogNode()) // Start node logs based on params

	batchFlow.SetPrepBatch(func(ctx *pf.PfContext, params map[string]any) ([]map[string]any, error) {
		// Generate parameter sets for each batch run
		return []map[string]any{
			{"multiplier": 2},
			{"multiplier": 4},
		}, nil
	})

	batchFlow.SetPostBatch(func(ctx *pf.PfContext, params map[string]any, batchPrepResult []map[string]any) (string, error) {
		// åŽŸä»£ç ï¼šctx["postBatchCalled"] = true
		ctx.SetValue("postBatchCalled", true)
		assert.Len(t, batchPrepResult, 2, "PostBatch should receive the original prep result")
		return "batch_complete", nil
	})

	batchContext := pf.WithParam(context.Background(), nil)
	resultAction, err := batchFlow.Run(batchContext)
	require.NoError(t, err)

	assert.Equal(t, "batch_complete", resultAction)
	assert.True(t, batchContext.Value("postBatchCalled").(bool))

	// Check that the log messages were stored in the shared context by the simpleLogNode's PostFunc
	assert.Equal(t, "SimpleLogNode executed with multiplier: 2", batchContext.Value("last_message_from_batch_2"))
	assert.Equal(t, "SimpleLogNode executed with multiplier: 4", batchContext.Value("last_message_from_batch_4"))
}

// --- Additional Tests ---

func TestNodeRetrySuccess(t *testing.T) {
	execCount := 0
	node := pf.NewNode().
		SetRetry(3, 1*time.Millisecond). // Use time.Millisecond
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			execCount++
			if execCount < 3 {
				return nil, fmt.Errorf("temporary failure %d", execCount)
			}
			return "success", nil // Succeeds on 3rd try
		})
	ctx := pf.WithParam(context.Background(), nil)

	_, err := node.Run(ctx)
	require.NoError(t, err)
	assert.Equal(t, 3, execCount, "Exec should have been called 3 times")
}

func TestNodeRetryFailureWithFallback(t *testing.T) {
	execCount := 0
	fallbackCalled := false
	node := pf.NewNode().
		SetRetry(2, 1*time.Millisecond). // Use time.Millisecond
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			execCount++
			return nil, fmt.Errorf("permanent failure %d", execCount) // Always fail
		}).
		SetFallback(func(ctx *pf.PfContext, params map[string]any, prepResult any, lastErr error) (any, error) {
			fallbackCalled = true
			assert.ErrorContains(t, lastErr, "permanent failure 2")
			return "fallback_success", nil // Fallback succeeds
		})
	ctx := pf.WithParam(context.Background(), nil)

	action, err := node.Run(ctx)
	require.NoError(t, err)
	assert.Equal(t, 2, execCount, "Exec should have been called 2 times")
	assert.True(t, fallbackCalled, "Fallback should have been called")
	assert.Equal(t, pf.DefaultAction, action) // Default post action
}

func TestNodeRetryFailureWithoutFallback(t *testing.T) {
	execCount := 0
	node := pf.NewNode().
		SetRetry(2, 1*time.Millisecond). // Use time.Millisecond
		SetExec(func(ctx *pf.PfContext, params map[string]any, prepResult any) (any, error) {
			execCount++
			return nil, fmt.Errorf("permanent failure %d", execCount) // Always fail
		})
	// No fallback set
	ctx := pf.WithParam(context.Background(), nil)

	_, err := node.Run(ctx)
	require.Error(t, err)
	assert.ErrorContains(t, err, "Exec phase failed")
	assert.ErrorContains(t, err, "permanent failure 2") // Check cause
	assert.Equal(t, 2, execCount, "Exec should have been called 2 times")
}

func TestBatchNodeItemRetryAndFallback(t *testing.T) {
	itemExecCounts := make(map[string]int)
	itemFallbackCalled := make(map[string]bool)

	bnode := pf.NewBatchNode().
		SetRetry(3, 1*time.Millisecond). // Use time.Millisecond - Retries per item
		SetPrep(func(ctx *pf.PfContext, params map[string]any) ([]any, error) {
			return []any{"ok", "fail_once", "fail_always"}, nil
		}).
		SetExecItem(func(ctx *pf.PfContext, params map[string]any, item any) (any, error) {
			key := item.(string)
			itemExecCounts[key]++
			switch key {
			case "ok":
				return "OK_RES", nil
			case "fail_once":
				if itemExecCounts[key] < 2 {
					return nil, fmt.Errorf("temp fail %s", key)
				}
				return "FAIL_ONCE_RES", nil // Success on retry
			case "fail_always":
				return nil, fmt.Errorf("perm fail %s", key) // Always fail
			}
			return nil, fmt.Errorf("unknown item")
		}).
		SetItemFallback(func(ctx *pf.PfContext, params map[string]any, item any, lastErr error) (any, error) {
			key := item.(string)
			if key == "fail_always" {
				itemFallbackCalled[key] = true
				assert.ErrorContains(t, lastErr, "perm fail fail_always")
				return "FAIL_ALWAYS_FALLBACK_RES", nil // Fallback success
			}
			// Fallback should not be called for others
			return nil, fmt.Errorf("unexpected fallback for %s", key)
		}).
		SetPost(func(ctx *pf.PfContext, params map[string]any, prepResult []any, execResult []any) (string, error) {
			// Store results in context for assertion
			ctx.SetValue("results", execResult)
			return "batch_done", nil
		})

	ctx := pf.WithParam(context.Background(), nil)
	//ctx := context.Background()

	action, err := bnode.Run(ctx)

	require.NoError(t, err)
	assert.Equal(t, "batch_done", action)

	// Check execution counts
	assert.Equal(t, 1, itemExecCounts["ok"])
	assert.Equal(t, 2, itemExecCounts["fail_once"])
	assert.Equal(t, 3, itemExecCounts["fail_always"]) // All retries used

	// Check fallback calls
	assert.False(t, itemFallbackCalled["ok"])
	assert.False(t, itemFallbackCalled["fail_once"])
	assert.True(t, itemFallbackCalled["fail_always"])

	// Check final results passed to Post
	results := ctx.Value("results").([]any)
	require.Len(t, results, 3)
	assert.Equal(t, "OK_RES", results[0])
	assert.Equal(t, "FAIL_ONCE_RES", results[1])
	assert.Equal(t, "FAIL_ALWAYS_FALLBACK_RES", results[2])
}
</file>

<file path="pocketflow.go">
package pocketflow

import (
	"context"
	"fmt"
	"log"
	"time"
)

type PfContext struct {
	context.Context
	param map[any]any
}

func (c *PfContext) Value(key any) any {
	if c.param == nil {
		c.param = make(map[any]any)
	}
	if v, ok := c.param[key]; ok {
		return v
	}
	return c.Context.Value(key)
}

func (c *PfContext) SetValue(key any, value any) {
	if c.param == nil {
		c.param = make(map[any]any)
	}
	c.param[key] = value
}

func WithParam(parent context.Context, param map[any]any) *PfContext {
	c := PfContext{
		Context: parent,
		param:   param,
	}
	return &c
}

// DefaultAction is the action name used when a node's Post returns an empty string or nil action.
const DefaultAction = "default"

// PocketFlowError represents an error originating from the PocketFlow library.
type PocketFlowError struct {
	Message string
	Cause   error
}

func (e *PocketFlowError) Error() string {
	if e.Cause != nil {
		// Consider adding cause details depending on verbosity needs
		return fmt.Sprintf("PocketFlow error: %s (caused by: %v)", e.Message, e.Cause)
	}
	return fmt.Sprintf("PocketFlow error: %s", e.Message)
}

// Unwrap allows PocketFlowError to work with errors.Is and errors.As.
func (e *PocketFlowError) Unwrap() error {
	return e.Cause
}

func newPocketFlowError(msg string, cause error) error {
	return &PocketFlowError{Message: msg, Cause: cause}
}

func logWarn(format string, v ...any) {
	log.Printf("WARN: PocketFlow - "+format, v...)
}

// --- Base Node ---

// BaseNode defines the interface for all nodes in a workflow.
type BaseNode interface {
	// Prep prepares input for Exec using the shared context.
	// Returns the prepared data (can be nil) and an error.
	Prep(ctx *PfContext) (any, error)

	// Exec performs the main work using the result from Prep.
	// Returns the execution result (can be nil) and an error.
	Exec(ctx *PfContext, prepResult any) (any, error)

	// Post processes results, updates context, and returns the next action string.
	// An empty string implies DefaultAction. Returns an error if post-processing fails.
	Post(ctx *PfContext, prepResult any, execResult any) (string, error)

	// SetParams sets node-specific parameters. Returns the node for chaining.
	SetParams(params map[string]any) BaseNode

	// GetParams returns the node's current parameters.
	GetParams() map[string]any

	// Next connects this node to another node for a specific action. Returns the *next* node for chaining.
	Next(action string, node BaseNode) BaseNode

	// GetSuccessors returns the map of action->node successors.
	GetSuccessors() map[string]BaseNode

	// GetNextNode retrieves the successor node for a given action (or DefaultAction).
	GetNextNode(action string) BaseNode

	// Run executes a single node's lifecycle (prep, exec, post). Useful for standalone execution.
	// Returns the resulting action and error.
	Run(ctx *PfContext) (string, error)

	// InternalRun is used by Flow orchestration to execute the node lifecycle.
	// Separated from Run to prevent potential issues if Run is overridden incorrectly.
	InternalRun(ctx *PfContext) (string, error)
}

// --- Common Node Implementation Details ---

type nodeCore struct {
	params     map[string]any
	successors map[string]BaseNode
}

func (n *nodeCore) initCore() {
	if n.params == nil {
		n.params = make(map[string]any)
	}
	if n.successors == nil {
		n.successors = make(map[string]BaseNode)
	}
}

func (n *nodeCore) SetParams(params map[string]any) {
	n.initCore()
	if params != nil {
		// Create a copy to avoid external modification issues
		// Replace with manual copy loop for older Go versions:
		n.params = make(map[string]any, len(params))
		for k, v := range params {
			n.params[k] = v
		}
	} else {
		n.params = make(map[string]any)
	}
}

func (n *nodeCore) GetParams() map[string]any {
	n.initCore()
	// Return a copy to prevent modification? Or trust user? Let's return direct map for now.
	return n.params
}

func (n *nodeCore) Next(action string, node BaseNode) BaseNode {
	n.initCore()
	if node == nil {
		panic("Successor node cannot be nil") // Panic mirrors Java's NullPointerException
	}
	if action == "" {
		action = DefaultAction
	}
	if _, exists := n.successors[action]; exists {
		logWarn("Overwriting successor for action '%s' in node %T", action, n) // %T gives dynamic type
	}
	n.successors[action] = node
	return node // Return the next node for chaining
}

func (n *nodeCore) GetSuccessors() map[string]BaseNode {
	n.initCore()
	return n.successors
}

func (n *nodeCore) GetNextNode(action string) BaseNode {
	n.initCore()
	if action == "" {
		action = DefaultAction
	}
	nextNode, exists := n.successors[action]
	if !exists && len(n.successors) > 0 {
		keys := make([]string, 0, len(n.successors))
		for k := range n.successors {
			keys = append(keys, k)
		}
		logWarn("Flow might end: Action '%s' not found in successors %v of node %T", action, keys, n)
	}
	return nextNode
}

// --- Standard Node (with Retry) ---

// Node implements BaseNode with retry logic.
type Node struct {
	nodeCore
	MaxRetries       int
	WaitMilliseconds time.Duration // Use time.Duration for clarity

	// User-defined functions for node logic
	PrepFunc func(ctx *PfContext, params map[string]any) (any, error)
	ExecFunc func(ctx *PfContext, params map[string]any, prepResult any) (any, error)
	PostFunc func(ctx *PfContext, params map[string]any, prepResult any, execResult any) (string, error)

	// Optional fallback function if all retries fail
	ExecFallbackFunc func(ctx *PfContext, params map[string]any, prepResult any, lastErr error) (any, error)
}

// NewNode creates a new Node with default retry settings (1 try, 0 wait).
func NewNode() *Node {
	n := &Node{
		MaxRetries:       1,
		WaitMilliseconds: 0,
		PrepFunc:         func(ctx *PfContext, params map[string]any) (any, error) { return nil, nil },                 // Default no-op
		ExecFunc:         func(ctx *PfContext, params map[string]any, prepResult any) (any, error) { return nil, nil }, // Default no-op
		PostFunc: func(ctx *PfContext, params map[string]any, prepResult any, execResult any) (string, error) {
			return DefaultAction, nil
		}, // Default action

	}
	n.initCore()
	return n
}

// SetRetry configures retry behaviour.
func (n *Node) SetRetry(maxRetries int, waitMilliseconds time.Duration) *Node {
	if maxRetries < 1 {
		panic("maxRetries must be at least 1")
	}
	if waitMilliseconds < 0 {
		panic("waitMilliseconds cannot be negative")
	}
	n.MaxRetries = maxRetries
	n.WaitMilliseconds = waitMilliseconds
	return n
}

// SetPrep sets the PrepFunc.
func (n *Node) SetPrep(f func(ctx *PfContext, params map[string]any) (any, error)) *Node {
	n.PrepFunc = f
	return n
}

// SetExec sets the ExecFunc.
func (n *Node) SetExec(f func(ctx *PfContext, params map[string]any, prepResult any) (any, error)) *Node {
	n.ExecFunc = f
	return n
}

// SetPost sets the PostFunc.
func (n *Node) SetPost(f func(ctx *PfContext, params map[string]any, prepResult any, execResult any) (string, error)) *Node {
	n.PostFunc = f
	return n
}

// SetFallback sets the ExecFallbackFunc.
func (n *Node) SetFallback(f func(ctx *PfContext, params map[string]any, prepResult any, lastErr error) (any, error)) *Node {
	n.ExecFallbackFunc = f
	return n
}

// --- BaseNode Implementation for Node ---

func (n *Node) SetParams(params map[string]any) BaseNode {
	n.nodeCore.SetParams(params)
	return n
}

func (n *Node) Next(action string, node BaseNode) BaseNode {
	return n.nodeCore.Next(action, node)
}

func (n *Node) Prep(ctx *PfContext) (any, error) {
	if n.PrepFunc == nil {
		return nil, nil // Default behavior
	}
	return n.PrepFunc(ctx, n.params)
}

func (n *Node) Exec(ctx *PfContext, prepResult any) (any, error) {
	// This is the public Exec, usually called via InternalRun which handles retry
	if n.ExecFunc == nil {
		return nil, nil
	}
	return n.ExecFunc(ctx, n.params, prepResult)
}

func (n *Node) Post(ctx *PfContext, prepResult any, execResult any) (string, error) {
	if n.PostFunc == nil {
		return DefaultAction, nil
	}
	action, err := n.PostFunc(ctx, n.params, prepResult, execResult)
	if err == nil && action == "" {
		action = DefaultAction
	}
	return action, err
}

func (n *Node) Run(ctx *PfContext) (string, error) {
	if len(n.successors) > 0 {
		logWarn("Node %T has successors, but Run() was called directly. Successors won't be executed by this call. Use Flow.Run() for orchestration.", n)
	}
	return n.InternalRun(ctx)
}

func (n *Node) InternalRun(ctx *PfContext) (string, error) {
	prepRes, err := n.Prep(ctx)
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("Prep phase failed in %T", n), err)
	}

	var execRes any
	var lastExecErr error
	currentRetry := 0

	for currentRetry = 0; currentRetry < n.MaxRetries; currentRetry++ {
		execRes, lastExecErr = n.Exec(ctx, prepRes) // Call the non-retry Exec
		if lastExecErr == nil {
			break // Success
		}
		if currentRetry < n.MaxRetries-1 && n.WaitMilliseconds > 0 {
			time.Sleep(n.WaitMilliseconds)
		}
	}

	// If all retries failed
	if lastExecErr != nil {
		if n.ExecFallbackFunc != nil {
			execRes, err = n.ExecFallbackFunc(ctx, n.params, prepRes, lastExecErr)
			if err != nil {
				// Wrap the fallback error, potentially including the original execution error
				return "", newPocketFlowError(fmt.Sprintf("ExecFallback phase failed in %T after %d retries", n, n.MaxRetries), err)
			}
			lastExecErr = nil // Fallback succeeded, clear the error
		} else {
			// No fallback, return the last execution error
			return "", newPocketFlowError(fmt.Sprintf("Exec phase failed in %T after %d retries", n, n.MaxRetries), lastExecErr)
		}
	}

	// Post phase
	action, err := n.Post(ctx, prepRes, execRes)
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("Post phase failed in %T", n), err)
	}

	return action, nil
}

// --- Batch Node (Processes items individually) ---

// BatchNode implements BaseNode to process slices of items.
type BatchNode struct {
	nodeCore
	MaxRetries       int
	WaitMilliseconds time.Duration

	// User-defined functions
	// Prep returns a slice (or error)
	PrepFunc func(ctx *PfContext, params map[string]any) ([]any, error)
	// ExecItem operates on a single item from the Prep slice
	ExecItemFunc func(ctx *PfContext, params map[string]any, item any) (any, error)
	// Post receives the original prep slice and the slice of exec results
	PostFunc func(ctx *PfContext, params map[string]any, prepResult []any, execResult []any) (string, error)

	// Optional fallback for individual item processing
	ExecItemFallbackFunc func(ctx *PfContext, params map[string]any, item any, lastErr error) (any, error)
}

// NewBatchNode creates a new BatchNode with default settings.
func NewBatchNode() *BatchNode {
	bn := &BatchNode{
		MaxRetries:       1,
		WaitMilliseconds: 0,
		PrepFunc:         func(ctx *PfContext, params map[string]any) ([]any, error) { return nil, nil },
		ExecItemFunc:     func(ctx *PfContext, params map[string]any, item any) (any, error) { return item, nil }, // Default pass-through
		PostFunc: func(ctx *PfContext, params map[string]any, prepResult []any, execResult []any) (string, error) {
			return DefaultAction, nil
		},
	}
	bn.initCore()
	return bn
}

// SetRetry configures retry behaviour.
func (bn *BatchNode) SetRetry(maxRetries int, waitMilliseconds time.Duration) *BatchNode {
	if maxRetries < 1 {
		panic("maxRetries must be at least 1")
	}
	if waitMilliseconds < 0 {
		panic("waitMilliseconds cannot be negative")
	}
	bn.MaxRetries = maxRetries
	bn.WaitMilliseconds = waitMilliseconds
	return bn
}

// SetPrep sets the PrepFunc. Expects a function returning []any.
func (bn *BatchNode) SetPrep(f func(ctx *PfContext, params map[string]any) ([]any, error)) *BatchNode {
	bn.PrepFunc = f
	return bn
}

// SetExecItem sets the ExecItemFunc for processing individual items.
func (bn *BatchNode) SetExecItem(f func(ctx *PfContext, params map[string]any, item any) (any, error)) *BatchNode {
	bn.ExecItemFunc = f
	return bn
}

// SetPost sets the PostFunc. Receives []any prep and []any exec results.
func (bn *BatchNode) SetPost(f func(ctx *PfContext, params map[string]any, prepResult []any, execResult []any) (string, error)) *BatchNode {
	bn.PostFunc = f
	return bn
}

// SetItemFallback sets the ExecItemFallbackFunc.
func (bn *BatchNode) SetItemFallback(f func(ctx *PfContext, params map[string]any, item any, lastErr error) (any, error)) *BatchNode {
	bn.ExecItemFallbackFunc = f
	return bn
}

// --- BaseNode Implementation for BatchNode ---

func (bn *BatchNode) SetParams(params map[string]any) BaseNode {
	bn.nodeCore.SetParams(params)
	return bn
}

func (bn *BatchNode) Next(action string, node BaseNode) BaseNode {
	return bn.nodeCore.Next(action, node)
}

// Prep calls the user-defined PrepFunc.
func (bn *BatchNode) Prep(ctx *PfContext) (any, error) {
	if bn.PrepFunc == nil {
		return nil, nil
	}
	// Prep returns the slice directly (as 'any')
	return bn.PrepFunc(ctx, bn.params)
}

// Exec iterates through the prepResult slice, calling ExecItemFunc for each item with retries.
func (bn *BatchNode) Exec(ctx *PfContext, prepResult any) (any, error) {
	if prepResult == nil {
		return []any{}, nil // Return empty slice if prep was nil
	}

	// Type assertion to get the slice from Prep result
	items, ok := prepResult.([]any)
	if !ok {
		return nil, newPocketFlowError(fmt.Sprintf("Prep phase of BatchNode %T did not return []any, got %T", bn, prepResult), nil)
	}

	if len(items) == 0 {
		return []any{}, nil // Return empty slice for empty input
	}

	results := make([]any, len(items))
	var itemResult any
	var lastItemErr error
	currentRetry := 0

	for i, item := range items {
		lastItemErr = nil // Reset error for each item
		itemSuccess := false
		for currentRetry = 0; currentRetry < bn.MaxRetries; currentRetry++ {
			itemResult, lastItemErr = bn.ExecItemFunc(ctx, bn.params, item)
			if lastItemErr == nil {
				itemSuccess = true
				break // Success for this item
			}
			if currentRetry < bn.MaxRetries-1 && bn.WaitMilliseconds > 0 {
				time.Sleep(bn.WaitMilliseconds)
			}
		}

		// If all retries failed for this item
		if !itemSuccess {
			if bn.ExecItemFallbackFunc != nil {
				fallbackResult, fallbackErr := bn.ExecItemFallbackFunc(ctx, bn.params, item, lastItemErr)
				if fallbackErr != nil {
					// Fallback failed, return error for the whole batch
					return nil, newPocketFlowError(fmt.Sprintf("ExecItemFallback failed for item %d (%v) in %T after %d retries", i, item, bn, bn.MaxRetries), fallbackErr)
				}
				itemResult = fallbackResult // Use fallback result
				lastItemErr = nil           // Mark as success via fallback
			} else {
				// No fallback, fail the whole batch
				return nil, newPocketFlowError(fmt.Sprintf("ExecItem failed for item %d (%v) in %T after %d retries", i, item, bn, bn.MaxRetries), lastItemErr)
			}
		}
		results[i] = itemResult
	}

	return results, nil // Return the slice of results
}

// Post calls the user-defined PostFunc.
func (bn *BatchNode) Post(ctx *PfContext, prepResult any, execResult any) (string, error) {
	// Type assertions needed as interface methods deal with 'any'
	prepSlice, okPrep := prepResult.([]any)
	if prepResult != nil && !okPrep { // Allow nil prepResult
		return "", newPocketFlowError(fmt.Sprintf("Internal error: prepResult in BatchNode %T Post was not []any (%T)", bn, prepResult), nil)
	}

	execSlice, okExec := execResult.([]any)
	if execResult != nil && !okExec { // Allow nil execResult (e.g., if prep was empty)
		return "", newPocketFlowError(fmt.Sprintf("Internal error: execResult in BatchNode %T Post was not []any (%T)", bn, execResult), nil)
	}
	// Ensure slices are not nil if they were originally nil/empty, matching Java behaviour somewhat
	if prepSlice == nil {
		prepSlice = []any{}
	}
	if execSlice == nil {
		execSlice = []any{}
	}

	if bn.PostFunc == nil {
		return DefaultAction, nil
	}
	action, err := bn.PostFunc(ctx, bn.params, prepSlice, execSlice)
	if err == nil && action == "" {
		action = DefaultAction
	}
	return action, err
}

func (bn *BatchNode) Run(ctx *PfContext) (string, error) {
	if len(bn.successors) > 0 {
		logWarn("Node %T has successors, but Run() was called directly. Successors won't be executed by this call. Use Flow.Run() for orchestration.", bn)
	}
	return bn.InternalRun(ctx)
}

// InternalRun implements the retry logic at the item level within Exec.
func (bn *BatchNode) InternalRun(ctx *PfContext) (string, error) {
	prepRes, err := bn.Prep(ctx) // prepRes should be []any
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("Prep phase failed in %T", bn), err)
	}

	// Exec handles its own item-level retry/fallback
	execRes, err := bn.Exec(ctx, prepRes) // execRes should be []any
	if err != nil {
		// Error from Exec already includes context about retries/fallbacks
		return "", err // Don't wrap again
	}

	// Post phase
	action, err := bn.Post(ctx, prepRes, execRes) // Post expects []any, []any
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("Post phase failed in %T", bn), err)
	}

	return action, nil
}

// --- Flow ---

// Flow orchestrates the execution of connected nodes.
type Flow struct {
	nodeCore  // Flow itself can have params, though less common for successors here
	startNode BaseNode
}

// NewFlow creates a new Flow, optionally with a starting node.
func NewFlow(startNode BaseNode) *Flow {
	f := &Flow{
		startNode: startNode,
	}
	f.initCore()
	return f
}

// Start sets the initial node for the flow. Returns the start node for chaining setup.
func (f *Flow) Start(node BaseNode) BaseNode {
	if node == nil {
		panic("Start node cannot be nil")
	}
	f.startNode = node
	return node
}

// --- BaseNode Implementation for Flow ---
// Most BaseNode methods are less relevant for Flow itself, focused on orchestration.

func (f *Flow) SetParams(params map[string]any) BaseNode {
	f.nodeCore.SetParams(params)
	return f
}

// Next for a Flow doesn't make logical sense in the standard execution model.
func (f *Flow) Next(action string, node BaseNode) BaseNode {
	logWarn("Calling Next() on a Flow is unusual. Successors set here are not used by standard Run() orchestration.")
	return f.nodeCore.Next(action, node)
}

// Prep for the Flow itself. Default is no-op. Can be overridden if needed.
func (f *Flow) Prep(ctx *PfContext) (any, error) {
	// Typically Flow prep is about setting up the context before orchestration starts
	return nil, nil
}

// Exec for the Flow initiates the orchestration. Should not be called directly by user.
func (f *Flow) Exec(ctx *PfContext, prepResult any) (any, error) {
	// This is called internally by InternalRun after Flow's Prep.
	// The 'prepResult' here is the result of Flow.Prep, not a node's prep.
	// The 'execResult' of a Flow is the final action string from orchestration.
	// We need the context here for orchestrate, assume prepResult is the context for simplicity
	// although Flow's Prep doesn't *have* to return the context. Let's pass ctx directly.
	// This requires changing the call site in InternalRun.
	sharedCtx, _ := prepResult.(map[string]any)
	finalAction, err := f.orchestrate(ctx, sharedCtx) // Run orchestration with the context
	if err != nil {
		return "", err // Return error, action is irrelevant if orchestration failed
	}
	return finalAction, nil // Return the final action as the result
}

// Post for the Flow runs after orchestration completes. Default returns the final action.
func (f *Flow) Post(ctx *PfContext, prepResult any, execResult any) (string, error) {
	// prepResult is from Flow.Prep, execResult is the final action string from Exec/orchestrate.
	finalAction, _ := execResult.(string) // Ignore error, default to "" if cast fails
	if finalAction == "" {
		finalAction = DefaultAction // Or maybe keep it empty? Let's default.
	}
	return finalAction, nil
}

// Run starts the flow execution.
func (f *Flow) Run(ctx *PfContext) (string, error) {
	// Use InternalRun to perform the standard Flow lifecycle (Prep, Exec(orchestrate), Post)
	return f.InternalRun(ctx)
}

// InternalRun executes the flow's lifecycle: Prep, Orchestrate (via Exec), Post.
func (f *Flow) InternalRun(ctx *PfContext) (string, error) {
	// 1. Run Flow's Prep phase
	flowPrepResult, err := f.Prep(ctx)
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("Prep phase failed for Flow %T", f), err)
	}

	// 2. Run Flow's Exec phase (which triggers orchestration)
	// Pass the *original* shared context to Exec, as Exec now expects it.
	flowExecResult, err := f.Exec(ctx, flowPrepResult) // Exec calls orchestrate
	if err != nil {
		// Error likely came from a node within orchestrate
		return "", err // Don't wrap again, error should be informative
	}

	// 3. Run Flow's Post phase
	finalAction, err := f.Post(ctx, flowPrepResult, flowExecResult)
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("Post phase failed for Flow %T", f), err)
	}

	return finalAction, nil
}

// orchestrate executes the node chain starting from startNode.
// initialParams are merged with the flow's own params for the *first* node.
// Returns the last action string and any error encountered.
func (f *Flow) orchestrate(ctx *PfContext, initialParams map[string]any) (string, error) {
	if f.startNode == nil {
		logWarn("Flow started with no start node.")
		return "", nil // No error, just nothing to run
	}

	currentNode := f.startNode
	lastAction := ""
	var err error

	// Prepare initial parameters for the first node run
	// Combine Flow's params and any specific initialParams for this run
	// Replace with manual copy loop:
	combinedParams := make(map[string]any, len(f.params))
	for k, v := range f.params {
		combinedParams[k] = v
	}

	if initialParams != nil {
		// Replace with manual copy loop:
		for k, v := range initialParams {
			combinedParams[k] = v // Add or overwrite keys from initialParams
		}
	}

	for currentNode != nil {
		// Set the combined params *before* running the node
		// Only apply combinedParams on the *first* iteration
		if combinedParams != nil {
			currentNode.SetParams(combinedParams)
			combinedParams = nil // Clear after first use
		} else {
			// Ensure subsequent nodes get at least the Flow's base params if theirs are unset.
			if len(currentNode.GetParams()) == 0 && len(f.params) > 0 {
				currentNode.SetParams(f.params) // Give it the flow's base params if it has none
			}
		}

		// Execute the node's full lifecycle (Prep, Exec, Post)
		lastAction, err = currentNode.InternalRun(ctx)
		if err != nil {
			// Error occurred within the node's execution
			return "", err // Return the error immediately
		}

		// Get the next node based on the action returned by Post
		currentNode = currentNode.GetNextNode(lastAction)

		// Parameter propagation logic for subsequent nodes (revisit if needed)
		// The current logic sets params once at the start or uses node's existing/flow base.
	}

	// Orchestration finished successfully, return the last action determined
	return lastAction, nil
}

// --- Batch Flow ---

// BatchFlow runs the entire flow sequence for each parameter set generated by PrepBatch.
type BatchFlow struct {
	Flow // Embed Flow to inherit its structure and orchestration logic

	// User-defined functions for batch behavior
	PrepBatchFunc func(ctx *PfContext, params map[string]any) ([]map[string]any, error)
	PostBatchFunc func(ctx *PfContext, params map[string]any, batchPrepResult []map[string]any) (string, error)
}

// NewBatchFlow creates a new BatchFlow.
func NewBatchFlow(startNode BaseNode) *BatchFlow {
	bf := &BatchFlow{
		Flow: Flow{ // Initialize embedded Flow
			startNode: startNode,
		},
		// Provide sensible defaults?
		PrepBatchFunc: func(ctx *PfContext, params map[string]any) ([]map[string]any, error) { return nil, nil },
		PostBatchFunc: func(ctx *PfContext, params map[string]any, batchPrepResult []map[string]any) (string, error) {
			return DefaultAction, nil
		},
	}
	bf.initCore()      // Initialize nodeCore for the BatchFlow itself
	bf.Flow.initCore() // Ensure embedded Flow's core is also initialized
	return bf
}

// SetPrepBatch sets the function to generate batch parameters.
func (bf *BatchFlow) SetPrepBatch(f func(ctx *PfContext, params map[string]any) ([]map[string]any, error)) *BatchFlow {
	bf.PrepBatchFunc = f
	return bf
}

// SetPostBatch sets the function to run after all batches complete.
func (bf *BatchFlow) SetPostBatch(f func(ctx *PfContext, params map[string]any, batchPrepResult []map[string]any) (string, error)) *BatchFlow {
	bf.PostBatchFunc = f
	return bf
}

// --- BaseNode Implementation Overrides for BatchFlow ---

// Prep for BatchFlow runs its PrepBatchFunc.
func (bf *BatchFlow) Prep(ctx *PfContext) (any, error) {
	if bf.PrepBatchFunc == nil {
		return nil, nil
	}
	// Returns []*pfContext
	return bf.PrepBatchFunc(ctx, bf.params)
}

// Exec for BatchFlow runs the orchestration for each batch item.
// The 'prepResult' here is the []*pfContext from BatchFlow.Prep.
func (bf *BatchFlow) Exec(prepResult any) (any, error) {
	// We need the original context for the orchestrate calls.
	// InternalRun should pass it. For now, let's assume prepResult contains it implicitly
	// or redesign how context is passed through BatchFlow's Exec.
	// Safest: Assume InternalRun passes the context correctly and prepResult is the list.
	// Let's adjust the call site in InternalRun.

	batchParamsList, ok := prepResult.([]*PfContext)
	if prepResult != nil && !ok {
		return "", newPocketFlowError(fmt.Sprintf("Internal error: prepResult in BatchFlow %T Exec was not []*pfContext (%T)", bf, prepResult), nil)
	}
	if batchParamsList == nil {
		batchParamsList = []*PfContext{}
	}

	// We need the actual *pfContext. Where does it come from?
	// It should be passed *alongside* the prepResult by InternalRun.
	// Let's redefine Exec slightly to accept it, or rely on a field.
	// Simpler: Let InternalRun handle context passing to orchestrate directly.
	// Exec just needs to return the batchParamsList for Post.

	// The actual orchestration happens in InternalRun using this list.
	// This function's role is primarily semantic within the BaseNode interface call chain.
	// It returns the data needed for Post.

	return batchParamsList, nil
}

// Post for BatchFlow runs its PostBatchFunc.
func (bf *BatchFlow) Post(ctx *PfContext, prepResult any, execResult any) (string, error) {
	// prepResult is the result of BatchFlow.Prep ([]*pfContext)
	// execResult is the result of BatchFlow.Exec (which we defined as the same []*pfContext)

	batchPrepResult, okPrep := prepResult.([]map[string]any)
	if prepResult != nil && !okPrep {
		return "", newPocketFlowError(fmt.Sprintf("Internal error: prepResult in BatchFlow %T Post was not []*pfContext (%T)", bf, prepResult), nil)
	}
	if batchPrepResult == nil {
		batchPrepResult = []map[string]any{}
	}

	if bf.PostBatchFunc == nil {
		return DefaultAction, nil
	}

	action, err := bf.PostBatchFunc(ctx, bf.params, batchPrepResult)
	if err == nil && action == "" {
		action = DefaultAction
	}
	return action, err
}

// Run starts the BatchFlow execution.
func (bf *BatchFlow) Run(ctx *PfContext) (string, error) {
	// Use InternalRun to perform the standard lifecycle (PrepBatch, Exec Batches, PostBatch)
	return bf.InternalRun(ctx)
}

// InternalRun executes the BatchFlow lifecycle: PrepBatch, Exec(orchestrate per batch), PostBatch.
func (bf *BatchFlow) InternalRun(ctx *PfContext) (string, error) {
	// 1. Run BatchFlow's Prep phase (PrepBatchFunc)
	// Should return []*pfContext
	prepBatchResultAny, err := bf.Prep(ctx)
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("PrepBatch phase failed for BatchFlow %T", bf), err)
	}

	batchParamsList, ok := prepBatchResultAny.([]map[string]any)
	if prepBatchResultAny != nil && !ok {
		return "", newPocketFlowError(fmt.Sprintf("Internal error: PrepBatch phase in BatchFlow %T did not return []*pfContext (%T)", bf, prepBatchResultAny), nil)
	}
	if batchParamsList == nil {
		batchParamsList = []map[string]any{}
	}

	// 2. Run the orchestration for each item in batchParamsList
	for i, batchParams := range batchParamsList {
		// Run the embedded Flow's orchestration logic for each parameter set.
		// Pass the *original* shared context and current batchParams.
		_, err := bf.Flow.orchestrate(ctx, batchParams)
		if err != nil {
			// If one batch run fails, fail the whole BatchFlow execution
			return "", newPocketFlowError(fmt.Sprintf("Orchestration failed for batch item %d in %T", i, bf), err)
		}
		// Result (lastAction) of individual orchestrate runs is ignored here; side effects matter.
	}

	// 3. Run BatchFlow's Post phase (PostBatchFunc)
	// The result of the "Exec" phase semantically is the list itself.
	execResult := batchParamsList
	finalAction, err := bf.Post(ctx, prepBatchResultAny, execResult)
	if err != nil {
		return "", newPocketFlowError(fmt.Sprintf("PostBatch phase failed for BatchFlow %T", bf), err)
	}

	return finalAction, nil
}
</file>

<file path="README.md">
# PocketFlow Go

A minimalist LLM framework concept, ported from Python to Go.

## Overview

PocketFlow Go is a port of the original [Python PocketFlow](https://github.com/The-Pocket/PocketFlow). It provides a lightweight, flexible system for building and executing LLM-based (or other sequential) workflows through a simple node-based architecture using Go interfaces and functions.

> **Note:** This is an initial synchronous implementation mirroring the Java version. It currently does not support asynchronous operations (goroutines for execution). Community contributors are welcome to help enhance and maintain this project, particularly with adding robust concurrency patterns if desired.

## Installation

Ensure you have Go (1.18 or later recommended, 1.21+ for map cloning functions) installed.

```bash
go get github.com/The-Pocket/PocketFlow-Go
```

## Usage

Here's a simple example of how to use PocketFlow Go in your application:

```go
package main

import (
	"fmt"
	"log"

	pf "github.com/The-Pocket/PocketFlow-Go" // Adjust import path
)

// Define node logic using PocketFlow's functional style

// myStartNode creates a node that starts the workflow.
func myStartNode() pf.BaseNode {
	return pf.NewNode().
		SetExec(func(prepResult any, params pf.SharedContext) (any, error) {
			log.Println("Starting workflow...")
			// Exec result can be used by Post to determine action
			return "started_data", nil
		}).
		SetPost(func(ctx pf.SharedContext, prepResult any, execResult any, params pf.SharedContext) (string, error) {
			// Use execResult to decide the next step
			log.Printf("Start node finished with data: %v\n", execResult)
			ctx["start_result"] = execResult // Optional: Update shared context
			return "started", nil            // Action name to trigger the next node
		})
}

// myEndNode creates a node that ends the workflow.
func myEndNode() pf.BaseNode {
	return pf.NewNode().
		SetPrep(func(ctx pf.SharedContext, params pf.SharedContext) (any, error) {
			// Prep can access the shared context
			startData := ctx["start_result"]
			prepMsg := fmt.Sprintf("Preparing to end workflow, received: %v", startData)
			log.Println(prepMsg)
			return prepMsg, nil // Prep result passed to Exec
		}).
		SetExec(func(prepResult any, params pf.SharedContext) (any, error) {
			prepMsg := prepResult.(string) // Assume prep result is string
			log.Printf("Ending workflow with: %s\n", prepMsg)
			// End nodes often don't need to return data
			return nil, nil
		})
	// Default Post (returns DefaultAction) is fine here
}

func main() {
	// Create instances of your nodes
	startNode := myStartNode()
	endNode := myEndNode()

	// Connect the nodes: start -> end (when action is "started")
	startNode.Next("started", endNode)

	// Create a flow with the start node
	flow := pf.NewFlow(startNode)

	// Create a context and run the flow
	context := make(pf.SharedContext)
	log.Println("Executing workflow...")
	finalAction, err := flow.Run(context)
	if err != nil {
		log.Fatalf("Workflow failed: %v\n", err)
	}

	log.Printf("Workflow completed successfully. Final action: %s\n", finalAction)
	log.Printf("Final Context: %v\n", context)
}

```

## Development

### Building the Project

```bash
go build ./...
```

### Running Tests

```bash
go test ./...
```
Or with coverage:
```bash
go test -coverprofile=coverage.out ./... && go tool cover -html=coverage.out
```

## Contributing

Contributions are welcome! We're particularly looking for volunteers to:

1.  Implement asynchronous operation support (e.g., using goroutines, channels, `context.Context`).
2.  Add more comprehensive test coverage, including edge cases and error handling.
3.  Improve documentation and provide more complex examples (e.g., LLM integration stubs).
4.  Refine the API for better Go idiomatic usage if applicable.

Please feel free to submit pull requests or open issues for discussion.

## License

[MIT License](LICENSE)
</file>

</files>
</file>

<file path=".docs/repomix-output-The-Pocket-PocketFlow-Rust.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    001-how_to_use.mdc
.github/
  workflows/
    publish.yml
examples/
  pocketflow-rs-rag/
    src/
      nodes/
        chunk_documents.rs
        create_index.rs
        embed_documents.rs
        embed_query.rs
        file_loader.rs
        generate_answer.rs
        mod.rs
        query_rewrite.rs
        retrieve_document.rs
      lib.rs
      main.rs
      state.rs
    Cargo.toml
    README.md
  text2sql/
    example_data/
      customers.csv
      orders.csv
    src/
      flow.rs
      lib.rs
      main.rs
    .gitignore
    Cargo.toml
  basic.rs
src/
  utils/
    embedding.rs
    llm_wrapper.rs
    mod.rs
    text_chunking.rs
    vector_db.rs
    viz_debug.rs
    web_search.rs
  context.rs
  flow.rs
  lib.rs
  node.rs
.gitignore
Cargo.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/001-how_to_use.mdc">
---
description: 
globs: 
alwaysApply: false
---
## How to create a workflow

### Build Node 

1. Define State(Optional)
Each node can define different states based on the possible execution of subsequent nodes. If the node logic is simple, it can be directly implemented using BaseState

A example for define state: the SQL executing node can have states such as: execution successful/execution failed SQL error/execution failed connection error. If the execution is successful, the node can proceed to the next stage. If the execution is incorrect, it will jump to generate SQL and regenerate the node. 

```rust
pub enum SqlExecutorState{
    SqlSyntaxError,
    SqlClientError
    Default, // Success
}
```

2. implement trait function.
+ prepare(optional): Sets up necessary preconditions, preprocess the data in context.
+ execute: Performs the main logic and produces a result.
+ post_process(optional): 
    + Evaluates the execute result, updates the Context.
    + Return the corresponding state based on the result, allowing the Flow runtime library to determine which node to call next by evaluating the edge conditions.
    + If the logic of the node is simple enough and does not require post-processing, it can be omitted and the default can be used.

## Build Flow:

You can use rust macro `build_flow` and `build_batch_flow` to create a workflow for LLM.

such as:

```rust
let flow = build_flow!(
    start: ("start", node1), // define begin node, node1 is object for Node and 'start' is alias.
    nodes: [("next", node2)], // define other nodes as start.
    edges: [
        ("start", "next", MyState::Default) // start -> next, when start post_process returned state is MyState::Default
    ]
);
```
</file>

<file path=".github/workflows/publish.yml">
name: Publish to crates.io

on:
  push:
    tags:
      - 'v*' # Trigger on version tags

env:
  CARGO_TERM_COLOR: always

jobs:
  publish:
    name: Publish
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      - name: Run tests
        run: cargo test --all-features

      - name: Run clippy
        run: cargo clippy --all-features -- -D warnings

      - name: Run rustfmt
        run: cargo fmt --all -- --check

      - name: Publish to crates.io
        run: cargo publish --token ${CRATES_TOKEN}
        env:
          CRATES_TOKEN: ${{ secrets.CRATES_TOKEN }}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/chunk_documents.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::text_chunking::{ChunkingOptions, ChunkingStrategy, TextChunker};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::{Value, json};
use tracing::info;

pub struct ChunkDocumentsNode {
    chunker: TextChunker,
    options: ChunkingOptions,
}

impl ChunkDocumentsNode {
    pub fn new(chunk_size: usize, overlap: usize, strategy: ChunkingStrategy) -> Self {
        Self {
            chunker: TextChunker::new(),
            options: ChunkingOptions {
                chunk_size,
                overlap,
                strategy,
            },
        }
    }
}

#[async_trait]
impl Node for ChunkDocumentsNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let documents = context
            .get("documents")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No documents found in context"))?;

        let mut chunks_meta = Vec::new();
        for doc_map in documents {
            let content = doc_map
                .get("content")
                .and_then(|v| v.as_str())
                .ok_or_else(|| anyhow::anyhow!("No content found in document"))?;
            let chunks = self.chunker.chunk_text(content, &self.options);
            info!(
                "Process: {:?}, Chunks lens: {:?}",
                doc_map.get("metadata").unwrap(),
                chunks.len()
            );
            chunks_meta.push(json!({
                "chunks": chunks,
                "metadata": doc_map.get("metadata").unwrap_or(&Value::Null),
            }));
        }

        Ok(Value::Array(chunks_meta))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("documents_chunked", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "documents_chunked".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::ChunkingError,
                format!("chunking_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/create_index.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::vector_db::{
    DistanceMetric, QdrantDB, VectorDB, VectorDBOptions, VectorRecord,
};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;

pub struct CreateIndexNode {
    db: Arc<QdrantDB>,
}

impl CreateIndexNode {
    pub async fn new(
        db_url: String,
        api_key: Option<String>,
        collection: String,
        dimension: usize,
        distance_metric: DistanceMetric,
    ) -> Result<Self> {
        let options = VectorDBOptions {
            collection_name: collection,
            dimension,
            distance_metric,
        };
        let db = QdrantDB::new(db_url, api_key, options).await?;
        Ok(Self { db: Arc::new(db) })
    }
}

#[async_trait]
impl Node for CreateIndexNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let chunks_embeddings = context
            .get("chunk_embeddings")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No embeddings found in context"))?;

        let mut records = Vec::new();
        for chunk_embedding in chunks_embeddings {
            let chunks = chunk_embedding
                .get("chunks")
                .and_then(|v| v.as_array())
                .ok_or_else(|| anyhow::anyhow!("No chunks found in document"))?;
            let embeddings = chunk_embedding
                .get("embeddings")
                .and_then(|v| v.as_array())
                .ok_or_else(|| anyhow::anyhow!("No embeddings found in document"))?;
            let metadata = chunk_embedding.get("metadata").unwrap_or(&Value::Null);

            let chunks_size = chunks.len();
            for i in 0..chunks_size {
                let chunk = chunks[i].to_string();
                let default_embedding = Vec::new();
                let embedding = embeddings[i].as_array().unwrap_or(&default_embedding);
                let embedding_vec: Vec<f32> = embedding
                    .iter()
                    .filter_map(|v| v.as_f64().map(|x| x as f32))
                    .collect();
                records.push(VectorRecord {
                    id: uuid::Uuid::new_v4().to_string(),
                    vector: embedding_vec,
                    metadata: serde_json::Map::from_iter(vec![
                        ("text".to_string(), serde_json::Value::String(chunk)),
                        ("file_metadata".to_string(), metadata.clone()),
                    ]),
                });
            }
        }

        if records.is_empty() {
            return Err(anyhow::anyhow!("No valid records to insert"));
        }

        self.db
            .insert(records)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to insert records: {}", e))?;
        Ok(Value::Null)
    }

    #[allow(unused_variables)]
    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(_) => Ok(ProcessResult::new(
                RagState::Default,
                "index_created".to_string(),
            )),
            Err(e) => Ok(ProcessResult::new(
                RagState::IndexCreationError,
                format!("index_creation_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/embed_documents.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::embedding::EmbeddingGenerator;
use pocketflow_rs::utils::embedding::{EmbeddingOptions, OpenAIEmbeddingGenerator};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::{Value, json};
use std::sync::Arc;
use tracing::{debug, info};

pub struct EmbedDocumentsNode {
    generator: Arc<OpenAIEmbeddingGenerator>,
}

impl EmbedDocumentsNode {
    pub fn new(api_key: String, endpoint: String, model: String, dimension: Option<usize>) -> Self {
        Self {
            generator: Arc::new(OpenAIEmbeddingGenerator::new(
                &api_key,
                &endpoint,
                EmbeddingOptions {
                    model,
                    dimensions: dimension,
                },
            )),
        }
    }
}

#[async_trait]
impl Node for EmbedDocumentsNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let documents_chunked = context
            .get("documents_chunked")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No chunks found in context"))?;
        info!("Documents chunked: {:?}", documents_chunked.len());

        let mut embed_result = Vec::new();
        for chunk in documents_chunked {
            let chunks = chunk
                .get("chunks")
                .and_then(|v| v.as_array())
                .ok_or_else(|| anyhow::anyhow!("No chunks found in document"))?;
            let chunk_text: Vec<String> = chunks
                .iter()
                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                .collect();
            debug!("Chunk text: {:?}", chunk_text);
            info!("Chunk text len: {:?}", chunk_text.len());
            let embeddings = self.generator.generate_embeddings(&chunk_text).await?;
            info!("Embeddings len: {:?}", embeddings.len());
            if embeddings.is_empty() {
                return Err(anyhow::anyhow!("Embeddings array is empty"));
            }
            info!("First Embeddings: {:?}", embeddings[0]);

            embed_result.push(json!(
                {
                    "chunks": chunk_text,
                    "embeddings": embeddings,
                    "metadata": chunk.get("metadata").unwrap_or(&Value::Null),
                }
            ));
        }

        Ok(Value::Array(embed_result))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("chunk_embeddings", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "chunks_embedded".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::EmbeddingError,
                format!("embedding_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/embed_query.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::embedding::{
    EmbeddingGenerator, EmbeddingOptions, OpenAIEmbeddingGenerator,
};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::{Value, json};
use std::sync::Arc;

pub struct EmbedQueryNode {
    generator: Arc<OpenAIEmbeddingGenerator>,
}

impl EmbedQueryNode {
    pub fn new(api_key: String, endpoint: String, model: String, dimension: Option<usize>) -> Self {
        Self {
            generator: Arc::new(OpenAIEmbeddingGenerator::new(
                &api_key,
                &endpoint,
                EmbeddingOptions {
                    model,
                    dimensions: dimension,
                },
            )),
        }
    }
}

#[async_trait]
impl Node for EmbedQueryNode {
    type State = RagState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<Value> {
        let query = context
            .get("rewritten_query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .to_string();
        let embedding = self.generator.generate_embedding(&query).await?;
        if embedding.is_empty() {
            return Err(anyhow::anyhow!("No embedding generated for query"));
        }
        Ok(Value::Array(
            embedding.into_iter().map(|x| json!(x)).collect(),
        ))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("query_embedding", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "query_embedded".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::QueryEmbeddingError,
                format!("query_embedding_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/file_loader.rs">
use crate::state::RagState;
use anyhow::{Context, Result};
use async_trait::async_trait;
use pdf_extract::extract_text;
use pocketflow_rs::{Context as FlowContext, Node, ProcessResult};
use reqwest::Client;
use serde_json::{Value, json};
use std::fs;
use std::path::Path;
use std::sync::Arc;
use std::time::SystemTime;
use tracing::info;

#[derive(Debug)]
struct Document {
    content: String,
    metadata: Value,
}

impl Document {
    fn new(content: String, url: &str, file_type: &str) -> Self {
        let metadata = json!({
            "url": url,
            "file_type": file_type,
            "timestamp": SystemTime::now()
                .duration_since(SystemTime::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            "content_length": content.len(),
        });
        Self { content, metadata }
    }
}

pub struct FileLoaderNode {
    urls: Vec<String>,
    client: Arc<Client>,
}

impl FileLoaderNode {
    pub fn new(urls: Vec<String>) -> Self {
        Self {
            urls,
            client: Arc::new(Client::new()),
        }
    }

    fn detect_file_type(path: &Path) -> Result<&'static str> {
        let extension = path
            .extension()
            .and_then(|ext| ext.to_str())
            .ok_or_else(|| anyhow::anyhow!("Could not determine file extension"))?;

        match extension.to_lowercase().as_str() {
            "pdf" => Ok("pdf"),
            "txt" => Ok("text"),
            _ => Err(anyhow::anyhow!("Unsupported file type: {}", extension)),
        }
    }

    async fn load_from_url(&self, url: &str) -> Result<Document> {
        info!("Loading content from URL: {}", url);
        if url.starts_with("http://") || url.starts_with("https://") {
            let response = self.client.get(url).send().await?;
            let content_type = response
                .headers()
                .get("content-type")
                .map(|header| header.to_str().unwrap_or("text/plain"));

            let mut file_type = "web";
            let content = match content_type {
                Some("text/plain") => response.text().await?,
                Some("application/pdf") => {
                    let bytes = response.bytes().await?;
                    file_type = "pdf";
                    pdf_extract::extract_text_from_mem(&bytes)?
                }
                _ => response.text().await?,
            };

            Ok(Document::new(content, url, file_type))
        } else {
            info!("Loading content from local file: {}", url);
            let path = Path::new(url);
            let file_type = Self::detect_file_type(path)?;
            let content = match file_type {
                "pdf" => extract_text(path)
                    .with_context(|| format!("Failed to extract text from PDF: {:?}", path))?,
                "text" => fs::read_to_string(path)
                    .with_context(|| format!("Failed to read text file: {:?}", path))?,
                _ => unreachable!(),
            };
            Ok(Document::new(content, url, file_type))
        }
    }
}

#[async_trait]
impl Node for FileLoaderNode {
    type State = RagState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &FlowContext) -> Result<Value> {
        let mut documents = Vec::new();

        for url in &self.urls {
            let doc = self
                .load_from_url(url)
                .await
                .with_context(|| format!("Failed to load content from URL: {}", url))?;
            info!("Document loaded: {:?}", doc.metadata);
            documents.push(json!({
                "content": doc.content,
                "metadata": doc.metadata
            }));
        }

        if documents.is_empty() {
            return Err(anyhow::anyhow!("No documents loaded from any URL"));
        }

        Ok(Value::Array(documents))
    }

    async fn post_process(
        &self,
        context: &mut FlowContext,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("documents", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "documents_loaded".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::FileLoadedError,
                format!("loading_error: {}", e),
            )),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs::File;
    use std::io::Write;
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_load_text_file() {
        // Create a temporary directory
        let dir = tempdir().unwrap();
        let file_path = dir.path().join("test.txt");

        // Create a test text file
        let mut file = File::create(&file_path).unwrap();
        writeln!(file, "Hello, World!").unwrap();

        // Test loading the text file
        let loader = FileLoaderNode::new(vec![file_path.to_str().unwrap().to_string()]);
        let result = loader.execute(&FlowContext::new()).await.unwrap();

        // Verify the result
        let documents = result.as_array().unwrap();
        assert_eq!(documents.len(), 1);

        let doc = &documents[0];
        assert_eq!(doc["content"].as_str().unwrap(), "Hello, World!\n");
        assert_eq!(doc["metadata"]["file_type"].as_str().unwrap(), "text");
    }

    #[tokio::test]
    async fn test_load_multiple_files() {
        let dir = tempdir().unwrap();

        let text_path = dir.path().join("test.txt");
        let mut text_file = File::create(&text_path).unwrap();
        writeln!(text_file, "Text content").unwrap();

        let urls = vec![
            text_path.to_str().unwrap().to_string(),
            "https://pdfobject.com/pdf/sample.pdf".to_string(),
        ];

        let loader = FileLoaderNode::new(urls);
        let result = loader.execute(&FlowContext::new()).await;

        if let Ok(result) = result {
            let documents = result.as_array().unwrap();
            assert!(documents.len() > 0);

            for doc in documents {
                assert!(doc["content"].is_string());
                assert!(doc["metadata"]["url"].is_string());
                assert!(doc["metadata"]["file_type"].is_string());
                assert!(doc["metadata"]["timestamp"].is_number());
                assert!(doc["metadata"]["content_length"].is_number());
            }
        }
    }

    #[tokio::test]
    async fn test_invalid_file_type() {
        let dir = tempdir().unwrap();
        let file_path = dir.path().join("test.xyz");

        let mut file = File::create(&file_path).unwrap();
        writeln!(file, "Some content").unwrap();

        let loader = FileLoaderNode::new(vec![file_path.to_str().unwrap().to_string()]);
        let result = loader.execute(&FlowContext::new()).await;

        assert!(result.is_err());
        let error = result.unwrap_err();
        assert!(
            error
                .to_string()
                .contains("Failed to load content from URL")
        );
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/generate_answer.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::llm_wrapper::{LLMWrapper, OpenAIClient};
use pocketflow_rs::vector_db::VectorRecord;
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;

pub struct GenerateAnswerNode {
    client: Arc<OpenAIClient>,
    query: String,
}

impl GenerateAnswerNode {
    pub fn new(api_key: String, model: String, endpoint: String, query: String) -> Self {
        Self {
            client: Arc::new(OpenAIClient::new(api_key, model, endpoint)),
            query,
        }
    }
}

#[async_trait]
impl Node for GenerateAnswerNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let retrieved_docs = context
            .get("retrieved_documents")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No retrieved documents found in context"))?;

        let retrieved_docs_array: Vec<VectorRecord> = retrieved_docs
            .iter()
            .map(VectorRecord::parse_by_value)
            .collect();

        let retrieved_text_with_meta = retrieved_docs_array
            .iter()
            .map(|v| {
                format!(
                    "{}: {}",
                    v.metadata
                        .get("file_metadata")
                        .unwrap()
                        .get("url")
                        .unwrap()
                        .as_str()
                        .unwrap(),
                    v.metadata.get("text").unwrap()
                )
            })
            .collect::<Vec<_>>()
            .join("\n\n");

        if retrieved_text_with_meta.is_empty() {
            return Ok(Value::String("I don't know.".to_string()));
        }

        let prompt = format!("
You are a helpful assistant. Based on the following context, please answer the question. If the answer cannot be found in the context, say 'I don't know'.\n\n
Output format using markdown and add reference links to the source documents. \n\n
You can use the following context to answer the question: \n{}\n\n
Question: {}\n\n
Answer:",
        retrieved_text_with_meta,
            self.query
        );

        let response = self.client.generate(&prompt).await?;
        if response.content.is_empty() {
            return Err(anyhow::anyhow!("Empty response from LLM"));
        }

        Ok(Value::String(response.content.trim().to_string()))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("result", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "answer_generated".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::GenerationError,
                format!("generation_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/mod.rs">
mod chunk_documents;
mod create_index;
mod embed_documents;
mod embed_query;
mod file_loader;
mod generate_answer;
mod query_rewrite;
mod retrieve_document;

pub use chunk_documents::ChunkDocumentsNode;
pub use create_index::CreateIndexNode;
pub use embed_documents::EmbedDocumentsNode;
pub use embed_query::EmbedQueryNode;
pub use file_loader::FileLoaderNode;
pub use generate_answer::GenerateAnswerNode;
pub use query_rewrite::QueryRewriteNode;
pub use retrieve_document::RetrieveDocumentNode;
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/query_rewrite.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::llm_wrapper::{LLMWrapper, OpenAIClient};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;
use tracing::info;

pub struct QueryRewriteNode {
    client: Arc<OpenAIClient>,
}

impl QueryRewriteNode {
    pub fn new(api_key: String, model: String, endpoint: String) -> Self {
        Self {
            client: Arc::new(OpenAIClient::new(api_key, model, endpoint)),
        }
    }
}

#[async_trait]
impl Node for QueryRewriteNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let user_query = context.get("user_query").unwrap();
        let prompt = format!("
**Role:** You are an AI Query Enhancer for a Retrieval-Augmented Generation (RAG) system.

**Goal:** Your task is to take a raw user query and rewrite it into an optimized query string suitable for vector database search. This involves identifying the user's core intent and transforming the query into a concise, keyword-focused format that maximizes the chances of retrieving relevant documents.

**Input:** You will receive a single \"Original User Query\".

**Instructions:**

1.  **Analyze Intent:** Carefully examine the \"Original User Query\" to understand the user's underlying information need or question. What are they *really* trying to find out?
2.  **Identify Keywords:** Extract the most critical entities, concepts, and keywords from the query.
3.  **Remove Filler:** Discard conversational filler, politeness phrases (e.g., \"please\", \"can you tell me\"), and vague phrasing (\"thing\", \"stuff\", \"how about\").
4.  **Rewrite for Clarity & Conciseness:** Construct a new query string that clearly represents the intent using the identified keywords. Make it specific and direct.
5.  **Consider Expansion (Optional but Recommended):** If the original query is very sparse or could benefit from clarification, cautiously add 1-2 highly relevant synonyms or closely related terms that specify the intent further (e.g., adding \"nutrition\" if the query is just \"apples\"). Avoid overly broad expansion.
6.  **Format for Embedding:** The final rewritten query should be a simple string, optimized for being turned into a vector embedding for semantic search.

**Output:** Respond with ONLY the rewritten query string. Do not include any explanations or introductory text.

**Example 1:**
Original User Query: \"Hey, could you tell me about the financial performance of Tesla last year?\"
Rewritten Query: `Tesla financial performance 2024 earnings report revenue analysis`

**Example 2:**
Original User Query: \"What's the deal with that new AI that makes pictures?\"
Rewritten Query: `AI image generation model technology explanation diffusion transformer`

**Example 3:**
Original User Query: \"I need help understanding how to mitigate risks in my supply chain in Europe.\"
Rewritten Query: `supply chain risk mitigation strategies Europe logistics management`

**Now, process the following input:**

Original User Query: \"{}\"
Rewritten Query:",user_query);
        let response = self.client.generate(&prompt).await?;
        info!("Query rewritten: {:?}", response.content);
        Ok(Value::String(response.content.replace("`", "")))
    }

    #[allow(unused_variables)]
    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        return match result {
            Ok(value) => {
                context.set("rewritten_query", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "query_rewritten".to_string(),
                ))
            }
            Err(e) => {
                info!("Error rewriting query: {:?}", e);
                Ok(ProcessResult::new(
                    RagState::QueryRewriteError,
                    "query_rewrite_error".to_string(),
                ))
            }
        };
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/retrieve_document.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::vector_db::{QdrantDB, VectorDB};
use pocketflow_rs::vector_db::{DistanceMetric, VectorDBOptions};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;
use tracing::{error, info};

pub struct RetrieveDocumentNode {
    db: Arc<QdrantDB>,
    k: usize,
}

impl RetrieveDocumentNode {
    pub async fn new(
        db_url: String,
        api_key: Option<String>,
        collection: String,
        dimension: usize,
        distance_metric: DistanceMetric,
        k: usize,
    ) -> Result<Self> {
        let db = QdrantDB::new(
            db_url,
            api_key,
            VectorDBOptions {
                collection_name: collection,
                dimension,
                distance_metric,
            },
        )
        .await?;
        Ok(Self {
            db: Arc::new(db),
            k,
        })
    }
}

#[async_trait]
impl Node for RetrieveDocumentNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let query_embedding = context
            .get("query_embedding")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_f64().map(|x| x as f32))
                    .collect::<Vec<f32>>()
            })
            .ok_or_else(|| anyhow::anyhow!("No query embedding found in context"))?;

        let records = self.db.search(query_embedding, self.k).await?;
        if records.is_empty() {
            error!("No documents retrieved");
            return Err(anyhow::anyhow!("No documents retrieved"));
        }

        info!("Retrieved documents line: {:?}", records.len());

        let result_array: Vec<Value> = records
            .into_iter()
            .map(|record| record.to_value())
            .collect();

        Ok(Value::Array(result_array))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("retrieved_documents", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "documents_retrieved".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::RetrievalError,
                format!("retrieval_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/lib.rs">
pub mod nodes;
pub mod state;

pub use nodes::*;
pub use state::*;
</file>

<file path="examples/pocketflow-rs-rag/src/main.rs">
use anyhow::Result;
use clap::{Parser, Subcommand};
use pocketflow_rs::utils::{text_chunking::ChunkingStrategy, vector_db::DistanceMetric};
use pocketflow_rs::{Context as FlowContext, build_flow};
use pocketflow_rs_rag::{
    QueryRewriteNode,
    nodes::{
        ChunkDocumentsNode, CreateIndexNode, EmbedDocumentsNode, EmbedQueryNode, FileLoaderNode,
        GenerateAnswerNode, RetrieveDocumentNode,
    },
    state::RagState,
};
use serde_json::json;
use tracing::Level;
use tracing_subscriber::FmtSubscriber;

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Process documents offline
    Offline {
        /// Qdrant database URL
        #[arg(long, default_value = "http://localhost:6333")]
        db_url: String,

        /// Collection name in Qdrant
        #[arg(long, default_value = "documents")]
        collection: String,

        /// OpenAI API key
        #[arg(long)]
        api_key: String,

        /// Qdrant API key
        #[arg(long)]
        qdrant_api_key: Option<String>,

        /// OpenAI API endpoint
        #[arg(short, long, default_value = "https://api.openai.com/v1")]
        endpoint: String,

        /// Chunk size for document splitting
        #[arg(long, default_value = "1000")]
        chunk_size: usize,

        /// Overlap between chunks
        #[arg(long, default_value = "200")]
        overlap: usize,

        /// OpenAI model to use
        #[arg(long, default_value = "text-embedding-ada-002")]
        model: String,

        #[arg(long, default_value = "1024")]
        dimension: usize,

        /// Paths to document files
        #[arg(required = true)]
        files: Vec<String>,
    },
    /// Online processing: answer questions based on indexed documents
    Online {
        /// Qdrant database URL
        #[arg(long, default_value = "http://localhost:6333")]
        db_url: String,

        /// Collection name in Qdrant
        #[arg(long, default_value = "documents")]
        collection: String,

        /// OpenAI API key
        #[arg(long)]
        api_key: String,

        /// OpenAI API endpoint
        #[arg(long, default_value = "https://api.openai.com/v1")]
        endpoint: String,

        /// Number of documents to retrieve
        #[arg(short, long, default_value = "3")]
        k: usize,

        /// chat mode
        #[arg(long, default_value = "chat")]
        chat_mode: String,

        /// embedding dimension
        #[arg(long, default_value = "1024")]
        dimension: usize,

        /// Qdrant API key
        #[arg(long)]
        qdrant_api_key: Option<String>,

        /// Embedding model
        #[arg(long, default_value = "text-embedding-ada-002")]
        embedding_model: String,

        /// Question to answer
        #[arg(required = true)]
        query: String,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();
    FmtSubscriber::builder().with_max_level(Level::INFO).init();

    match cli.command {
        Commands::Offline {
            files,
            db_url,
            collection,
            api_key,
            qdrant_api_key,
            endpoint,
            chunk_size,
            overlap,
            model,
            dimension,
        } => {
            let file_loader = FileLoaderNode::new(files);
            let chunk_documents =
                ChunkDocumentsNode::new(chunk_size, overlap, ChunkingStrategy::Sentence);
            let embed_documents = EmbedDocumentsNode::new(
                api_key.clone(),
                endpoint.clone(),
                model.clone(),
                Some(dimension),
            );
            let create_index = CreateIndexNode::new(
                db_url,
                qdrant_api_key,
                collection,
                dimension,
                DistanceMetric::Cosine,
            )
            .await?;

            let flow = build_flow!(
                start: ("file_loader", file_loader),
                nodes: [
                    ("chunk_documents", chunk_documents),
                    ("embed_documents", embed_documents),
                    ("create_index", create_index)
                ],
                edges: [
                    ("file_loader", "chunk_documents", RagState::Default),
                    ("chunk_documents", "embed_documents", RagState::Default),
                    ("embed_documents", "create_index", RagState::Default)
                ]
            );

            flow.run(FlowContext::new()).await?;
        }
        Commands::Online {
            query,
            db_url,
            collection,
            api_key,
            endpoint,
            k,
            chat_mode,
            dimension,
            qdrant_api_key,
            embedding_model,
        } => {
            let mut context = FlowContext::new();
            context.set("user_query", json!(query.clone()));

            let query_rewrite_node =
                QueryRewriteNode::new(api_key.clone(), chat_mode.clone(), endpoint.clone());

            let embed_query_node = EmbedQueryNode::new(
                api_key.clone(),
                endpoint.clone(),
                embedding_model.clone(),
                Some(dimension),
            );

            let retrieve_node = RetrieveDocumentNode::new(
                db_url,
                qdrant_api_key,
                collection,
                dimension,
                DistanceMetric::Cosine,
                k,
            )
            .await?;

            let generate_node = GenerateAnswerNode::new(api_key, chat_mode, endpoint, query);

            // Build and execute online flow
            let flow = build_flow!(
                start: ("query_rewrite", query_rewrite_node),
                nodes: [
                    ("embed_query", embed_query_node),
                    ("retrieve", retrieve_node),
                    ("generate", generate_node)
                ],
                edges: [
                    ("query_rewrite", "embed_query", RagState::Default),
                    ("embed_query", "retrieve", RagState::Default),
                    ("retrieve", "generate", RagState::Default)
                ]
            );

            let result = flow.run(context).await?;

            termimad::print_text(result.as_str().unwrap());
        }
    }

    Ok(())
}
</file>

<file path="examples/pocketflow-rs-rag/src/state.rs">
use pocketflow_rs::ProcessState;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum RagState {
    // Offline states
    FileLoadedError,
    DocumentsLoaded,
    DocumentsChunked,
    ChunksEmbedded,
    IndexCreated,
    // Offline error states
    DocumentLoadError,
    ChunkingError,
    EmbeddingError,
    IndexCreationError,
    // Online states
    QueryEmbedded,
    DocumentsRetrieved,
    AnswerGenerated,
    // Online error states
    QueryEmbeddingError,
    RetrievalError,
    GenerationError,
    Default,
    QueryRewriteError,
}

impl ProcessState for RagState {
    fn is_default(&self) -> bool {
        matches!(self, RagState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            // Offline states
            RagState::FileLoadedError => "file_loaded_error".to_string(),
            RagState::DocumentsLoaded => "documents_loaded".to_string(),
            RagState::DocumentsChunked => "documents_chunked".to_string(),
            RagState::ChunksEmbedded => "chunks_embedded".to_string(),
            RagState::IndexCreated => "index_created".to_string(),
            // Offline error states
            RagState::DocumentLoadError => "document_load_error".to_string(),
            RagState::ChunkingError => "chunking_error".to_string(),
            RagState::EmbeddingError => "embedding_error".to_string(),
            RagState::IndexCreationError => "index_creation_error".to_string(),
            // Online states
            RagState::QueryEmbedded => "query_embedded".to_string(),
            RagState::DocumentsRetrieved => "documents_retrieved".to_string(),
            RagState::AnswerGenerated => "answer_generated".to_string(),
            // Online error states
            RagState::QueryEmbeddingError => "query_embedding_error".to_string(),
            RagState::RetrievalError => "retrieval_error".to_string(),
            RagState::GenerationError => "generation_error".to_string(),
            RagState::Default => "default".to_string(),
            RagState::QueryRewriteError => "query_rewrite_error".to_string(),
        }
    }
}

impl Default for RagState {
    fn default() -> Self {
        RagState::Default
    }
}
</file>

<file path="examples/pocketflow-rs-rag/Cargo.toml">
[package]
name = "pocketflow-rs-rag"
version = "0.1.0"
edition = "2024"

[dependencies]
pocketflow_rs = { path = "../../", features = ["openai", "qdrant", "debug"] }
anyhow = "1.0"
tokio = { version = "1.0", features = ["full"] }
tracing = "0.1"
tracing-subscriber = "0.3"
serde_json = "1.0"
async-trait = "0.1"
faiss = "0.12.1"
clap = { version = "4.5", features = ["derive"] }
pdf-extract = "0.9"
reqwest = { version = "0.12.15", features = ["json"] }
uuid = { version = "1.16.0", features = ["v4"] }
qdrant-client = "1.14.0"
termimad = "0.31.3"

[dev-dependencies]
tempfile = "3.8"
</file>

<file path="examples/pocketflow-rs-rag/README.md">
# PocketFlow RAG Example

## Overview

This example demonstrates how to use PocketFlow to build a Retrieval-Augmented Generation (RAG) pipeline. The implementation consists of two main components: an offline pipeline for document processing and indexing, and an online pipeline for question answering.

### Offline Pipeline

The offline pipeline processes and indexes documents for later retrieval. It consists of the following nodes:

- `FileLoaderNode`: Loads documents from local files or URLs, supporting various formats including PDF, text, and web pages.
- `ChunkDocumentsNode`: Splits documents into smaller chunks using configurable chunk size and overlap, with support for different chunking strategies.
- `EmbedDocumentsNode`: Converts document chunks into vector embeddings using OpenAI's embedding models.
- `CreateIndexNode`: Stores the embedded chunks in a Qdrant vector database with configurable distance metrics.

### Online Pipeline

The online pipeline handles real-time question answering using the indexed documents. It includes:

- `QueryRewriteNode`: Enhances the user's query using LLM to improve retrieval quality.
- `EmbedQueryNode`: Converts the rewritten query into a vector embedding.
- `RetrieveDocumentNode`: Retrieves the most relevant document chunks from the vector database.
- `GenerateAnswerNode`: Generates a comprehensive answer based on the retrieved context and the original query.

The pipeline supports various configuration options including:

- Customizable embedding models and dimensions
- Configurable chunk sizes and overlap
- Adjustable number of retrieved documents
- Different chat modes for answer generation
- Flexible vector database settings

## Workflow Diagram

```mermaid
graph TB
    subgraph Offline["Offline Pipeline"]
        direction LR
        FL[FileLoaderNode] --> CD[ChunkDocumentsNode]
        CD --> ED[EmbedDocumentsNode]
        ED --> CI[CreateIndexNode]
    end

    subgraph Online["Online Pipeline"]
        direction LR
        QR[QueryRewriteNode] --> EQ[EmbedQueryNode]
        EQ --> RD[RetrieveDocumentNode]
        RD --> GA[GenerateAnswerNode]
    end

    style Offline fill:#f9f,stroke:#333,stroke-width:2px
    style Online fill:#bbf,stroke:#333,stroke-width:2px
```

## Example Usage

### run offline pipeline

```bash
cargo run -- offline --db-url <qdrant-db-url> --collection <collection-name> --api-key <openai-api-key> --qdrant-api-key <qdrant-api-key> --endpoint <openai-endpoint> --chunk-size <chunk-size> --overlap <overlap> --model <embedding-model> --dimension <dimension> https://www.usenix.org/system/files/fast23-li-qiang_more.pdf https://www.usenix.org/system/files/fast23-li-qiang.pdf
```

### run online pipeline

```bash
cargo run -- online --db-url <qdrant-db-url> --collection <collection-name> --api-key <openai-api-key> --qdrant-api-key <qdrant-api-key> --endpoint <openai-endpoint> --embedding-model <embedding-model> --chat-mode <chat-mode> --dimension <dimension> --k <k> "Introduce Alibaba Cloud's Pangu distributed file system"
```

### Output

```markdown
Alibaba Cloud's Pangu is a large-scale, distributed storage system that has been in development and deployment since 2009. It serves as a unified storage platform for Alibaba Group and Alibaba Cloud, providing scalable, high-performance, and reliable storage services to support core businesses such as Taobao, Tmall, AntFin, and Alimama. A variety of cloud services, including Elastic Block Storage (EBS), Object Storage Service (OSS), Network-Attached Storage (NAS), PolarDB, and MaxCompute, are built on top of Pangu. Over more than a decade, Pangu has grown into a global storage system managing exabytes of data and trillions of files.

### Evolution of Pangu

Pangu's evolution can be divided into two main phases:

1. **Pangu 1.0 (2009-2015)**: This version was designed on an infrastructure composed of servers with commodity CPUs and hard disk drives (HDDs), which have millisecond-level I/O latency, and Gbps-level datacenter networks. Pangu 1.0 featured a distributed kernel-space file system based on Linux Ext4 and kernel-space TCP, gradually adding support for multiple file types (e.g., TempFile, LogFile, and random access files) as required by different storage services. During this period, the primary focus was on providing large volumes of storage space rather than high performance.

2. **Pangu 2.0 (Since 2015)**: In response to the emergence of new hardware technologies, particularly solid-state drives (SSDs) and remote direct memory access (RDMA), Pangu 2.0 was developed to provide high-performance storage services with a 100Âµs-level I/O latency. Key innovations include:
   - **Embracing SSD and RDMA**: To leverage the low latency of SSDs and RDMA, Pangu 2.0 introduced a series of new designs in its file system and developed a user-space storage operating system.
   - **High Throughput and IOPS**: Pangu 2.0 aims to achieve high throughput and IOPS, with an effective throughput on storage servers approaching their capacity.
   - **Unified High-Performance Support**: The system provides unified high-performance support to all services running on top of it, such as online search, data streaming analytics, EBS, OSS, and databases.

### Design Goals of Pangu 2.0

- **Low Latency**: Pangu 2.0 targets an average 100Âµs-level I/O latency in a computation-storage disaggregated architecture, even under dynamic environments like network traffic jitters and server failures.
- **High Throughput**: The system aims to reach an effective throughput on storage servers that approaches their capacity.
- **Unified High-Performance Support**: Pangu 2.0 provides unified high-performance support to all services, ensuring that all applications benefit from the advancements in hardware and software.

### Related Work

Pangu is part of a broader ecosystem of distributed storage systems, both open-source (e.g., HDFS and Ceph) and proprietary (e.g., GFS, Tectonic, and AWS). Alibaba has shared its experiences in various aspects of Pangu, including the large-scale deployment of RDMA, key-value engines for scale-out cloud storage, co-design of network and storage software stacks for EBS, and key designs of the namespace metadata service.

For more detailed information, you can refer to the following sources:

- [FAST '23 Paper: "Fisc: A Lightweight Client for Large-Scale Distributed File Systems"](https://www.usenix.org/system/files/fast23-li-qiang.pdf)_more.pdf)

These documents provide in-depth insights into the design, implementation, and operational experiences of Pangu.
```
</file>

<file path="examples/text2sql/example_data/customers.csv">
id,name,city,email,signup_date
1,John Doe,New York,john.doe@example.com,2023-01-15
2,Jane Smith,San Francisco,jane.smith@example.com,2023-02-20
3,Robert Johnson,New York,robert.j@example.com,2023-03-05
4,Emily Davis,Chicago,emily.d@example.com,2023-01-30
5,Michael Brown,Boston,michael.b@example.com,2023-04-10
6,Lisa Wang,Chicago,lisa.w@example.com,2023-05-12
7,David Lee,San Francisco,david.l@example.com,2023-02-28
8,Sarah Miller,Boston,sarah.m@example.com,2023-06-01
9,James Taylor,New York,james.t@example.com,2023-04-22
10,Amanda Garcia,Chicago,amanda.g@example.com,2023-03-18
</file>

<file path="examples/text2sql/example_data/orders.csv">
order_id,customer_id,product,amount,order_date
101,1,Laptop,1299.99,2023-02-10
102,3,Phone,799.99,2023-03-15
103,2,Headphones,149.99,2023-02-22
104,5,Tablet,499.99,2023-04-18
105,1,Monitor,349.99,2023-05-05
106,4,Keyboard,89.99,2023-03-10
107,3,Mouse,59.99,2023-04-02
108,7,Printer,279.99,2023-03-28
109,9,Speakers,129.99,2023-05-15
110,10,Webcam,79.99,2023-04-20
</file>

<file path="examples/text2sql/src/flow.rs">
use anyhow::{Context as AnyhowContext, Result};
use async_trait::async_trait;
use chrono::NaiveDate;
use duckdb::types::ValueRef;
use duckdb::{Connection, Result as DuckResult};
use openai_api_rust::chat::*;
use openai_api_rust::*;
use pocketflow_rs::{Context, Node, ProcessResult, ProcessState};
use serde_json::{Value, json};
use tracing::{error, info};

#[derive(Debug, Clone, PartialEq)]
pub enum SqlExecutorState {
    SchemaRetrieved,
    SqlGenerated,
    SqlExecuted,
    Default,
}

impl ProcessState for SqlExecutorState {
    fn is_default(&self) -> bool {
        matches!(self, SqlExecutorState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            SqlExecutorState::SchemaRetrieved => "schema_retrieved".to_string(),
            SqlExecutorState::SqlGenerated => "sql_generated".to_string(),
            SqlExecutorState::SqlExecuted => "sql_executed".to_string(),
            SqlExecutorState::Default => "default".to_string(),
        }
    }
}

impl Default for SqlExecutorState {
    fn default() -> Self {
        SqlExecutorState::Default
    }
}

#[derive(Debug, thiserror::Error)]
pub enum WorkflowError {
    #[error("NodeExecution: {0}")]
    NodeExecution(String),
}

pub struct SchemaRetrievalNode {
    db_path: String,
}

impl SchemaRetrievalNode {
    pub fn new(db_path: String) -> Self {
        Self { db_path }
    }
}

#[async_trait]
impl Node for SchemaRetrievalNode {
    type State = SqlExecutorState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<Value> {
        info!("Exec SchemaRetrievalNode");
        let conn = Connection::open(&self.db_path)?;

        let query = "SELECT table_name FROM information_schema.tables WHERE table_schema='main'";
        let mut stmt = conn.prepare(query)?;
        let tables = stmt.query_map([], |row| Ok(row.get(0)?));

        let tables = tables.context("èŽ·å–è¡¨åå¤±è´¥")?;

        let mut schema = serde_json::Map::new();
        for table in tables {
            let table_name = table?;
            let query = format!(
                "SELECT column_name, data_type, is_nullable, column_default
                 FROM information_schema.columns
                 WHERE table_name='{}' AND table_schema='main'",
                table_name
            );

            let mut stmt = conn.prepare(&query)?;
            let columns = stmt
                .query_map([], |row| {
                    Ok(json!({
                        "name": row.get::<_, String>(0)?,
                        "type": row.get::<_, String>(1)?,
                        "nullable": row.get::<_, String>(2)? == "YES",
                        "default_value": row.get::<_, Option<String>>(3)?,
                    }))
                })?
                .collect::<DuckResult<Vec<Value>>>()
                .context("Get Column Info Failed")?;

            schema.insert(table_name, Value::Array(columns));
        }
        info!("Get Result Final");

        Ok(Value::Object(schema))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<SqlExecutorState>> {
        context.set("result", result.as_ref().unwrap().clone());
        Ok(ProcessResult::new(
            SqlExecutorState::SchemaRetrieved,
            "schema_retrieved".to_string(),
        ))
    }
}

pub struct OpenAISQLGenerationNode {
    api_key: String,
    user_query: String,
}

impl OpenAISQLGenerationNode {
    pub fn new(api_key: String, user_query: String) -> Self {
        Self {
            api_key,
            user_query,
        }
    }
}

pub fn print_table(headers: &[String], data: &[Vec<String>]) {
    if headers.is_empty() {
        println!("Query returned no columns.");
        return;
    }

    // Calculate column widths based on headers and data
    let mut widths: Vec<usize> = headers.iter().map(|h| h.len()).collect();
    for row in data {
        for (i, cell) in row.iter().enumerate() {
            if i < widths.len() {
                widths[i] = widths[i].max(cell.len());
            }
        }
    }

    // Print Header
    let header_line = headers
        .iter()
        .zip(&widths)
        .map(|(h, w)| format!("{:<width$}", h, width = w))
        .collect::<Vec<_>>()
        .join(" | ");
    println!("\n{}", header_line);

    // Print Separator
    let separator_line = widths
        .iter()
        .map(|w| "-".repeat(*w))
        .collect::<Vec<_>>()
        .join("-+-");
    println!("{}", separator_line);

    // Print Data Rows
    if data.is_empty() {
        println!("(No rows returned)");
    } else {
        for row in data {
            let row_line = row
                .iter()
                .zip(&widths)
                .map(|(cell, w)| format!("{:<width$}", cell, width = w))
                .collect::<Vec<_>>()
                .join(" | ");
            println!("{}", row_line);
        }
    }
}

#[async_trait]
impl Node for OpenAISQLGenerationNode {
    type State = SqlExecutorState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let schema = context.get("result").ok_or_else(|| {
            WorkflowError::NodeExecution("Failed to get database schema".to_string())
        })?;

        let system_prompt = "You are a SQL expert. Based on the provided database schema and user query, generate the correct SQL query. Only return the SQL query, do not include any explanation or other text. The condition content uses English, you can choose to query some fields first, then make a general query.";

        let schema_json =
            serde_json::to_string_pretty(schema).context("Failed to serialize database schema")?;

        let user_prompt = format!(
            "database schema:\n{}\n\nuser query:\n{}\n\nPlease generate a SQL query to answer this question.",
            schema_json, self.user_query
        );

        let auth = Auth::new(self.api_key.as_str());
        let openai = OpenAI::new(auth, "https://dashscope.aliyuncs.com/compatible-mode/v1/");
        let body = ChatBody {
            model: "qwen-plus".to_string(),
            max_tokens: Some(1024),
            temperature: Some(0.8_f32),
            top_p: Some(0_f32),
            n: Some(1),
            stream: Some(false),
            stop: None,
            presence_penalty: None,
            frequency_penalty: None,
            logit_bias: None,
            user: None,
            messages: vec![
                Message {
                    role: Role::System,
                    content: system_prompt.to_string(),
                },
                Message {
                    role: Role::User,
                    content: user_prompt,
                },
            ],
        };
        let rs = openai.chat_completion_create(&body);
        if rs.is_err() {
            error!("OpenAI Error {}", rs.as_ref().err().unwrap().to_string());
        }
        let choice = rs.unwrap().choices;
        let message = &choice[0].message.as_ref().unwrap();

        let sql = message.content.clone();

        println!("ç”Ÿæˆçš„SQLæŸ¥è¯¢: {}", sql);

        Ok(Value::String(sql))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<SqlExecutorState>> {
        context.set("result", result.as_ref().unwrap().clone());
        Ok(ProcessResult::new(
            SqlExecutorState::SqlGenerated,
            "sql_generated".to_string(),
        ))
    }
}

pub struct ExecuteSQLNode {
    db_path: String,
}

impl ExecuteSQLNode {
    pub fn new(db_path: String) -> Self {
        Self { db_path }
    }
}

#[async_trait]
impl Node for ExecuteSQLNode {
    type State = SqlExecutorState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let conn = Connection::open(&self.db_path)?;

        let sql = context
            .get("result")
            .and_then(|v| v.as_str())
            .ok_or_else(|| {
                WorkflowError::NodeExecution("SQL query not found in context".to_string())
            })?;

        info!("ExecuteSQLNode: Get Sql: {}", sql);

        let mut stmt = conn.prepare(sql)?;
        let mut rows = stmt.query([])?;

        let mut headers = Vec::new();
        let mut data_rows = Vec::new();

        if let Some(first_row) = rows.next()? {
            // Get column names from the first row
            headers = first_row.as_ref().column_names();
            let column_count = headers.len();

            // Process first row
            let mut row_values = Vec::with_capacity(column_count);
            for i in 0..column_count {
                let value_ref = first_row.get_ref(i)?;
                let string_value = match value_ref {
                    ValueRef::Null => "NULL".to_string(),
                    ValueRef::Boolean(b) => b.to_string(),
                    ValueRef::TinyInt(i) => i.to_string(),
                    ValueRef::SmallInt(i) => i.to_string(),
                    ValueRef::Int(i) => i.to_string(),
                    ValueRef::BigInt(i) => i.to_string(),
                    ValueRef::Float(f) => f.to_string(),
                    ValueRef::Double(d) => d.to_string(),
                    ValueRef::Text(bytes) => String::from_utf8_lossy(bytes).to_string(),
                    ValueRef::Blob(_) => "[BLOB]".to_string(),
                    ValueRef::Date32(d) => {
                        let date = NaiveDate::from_num_days_from_ce_opt(d as i32 + 719163).unwrap();
                        date.format("%Y-%m-%d").to_string()
                    }
                    _ => format!("Unsupported: {:?}", value_ref),
                };
                row_values.push(string_value);
            }
            data_rows.push(row_values);

            // Process remaining rows
            while let Some(row) = rows.next()? {
                let mut row_values = Vec::with_capacity(column_count);
                for i in 0..column_count {
                    let value_ref = row.get_ref(i)?;
                    let string_value = match value_ref {
                        ValueRef::Null => "NULL".to_string(),
                        ValueRef::Boolean(b) => b.to_string(),
                        ValueRef::TinyInt(i) => i.to_string(),
                        ValueRef::SmallInt(i) => i.to_string(),
                        ValueRef::Int(i) => i.to_string(),
                        ValueRef::BigInt(i) => i.to_string(),
                        ValueRef::Float(f) => f.to_string(),
                        ValueRef::Double(d) => d.to_string(),
                        ValueRef::Text(bytes) => String::from_utf8_lossy(bytes).to_string(),
                        ValueRef::Blob(_) => "[BLOB]".to_string(),
                        ValueRef::Date32(d) => {
                            let date =
                                NaiveDate::from_num_days_from_ce_opt(d as i32 + 719163).unwrap();
                            date.format("%Y-%m-%d").to_string()
                        }
                        _ => format!("Unsupported: {:?}", value_ref),
                    };
                    row_values.push(string_value);
                }
                data_rows.push(row_values);
            }
        }

        print_table(&headers, &data_rows);

        Ok(json!({
            "columns": headers,
            "data": data_rows
        }))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<SqlExecutorState>> {
        context.set("result", result.as_ref().unwrap().clone());
        Ok(ProcessResult::new(
            SqlExecutorState::SqlExecuted,
            "sql_executed".to_string(),
        ))
    }
}
</file>

<file path="examples/text2sql/src/lib.rs">
pub mod flow;
</file>

<file path="examples/text2sql/src/main.rs">
use std::env;

use anyhow::Result;
use duckdb::Connection;
use pocketflow_rs::{Context, build_flow};
use text2sql::flow::{ExecuteSQLNode, OpenAISQLGenerationNode, SchemaRetrievalNode};

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt::init();

    let db_path = "ecommerce.duckdb";
    let conn = Connection::open(db_path)?;

    conn.execute(&format!(
        "CREATE TABLE IF NOT EXISTS customers AS SELECT * FROM read_csv_auto('{}', AUTO_DETECT=TRUE)",
        "example_data/customers.csv"
    ), [])?;

    conn.execute(&format!(
        "CREATE TABLE IF NOT EXISTS orders AS SELECT * FROM read_csv_auto('{}', AUTO_DETECT=TRUE)",
        "example_data/orders.csv"
    ), [])?;

    println!("please input your query using natural language?");
    let mut user_query = String::new();
    std::io::stdin().read_line(&mut user_query)?;
    user_query = user_query.trim().to_string();

    let schema_retrieval = SchemaRetrievalNode::new(db_path.to_string());
    let openai_sql_gen =
        OpenAISQLGenerationNode::new(env::var("DASH_SCOPE_API_KEY").unwrap(), user_query);
    let execute_sql = ExecuteSQLNode::new(db_path.to_string());

    let flow = build_flow! (
        start: ("start", schema_retrieval),
        nodes: [
            ("generate_sql", openai_sql_gen),
            ("execute_sql", execute_sql),
        ],
        edges: [
            ("start", "generate_sql", text2sql::flow::SqlExecutorState::Default),
            ("generate_sql", "execute_sql", text2sql::flow::SqlExecutorState::Default)
        ]
    );
    let context = Context::new();

    let result = flow.run(context).await?;
    println!("result: {:?}", result);

    Ok(())
}
</file>

<file path="examples/text2sql/.gitignore">
ecommerce.duckdb
</file>

<file path="examples/text2sql/Cargo.toml">
[package]
name = "text2sql"
version = "0.1.0"
edition = "2024"

[[bin]]
name = "text2sql"

[dependencies]
duckdb = {version="1.2.2", features = ["bundled"]}
pocketflow_rs = { path = '../..'}
serde_json = "1.0.140"
async-trait = "0.1"
tokio = { version = "1.0", features = ["full"] }
anyhow = "1.0.98"
thiserror = "1.0.69"
openai_api_rust = "0.1.9"
chrono = "0.4.41"
tracing = "0.1.41"
tracing-subscriber = "0.3.19"
</file>

<file path="examples/basic.rs">
use anyhow::Result;
use pocketflow_rs::{Context, Node, ProcessResult, ProcessState, build_flow};
use rand::Rng;
use serde_json::Value;

#[derive(Debug, Clone, PartialEq, Default)]
enum NumberState {
    Small,
    Medium,
    Large,
    #[default]
    Default,
}

impl ProcessState for NumberState {
    fn is_default(&self) -> bool {
        matches!(self, NumberState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            NumberState::Small => "small".to_string(),
            NumberState::Medium => "medium".to_string(),
            NumberState::Large => "large".to_string(),
            NumberState::Default => "default".to_string(),
        }
    }
}

// A simple node that prints a message
struct PrintNode {
    message: String,
}

impl PrintNode {
    fn new(message: &str) -> Self {
        Self {
            message: message.to_string(),
        }
    }
}

#[async_trait::async_trait]
impl Node for PrintNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        println!("PrintNode: {}, Context: {}", self.message, context);
        Ok(Value::String(self.message.clone()))
    }
}

// A node that generates a random number
struct RandomNumberNode {
    max: i64,
}

impl RandomNumberNode {
    fn new(max: i64) -> Self {
        Self { max }
    }
}

#[async_trait::async_trait]
impl Node for RandomNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = rand::thread_rng().gen_range(0..self.max);
        println!(
            "RandomNumberNode: Generated number {}, Context: {}",
            num, context
        );
        Ok(Value::Number(num.into()))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<NumberState>> {
        let num = result.as_ref().unwrap().as_i64().unwrap_or(0);
        context.set("number", Value::Number(num.into()));
        // Return different states based on the number
        let state = if num < self.max / 3 {
            NumberState::Small
        } else if num < 2 * self.max / 3 {
            NumberState::Medium
        } else {
            NumberState::Large
        };
        let condition = state.to_condition();
        Ok(ProcessResult::new(state, condition))
    }
}

// A node that processes small numbers
struct SmallNumberNode;

#[async_trait::async_trait]
impl Node for SmallNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = context.get("number").and_then(|v| v.as_i64()).unwrap_or(0);
        println!("SmallNumberNode: Processing small number {}", num);
        Ok(Value::String(format!("Small number processed: {}", num)))
    }
}

// A node that processes medium numbers
struct MediumNumberNode;

#[async_trait::async_trait]
impl Node for MediumNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = context.get("number").and_then(|v| v.as_i64()).unwrap_or(0);
        println!("MediumNumberNode: Processing medium number {}", num);
        Ok(Value::String(format!("Medium number processed: {}", num)))
    }
}

// A node that processes large numbers
struct LargeNumberNode;

#[async_trait::async_trait]
impl Node for LargeNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = context.get("number").and_then(|v| v.as_i64()).unwrap_or(0);
        println!("LargeNumberNode: Processing large number {}", num);
        Ok(Value::String(format!("Large number processed: {}", num)))
    }
}

#[tokio::main]
async fn main() -> std::result::Result<(), Box<dyn std::error::Error>> {
    // Create nodes
    let begin_node = PrintNode::new("Begin Node");
    let random_node = RandomNumberNode::new(100);
    let small_node = SmallNumberNode;
    let medium_node = MediumNumberNode;
    let large_node = LargeNumberNode;

    // Create flow using macro
    let flow = build_flow!(
        start: ("start", begin_node),
        nodes: [
            ("rand", random_node),
            ("small", small_node),
            ("medium", medium_node),
            ("large", large_node)
        ],
        edges: [
            ("start", "rand", NumberState::Default),
            ("rand", "small", NumberState::Small),
            ("rand", "medium", NumberState::Medium),
            ("rand", "large", NumberState::Large)
        ]
    );

    // Create context
    let context = Context::new();

    // Run the flow
    println!("Starting flow execution...");
    flow.run(context).await?;
    println!("Flow execution completed!");

    Ok(())
}
</file>

<file path="src/utils/embedding.rs">
#![cfg(feature = "openai")]

use async_trait::async_trait;
use openai_api_rust::embeddings::*;
use openai_api_rust::*;
use tracing::info;

#[derive(Debug, Clone)]
pub struct EmbeddingOptions {
    pub model: String,
    pub dimensions: Option<usize>,
}

impl Default for EmbeddingOptions {
    fn default() -> Self {
        Self {
            model: "text-embedding-ada-002".to_string(),
            dimensions: None,
        }
    }
}

#[async_trait]
pub trait EmbeddingGenerator {
    async fn generate_embedding(&self, text: &str) -> anyhow::Result<Vec<f64>>;
    async fn generate_embeddings(&self, texts: &[String]) -> anyhow::Result<Vec<Vec<f64>>>;
}

#[allow(dead_code)]
pub struct OpenAIEmbeddingGenerator {
    api_key: String,
    options: EmbeddingOptions,
    client: OpenAI,
}

impl OpenAIEmbeddingGenerator {
    pub fn new(api_key: &str, endpoint: &str, options: EmbeddingOptions) -> Self {
        let auth = Auth::new(api_key);
        let client = OpenAI::new(auth, endpoint);
        Self {
            api_key: api_key.to_string(),
            options,
            client,
        }
    }
}

#[async_trait]
impl EmbeddingGenerator for OpenAIEmbeddingGenerator {
    async fn generate_embedding(&self, text: &str) -> anyhow::Result<Vec<f64>> {
        let embeds = self.generate_embeddings(&[text.to_string()]).await?;
        let result: Vec<f64> = embeds[0].to_vec();
        Ok(result)
    }

    async fn generate_embeddings(&self, texts: &[String]) -> anyhow::Result<Vec<Vec<f64>>> {
        // chunked by 10
        let chunks = texts.chunks(10).collect::<Vec<_>>();
        let mut results = Vec::new();
        for chunk in chunks {
            let embedding = EmbeddingsBody {
                model: self.options.model.clone(),
                input: chunk.to_vec(),
                user: None,
            };

            info!("Sending request to OpenAI Embedding API");
            let response = self.client.embeddings_create(&embedding).unwrap();
            let data = response.data.unwrap();
            let result: Vec<Vec<f64>> = data
                .into_iter()
                .map(|x: EmbeddingData| x.embedding.unwrap())
                .collect();
            results.extend(result);
        }
        Ok(results)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::env;

    #[tokio::test]
    #[ignore = "E2E case, requires API keys"]
    async fn test_e2e_embedding_generator() {
        let generator = OpenAIEmbeddingGenerator::new(
            &env::var("DASH_SCOPE_API_KEY").unwrap(),
            "https://dashscope.aliyuncs.com/compatible-mode/v1/",
            EmbeddingOptions {
                model: "text-embedding-v3".to_string(),
                dimensions: Some(64),
            },
        );
        let text = "Hello, world!";
        let embedding = generator.generate_embedding(text).await.unwrap();
        println!("{:?}", embedding);
    }
}
</file>

<file path="src/utils/llm_wrapper.rs">
#![cfg(feature = "openai")]

use std::{collections::HashMap, hash::RandomState};

use async_trait::async_trait;
use openai_api_rust::chat::*;
use openai_api_rust::*;
use serde::{Deserialize, Serialize};
use tracing::info;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMResponse {
    pub content: String,
    pub usage: Option<LLMUsage>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMUsage {
    pub prompt_tokens: Option<u32>,
    pub completion_tokens: Option<u32>,
    pub total_tokens: Option<u32>,
}

#[async_trait]
pub trait LLMWrapper {
    async fn generate(&self, prompt: &str) -> anyhow::Result<LLMResponse>;
    async fn generate_with_options(
        &self,
        prompt: &str,
        options: LLMOptions,
    ) -> anyhow::Result<LLMResponse>;
}

#[derive(Debug, Clone, Default)]
pub struct LLMOptions {
    pub temperature: Option<f32>,
    pub max_tokens: Option<i32>,
    pub top_p: Option<f32>,
    pub frequency_penalty: Option<f32>,
    pub presence_penalty: Option<f32>,
    pub stop: Option<Vec<String>>,
    pub logit_bias: Option<HashMap<String, String, RandomState>>,
}

#[allow(dead_code)]
pub struct OpenAIClient {
    api_key: String,
    model: String,
    endpoint: String,
    client: OpenAI,
}

impl OpenAIClient {
    pub fn new(api_key: String, model: String, endpoint: String) -> Self {
        let auth = Auth::new(&api_key);
        let client = OpenAI::new(auth, &endpoint);
        Self {
            api_key,
            model,
            endpoint,
            client,
        }
    }
}

#[async_trait]
impl LLMWrapper for OpenAIClient {
    async fn generate(&self, prompt: &str) -> anyhow::Result<LLMResponse> {
        self.generate_with_options(prompt, LLMOptions::default())
            .await
    }

    async fn generate_with_options(
        &self,
        prompt: &str,
        options: LLMOptions,
    ) -> anyhow::Result<LLMResponse> {
        let chat = ChatBody {
            model: self.model.clone(),
            temperature: options.temperature,
            max_tokens: options.max_tokens,
            presence_penalty: options.presence_penalty,
            frequency_penalty: options.frequency_penalty,
            logit_bias: options.logit_bias,
            top_p: options.top_p,
            stream: Some(false),
            stop: options.stop,
            user: None,
            n: Some(1),
            messages: vec![Message {
                role: Role::User,
                content: prompt.to_string(),
            }],
        };

        info!("Sending request to OpenAI API");
        let response = self.client.chat_completion_create(&chat).unwrap();
        let choice = response.choices;
        let content = &choice[0].message.as_ref().unwrap().content;
        let u = response.usage;
        let usage = LLMUsage {
            prompt_tokens: u.prompt_tokens,
            completion_tokens: u.completion_tokens,
            total_tokens: u.total_tokens,
        };

        Ok(LLMResponse {
            content: content.clone(),
            usage: Some(usage),
        })
    }
}
</file>

<file path="src/utils/mod.rs">
pub mod embedding;
pub mod llm_wrapper;
pub mod text_chunking;
pub mod vector_db;
pub mod viz_debug;
pub mod web_search;
</file>

<file path="src/utils/text_chunking.rs">
use regex::Regex;
use tracing::info;

#[derive(Debug, Clone)]
pub struct ChunkingOptions {
    pub chunk_size: usize,
    pub overlap: usize,
    pub strategy: ChunkingStrategy,
}

#[derive(Debug, Clone)]
pub enum ChunkingStrategy {
    FixedSize,
    Sentence,
    Paragraph,
}

impl Default for ChunkingOptions {
    fn default() -> Self {
        Self {
            chunk_size: 1000,
            overlap: 100,
            strategy: ChunkingStrategy::FixedSize,
        }
    }
}

pub struct TextChunker {
    sentence_regex: Regex,
    paragraph_regex: Regex,
}

impl Default for TextChunker {
    fn default() -> Self {
        Self::new()
    }
}

impl TextChunker {
    pub fn new() -> Self {
        Self {
            sentence_regex: Regex::new(r"[.!?]+[\s]+").unwrap(),
            paragraph_regex: Regex::new(r"\n\s*\n").unwrap(),
        }
    }

    pub fn chunk_text(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        info!("Chunking text with strategy: {:?}", options.strategy);
        match options.strategy {
            ChunkingStrategy::FixedSize => self.chunk_by_size(text, options),
            ChunkingStrategy::Sentence => self.chunk_by_sentence(text, options),
            ChunkingStrategy::Paragraph => self.chunk_by_paragraph(text, options),
        }
    }

    fn chunk_by_size(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        let mut chunks = Vec::new();
        let mut start = 0;
        let text_size = text.len();

        while start < text_size {
            let end = (start + options.chunk_size).min(text_size);

            // Try to find a good breaking point (space or punctuation)
            let mut actual_end = end;
            if actual_end < text_size {
                while actual_end > start && !text[actual_end..].starts_with(char::is_whitespace) {
                    actual_end -= 1;
                }
                // If we couldn't find a good breaking point, force a break at the chunk size
                if actual_end == start {
                    actual_end = end;
                }
            }

            let chunk = text[start..actual_end].trim().to_string();
            if !chunk.is_empty() {
                chunks.push(chunk);
            }

            // Ensure we always advance by at least 1 character to prevent infinite loop
            let new_start = actual_end.saturating_sub(options.overlap);
            if new_start <= start {
                start = actual_end;
            } else {
                start = new_start;
            }
        }

        chunks
    }

    fn chunk_by_sentence(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();

        for sentence in self.sentence_regex.split(text) {
            let sentence = sentence.trim();
            if sentence.is_empty() {
                continue;
            }

            if current_chunk.len() + sentence.len() < options.chunk_size {
                if !current_chunk.is_empty() {
                    current_chunk.push(' ');
                }
                current_chunk.push_str(sentence);
            } else {
                if !current_chunk.is_empty() {
                    chunks.push(current_chunk);
                }
                current_chunk = sentence.to_string();
            }
        }

        if !current_chunk.is_empty() {
            chunks.push(current_chunk);
        }

        // Add overlap between chunks
        if options.overlap > 0 && chunks.len() > 1 {
            let mut overlapped_chunks = Vec::with_capacity(chunks.len());
            overlapped_chunks.push(chunks[0].clone());

            for i in 1..chunks.len() {
                let prev_chunk = &chunks[i - 1];
                let current_chunk = &chunks[i];

                // Find the last sentence in the previous chunk
                let last_sentences: Vec<&str> = self
                    .sentence_regex
                    .split(prev_chunk)
                    .filter(|s| !s.trim().is_empty())
                    .collect();

                if let Some(last_sentence) = last_sentences.last() {
                    let mut new_chunk = last_sentence.trim().to_string();
                    new_chunk.push(' ');
                    new_chunk.push_str(current_chunk);
                    overlapped_chunks.push(new_chunk);
                } else {
                    overlapped_chunks.push(current_chunk.clone());
                }
            }

            chunks = overlapped_chunks;
        }

        chunks
    }

    fn chunk_by_paragraph(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();

        for paragraph in self.paragraph_regex.split(text) {
            let paragraph = paragraph.trim();
            if paragraph.is_empty() {
                continue;
            }

            if current_chunk.len() + paragraph.len() + 2 <= options.chunk_size {
                if !current_chunk.is_empty() {
                    current_chunk.push_str("\n\n");
                }
                current_chunk.push_str(paragraph);
            } else {
                if !current_chunk.is_empty() {
                    chunks.push(current_chunk);
                }
                current_chunk = paragraph.to_string();
            }
        }

        if !current_chunk.is_empty() {
            chunks.push(current_chunk);
        }

        // Add overlap between chunks
        if options.overlap > 0 && chunks.len() > 1 {
            let mut overlapped_chunks = Vec::with_capacity(chunks.len());
            overlapped_chunks.push(chunks[0].clone());

            for i in 1..chunks.len() {
                let prev_chunk = &chunks[i - 1];
                let current_chunk = &chunks[i];

                // Find the last paragraph in the previous chunk
                let last_paragraphs: Vec<&str> = self
                    .paragraph_regex
                    .split(prev_chunk)
                    .filter(|p| !p.trim().is_empty())
                    .collect();

                if let Some(last_paragraph) = last_paragraphs.last() {
                    let mut new_chunk = last_paragraph.trim().to_string();
                    new_chunk.push_str("\n\n");
                    new_chunk.push_str(current_chunk);
                    overlapped_chunks.push(new_chunk);
                } else {
                    overlapped_chunks.push(current_chunk.clone());
                }
            }

            chunks = overlapped_chunks;
        }

        chunks
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_fixed_size_chunking() {
        let chunker = TextChunker::new();
        let text = "This is a test. This is another test. This is a third test.";
        let options = ChunkingOptions {
            chunk_size: 20,
            overlap: 5,
            strategy: ChunkingStrategy::FixedSize,
        };

        let chunks = chunker.chunk_text(text, &options);
        assert_eq!(chunks.len(), 5);
        for chunk in chunks {
            assert!(chunk.len() <= 20);
        }
    }

    #[test]
    fn test_sentence_chunking() {
        let chunker = TextChunker::new();
        let text = "This is a test. This is another test. This is a third test.";
        let options = ChunkingOptions {
            chunk_size: 30,
            overlap: 10,
            strategy: ChunkingStrategy::Sentence,
        };

        let chunks = chunker.chunk_text(text, &options);
        assert_eq!(chunks.len(), 3);
        assert!(chunks[0].contains("This is a test"));
        assert!(chunks[1].contains("This is another test"));
        assert!(chunks[2].contains("This is a third test"));
    }

    #[test]
    fn test_paragraph_chunking() {
        let chunker = TextChunker::new();
        let text = "This is a test.\n\nThis is another test.\n\nThis is a third test.";
        let options = ChunkingOptions {
            chunk_size: 30,
            overlap: 10,
            strategy: ChunkingStrategy::Paragraph,
        };

        let chunks = chunker.chunk_text(text, &options);
        assert_eq!(chunks.len(), 3);
        assert!(chunks[0].contains("This is a test"));
        assert!(chunks[1].contains("This is another test"));
        assert!(chunks[2].contains("This is a third test"));
    }
}
</file>

<file path="src/utils/vector_db.rs">
#![cfg(feature = "qdrant")]

use async_trait::async_trait;
use qdrant_client::Qdrant;
use qdrant_client::qdrant::{
    CreateCollectionBuilder, DeletePointsBuilder, Distance, PointStruct, ScoredPoint,
    SearchPointsBuilder, UpsertPointsBuilder, VectorParamsBuilder,
};
use qdrant_client::qdrant::{Value as QdrantValue, value::Kind as QdrantKind};

use serde_json::{Map as SerdeMap, Number as SerdeNumber, Value as SerdeValue, json};

use tracing::info;

#[derive(Debug, Clone)]
pub struct VectorDBOptions {
    pub collection_name: String,
    pub dimension: usize,
    pub distance_metric: DistanceMetric,
}

#[derive(Debug, Clone)]
pub enum DistanceMetric {
    Cosine,
    Euclidean,
    DotProduct,
}

#[derive(Debug, Clone)]
pub struct VectorRecord {
    pub id: String,
    pub vector: Vec<f32>,
    pub metadata: serde_json::Map<String, serde_json::Value>,
}

impl VectorRecord {
    pub fn parse_by_value(value: &serde_json::Value) -> Self {
        let id = value.get("id").unwrap().as_str().unwrap().to_string();
        let vector = value
            .get("vector")
            .unwrap()
            .as_array()
            .unwrap()
            .iter()
            .map(|v| v.as_f64().unwrap() as f32)
            .collect();
        let metadata = value.get("metadata").unwrap().as_object().unwrap().clone();
        Self {
            id,
            vector,
            metadata,
        }
    }

    pub fn to_value(&self) -> serde_json::Value {
        json!({
            "id": self.id,
            "vector": self.vector,
            "metadata": self.metadata
        })
    }
}

fn qdrant_value_to_serde_json(q_val: QdrantValue) -> SerdeValue {
    match q_val.kind {
        Some(QdrantKind::NullValue(_)) => SerdeValue::Null,
        Some(QdrantKind::BoolValue(b)) => SerdeValue::Bool(b),
        Some(QdrantKind::DoubleValue(d)) => {
            SerdeNumber::from_f64(d).map_or(SerdeValue::Null, SerdeValue::Number)
        }
        Some(QdrantKind::IntegerValue(i)) => SerdeValue::Number(i.into()),
        Some(QdrantKind::StringValue(s)) => SerdeValue::String(s),
        Some(QdrantKind::ListValue(list_value)) => {
            let serde_list: Vec<SerdeValue> = list_value
                .values
                .into_iter()
                .map(qdrant_value_to_serde_json)
                .collect();
            SerdeValue::Array(serde_list)
        }
        Some(QdrantKind::StructValue(struct_value)) => {
            let mut serde_map = SerdeMap::new();
            for (key, val) in struct_value.fields {
                serde_map.insert(key, qdrant_value_to_serde_json(val));
            }
            SerdeValue::Object(serde_map)
        }
        None => SerdeValue::Null, // Treat absence of kind as Null
    }
}

impl VectorRecord {
    pub fn from_scored_point(point: ScoredPoint) -> Option<Self> {
        let id_str = match point.id {
            Some(point_id) => match point_id.point_id_options {
                Some(qdrant_client::qdrant::point_id::PointIdOptions::Num(n)) => n.to_string(),
                Some(qdrant_client::qdrant::point_id::PointIdOptions::Uuid(s)) => s,
                None => return None,
            },
            None => return None,
        };
        let vector_data = match point.vectors {
            Some(vector) => match vector.vectors_options {
                Some(qdrant_client::qdrant::vectors_output::VectorsOptions::Vector(v)) => v.data,
                _ => return None,
            },
            None => return None,
        };
        // 3. Convert Payload
        let metadata_map: SerdeMap<String, SerdeValue> = point
            .payload
            .into_iter()
            .map(|(key, q_val)| (key, qdrant_value_to_serde_json(q_val)))
            .collect();

        Some(VectorRecord {
            id: id_str,
            vector: vector_data,
            metadata: metadata_map,
        })
    }
}

#[async_trait]
pub trait VectorDB {
    async fn insert(&self, records: Vec<VectorRecord>) -> anyhow::Result<()>;
    async fn search(&self, query: Vec<f32>, k: usize) -> anyhow::Result<Vec<VectorRecord>>;
    async fn delete(&self, ids: Vec<String>) -> anyhow::Result<()>;
}

pub struct QdrantDB {
    client: Qdrant,
    options: VectorDBOptions,
}

impl QdrantDB {
    pub async fn new(
        db_url: String,
        api_key: Option<String>,
        options: VectorDBOptions,
    ) -> anyhow::Result<Self> {
        let client = match api_key {
            Some(api_key) => Qdrant::from_url(db_url.as_str()).api_key(api_key).build()?,
            None => Qdrant::from_url(db_url.as_str()).build()?,
        };

        // Create collection if it doesn't exist
        let collections = client.list_collections().await?;
        if !collections
            .collections
            .iter()
            .any(|c| c.name == options.collection_name)
        {
            let distance = match options.distance_metric {
                DistanceMetric::Cosine => Distance::Cosine,
                DistanceMetric::Euclidean => Distance::Euclid,
                DistanceMetric::DotProduct => Distance::Dot,
            };
            let request = CreateCollectionBuilder::new(options.collection_name.clone())
                .vectors_config(VectorParamsBuilder::new(options.dimension as u64, distance));
            client.create_collection(request).await?;
        }

        Ok(Self { client, options })
    }
}

#[async_trait]
impl VectorDB for QdrantDB {
    async fn insert(&self, records: Vec<VectorRecord>) -> anyhow::Result<()> {
        let points: Vec<PointStruct> = records
            .into_iter()
            .map(|record| PointStruct::new(record.id, record.vector, record.metadata))
            .collect();
        let points_request = UpsertPointsBuilder::new(&self.options.collection_name, points);

        info!("Inserting points into Qdrant");
        self.client.upsert_points(points_request).await?;
        Ok(())
    }

    async fn search(&self, query: Vec<f32>, k: usize) -> anyhow::Result<Vec<VectorRecord>> {
        info!(
            "Searching points in Qdrant, collection: {}",
            self.options.collection_name
        );
        let response = self
            .client
            .search_points(
                SearchPointsBuilder::new(&self.options.collection_name, query, k as u64)
                    .with_payload(true)
                    .with_vectors(true),
            )
            .await?;
        let results = response
            .result
            .into_iter()
            .filter_map(VectorRecord::from_scored_point)
            .collect::<Vec<_>>();
        info!("Retrieved results len: {:?}", results.len());

        Ok(results)
    }

    async fn delete(&self, ids: Vec<String>) -> anyhow::Result<()> {
        info!("Deleting points from Qdrant");
        self.client
            .delete_points(DeletePointsBuilder::new(&self.options.collection_name).points(ids))
            .await?;
        Ok(())
    }
}
</file>

<file path="src/utils/viz_debug.rs">
#![cfg(feature = "debug")]
use std::fmt::Debug;

pub trait DebugVisualizer {
    fn visualize<T: Debug>(&self, data: &T) -> String;
    fn visualize_flow(&self, flow_data: &[u8]) -> String;
}

pub struct ConsoleDebugVisualizer;

impl DebugVisualizer for ConsoleDebugVisualizer {
    fn visualize<T: Debug>(&self, data: &T) -> String {
        format!("{:?}", data)
    }

    #[allow(unused_variables)]
    fn visualize_flow(&self, flow_data: &[u8]) -> String {
        // TODO: Implement flow visualization
        "Flow visualization not implemented".to_string()
    }
}

pub struct GraphDebugVisualizer;

impl DebugVisualizer for GraphDebugVisualizer {
    fn visualize<T: Debug>(&self, data: &T) -> String {
        // TODO: Implement graph visualization
        format!("Graph visualization of {:?}", data)
    }

    #[allow(unused_variables)]
    fn visualize_flow(&self, flow_data: &[u8]) -> String {
        // TODO: Implement flow graph visualization
        "Flow graph visualization not implemented".to_string()
    }
}
</file>

<file path="src/utils/web_search.rs">
#![cfg(feature = "websearch")]

use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use tracing::info;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    pub title: String,
    pub url: String,
    pub snippet: String,
}

#[async_trait]
pub trait WebSearcher {
    async fn search(&self, query: &str) -> anyhow::Result<Vec<SearchResult>>;
    async fn search_with_options(
        &self,
        query: &str,
        options: SearchOptions,
    ) -> anyhow::Result<Vec<SearchResult>>;
}

#[derive(Debug, Clone, Default)]
pub struct SearchOptions {
    pub max_results: Option<usize>,
    pub language: Option<String>,
    pub region: Option<String>,
}

pub struct GoogleSearcher {
    api_key: String,
    search_engine_id: String,
    client: Client,
}

impl GoogleSearcher {
    pub fn new(api_key: String, search_engine_id: String) -> Self {
        Self {
            api_key,
            search_engine_id,
            client: Client::new(),
        }
    }
}

#[async_trait]
impl WebSearcher for GoogleSearcher {
    async fn search(&self, query: &str) -> anyhow::Result<Vec<SearchResult>> {
        self.search_with_options(query, SearchOptions::default())
            .await
    }

    async fn search_with_options(
        &self,
        query: &str,
        options: SearchOptions,
    ) -> anyhow::Result<Vec<SearchResult>> {
        let mut url = format!(
            "https://www.googleapis.com/customsearch/v1?key={}&cx={}&q={}",
            self.api_key, self.search_engine_id, query
        );

        if let Some(lang) = options.language {
            url.push_str(&format!("&lr=lang_{}", lang));
        }
        if let Some(region) = options.region {
            url.push_str(&format!("&cr=country{}", region));
        }
        if let Some(max_results) = options.max_results {
            url.push_str(&format!("&num={}", max_results));
        }

        info!("Sending request to Google Search API");
        let response = self.client.get(&url).send().await?;
        let search_response: serde_json::Value = response.json().await?;
        let default_val: Vec<serde_json::Value> = vec![];
        let items = search_response["items"].as_array().unwrap_or(&default_val);
        let results = items
            .iter()
            .map(|item| SearchResult {
                title: item["title"].as_str().unwrap_or("").to_string(),
                url: item["link"].as_str().unwrap_or("").to_string(),
                snippet: item["snippet"].as_str().unwrap_or("").to_string(),
            })
            .collect();

        Ok(results)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::env;

    #[tokio::test]
    #[ignore = "E2E case, requires API keys"]
    async fn test_e2e_google_searcher() {
        let searcher = GoogleSearcher::new(
            env::var("GOOGLE_API_KEY").unwrap(),
            env::var("GOOGLE_SEARCH_ENGINE_ID").unwrap(),
        );
        let results = searcher
            .search("Beijing's temperature today")
            .await
            .unwrap();
        println!("{:?}", results);
    }
}
</file>

<file path="src/context.rs">
use serde_json::Value;
use std::collections::HashMap;
use std::fmt;

#[derive(Debug, Clone, Default)]
pub struct Context {
    data: HashMap<String, Value>,
    metadata: HashMap<String, Value>,
}

impl Context {
    pub fn new() -> Self {
        Self {
            data: HashMap::new(),
            metadata: HashMap::new(),
        }
    }

    pub fn from_data(data: HashMap<String, Value>) -> Self {
        Self {
            data,
            metadata: HashMap::new(),
        }
    }

    pub fn get(&self, key: &str) -> Option<&Value> {
        self.data.get(key)
    }

    pub fn get_metadata(&self, key: &str) -> Option<&Value> {
        self.metadata.get(key)
    }

    pub fn set(&mut self, key: &str, value: Value) {
        self.data.insert(key.to_string(), value);
    }

    pub fn set_metadata(&mut self, key: &str, value: Value) {
        self.metadata.insert(key.to_string(), value);
    }

    pub fn remove(&mut self, key: &str) -> Option<Value> {
        self.data.remove(key)
    }

    pub fn remove_metadata(&mut self, key: &str) -> Option<Value> {
        self.metadata.remove(key)
    }

    pub fn get_all_data(&self) -> &HashMap<String, Value> {
        &self.data
    }

    pub fn get_all_metadata(&self) -> &HashMap<String, Value> {
        &self.metadata
    }

    pub fn merge(&mut self, other: &Context) {
        for (key, value) in &other.data {
            self.data.insert(key.clone(), value.clone());
        }
        for (key, value) in &other.metadata {
            self.metadata.insert(key.clone(), value.clone());
        }
    }

    pub fn clear(&mut self) {
        self.data.clear();
        self.metadata.clear();
    }

    pub fn contains_key(&self, key: &str) -> bool {
        self.data.contains_key(key)
    }

    pub fn contains_metadata_key(&self, key: &str) -> bool {
        self.metadata.contains_key(key)
    }
}

impl fmt::Display for Context {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        writeln!(f, "Context {{")?;

        // Display data
        writeln!(f, "  data: {{")?;
        for (key, value) in &self.data {
            writeln!(f, "    \"{}\": {},", key, value)?;
        }
        writeln!(f, "  }},")?;

        // Display metadata
        writeln!(f, "  metadata: {{")?;
        for (key, value) in &self.metadata {
            writeln!(f, "    \"{}\": {},", key, value)?;
        }
        writeln!(f, "  }}")?;

        write!(f, "}}")
    }
}

impl From<HashMap<String, Value>> for Context {
    fn from(data: HashMap<String, Value>) -> Self {
        Self::from_data(data)
    }
}
</file>

<file path="src/flow.rs">
use crate::{
    context::Context,
    node::{Node, ProcessState},
};
use anyhow::Result;
use serde_json::Value;
use std::collections::HashMap;
use std::sync::Arc;
use tracing::info;

pub struct Flow<S: ProcessState + Default> {
    nodes: HashMap<String, Arc<dyn Node<State = S>>>,
    edges: HashMap<String, Vec<(String, String)>>, // (to_node, condition)
    start_node: String,
}

impl<S: ProcessState + Default> Flow<S> {
    pub fn new(start_node_name: &str, start_node: Arc<dyn Node<State = S>>) -> Self {
        let mut nodes = HashMap::new();
        nodes.insert(start_node_name.to_string(), start_node);

        Self {
            nodes,
            edges: HashMap::new(),
            start_node: start_node_name.to_string(),
        }
    }

    pub fn add_node(&mut self, name: &str, node: Arc<dyn Node<State = S>>) {
        self.nodes.insert(name.to_string(), node);
    }

    pub fn add_edge(&mut self, from: &str, to: &str, condition: S) {
        self.edges
            .entry(from.to_string())
            .or_default()
            .push((to.to_string(), condition.to_condition()));
    }

    pub async fn run(&self, mut context: Context) -> Result<Value> {
        let mut current_node = self.start_node.clone();

        while let Some(node) = self.nodes.get(&current_node) {
            // Prepare
            info!("Preparing node: {}", current_node);
            node.prepare(&mut context).await?;

            // Execute
            info!("Executing node: {}", current_node);
            let result = node.execute(&context).await;

            // Post process
            info!("Post processing node: {}", current_node);
            let process_result = node.post_process(&mut context, &result).await?;

            // Find next node based on the state returned by post_process
            if let Some(edges) = self.edges.get(&current_node) {
                // Get the condition from the node state
                let condition = process_result.state.to_condition();

                // Try to find an edge matching the condition
                let next_node_info = edges
                    .iter()
                    .find(|(_, edge_condition)| edge_condition == &condition);

                if let Some((next, _)) = next_node_info {
                    current_node = next.clone();
                } else {
                    // If no matching edge found, try the default condition
                    let default_edge = edges
                        .iter()
                        .find(|(_, edge_condition)| edge_condition == "default");

                    if let Some((next, _)) = default_edge {
                        current_node = next.clone();
                    } else {
                        info!(
                            "No edge found for node '{}' with condition '{}'. Stopping flow.",
                            current_node, condition
                        );
                        break;
                    }
                }
            } else {
                info!(
                    "Node '{}' has no outgoing edges. Stopping flow.",
                    current_node
                );
                break;
            }
        }

        Ok(context.get("result").unwrap_or(&Value::Null).clone())
    }
}

#[allow(dead_code)]
pub struct BatchFlow<S: ProcessState + Default> {
    flow: Flow<S>,
    batch_size: usize,
}

impl<S: ProcessState + Default> BatchFlow<S> {
    pub fn new(
        start_node_name: &str,
        start_node: Arc<dyn Node<State = S>>,
        batch_size: usize,
    ) -> Self {
        Self {
            flow: Flow::new(start_node_name, start_node),
            batch_size,
        }
    }

    pub async fn run_batch(&self, contexts: Vec<Context>) -> Result<()> {
        info!(
            "Starting batch flow execution with {} items",
            contexts.len()
        );

        for context in contexts {
            self.flow.run(context).await?;
        }

        info!("Batch flow execution completed");
        Ok(())
    }
}

#[macro_export]
macro_rules! build_flow {
    (start: ($name: expr, $node:expr)) => {{
        $crate::flow::Flow::new($name, std::sync::Arc::new($node))
    }};

    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?]
    ) => {{
        let mut g = $crate::flow::Flow::new($start_name, std::sync::Arc::new($start_node));
        $(
            g.add_node($name, std::sync::Arc::new($node));
        )*
        g
    }};

    // Complete version with proper-edge handling
    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?],
        edges: [
            $($edge:tt),* $(,)?
        ]
    ) => {{
        let mut g = $crate::flow::Flow::new($start_name, std::sync::Arc::new($start_node));
        // Add all nodes first
        $(
            g.add_node($name, std::sync::Arc::new($node));
        )*
        // Handle edges appropriately
        $(
            build_flow!(@edge g, $edge);
        )*
        g
    }};


    (@edge $g:expr, ($from:expr, $to:expr, $condition:expr)) => {
        $g.add_edge($from, $to, $condition);
    };
}

#[macro_export]
macro_rules! build_batch_flow {
    (start: ($name: expr, $node:expr), batch_size: $batch_size:expr) => {{
        BatchFlow::new($name, std::sync::Arc::new($node), $batch_size)
    }};

    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?],
        batch_size: $batch_size:expr
    ) => {{
        let mut g = BatchFlow::new($start_name, std::sync::Arc::new($start_node), $batch_size);
        $(
            g.flow.add_node($name, std::sync::Arc::new($node));
        )*
        g
    }};

    // Complete version with proper-edge handling
    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?],
        edges: [
            $($edge:tt),* $(,)?
        ],
        batch_size: $batch_size:expr
    ) => {{
        let mut g = BatchFlow::new($start_name, std::sync::Arc::new($start_node), $batch_size);
        // Add all nodes first
        $(
            g.flow.add_node($name, std::sync::Arc::new($node));
        )*
        // Handle edges appropriately
        $(
            build_flow!(@edge g.flow, $edge);
        )*
        g
    }};
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::node::{Node, ProcessResult, ProcessState};
    use async_trait::async_trait;
    use serde_json::json;

    #[derive(Debug, Clone, PartialEq)]
    #[allow(dead_code)]
    #[derive(Default)]
    enum CustomState {
        Success,
        Failure,
        #[default]
        Default,
    }

    impl ProcessState for CustomState {
        fn is_default(&self) -> bool {
            matches!(self, CustomState::Default)
        }

        fn to_condition(&self) -> String {
            match self {
                CustomState::Success => "success".to_string(),
                CustomState::Failure => "failure".to_string(),
                CustomState::Default => "default".to_string(),
            }
        }
    }

    struct TestNode {
        result: Value,
        state: CustomState,
    }

    impl TestNode {
        fn new(result: Value, state: CustomState) -> Self {
            Self { result, state }
        }
    }

    #[async_trait]
    impl Node for TestNode {
        type State = CustomState;

        async fn execute(&self, _context: &Context) -> Result<Value> {
            Ok(self.result.clone())
        }

        async fn post_process(
            &self,
            context: &mut Context,
            result: &Result<Value>,
        ) -> Result<ProcessResult<CustomState>> {
            match result {
                Ok(value) => {
                    context.set("result", value.clone());
                    Ok(ProcessResult::new(self.state.clone(), "test".to_string()))
                }
                Err(e) => {
                    context.set("error", json!(e.to_string()));
                    Ok(ProcessResult::new(CustomState::Default, e.to_string()))
                }
            }
        }
    }

    #[tokio::test]
    async fn test_flow_with_custom_state() {
        let node1 = Arc::new(TestNode::new(
            json!({"data": "test1"}),
            CustomState::Success,
        ));
        let node2 = Arc::new(TestNode::new(
            json!({"data": "test2"}),
            CustomState::Default,
        ));
        let end_node = Arc::new(TestNode::new(
            json!({"final_result": "finished"}),
            CustomState::Default,
        ));

        let mut flow = Flow::<CustomState>::new("start", node1);
        flow.add_node("next", node2);
        flow.add_node("end", end_node);

        flow.add_edge("start", "next", CustomState::Success);
        flow.add_edge("next", "end", CustomState::Default);

        let context = Context::new();
        let result = flow.run(context).await.unwrap();

        assert_eq!(result, json!({"final_result": "finished"}));
    }

    #[tokio::test]
    async fn test_batch_flow() {
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let node2 = TestNode::new(json!({"data": "test2"}), CustomState::Default);

        let mut batch_flow = BatchFlow::<CustomState>::new("start", Arc::new(node1), 10);
        batch_flow.flow.add_node("next", Arc::new(node2));
        batch_flow
            .flow
            .add_edge("start", "next", CustomState::Success);
        batch_flow
            .flow
            .add_edge("next", "end", CustomState::Default);

        let contexts = vec![Context::new(), Context::new()];
        batch_flow.run_batch(contexts).await.unwrap();
    }

    #[tokio::test]
    async fn test_build_flow_macro() {
        // Test basic flow with start node only
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let flow1 = build_flow!(
            start: ("start", node1)
        );
        let context = Context::new();
        let result = flow1.run(context).await.unwrap();
        assert_eq!(result, json!({"data": "test1"}));

        // Test flow with multiple nodes
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let node2 = TestNode::new(json!({"data": "test2"}), CustomState::Default);
        let end_node = TestNode::new(json!({"final_result": "finished"}), CustomState::Default);
        let flow2 = build_flow!(
            start: ("start", node1),
            nodes: [("next", node2), ("end", end_node)],
            edges: [
                ("start", "next", CustomState::Success),
                ("next", "end", CustomState::Default)
            ]
        );
        let context = Context::new();
        let result = flow2.run(context).await.unwrap();
        assert_eq!(result, json!({"final_result": "finished"}));

        // Test flow with default edges
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let node2 = TestNode::new(json!({"data": "test2"}), CustomState::Default);
        let flow3 = build_flow!(
            start: ("start", node1),
            nodes: [("next", node2)],
            edges: [
                ("start", "next", CustomState::Default)
            ]
        );
        let context = Context::new();
        let result = flow3.run(context).await.unwrap();
        assert_eq!(result, json!({"data": "test2"}));
    }
}
</file>

<file path="src/lib.rs">
pub mod context;
pub mod flow;
pub mod node;
pub mod utils;

pub use context::Context;
pub use flow::*;
pub use node::*;
pub use utils::*;

pub type Params = std::collections::HashMap<String, serde_json::Value>;
</file>

<file path="src/node.rs">
use crate::{Params, context::Context};
use anyhow::Result;
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;

pub trait ProcessState: Send + Sync {
    fn is_default(&self) -> bool;
    fn to_condition(&self) -> String;
}

#[derive(Debug, Clone, PartialEq, Default)]
pub enum BaseState {
    Success,
    Failure,
    #[default]
    Default,
}

impl ProcessState for BaseState {
    fn is_default(&self) -> bool {
        matches!(self, BaseState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            BaseState::Success => "success".to_string(),
            BaseState::Failure => "failure".to_string(),
            BaseState::Default => "default".to_string(),
        }
    }
}

#[derive(Debug, Clone, PartialEq)]
pub struct ProcessResult<S: ProcessState> {
    pub state: S,
    pub message: String,
}

impl<S: ProcessState> ProcessResult<S> {
    pub fn new(state: S, message: String) -> Self {
        Self { state, message }
    }
}

impl<S: ProcessState + Default> Default for ProcessResult<S> {
    fn default() -> Self {
        Self {
            state: S::default(),
            message: "default".to_string(),
        }
    }
}

#[async_trait]
pub trait Node: Send + Sync {
    type State: ProcessState + Default;

    #[allow(unused_variables)]
    async fn prepare(&self, context: &mut Context) -> Result<()> {
        Ok(())
    }

    async fn execute(&self, context: &Context) -> Result<serde_json::Value>;

    #[allow(unused_variables)]
    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<serde_json::Value>,
    ) -> Result<ProcessResult<Self::State>> {
        match result {
            Ok(value) => {
                context.set("result", value.clone());
                Ok(ProcessResult::default())
            }
            Err(e) => {
                context.set("error", serde_json::Value::String(e.to_string()));
                Ok(ProcessResult::new(Self::State::default(), e.to_string()))
            }
        }
    }
}

pub trait BaseNodeTrait: Node<State = BaseState> {}

#[allow(dead_code)]
pub struct BaseNode {
    params: Params,
    next_nodes: HashMap<String, Arc<dyn BaseNodeTrait>>,
}

impl BaseNode {
    pub fn new(params: Params) -> Self {
        Self {
            params,
            next_nodes: HashMap::new(),
        }
    }

    pub fn add_next(&mut self, action: String, node: Arc<dyn BaseNodeTrait>) {
        self.next_nodes.insert(action, node);
    }
}

#[async_trait]
impl Node for BaseNode {
    type State = BaseState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<serde_json::Value> {
        Ok(serde_json::Value::Null)
    }
}

impl BaseNodeTrait for BaseNode {}

#[allow(dead_code)]
pub struct BatchNode {
    base: BaseNode,
    batch_size: usize,
}

impl BatchNode {
    pub fn new(params: Params, batch_size: usize) -> Self {
        Self {
            base: BaseNode::new(params),
            batch_size,
        }
    }
}

#[async_trait]
impl Node for BatchNode {
    type State = BaseState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<serde_json::Value> {
        Ok(serde_json::Value::Null)
    }
}

impl BaseNodeTrait for BatchNode {}
</file>

<file path=".gitignore">
/target
.idea/

target
.env

Cargo.lock
.vscode/
</file>

<file path="Cargo.toml">
[package]
name = "pocketflow_rs"
version = "0.1.0"
edition = "2024"
description = "PocketFlow implemented by rust"
authors = ["Yan Lu <luyanfcp@gmail.com>"]
license = "MIT"

[lib]
name = "pocketflow_rs"
path = "src/lib.rs"

[[example]]
name = "basic"
path = "examples/basic.rs"

[workspace]
members = [ 
    "examples/pocketflow-rs-rag",
    "examples/text2sql"
]

[dependencies]
anyhow = "1.0"
async-trait = "0.1"
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
thiserror = "1.0"
tracing = "0.1"
rand = "0.8"
openai_api_rust = { version = "0.1.9", optional = true}
regex = "1.11.1"
qdrant-client = {version = "1.14.0", optional = true}
reqwest = { version = "0.12", features = ["json"], optional = true }

[features]
openai = ["dep:openai_api_rust"]
websearch = ["dep:reqwest"]
qdrant = ["dep:qdrant-client"]
debug = []
default = [
    "openai",
]
</file>

<file path="README.md">
<div align="center">
  <img src="./static/pocketflow_rust_title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="400"/>
</div>

A Rust implementation of [PocketFlow](https://github.com/The-Pocket/PocketFlow), a minimalist flow-based programming framework.

ðŸ“‹ [Get started quickly with our template â†’](#template)

## Features

- Type-safe state transitions using enums
- Macro-based flow construction
- Async node execution and post-processing
- Batch flow support
- Custom state management
- Extensible node system

## Quick Start

### 0. Setup

```bash
cargo add pocketflow_rs
```

### 1. Define Custom States

```rust
use pocketflow_rs::ProcessState;

#[derive(Debug, Clone, PartialEq)]
pub enum MyState {
    Success,
    Failure,
    Default,
}

impl ProcessState for MyState {
    fn is_default(&self) -> bool {
        matches!(self, MyState::Default)
    }
    fn to_condition(&self) -> String {
        match self {
            MyState::Success => "success".to_string(),
            MyState::Failure => "failure".to_string(),
            MyState::Default => "default".to_string(),
        }
    }
}

impl Default for MyState {
    fn default() -> Self {
        MyState::Default
    }
}
```

### 2. Implement Nodes

```rust
use pocketflow_rs::{Node, ProcessResult, Context};
use anyhow::Result;
use async_trait::async_trait;

struct MyNode;

#[async_trait]
impl Node for MyNode {
    type State = MyState;

    async fn execute(&self, context: &Context) -> Result<serde_json::Value> {
        // Your node logic here
        Ok(serde_json::json!({"data": 42}))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<serde_json::Value>,
    ) -> Result<ProcessResult<MyState>> {
        // Your post-processing logic here
        Ok(ProcessResult::new(MyState::Success, "success".to_string()))
    }
}
```

### 3. Build Flows

```rust
use pocketflow_rs::{build_flow, Context};

let node1 = MyNode;
let node2 = MyNode;

let flow = build_flow!(
    start: ("start", node1),
    nodes: [("next", node2)],
    edges: [
        ("start", "next", MyState::Success)
    ]
);

let context = Context::new();
let result = flow.run(context).await?;
```

### 4. Batch Processing

```rust
use pocketflow_rs::build_batch_flow;

let batch_flow = build_batch_flow!(
    start: ("start", node1),
    nodes: [("next", node2)],
    edges: [
        ("start", "next", MyState::Success)
    ],
    batch_size: 10
);

let contexts = vec![Context::new(); 10];
batch_flow.run_batch(contexts).await?;
```

## Advanced Usage

### Custom State Management

Define your own states to control flow transitions:

```rust
#[derive(Debug, Clone, PartialEq)]
pub enum WorkflowState {
    Initialized,
    Processing,
    Completed,
    Error,
    Default,
}

impl ProcessState for WorkflowState {
    fn is_default(&self) -> bool {
        matches!(self, WorkflowState::Default)
    }
    fn to_condition(&self) -> String {
        match self {
            WorkflowState::Initialized => "initialized".to_string(),
            WorkflowState::Processing => "processing".to_string(),
            WorkflowState::Completed => "completed".to_string(),
            WorkflowState::Error => "error".to_string(),
            WorkflowState::Default => "default".to_string(),
        }
    }
}
```

### Complex Flow Construction

Build complex workflows with multiple nodes and state transitions:

```rust
let flow = build_flow!(
    start: ("start", node1),
    nodes: [
        ("process", node2),
        ("validate", node3),
        ("complete", node4)
    ],
    edges: [
        ("start", "process", WorkflowState::Initialized),
        ("process", "validate", WorkflowState::Processing),
        ("validate", "process", WorkflowState::Error),
        ("validate", "complete", WorkflowState::Completed)
    ]
);
```

## Available Features

The following features are available: (feature for [utility_function](https://the-pocket.github.io/PocketFlow/utility_function/))

- `openai` (default): Enable OpenAI API integration for LLM capabilities
- `websearch`: Enable web search functionality using Google Custom Search API
- `qdrant`: Enable vector database integration using Qdrant
- `debug`: Enable additional debug logging and information

To use specific features, add them to your `Cargo.toml`:

```toml
[dependencies]
pocketflow_rs = { version = "0.1.0", features = ["openai", "websearch"] }
```

Or use them in the command line:

```bash
cargo add pocketflow_rs --features "openai websearch"
```

## Examples

Check out the `examples/` directory for more detailed examples:

- basic.rs: Basic flow with custom states
- text2sql: Text-to-SQL workflow example
- [pocketflow-rs-rag](./examples/pocketflow-rs-rag/README.md): Retrieval-Augmented Generation (RAG) workflow example

## Template

Fork the [PocketFlow-Template-Rust](https://github.com/The-Pocket/PocketFlow-Template-Rust) repository and use it as a template for your own project.

## License

MIT
</file>

</files>
</file>

<file path=".docs/repomix-output-The-Pocket-PocketFlow-Typescript.xml">
This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  ISSUE_TEMPLATE/
    bug_report.md
    feature_request.md
  workflows/
    ci.yml
    code-quality.yml
  PULL_REQUEST_TEMPLATE.md
cookbook/
  pocketflow-batch-flow/
    src/
      flow.ts
      main.ts
      nodes.ts
      type.ts
    package.json
    README.md
  pocketflow-batch-node/
    src/
      flow.ts
      main.ts
      nodes.ts
      types.ts
    package.json
    README.md
  pocketflow-hello-world/
    hello-world.ts
    package.json
    README.md
  pocketflow-parallel-batch-flow/
    output/
      report.txt
    src/
      flows/
        image_processing_flow.ts
      nodes/
        completion_report_node.ts
        image_processing_node.ts
        image_scanner_node.ts
        index.ts
      utils/
        index.ts
        process_image.ts
        read_directory.ts
      index.ts
      types.ts
    design.md
    package.json
    README.md
docs/
  core_abstraction/
    batch.md
    communication.md
    flow.md
    index.md
    node.md
    parallel.md
  design_pattern/
    agent.md
    index.md
    mapreduce.md
    multi_agent.md
    rag.md
    structure.md
    workflow.md
  utility_function/
    chunking.md
    embedding.md
    index.md
    llm.md
    text_to_speech.md
    vector.md
    viz.md
    websearch.md
  _config.yml
  guide.md
  index.md
src/
  index.ts
tests/
  batch-flow.test.ts
  batch-node.test.ts
  core-abstraction-examples.test.ts
  fallback.test.ts
  flow-basic.test.ts
  flow-composition.test.ts
  mapreduce-pattern.test.ts
  multi-agent-pattern.test.ts
  parallel-batch-flow.test.ts
  parallel-batch-node.test.ts
  qa-pattern.test.ts
  rag-pattern.test.ts
.cursorrules
.gitignore
.npmignore
CODE_OF_CONDUCT.md
CONTRIBUTING.md
jest.config.js
LICENSE
package.json
README.md
tsconfig.json
tsup.config.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/ISSUE_TEMPLATE/bug_report.md">
---
name: Bug Report
about: Create a report to help us improve
title: "[BUG] "
labels: bug
assignees: ""
---

## Bug Description

A clear and concise description of the bug.

## Steps To Reproduce

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

## Expected Behavior

A clear and concise description of what you expected to happen.

## Actual Behavior

A clear and concise description of what actually happened.

## Screenshots

If applicable, add screenshots to help explain your problem.

## Environment

- OS: [e.g. Windows 10, macOS 12.3, Ubuntu 22.04]
- Node.js version: [e.g. 16.14.2]
- npm/yarn version: [e.g. npm 8.5.0]
- PocketFlow version: [e.g. 1.0.0]

## Additional Context

Add any other context about the problem here.
</file>

<file path=".github/ISSUE_TEMPLATE/feature_request.md">
---
name: Feature Request
about: Suggest an idea for this project
title: "[FEATURE] "
labels: enhancement
assignees: ""
---

## Problem Statement

A clear and concise description of the problem you're experiencing or the use case this feature would address. Ex. I'm always frustrated when [...]

## Proposed Solution

A clear and concise description of what you want to happen.

## Alternative Solutions

A clear and concise description of any alternative solutions or features you've considered.

## Example Implementation

If possible, provide a sketch, mockup, or example of how this feature might work.

## Additional Context

Add any other context, references, or screenshots about the feature request here.
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  test-and-build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run tests
        run: npm test

      - name: Build
        run: npm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-${{ matrix.node-version }}
          path: dist/
</file>

<file path=".github/workflows/code-quality.yml">
name: Code Quality

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  typescript-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18.x"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: TypeScript compilation check
        run: npx tsc --noEmit
</file>

<file path=".github/PULL_REQUEST_TEMPLATE.md">
## Description

<!-- Provide a brief description of the changes made in this PR -->

## Related Issue

<!-- Link to the related issue(s) that this PR addresses -->
<!-- Use syntax: "Fixes #123" or "Resolves #123" to automatically close the issue when the PR is merged -->

## Type of Change

<!-- Please delete options that are not relevant -->

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation update
- [ ] Performance improvement
- [ ] Code cleanup or refactor

## How Has This Been Tested?

<!-- Please describe the tests that you ran to verify your changes -->
<!-- Include relevant details for your test configuration -->

## Checklist

<!-- Please check all that apply -->

- [ ] My code follows the code style of this project
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] I have updated the documentation accordingly
- [ ] My changes generate no new warnings
- [ ] Any dependent changes have been merged and published

## Screenshots (if appropriate)

<!-- Add screenshots to help explain your changes if applicable -->

## Additional Notes

<!-- Add any other information about the PR here -->
</file>

<file path="cookbook/pocketflow-batch-flow/src/flow.ts">
import { Flow, BatchFlow } from 'pocketflow';
import { loadImageNode, applyFilterNode, saveImageNode } from './nodes';
import { SharedData, FilterType } from './type';

// Types for the image processing parameters
export type ImageProcessingParams = {
  imageName: string;
  filterType: FilterType;
};

// Connect nodes to create the processing pipeline
// Each node passes its output to the next node as input
loadImageNode.on("apply_filter", applyFilterNode);
applyFilterNode.on("save", saveImageNode);

// Create the base flow for processing a single image with a specific filter
export const imageProcessingFlow = new Flow<SharedData>(loadImageNode);

// Create the batch flow for processing multiple images with different filters
export const batchImageProcessingFlow = new BatchFlow<SharedData>(loadImageNode);

// Set batch parameters in main.ts before running
</file>

<file path="cookbook/pocketflow-batch-flow/src/main.ts">
import fs from 'fs';
import path from 'path';
import { batchImageProcessingFlow, ImageProcessingParams } from './flow';
import { SharedData } from './type';

async function main() {
  // Get all image files from the images directory
  const imageDir = path.join(__dirname, 'images');
  const imageFiles = fs.readdirSync(imageDir)
    .filter(file => /\.(jpg|jpeg|png)$/i.test(file));

  console.log('PocketFlow BatchFlow Example - Image Processor');
  console.log('==============================================');
  
  // Define the filter types to use
  const filterTypes: Array<'grayscale' | 'blur' | 'sepia'> = ['grayscale', 'blur', 'sepia'];
  
  // Create a list of parameters for batch processing
  const batchParams: ImageProcessingParams[] = [];
  
  // Create combinations of images and filters
  for (const image of imageFiles) {
    for (const filter of filterTypes) {
      batchParams.push({
        imageName: image,
        filterType: filter
      });
    }
  }
  
  console.log(`\nProcessing ${imageFiles.length} images with ${filterTypes.length} filters...`);
  console.log(`Total operations: ${batchParams.length}\n`);
  
  try {
    // Define the prep method for the BatchFlow to return our batch parameters
    const originalPrep = batchImageProcessingFlow.prep;
    batchImageProcessingFlow.prep = async () => {
      return batchParams;
    };

    // Create shared context - can be used to track progress or share data between runs
    const sharedContext: SharedData = {};

    // Execute the batch flow with the shared context
    console.time('Batch processing time');
    await batchImageProcessingFlow.run(sharedContext);
    console.timeEnd('Batch processing time');
    
    // Restore original prep method
    batchImageProcessingFlow.prep = originalPrep;
    
    console.log('\nAll images processed successfully!');
    console.log('Check the \'output\' directory for results.');
    
    // Output summary of results
    console.log('\nProcessed images:');
    batchParams.forEach((params) => {
      console.log(`- ${params.imageName} with ${params.filterType} filter`);
    });
  } catch (error) {
    console.error('Error processing images:', error);
  }
}

// Run the main function
main().catch(console.error);
</file>

<file path="cookbook/pocketflow-batch-flow/src/nodes.ts">
import fs from 'fs';
import path from 'path';
import { Node } from 'pocketflow';
import sharp from 'sharp';
import { FilterType, SharedData } from './type';

// Create output directory if it doesn't exist
if (!fs.existsSync(path.join(__dirname, '../output'))) {
  fs.mkdirSync(path.join(__dirname, '../output'));
}

// Define interfaces for node parameters and results that satisfy NonIterableObject constraint
type LoadImageParams ={
  imageName: string;
}

type LoadImageResult = {
  imageBuffer: Buffer;
}

type ApplyFilterParams = {
  filterType: FilterType;
}

type SaveImageResult = {
  outputPath: string;
}

// Node to load an image
export class LoadImageNode extends Node<SharedData, LoadImageParams> {

  async prep(): Promise<string> {
    const imageName = this._params.imageName;
    return imageName;
  }

  async exec(prepRes: string): Promise<LoadImageResult> {
    console.log(`Loading image: ${prepRes}...`);
    const imagePath = path.join(__dirname, 'images', prepRes);
    const imageBuffer = fs.readFileSync(imagePath);
    
    return { imageBuffer };
  }

  async post(shared: SharedData, prepRes: string, execRes: LoadImageResult): Promise<string> {
    shared.imageBuffer = execRes.imageBuffer;
    shared.imageName = prepRes;
    return "apply_filter";
  }
}

// Node to apply a filter to an image
export class ApplyFilterNode extends Node<SharedData, ApplyFilterParams> {

  async prep(shared: SharedData): Promise<{
    filterType: FilterType;
    imageBuffer: Buffer;
  }> {
    const filterType = this._params.filterType;
    return {
      filterType,
      imageBuffer: shared.imageBuffer || Buffer.from([])
    }
  }

  async exec(prepRes: {
    filterType: FilterType;
    imageBuffer: Buffer;
  }): Promise<Buffer> {
    const filterType = prepRes.filterType;
    
    console.log(`Processing image with ${filterType} filter...`);
    let processedImage;
    switch (filterType) {
      case 'grayscale':
        processedImage = await sharp(prepRes.imageBuffer).grayscale().toBuffer();
        break;
      case 'blur':
        processedImage = await sharp(prepRes.imageBuffer).blur(10).toBuffer();
        break;
      case 'sepia':
        processedImage = await sharp(prepRes.imageBuffer)
          .modulate({ brightness: 1, saturation: 0.8 })
          .tint({ r: 255, g: 220, b: 180 })
          .toBuffer();
        break;
      default:
        throw new Error(`Unsupported filter type: ${filterType}`);
    }
    
    return processedImage;
  }

  async post(shared: SharedData, prepRes: {
    filterType: FilterType;
  }, execRes: Buffer): Promise<string> {
    shared.processedImage = execRes;
    shared.filterType = prepRes.filterType;
    return "save";
  }
}

// Node to save a processed image
export class SaveImageNode extends Node<SharedData> {

  async prep(shared: SharedData): Promise<{
    imageBuffer: Buffer;
    imageName: string;
    filterType: FilterType;
  }> {
    return {
      imageBuffer: shared.processedImage || Buffer.from([]),
      imageName: shared.imageName || "",
      filterType: shared.filterType || "grayscale"
    }
  }

  async exec(prepRes: {
    imageBuffer: Buffer;
    imageName: string;
    filterType: FilterType;
  }): Promise<SaveImageResult> {
    const outputName = `${path.parse(prepRes.imageName).name}_${prepRes.filterType}${path.parse(prepRes.imageName).ext}`;
    const outputPath = path.join(__dirname, '../output', outputName);
    
    fs.writeFileSync(outputPath, prepRes.imageBuffer);
    console.log(`Saved processed image to: ${outputPath}`);
    
    return { outputPath };
  }
}

export const loadImageNode = new LoadImageNode();
export const applyFilterNode = new ApplyFilterNode();
export const saveImageNode = new SaveImageNode();
</file>

<file path="cookbook/pocketflow-batch-flow/src/type.ts">
export type FilterType = 'grayscale' | 'blur' | 'sepia';

export interface SharedData {
  imageName?: string;
  imageBuffer?: Buffer;
  processedImage?: Buffer;
  filterType?: FilterType;
}
</file>

<file path="cookbook/pocketflow-batch-flow/package.json">
{
    "name": "pocketflow-batch-flow",
    "dependencies": {
      "pocketflow": ">=1.0.3",
      "sharp": "^0.32.6"
    },
    "devDependencies": {
      "@types/node": "^18.0.0",
      "ts-node": "^10.9.1",
      "typescript": "^5.0.0"
    },
    "scripts": {
        "main": "npx ts-node src/main.ts"
    }
}
</file>

<file path="cookbook/pocketflow-batch-flow/README.md">
# PocketFlow BatchFlow Example

This example demonstrates the BatchFlow concept in PocketFlow by implementing an image processor that applies different filters to multiple images.

## What this Example Demonstrates

- How to use BatchFlow to run a Flow multiple times with different parameters
- Key concepts of BatchFlow:
  1. Creating a base Flow for single-item processing
  2. Using BatchFlow to process multiple items with different parameters
  3. Managing parameters across multiple Flow executions

## Project Structure

```
pocketflow-batch-flow/
â”œâ”€â”€ README.md
â”œâ”€â”€ package.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ cat.jpg        # Sample image 1
â”‚   â”‚   â”œâ”€â”€ dog.jpg        # Sample image 2
â”‚   â”‚   â””â”€â”€ bird.jpg       # Sample image 3
â”‚   â”œâ”€â”€ main.ts            # Entry point
â”‚   â”œâ”€â”€ flow.ts            # Flow and BatchFlow definitions
â”‚   â””â”€â”€ nodes.ts           # Node implementations for image processing
```

## How it Works

The example processes multiple images with different filters:

1. **Base Flow**: Processes a single image

   - Load image
   - Apply filter (grayscale, blur, or sepia)
   - Save processed image

2. **BatchFlow**: Processes multiple image-filter combinations
   - Takes a list of parameters (image + filter combinations)
   - Runs the base Flow for each parameter set
   - Organizes output in a structured way

## Installation

```bash
npm install
```

## Usage

```bash
npm run main
```

## Sample Output

```
Processing images with filters...

Processing cat.jpg with grayscale filter...
Processing cat.jpg with blur filter...
Processing dog.jpg with sepia filter...
...

All images processed successfully!
Check the 'output' directory for results.
```

## Key Concepts Illustrated

1. **Parameter Management**: Shows how BatchFlow manages different parameter sets
2. **Flow Reuse**: Demonstrates running the same Flow multiple times
3. **Batch Processing**: Shows how to process multiple items efficiently
4. **Real-world Application**: Provides a practical example of batch processing
</file>

<file path="cookbook/pocketflow-batch-node/src/flow.ts">
import { Flow } from 'pocketflow';
import { csvProcessorNode, displayResultsNode } from './nodes';
import { SharedData } from './types';

// Connect nodes to create the processing pipeline
csvProcessorNode.on("display_results", displayResultsNode);

// Create and export the flow
export const csvProcessingFlow = new Flow<SharedData>(csvProcessorNode);
</file>

<file path="cookbook/pocketflow-batch-node/src/main.ts">
import path from 'path';
import { csvProcessingFlow } from './flow';
import { SharedData } from './types';
import { csvProcessorNode } from './nodes';

async function main() {
  console.log('PocketFlow BatchNode Example - CSV Processor');
  console.log('=============================================');
  
  const filePath = path.join(__dirname, '../data/sales.csv');
  console.log(`\nProcessing ${path.basename(filePath)} in chunks...`);
  
  try {
    // Create shared context with the file path
    const sharedContext: SharedData = {
      filePath
    };

    // Set parameters for the CSV processor node
    csvProcessorNode.setParams({
      filePath,
      chunkSize: 5 // Process 5 records at a time
    });

    // Execute the flow with the shared context
    console.time('Processing time');
    await csvProcessingFlow.run(sharedContext);
    console.timeEnd('Processing time');
    
    console.log('\nProcessing completed successfully!');
  } catch (error) {
    console.error('Error processing CSV:', error);
  }
}

// Run the main function
main().catch(console.error);
</file>

<file path="cookbook/pocketflow-batch-node/src/nodes.ts">
import fs from 'fs';
import path from 'path';
import { BatchNode, Node } from 'pocketflow';
import csv from 'csv-parser';
import { SalesRecord, ChunkResult, SharedData } from './types';

// Number of records per chunk
const CHUNK_SIZE = 5;

// Define parameters type for the CSV processor with index signature
type CsvProcessorParams = {
  filePath: string;
  chunkSize?: number;
}

// Node to process CSV file in batches
export class CsvProcessorNode extends BatchNode<SharedData, CsvProcessorParams> {
  async prep(shared: SharedData): Promise<SalesRecord[][]> {
    // Use filePath from params or shared context
    const filePath = this._params?.filePath || shared.filePath;
    // Use chunkSize from params or default
    const chunkSize = this._params?.chunkSize || CHUNK_SIZE;
    
    console.log(`Reading CSV file: ${filePath}`);
    
    // Read the entire CSV file
    const allRecords: SalesRecord[] = [];
    
    // Return a promise that resolves when the CSV is fully read
    return new Promise((resolve, reject) => {
      fs.createReadStream(filePath || '')
        .pipe(csv())
        .on('data', (data: SalesRecord) => {
          allRecords.push(data);
        })
        .on('end', () => {
          console.log(`Total records loaded: ${allRecords.length}`);
          
          // Split all records into chunks of specified size
          const chunks: SalesRecord[][] = [];
          for (let i = 0; i < allRecords.length; i += chunkSize) {
            chunks.push(allRecords.slice(i, i + chunkSize));
          }
          
          console.log(`Split into ${chunks.length} chunks of ~${chunkSize} records each`);
          resolve(chunks);
        })
        .on('error', (error) => {
          reject(error);
        });
    });
  }

  async exec(chunk: SalesRecord[]): Promise<ChunkResult> {
    console.log(`Processing chunk with ${chunk.length} records...`);
    
    let totalSales = 0;
    let sumForAverage = 0;
    const totalTransactions = chunk.length;
    
    // Calculate statistics for this chunk
    for (const record of chunk) {
      const saleAmount = parseFloat(record.amount);
      
      totalSales += saleAmount;
      sumForAverage += saleAmount;
    }
    
    return {
      totalSales,
      totalTransactions,
      sumForAverage
    };
  }

  async post(shared: SharedData, chunks: SalesRecord[][], results: ChunkResult[]): Promise<string> {
    console.log(`Combining results from ${results.length} chunks...`);
    
    // Store batch results in shared data
    shared.batchResults = results;
    
    // Aggregate final statistics
    const totalSales = results.reduce((sum, chunk) => sum + chunk.totalSales, 0);
    const totalTransactions = results.reduce((sum, chunk) => sum + chunk.totalTransactions, 0);
    const sumForAverage = results.reduce((sum, chunk) => sum + chunk.sumForAverage, 0);
    const averageSale = sumForAverage / totalTransactions;
    
    // Store final statistics in shared data
    shared.finalStats = {
      totalSales,
      averageSale,
      totalTransactions
    };
    
    return "display_results";
  }
}

// Node to display the final results
export class DisplayResultsNode extends Node<SharedData> {

  async prep(shared: SharedData): Promise<SharedData['finalStats']> {
    return shared.finalStats;
  }

  async exec(finalStats: SharedData['finalStats']): Promise<void> {
    // Check if finalStats exists to avoid errors
    if (!finalStats) {
      console.error("Error: Final statistics not available");
      return;
    }
    
    console.log("\nFinal Statistics:");
    console.log(`- Total Sales: $${finalStats.totalSales.toLocaleString(undefined, { 
      minimumFractionDigits: 2, 
      maximumFractionDigits: 2 
    })}`);
    console.log(`- Average Sale: $${finalStats.averageSale.toLocaleString(undefined, { 
      minimumFractionDigits: 2, 
      maximumFractionDigits: 2 
    })}`);
    console.log(`- Total Transactions: ${finalStats.totalTransactions.toLocaleString()}`);
  }
}

// Create instances of the nodes
export const csvProcessorNode = new CsvProcessorNode();
export const displayResultsNode = new DisplayResultsNode();
</file>

<file path="cookbook/pocketflow-batch-node/src/types.ts">
// Define the structure of a sales record from the CSV
export interface SalesRecord {
  date: string;
  product: string;
  amount: string;
}

// Define the analysis result for a single chunk
export interface ChunkResult {
  totalSales: number;
  totalTransactions: number;
  sumForAverage: number;
}

// Define the shared data structure for our flow
export interface SharedData {
  filePath?: string;
  batchResults?: ChunkResult[];
  finalStats?: {
    totalSales: number;
    averageSale: number;
    totalTransactions: number;
  };
}
</file>

<file path="cookbook/pocketflow-batch-node/package.json">
{
  "name": "pocketflow-batch-node",
  "dependencies": {
    "pocketflow": ">=1.0.3",
    "csv-parser": "^3.0.0",
    "fs-extra": "^11.1.1"
  },
  "devDependencies": {
    "@types/fs-extra": "^11.0.2",
    "@types/node": "^18.0.0",
    "ts-node": "^10.9.1",
    "typescript": "^5.0.0"
  },
  "scripts": {
    "main": "npx ts-node src/main.ts"
  }
}
</file>

<file path="cookbook/pocketflow-batch-node/README.md">
# PocketFlow BatchNode Example

This example demonstrates the BatchNode concept in PocketFlow by implementing a CSV processor that handles large files by processing them in chunks.

## What this Example Demonstrates

- How to use BatchNode to process large inputs in chunks
- The three key methods of BatchNode:
  1. `prep`: Splits input into chunks
  2. `exec`: Processes each chunk independently
  3. `post`: Combines results from all chunks

## Project Structure

```
pocketflow-batch-node/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales.csv      # Sample large CSV file
â”œâ”€â”€ package.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.ts            # Entry point
â”‚   â”œâ”€â”€ flow.ts            # Flow definition
â”‚   â””â”€â”€ nodes.ts           # BatchNode implementation
```

## How it Works

The example processes a large CSV file containing sales data:

1. **Chunking (prep)**: The CSV file is read and split into chunks of N rows
2. **Processing (exec)**: Each chunk is processed to calculate:
   - Total sales
   - Average sale value
   - Number of transactions
3. **Combining (post)**: Results from all chunks are aggregated into final statistics

## Installation

```bash
npm install
```

## Usage

```bash
npm run main
```

## Sample Output

```
Processing sales.csv in chunks...

Final Statistics:
- Total Sales: $999,359.04
- Average Sale: $99.94
- Total Transactions: 10,000
```

## Key Concepts Illustrated

1. **Chunk-based Processing**: Shows how BatchNode handles large inputs by breaking them into manageable pieces
2. **Independent Processing**: Demonstrates how each chunk is processed separately
3. **Result Aggregation**: Shows how individual results are combined into a final output
</file>

<file path="cookbook/pocketflow-hello-world/hello-world.ts">
import { Node, Flow } from 'pocketflow';

// Define a shared storage
type SharedStorage = { text?: string };

// Node that stores text
class StoreTextNode extends Node<SharedStorage> {
  constructor(private text: string) {
    super();
  }
  
  async prep(shared: SharedStorage): Promise<void> {
    shared.text = this.text;
  }
}

// Node that prints text
class PrintTextNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<void> {
    console.log(shared.text || "No text");
  }
}

// Run example
async function main() {
  const shared: SharedStorage = {};
  
  const storeNode = new StoreTextNode("Hello World");
  const printNode = new PrintTextNode();
  storeNode.next(printNode);
  
  const flow = new Flow(storeNode);
  await flow.run(shared);
}

main();
</file>

<file path="cookbook/pocketflow-hello-world/package.json">
{
    "name": "hello-pocketflow",
    "dependencies": {
      "pocketflow": ">=1.0.3"
    }
}
</file>

<file path="cookbook/pocketflow-hello-world/README.md">
- `npm install`
- `npx ts-node hello-world.ts`
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/output/report.txt">
===================================================
         IMAGE PROCESSING COMPLETION REPORT        
===================================================

Report generated on: 30/3/2025 ä¸Šåˆ9:31:46
Total images processed: 3

PROCESSING TIME SUMMARY:
Total processing time: 0.07 seconds
Theoretical sequential time: 0.39 seconds
Speedup factor (parallel vs sequential): 5.43x
Average processing time per task: 0.043 seconds

PROCESSED IMAGES:

- bird.jpg
  Filters applied: blur, grayscale, sepia
  Processing times:
    - blur: 0.037 seconds
    - grayscale: 0.032 seconds
    - sepia: 0.031 seconds

- cat.jpg
  Filters applied: blur, grayscale, sepia
  Processing times:
    - blur: 0.032 seconds
    - grayscale: 0.043 seconds
    - sepia: 0.043 seconds

- dog.jpg
  Filters applied: blur, grayscale, sepia
  Processing times:
    - blur: 0.066 seconds
    - grayscale: 0.052 seconds
    - sepia: 0.055 seconds

===================================================
                 END OF REPORT                     
===================================================
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/flows/image_processing_flow.ts">
import { Flow } from 'pocketflow';
import { ImageScannerNode } from '../nodes/image_scanner_node';
import { ImageProcessingNode } from '../nodes/image_processing_node';
import { CompletionReportNode } from '../nodes/completion_report_node';
import { SharedData } from '../types';

/**
 * Image Processing Flow
 * Orchestrates the entire image processing pipeline
 */
export class ImageProcessingFlow extends Flow {
  /**
   * Constructor
   * @param itemsPerBatch Number of images to process in each batch
   * @param concurrency Number of parallel batches to run
   */
  constructor(itemsPerBatch: number = 5, concurrency: number = 3) {
    // Create nodes
    const scannerNode = new ImageScannerNode();
    const processingNode = new ImageProcessingNode();
    const reportNode = new CompletionReportNode();
    
    // Configure parallel batch processing if custom values are provided
    if (itemsPerBatch !== 5 || concurrency !== 3) {
      processingNode.setParams({
        itemsPerBatch,
        concurrency
      });
    }
    
    // Connect nodes
    scannerNode.next(processingNode);
    processingNode.next(reportNode);
    
    // Create flow with the scanner node as the starting point
    super(scannerNode);
  }

  /**
   * Run the flow with initial shared data
   */
  async process(): Promise<SharedData> {
    const shared: SharedData = {
      inputImages: [],
      filters: [],
      outputFolder: '',
      processedImages: []
    };
    
    await this.run(shared);
    
    return shared;
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/completion_report_node.ts">
import { Node } from 'pocketflow';
import { SharedData } from '../types';
import fs from 'fs';
import path from 'path';

/**
 * Completion Report Node
 * Generates a report of all processed images
 */
export class CompletionReportNode extends Node<SharedData> {
  /**
   * Read processedImages from shared store
   */
  async prep(shared: SharedData): Promise<SharedData> {
    // Pass the entire shared data to include timing information
    return shared;
  }

  /**
   * Generate a summary report of all images and filters applied
   */
  async exec(shared: SharedData): Promise<string> {
    const { processedImages, startTime, endTime, processingTimes } = shared;
    
    // Calculate overall processing time
    const totalProcessingTimeMs = endTime && startTime ? endTime - startTime : 0;
    const totalProcessingTimeSec = (totalProcessingTimeMs / 1000).toFixed(2);
    
    // Calculate average processing time per image/filter
    const avgProcessingTimeMs = processingTimes && processingTimes.length > 0 
      ? processingTimes.reduce((sum, item) => sum + item.timeMs, 0) / processingTimes.length
      : 0;
    
    // Calculate theoretical sequential time (sum of all processing times)
    const sequentialTimeMs = processingTimes 
      ? processingTimes.reduce((sum, item) => sum + item.timeMs, 0) 
      : 0;
    const sequentialTimeSec = (sequentialTimeMs / 1000).toFixed(2);
    
    // Calculate speedup factor
    const speedupFactor = sequentialTimeMs > 0 && totalProcessingTimeMs > 0
      ? (sequentialTimeMs / totalProcessingTimeMs).toFixed(2) 
      : 'N/A';
    
    const reportLines = [
      '===================================================',
      '         IMAGE PROCESSING COMPLETION REPORT        ',
      '===================================================',
      '',
      `Report generated on: ${new Date().toLocaleString()}`,
      `Total images processed: ${processedImages.length}`,
      '',
      'PROCESSING TIME SUMMARY:',
      `Total processing time: ${totalProcessingTimeSec} seconds`,
      `Theoretical sequential time: ${sequentialTimeSec} seconds`,
      `Speedup factor (parallel vs sequential): ${speedupFactor}x`,
      `Average processing time per task: ${(avgProcessingTimeMs / 1000).toFixed(3)} seconds`,
      '',
      'PROCESSED IMAGES:',
      ''
    ];
    
    // Add details for each processed image
    for (const image of processedImages) {
      const imageName = path.basename(image.imagePath);
      reportLines.push(`- ${imageName}`);
      reportLines.push(`  Filters applied: ${image.appliedFilters.join(', ')}`);
      
      // Add processing times for this image
      if (processingTimes) {
        const imageTimes = processingTimes.filter(t => t.imagePath === imageName);
        if (imageTimes.length > 0) {
          reportLines.push('  Processing times:');
          for (const time of imageTimes) {
            reportLines.push(`    - ${time.filter}: ${(time.timeMs / 1000).toFixed(3)} seconds`);
          }
        }
      }
      
      reportLines.push('');
    }
    
    reportLines.push('===================================================');
    reportLines.push('                 END OF REPORT                     ');
    reportLines.push('===================================================');
    
    return reportLines.join('\n');
  }

  /**
   * Write report to output folder
   */
  async post(shared: SharedData, _: SharedData, execRes: string): Promise<string | undefined> {
    // Create output directory if it doesn't exist
    if (!fs.existsSync(shared.outputFolder)) {
      fs.mkdirSync(shared.outputFolder, { recursive: true });
    }
    
    // Write report to file
    const reportPath = path.join(shared.outputFolder, 'report.txt');
    fs.writeFileSync(reportPath, execRes);
    
    // Log summary of timing information to console
    const totalTime = shared.endTime && shared.startTime 
      ? (shared.endTime - shared.startTime) / 1000 
      : 0;
    
    console.log(`Report generated at ${reportPath}`);
    console.log(`Total processing time: ${totalTime.toFixed(2)} seconds`);
    
    if (shared.processingTimes && shared.processingTimes.length > 0) {
      const sequentialTime = shared.processingTimes.reduce((sum, item) => sum + item.timeMs, 0) / 1000;
      const speedup = (sequentialTime / totalTime).toFixed(2);
      console.log(`Sequential processing would have taken approximately ${sequentialTime.toFixed(2)} seconds`);
      console.log(`Parallel processing achieved a ${speedup}x speedup`);
    }
    
    return undefined; // End of flow
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/image_processing_node.ts">
import { ParallelBatchNode } from 'pocketflow';
import { SharedData } from '../types';
import { processImage } from '../utils';
import path from 'path';

/**
 * Image Processing Input
 * Structure for each batch item in the ParallelBatchNode
 */
interface ImageProcessingInput {
  imagePath: string;
  filter: string;
}

/**
 * Image Processing Result
 * Structure for the output of processing each image
 */
interface ImageProcessingResult {
  originalPath: string;
  processedPath: string;
  filter: string;
  success: boolean;
  processingTimeMs: number; // Time taken to process
}

/**
 * Image Processing Node
 * Uses ParallelBatchFlow to apply filters to images in parallel
 */
export class ImageProcessingNode extends ParallelBatchNode<SharedData> {
  // Set default batch parameters
  constructor() {
    super();
    this.setParams({
      itemsPerBatch: 5,
      concurrency: 3
    });
  }

  /**
   * Create a batch of image processing tasks
   * Each task consists of an image and a filter to apply
   */
  async prep(shared: SharedData): Promise<ImageProcessingInput[]> {
    const tasks: ImageProcessingInput[] = [];
    
    // For each image, create a task for each filter
    for (const imagePath of shared.inputImages) {
      for (const filter of shared.filters) {
        tasks.push({
          imagePath,
          filter
        });
      }
    }
    
    console.log(`Created ${tasks.length} image processing tasks`);
    return tasks;
  }

  /**
   * Process a single image with the specified filter
   */
  async exec(input: ImageProcessingInput): Promise<ImageProcessingResult> {
    console.log(`Processing ${path.basename(input.imagePath)} with ${input.filter} filter`);
    
    const startTime = Date.now();
    try {
      const processedPath = await processImage(input.imagePath, input.filter);
      const endTime = Date.now();
      const processingTimeMs = endTime - startTime;
      
      return {
        originalPath: input.imagePath,
        processedPath,
        filter: input.filter,
        success: true,
        processingTimeMs
      };
    } catch (error) {
      console.error(`Failed to process ${input.imagePath} with ${input.filter}:`, error);
      const endTime = Date.now();
      const processingTimeMs = endTime - startTime;
      
      return {
        originalPath: input.imagePath,
        processedPath: '',
        filter: input.filter,
        success: false,
        processingTimeMs
      };
    }
  }

  /**
   * Update the shared store with the results of all processed images
   */
  async post(
    shared: SharedData, 
    _: ImageProcessingInput[], 
    results: ImageProcessingResult[]
  ): Promise<string | undefined> {
    // Group results by original image path
    const imageMap = new Map<string, string[]>();
    
    // Record processing times and update processed images
    for (const result of results) {
      // Store processing time
      if (!shared.processingTimes) {
        shared.processingTimes = [];
      }
      
      shared.processingTimes.push({
        imagePath: path.basename(result.originalPath),
        filter: result.filter,
        timeMs: result.processingTimeMs
      });
      
      if (result.success) {
        const appliedFilters = imageMap.get(result.originalPath) || [];
        appliedFilters.push(result.filter);
        imageMap.set(result.originalPath, appliedFilters);
      }
    }
    
    // Update processedImages in shared store
    for (const [imagePath, appliedFilters] of imageMap.entries()) {
      shared.processedImages.push({
        imagePath,
        appliedFilters
      });
    }
    
    console.log(`Successfully processed ${shared.processedImages.length} images`);
    
    // Set the end time
    shared.endTime = Date.now();
    
    return 'default'; // Go to the next node
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/image_scanner_node.ts">
import { Node } from 'pocketflow';
import { SharedData } from '../types';
import { readDirectory } from '../utils';
import path from 'path';

/**
 * Image Scanner Node
 * Scans the src/images directory to find all input images
 */
export class ImageScannerNode extends Node<SharedData> {
  /**
   * Initialize empty inputImages array in shared store
   */
  async prep(shared: SharedData): Promise<null> {
    shared.inputImages = [];
    // Initialize timing data
    shared.startTime = Date.now();
    shared.processingTimes = [];
    return null;
  }

  /**
   * Read all files from src/images directory, filter for image files
   */
  async exec(_: null): Promise<string[]> {
    const imagesPath = path.join(process.cwd(), 'src', 'images');
    console.log(`Scanning for images in ${imagesPath}`);
    return readDirectory(imagesPath);
  }

  /**
   * Write image paths to inputImages in shared store and initialize filters array
   */
  async post(shared: SharedData, _: null, execRes: string[]): Promise<string | undefined> {
    shared.inputImages = execRes;
    shared.filters = ['blur', 'grayscale', 'sepia'];
    shared.outputFolder = path.join(process.cwd(), 'output');
    shared.processedImages = [];
    
    console.log(`Found ${shared.inputImages.length} images to process`);
    
    return 'default'; // Go to the next node
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/index.ts">
export { ImageScannerNode } from './image_scanner_node';
export { ImageProcessingNode } from './image_processing_node';
export { CompletionReportNode } from './completion_report_node';
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/utils/index.ts">
export { readDirectory } from './read_directory';
export { processImage } from './process_image';
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/utils/process_image.ts">
import fs from 'fs';
import path from 'path';
import sharp from 'sharp';

/**
 * Applies a filter to an image
 * @param imagePath Path to the image file
 * @param filter Filter to apply (blur, grayscale, sepia)
 * @returns Path to the processed image
 */
export async function processImage(imagePath: string, filter: string): Promise<string> {
  try {
    // Create a sharp instance with the input image
    let sharpImage = sharp(imagePath);
    
    // Apply the filter
    switch (filter) {
      case 'blur':
        sharpImage = sharpImage.blur(5);
        break;
      case 'grayscale':
        sharpImage = sharpImage.grayscale();
        break;
      case 'sepia':
        // Sepia is implemented using a color matrix
        sharpImage = sharpImage.recomb([
          [0.393, 0.769, 0.189],
          [0.349, 0.686, 0.168],
          [0.272, 0.534, 0.131]
        ]);
        break;
      default:
        console.warn(`Unknown filter: ${filter}`);
    }
    
    // Generate output path
    const { dir, name, ext } = path.parse(imagePath);
    const outputDir = path.join(process.cwd(), 'output');
    
    // Create output directory if it doesn't exist
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }
    
    const outputPath = path.join(outputDir, `${name}_${filter}${ext}`);
    
    // Save the processed image
    await sharpImage.toFile(outputPath);
    
    return outputPath;
  } catch (error) {
    console.error(`Error processing image ${imagePath} with filter ${filter}:`, error);
    throw error;
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/utils/read_directory.ts">
import fs from 'fs';
import path from 'path';

/**
 * Reads all files from a directory
 * @param directoryPath Path to the directory to read
 * @returns Array of file paths
 */
export function readDirectory(directoryPath: string): string[] {
  try {
    const files = fs.readdirSync(directoryPath);
    // Filter for image files (jpg, png, jpeg, gif)
    return files
      .filter(file => 
        /\.(jpg|jpeg|png|gif)$/i.test(file))
      .map(file => path.join(directoryPath, file));
  } catch (error) {
    console.error(`Error reading directory ${directoryPath}:`, error);
    return [];
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/index.ts">
import { ImageProcessingFlow } from './flows/image_processing_flow';
import fs from 'fs';
import path from 'path';

/**
 * Main function to run the image filter application
 */
async function main() {
  console.log("Starting Image Filter Application...");
  
  // Ensure the images directory exists
  const imagesDir = path.join(process.cwd(), 'src', 'images');
  if (!fs.existsSync(imagesDir)) {
    fs.mkdirSync(imagesDir, { recursive: true });
    console.log(`Created images directory at: ${imagesDir}`);
    console.log("Please add some image files to this directory and run the application again.");
    return;
  }
  
  // Check if there are any images in the directory
  const files = fs.readdirSync(imagesDir);
  const imageFiles = files.filter(file => /\.(jpg|jpeg|png|gif)$/i.test(file));
  
  if (imageFiles.length === 0) {
    console.log("No image files found in the images directory.");
    console.log("Please add some image files to the src/images directory and run the application again.");
    return;
  }
  
  console.log("--------------------------------------------------");
  console.log("Starting parallel image processing...");
  console.log("--------------------------------------------------");
  
  // Create and run the image processing flow
  try {
    // Start time for overall application
    const appStartTime = Date.now();
    
    // 5 images per batch, 3 parallel batches
    const processingFlow = new ImageProcessingFlow(5, 3);
    const result = await processingFlow.process();
    
    // End time for overall application
    const appEndTime = Date.now();
    const totalAppTime = (appEndTime - appStartTime) / 1000;
    
    console.log("--------------------------------------------------");
    console.log("Image processing completed successfully!");
    console.log(`Processed ${imageFiles.length} images with 3 filters (blur, grayscale, sepia).`);
    console.log(`Output files and report can be found in the 'output' directory.`);
    console.log("--------------------------------------------------");
    console.log(`Total application time: ${totalAppTime.toFixed(2)} seconds`);
    console.log("--------------------------------------------------");
  } catch (error) {
    console.error("Error processing images:", error);
  }
}

// Run the main function
main().catch(console.error);
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/types.ts">
/**
 * Interface for the shared memory data structure
 */
export interface SharedData {
  // Input data
  inputImages: string[]; // Paths to input images

  // Processing data
  filters: string[]; // List of filters to apply (blur, grayscale, sepia)

  // Output data
  outputFolder: string; // Path to output folder
  processedImages: {
    // Tracking processed images
    imagePath: string;
    appliedFilters: string[];
  }[];

  // Timing data
  startTime?: number; // Start time in milliseconds
  endTime?: number; // End time in milliseconds
  processingTimes?: {
    // Individual processing times for each image/filter combination
    imagePath: string;
    filter: string;
    timeMs: number;
  }[];
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/design.md">
# Design Doc: Image Filter Application

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

1. The application must process multiple images located in the src/images directory
2. Three filters must be applied to each image: blur, grayscale, and sepia
3. Each image must be processed with all three filters, generating three output images
4. All processed images must be saved in an output folder with naming pattern originalName_filterName
5. Image processing must be parallelized using ParallelBatchFlow for efficiency
6. The application must generate a report of all processed images
7. The application must be compatible with Node.js environments

**User Stories:**

- As a user, I want to process multiple images with different filters so I can choose the best visual effect
- As a user, I want the processing to happen in parallel to save time
- As a user, I want processed images to be named consistently so I can easily identify them
- As a user, I want a report of all processed images to track what was done
- As a user, I want the application to run in any Node.js environment without special dependencies

## Technology Stack

- **Node.js**: Runtime environment
- **TypeScript**: Programming language
- **PocketFlow**: Framework for parallel processing
- **Sharp**: High-performance image processing library for Node.js

## Flow Design

> Notes for AI:
>
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

ParallelBatchFlow is perfect for this use case because:

1. Each image needs the same set of filters applied (consistent workload)
2. The filter operations are independent and can run in parallel
3. Batch processing provides efficient resource utilization

### Flow high-level Design:

1. **Image Scanner Node**: Finds all images in src/images directory
2. **Image Processing Node**: Uses ParallelBatchFlow to apply filters to images
3. **Completion Report Node**: Generates a report of all processed images

```mermaid
flowchart TD
    scanNode[Image Scanner Node] --> processNode[Image Processing Node]
    processNode --> reportNode[Completion Report Node]

    subgraph processNode[Image Processing Node - ParallelBatchFlow]
        subgraph batch1[Batch 1]
            image1[Image 1] --> filter1_1[Apply Filters]
            image2[Image 2] --> filter1_2[Apply Filters]
        end
        subgraph batch2[Batch 2]
            image3[Image 3] --> filter2_1[Apply Filters]
            image4[Image 4] --> filter2_2[Apply Filters]
        end
    end
```

## Utility Functions

> Notes for AI:
>
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1. **Read Directory** (`src/utils/read_directory.ts`)

   - _Input_: directoryPath (string)
   - _Output_: files (string[])
   - _Implementation_: Uses Node.js fs module to read directory and filter for image files
   - Used by Image Scanner Node to get all image files

2. **Image Processing** (`src/utils/process_image.ts`)
   - _Input_: imagePath (string), filter (string)
   - _Output_: processedImagePath (string)
   - _Implementation_: Uses Sharp library to apply filters to images
   - Filter implementations:
     - Blur: Uses Sharp's blur function with radius 5
     - Grayscale: Uses Sharp's built-in grayscale function
     - Sepia: Uses Sharp's recomb function with a sepia color matrix
   - Used by Image Processing Node to apply filters to images

## Node Design

### Shared Memory

> Notes for AI: Try to minimize data redundancy

The shared memory structure is organized as follows:

```typescript
interface SharedData {
  // Input data
  inputImages: string[]; // Paths to input images

  // Processing data
  filters: string[]; // List of filters to apply (blur, grayscale, sepia)

  // Output data
  outputFolder: string; // Path to output folder
  processedImages: {
    // Tracking processed images
    imagePath: string;
    appliedFilters: string[];
  }[];
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1. Image Scanner Node

- _Purpose_: Scan the src/images directory to find all input images
- _Type_: Regular
- _Steps_:
  - _prep_: Initialize empty inputImages array in shared store
  - _exec_: Read all files from src/images directory, filter for image files
  - _post_: Write image paths to inputImages in shared store and initialize filters array with ["blur", "grayscale", "sepia"]

2. Image Processing Node

- _Purpose_: Apply filters to images in parallel
- _Type_: ParallelBatchFlow
- _Batch Configuration_:
  - _itemsPerBatch_: 5 (process 5 images at a time)
  - _concurrency_: 3 (run 3 parallel batches)
- _Sub-flow_:
  - Apply Filter Node (for each image)
    - _Type_: Batch
    - _Input_: Single image path and array of filters
    - _Steps_:
      - _prep_: Load the image data using Sharp
      - _exec_: For each filter, apply the filter to the image using Sharp's API
      - _post_: Save processed images to output folder with naming pattern originalName_filterName

3. Completion Report Node

- _Purpose_: Generate a report of all processed images
- _Type_: Regular
- _Steps_:
  - _prep_: Read processedImages from shared store
  - _exec_: Generate a summary report of all images and filters applied
  - _post_: Write report to output folder
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/package.json">
{
  "name": "image-filter-app",
  "scripts": {
    "start": "npx ts-node src/index.ts"
  },
  "dependencies": {
    "sharp": "^0.32.1",
    "pocketflow": "^1.0.0"
  },
  "devDependencies": {
    "@types/node": "^14.18.63",
    "typescript": "^4.9.5"
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/README.md">
# Parallel Image Processor

Demonstrates how AsyncParallelBatchFlow processes multiple images with multiple filters >8x faster than sequential processing.

## Features

```mermaid
graph TD
    subgraph AsyncParallelBatchFlow[Image Processing Flow]
        subgraph AsyncFlow[Per Image-Filter Flow]
            A[Load Image] --> B[Apply Filter]
            B --> C[Save Image]
        end
    end
```

- Processes images with multiple filters in parallel
- Applies three different filters (grayscale, blur, sepia)
- Shows significant speed improvement over sequential processing
- Manages system resources with semaphores

## Run It

```bash
npm install
npm run start
```

## Output

```
Starting Image Filter Application...
Scanning for images in C:\WorkSpace\PocketFlow-Typescript\cookbook\pocketflow-parallel-batch-flow\src\images
Found 3 images to process
Created 9 image processing tasks
Processing bird.jpg with blur filter
Processing bird.jpg with grayscale filter
Processing bird.jpg with sepia filter
Processing cat.jpg with blur filter
Processing cat.jpg with grayscale filter
Processing cat.jpg with sepia filter
Processing dog.jpg with blur filter
Processing dog.jpg with grayscale filter
Processing dog.jpg with sepia filter
Successfully processed 3 images
Report generated at report.txt
Image processing completed successfully!
Processed 3 images with 3 filters (blur, grayscale, sepia).
Output files and report can be found in the 'output' directory.
```

## Key Points

- **Sequential**: Total time = sum of all item times

  - Good for: Rate-limited APIs, maintaining order

- **Parallel**: Total time â‰ˆ longest single item time
  - Good for: I/O-bound tasks, independent operations
</file>

<file path="docs/core_abstraction/batch.md">
---
layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4
---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:

- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **array** of items to process.
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prepRes, execResList)`**: after all items are processed, receives a **list** of results (`execResList`) and returns an **Action**.

### Example: Summarize a Large File

```typescript
type SharedStorage = {
  data: string;
  summary?: string;
};

class MapSummaries extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // Chunk content into manageable pieces
    const content = shared.data;
    const chunks: string[] = [];
    const chunkSize = 10000;

    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize));
    }

    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    _: string[],
    summaries: string[]
  ): Promise<string> {
    shared.summary = summaries.join("\n");
    return "default";
  }
}

// Usage
const flow = new Flow(new MapSummaries());
await flow.run({ data: "very long text content..." });
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Example: Summarize Many Files

```typescript
type SharedStorage = {
  files: string[];
};

type FileParams = {
  filename: string;
};

class SummarizeAllFiles extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    return shared.files.map((filename) => ({ filename }));
  }
}

// Create a per-file summarization flow
const summarizeFile = new SummarizeFile();
const summarizeAllFiles = new SummarizeAllFiles(summarizeFile);

await summarizeAllFiles.run({ files: ["file1.txt", "file2.txt"] });
```

### Under the Hood

1. `prep(shared)` returns a list of param objectsâ€”e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each object and:
   - Merges it with the BatchFlow's own `params`
   - Calls `flow.run(shared)` using the merged result
3. This means the sub-Flow runs **repeatedly**, once for every param object.

---

## 3. Nested Batches

You can nest BatchFlows to handle hierarchical data processing:

```typescript
type DirectoryParams = {
  directory: string;
};

type FileParams = DirectoryParams & {
  filename: string;
};

class FileBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    const directory = this._params.directory;
    const files = await getFilesInDirectory(directory).filter((f) =>
      f.endsWith(".txt")
    );

    return files.map((filename) => ({
      directory, // Pass on directory from parent
      filename, // Add filename for this batch item
    }));
  }
}

class DirectoryBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<DirectoryParams[]> {
    return ["/path/to/dirA", "/path/to/dirB"].map((directory) => ({
      directory,
    }));
  }
}

// Process all files in all directories
const processingNode = new ProcessingNode();
const fileFlow = new FileBatchFlow(processingNode);
const dirFlow = new DirectoryBatchFlow(fileFlow);
await dirFlow.run({});
```
</file>

<file path="docs/core_abstraction/communication.md">
---
layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3
---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)**

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate _Data Schema_ from _Compute Logic_! This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](./batch.md).
     > {: .best-practice }

2. **Params (only for [Batch](./batch.md))**
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:

```typescript
interface SharedStore {
  data: Record<string, unknown>;
  summary: Record<string, unknown>;
  config: Record<string, unknown>;
  // ...other properties
}

const shared: SharedStore = { data: {}, summary: {}, config: {} /* ... */ };
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```typescript
interface SharedStore {
  data: string;
  summary: string;
}

class LoadData extends Node<SharedStore> {
  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write data to shared store
    shared.data = "Some text content";
    return "default";
  }
}

class Summarize extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<unknown> {
    // We read data from shared store
    return shared.data;
  }

  async exec(prepRes: unknown): Promise<unknown> {
    // Call LLM to summarize
    const prompt = `Summarize: ${prepRes}`;
    const summary = await callLlm(prompt);
    return summary;
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write summary to shared store
    shared.summary = execRes as string;
    return "default";
  }
}

const loadData = new LoadData();
const summarize = new Summarize();
loadData.next(summarize);
const flow = new Flow(loadData);

const shared: SharedStore = { data: "", summary: "" };
flow.run(shared);
```

Here:

- `LoadData` writes to `shared.data`.
- `Summarize` reads from `shared.data`, summarizes, and writes to `shared.summary`.

---

## 2. Params

**Params** let you store _per-Node_ or _per-Flow_ config that doesn't need to live in the shared store. They are:

- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `setParams()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow.
>
> If you need to set child node params, see [Batch](./batch.md).
> {: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```typescript
interface SharedStore {
  data: Record<string, string>;
  summary: Record<string, string>;
}

interface SummarizeParams {
  filename: string;
}

// 1) Create a Node that uses params
class SummarizeFile extends Node<SharedStore, SummarizeParams> {
  async prep(shared: SharedStore): Promise<unknown> {
    // Access the node's param
    const filename = this._params.filename;
    return shared.data[filename] || "";
  }

  async exec(prepRes: unknown): Promise<unknown> {
    const prompt = `Summarize: ${prepRes}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    const filename = this._params.filename;
    shared.summary[filename] = execRes as string;
    return "default";
  }
}

// 2) Set params
const node = new SummarizeFile();

// 3) Set Node params directly (for testing)
node.setParams({ filename: "doc1.txt" });
node.run(shared);

// 4) Create Flow
const flow = new Flow<SharedStore>(node);

// 5) Set Flow params (overwrites node params)
flow.setParams({ filename: "doc2.txt" });
flow.run(shared); // The node summarizes doc2, not doc1
```
</file>

<file path="docs/core_abstraction/flow.md">
---
layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2
---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `nodeA.next(nodeB)`
   This means if `nodeA.post()` returns `"default"`, go to `nodeB`.
   (Equivalent to `nodeA.on("default", nodeB)`)

2. **Named action transition**: `nodeA.on("actionName", nodeB)`
   This means if `nodeA.post()` returns `"actionName"`, go to `nodeB`.

It's possible to create loops, branching, or multi-step flows.

## 2. Method Chaining

The transition methods support **chaining** for more concise flow creation:

### Chaining `on()` Methods

The `on()` method returns the current node, so you can chain multiple action definitions:

```typescript
// All transitions from the same node
nodeA.on("approved", nodeB).on("rejected", nodeC).on("needs_review", nodeD);

// Equivalent to:
nodeA.on("approved", nodeB);
nodeA.on("rejected", nodeC);
nodeA.on("needs_review", nodeD);
```

### Chaining `next()` Methods

The `next()` method returns the target node, allowing you to create linear sequences in a single expression:

```typescript
// Creates a linear A â†’ B â†’ C â†’ D sequence
nodeA.next(nodeB).next(nodeC).next(nodeD);

// Equivalent to:
nodeA.next(nodeB);
nodeB.next(nodeC);
nodeC.next(nodeD);
```

### Combining Chain Types

You can combine both chaining styles for complex flows:

```typescript
nodeA
  .on("action1", nodeB.next(nodeC).next(nodeD))
  .on("action2", nodeE.on("success", nodeF).on("failure", nodeG));
```

## 3. Creating a Flow

A **Flow** begins with a **start** node. You call `const flow = new Flow(someNode)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```typescript
nodeA.next(nodeB);
const flow = new Flow(nodeA);
flow.run(shared);
```

- When you run the flow, it executes `nodeA`.
- Suppose `nodeA.post()` returns `"default"`.
- The flow then sees `"default"` Action is linked to `nodeB` and runs `nodeB`.
- `nodeB.post()` returns `"default"` but we didn't define a successor for `nodeB`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```typescript
// Define the flow connections
review.on("approved", payment); // If approved, process payment
review.on("needs_revision", revise); // If needs changes, go to revision
review.on("rejected", finish); // If rejected, finish the process

revise.next(review); // After revision, go back for another review
payment.next(finish); // After payment, finish the process

const flow = new Flow(review);
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action.
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
>
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
{: .warning }

## 4. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.
2. Combine multiple smaller Flows into a larger Flow for reuse.
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `undefined` for `execRes` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```typescript
// Create a sub-flow
nodeA.next(nodeB);
const subflow = new Flow(nodeA);

// Connect it to another node
subflow.next(nodeC);

// Create the parent flow
const parentFlow = new Flow(subflow);
```

When `parentFlow.run()` executes:

1. It starts `subflow`
2. `subflow` runs through its nodes (`nodeA->nodeB`)
3. After `subflow` completes, execution continues to `nodeC`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation);
const paymentFlow = new Flow(validatePayment);

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory);
const inventoryFlow = new Flow(checkStock);

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup);
const shippingFlow = new Flow(createLabel);

// Connect the flows into a main order pipeline
paymentFlow.next(inventoryFlow).next(shippingFlow);

// Create the master flow
const orderPipeline = new Flow(paymentFlow);

// Run the entire pipeline
orderPipeline.run(sharedData);
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```
</file>

<file path="docs/core_abstraction/index.md">
---
layout: default
title: "Core Abstraction"
nav_order: 2
has_children: true
---
</file>

<file path="docs/core_abstraction/node.md">
---
layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1
---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`

   - **Read and preprocess data** from `shared` store.
   - Examples: _query DB, read files, or serialize data into a string_.
   - Return `prepRes`, which is used by `exec()` and `post()`.

2. `exec(prepRes)`

   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: _(mostly) LLM calls, remote APIs, tool use_.
   - âš ï¸ This shall be only for compute and **NOT** access `shared`.
   - âš ï¸ If retries enabled, ensure idempotent implementation.
   - Return `execRes`, which is passed to `post()`.

3. `post(shared, prepRes, execRes)`
   - **Postprocess and write data** back to `shared`.
   - Examples: _update DB, change states, log results_.
   - **Decide the next action** by returning a _string_ (`action = "default"` if _None_).

> **Why 3 steps?** To enforce the principle of _separation of concerns_. The data storage and data processing are operated separately.
>
> All steps are _optional_. E.g., you can only implement `prep` and `post` if you just need to process data.
> {: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting).
  `wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```typescript
const myNode = new SummarizeFile(3, 10); // maxRetries = 3, wait = 10 seconds
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `maxRetries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `this.currentRetry`.

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```typescript
execFallback(prepRes: unknown, error: Error): unknown {
  return "There was an error processing your request.";
}
```

By default, it just re-raises the exception.

### Example: Summarize file

```typescript
type SharedStore = {
  data: string;
  summary?: string;
};

class SummarizeFile extends Node<SharedStore> {
  prep(shared: SharedStore): string {
    return shared.data;
  }

  exec(content: string): string {
    if (!content) return "Empty file content";

    const prompt = `Summarize this text in 10 words: ${content}`;
    return callLlm(prompt);
  }

  execFallback(_: string, error: Error): string {
    return "There was an error processing your request.";
  }

  post(shared: SharedStore, _: string, summary: string): string | undefined {
    shared.summary = summary;
    return undefined; // "default" action
  }
}

// Example usage
const node = new SummarizeFile(3); // maxRetries = 3
const shared: SharedStore = { data: "Long text to summarize..." };
const action = node.run(shared);

console.log("Action:", action);
console.log("Summary:", shared.summary);
```
</file>

<file path="docs/core_abstraction/parallel.md">
---
layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6
---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple operations **concurrently**â€”for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute.

> Parallel nodes and flows excel at overlapping I/O-bound workâ€”like LLM calls, database queries, API requests, or file I/O. TypeScript's Promise-based implementation allows for truly concurrent execution of asynchronous operations.
> {: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
>
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism.
>
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
>   {: .best-practice }

## ParallelBatchNode

Like **BatchNode**, but runs operations in **parallel** using Promise.all():

```typescript
class TextSummarizer extends ParallelBatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // e.g., multiple texts
    return shared.texts || [];
  }

  async exec(text: string): Promise<string> {
    const prompt = `Summarize: ${text}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.summaries = execRes;
    return "default";
  }
}

const node = new TextSummarizer();
const flow = new Flow(node);
```

## ParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using Promise.all():

```typescript
class SummarizeMultipleFiles extends ParallelBatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, any>[]> {
    return (shared.files || []).map((f) => ({ filename: f }));
  }
}

const subFlow = new Flow(new LoadAndSummarizeFile());
const parallelFlow = new SummarizeMultipleFiles(subFlow);
await parallelFlow.run(shared);
```
</file>

<file path="docs/design_pattern/agent.md">
---
layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodesâ€”and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide actionâ€”for example:

```typescript
`
### CONTEXT
Task: ${task}
Previous Actions: ${prevActions}
Current State: ${state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters: query (str)

[2] answer
  Description: Conclude based on the results
  Parameters: result (str)

### NEXT ACTION
Decide the next action based on the current context.
Return your response in YAML format:

\`\`\`yaml
thinking: <reasoning process>
action: <action_name>
parameters: <parameters>
\`\`\``;
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md).

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actionsâ€”avoiding overlap like separate `read_databases` or `read_csvs`.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks instead of all at once.
- **Overview-zoom-in:** First provide high-level structure, then allow drilling into details.
- **Parameterized/Programmable:** Enable parameterized or programmable actions.
- **Backtracking:** Let the agent undo the last step instead of restarting entirely.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
interface SharedState {
  query?: string;
  context?: Array<{ term: string; result: string }>;
  search_term?: string;
  answer?: string;
}

class DecideAction extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    const context = shared.context
      ? JSON.stringify(shared.context)
      : "No previous search";
    return [shared.query || "", context];
  }

  async exec([query, context]: [string, string]): Promise<any> {
    const prompt = `
Given input: ${query}
Previous search results: ${context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
\`\`\`yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
\`\`\``;
    const resp = await callLlm(prompt);
    const yamlStr = resp.split("```yaml")[1].split("```")[0].trim();
    return yaml.load(yamlStr);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    result: any
  ): Promise<string> {
    if (result.action === "search") {
      shared.search_term = result.search_term;
    }
    return result.action;
  }
}

class SearchWeb extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.search_term || "";
  }

  async exec(searchTerm: string): Promise<string> {
    return await searchWeb(searchTerm);
  }

  async post(shared: SharedState, _: string, execRes: string): Promise<string> {
    shared.context = [
      ...(shared.context || []),
      { term: shared.search_term || "", result: execRes },
    ];
    return "decide";
  }
}

class DirectAnswer extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    return [
      shared.query || "",
      shared.context ? JSON.stringify(shared.context) : "",
    ];
  }

  async exec([query, context]: [string, string]): Promise<string> {
    return await callLlm(`Context: ${context}\nAnswer: ${query}`);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    execRes: string
  ): Promise<undefined> {
    shared.answer = execRes;
    return undefined;
  }
}

// Connect nodes
const decide = new DecideAction();
const search = new SearchWeb();
const answer = new DirectAnswer();

decide.on("search", search);
decide.on("answer", answer);
search.on("decide", decide); // Loop back

const flow = new Flow(decide);
await flow.run({ query: "Who won the Nobel Prize in Physics 2024?" });
````
</file>

<file path="docs/design_pattern/index.md">
---
layout: default
title: "Design Pattern"
nav_order: 3
has_children: true
---
</file>

<file path="docs/design_pattern/mapreduce.md">
---
layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:

- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```typescript
type SharedStorage = {
  files?: Record<string, string>;
  file_summaries?: Record<string, string>;
  all_files_summary?: string;
};

class SummarizeAllFiles extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<[string, string][]> {
    return Object.entries(shared.files || {}); // [["file1.txt", "aaa..."], ["file2.txt", "bbb..."], ...]
  }

  async exec([filename, content]: [string, string]): Promise<[string, string]> {
    const summary = await callLLM(`Summarize the following file:\n${content}`);
    return [filename, summary];
  }

  async post(
    shared: SharedStorage,
    _: [string, string][],
    summaries: [string, string][]
  ): Promise<string> {
    shared.file_summaries = Object.fromEntries(summaries);
    return "summarized";
  }
}

class CombineSummaries extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, string>> {
    return shared.file_summaries || {};
  }

  async exec(summaries: Record<string, string>): Promise<string> {
    const text_list = Object.entries(summaries).map(
      ([fname, summ]) => `${fname} summary:\n${summ}\n`
    );

    return await callLLM(
      `Combine these file summaries into one final summary:\n${text_list.join(
        "\n---\n"
      )}`
    );
  }

  async post(
    shared: SharedStorage,
    _: Record<string, string>,
    finalSummary: string
  ): Promise<string> {
    shared.all_files_summary = finalSummary;
    return "combined";
  }
}

// Create and connect flow
const batchNode = new SummarizeAllFiles();
const combineNode = new CombineSummaries();
batchNode.on("summarized", combineNode);

// Run the flow with test data
const flow = new Flow(batchNode);
flow.run({
  files: {
    "file1.txt":
      "Alice was beginning to get very tired of sitting by her sister...",
    "file2.txt": "Some other interesting text ...",
  },
});
```

> **Performance Tip**: The example above works sequentially. You can speed up the map phase by using `ParallelBatchNode` instead of `BatchNode`. See [(Advanced) Parallel](../core_abstraction/parallel.md) for more details.
> {: .note }
</file>

<file path="docs/design_pattern/multi_agent.md">
---
layout: default
title: "(Advanced) Multi-Agents"
parent: "Design Pattern"
nav_order: 6
---

# (Advanced) Multi-Agents

Multiple [Agents](./flow.md) can work together by handling subtasks and communicating the progress.
Communication between agents is typically implemented using message queues in shared storage.

> Most of time, you don't need Multi-Agents. Start with a simple solution first.
> {: .best-practice }

### Example Agent Communication: Message Queue

Here's a simple example showing how to implement agent communication using a TypeScript message queue.
The agent listens for messages, processes them, and continues listening:

```typescript
import { Node, Flow } from "../src/index";

// Define shared storage with message queue
type SharedStorage = {
  messages: string[];
  processing?: boolean;
};

class AgentNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    // Check if there are messages to process
    if (shared.messages.length === 0) {
      return undefined;
    }
    // Get the next message
    return shared.messages.shift();
  }

  async exec(message: string | undefined): Promise<string | undefined> {
    if (!message) {
      return undefined;
    }
    console.log(`Agent received: ${message}`);
    return message;
  }

  async post(
    shared: SharedStorage,
    prepRes: string | undefined,
    execRes: string | undefined
  ): Promise<string> {
    if (shared.messages.length === 0) {
      // Add a small delay to avoid tight loop CPU consumption
      await new Promise((resolve) => setTimeout(resolve, 100));
    }
    // Continue processing messages
    return "continue";
  }
}

// Message sender function
function sendSystemMessages(shared: SharedStorage) {
  let counter = 0;
  const messages = [
    "System status: all systems operational",
    "Memory usage: normal",
    "Network connectivity: stable",
    "Processing load: optimal",
  ];

  // Add a new message every second
  let intervalId: NodeJS.Timeout;
  intervalId = setInterval(() => {
    const message = `${
      messages[counter % messages.length]
    } | timestamp_${counter}`;
    shared.messages.push(message);
    counter++;

    // Stop after a few messages for demonstration
    if (counter >= 10) {
      clearInterval(intervalId);
    }
  }, 1000);
}

async function main() {
  // Create shared storage with empty message queue
  const shared: SharedStorage = {
    messages: [],
  };

  // Create agent node
  const agent = new AgentNode();
  agent.on("continue", agent); // Connect to self to continue processing

  // Create flow
  const flow = new Flow(agent);

  // Start sending messages
  sendSystemMessages(shared);

  // Run the flow
  await flow.run(shared);
}

main().catch(console.error);
```

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
Agent received: System status: all systems operational | timestamp_4
Agent received: Memory usage: normal | timestamp_5
Agent received: Network connectivity: stable | timestamp_6
Agent received: Processing load: optimal | timestamp_7
Agent received: System status: all systems operational | timestamp_8
Agent received: Memory usage: normal | timestamp_9
```

### Interactive Multi-Agent Example: Taboo Game

Here's a more complex example where two agents play the word-guessing game Taboo.
One agent provides hints while avoiding forbidden words, and another agent tries to guess the target word:

```typescript
import { Node, Flow } from "../src/index";

// Define shared storage for the game
type SharedStorage = {
  targetWord: string;
  forbiddenWords: string[];
  pastGuesses: string[];
  hinterQueue: string[];
  guesserQueue: string[];
  gameOver: boolean;
};

// Utility function to simulate LLM call
function callLLM(prompt: string): string {
  // In a real implementation, this would call an actual LLM API
  console.log(`[LLM PROMPT]: ${prompt}`);

  // For demonstration, return predefined responses
  if (prompt.includes("Generate hint")) {
    if (prompt.includes("popsicle")) {
      return "When childhood cartoons make you emotional";
    }
    if (prompt.includes("nostalgic")) {
      return "When old songs move you";
    }
    if (prompt.includes("memories")) {
      return "That warm emotion about childhood";
    }
    return "Thinking of childhood summer days";
  } else if (prompt.includes("Given hint")) {
    if (prompt.includes("Thinking of childhood summer days")) {
      return "popsicle";
    }
    if (prompt.includes("When childhood cartoons make you emotional")) {
      return "nostalgic";
    }
    if (prompt.includes("When old songs move you")) {
      return "memories";
    }
    if (prompt.includes("That warm emotion about childhood")) {
      return "nostalgia";
    }
    return "unknown";
  }
  return "no response";
}

// Hinter agent that provides clues
class Hinter extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    if (shared.gameOver) {
      return null;
    }

    // Wait for a message in the hinter queue
    while (shared.hinterQueue.length === 0) {
      await new Promise((resolve) => setTimeout(resolve, 100));
      if (shared.gameOver) return null;
    }

    const guess = shared.hinterQueue.shift();
    if (guess === "GAME_OVER") {
      shared.gameOver = true;
      return null;
    }

    return {
      target: shared.targetWord,
      forbidden: shared.forbiddenWords,
      pastGuesses: shared.pastGuesses,
    };
  }

  async exec(inputs: any): Promise<string | null> {
    if (inputs === null) {
      return null;
    }

    const { target, forbidden, pastGuesses } = inputs;
    let prompt = `Generate hint for '${target}'\nForbidden words: ${forbidden.join(
      ", "
    )}`;

    if (pastGuesses && pastGuesses.length > 0) {
      prompt += `\nPrevious wrong guesses: ${pastGuesses.join(
        ", "
      )}\nMake hint more specific.`;
    }

    prompt += "\nUse at most 5 words.";

    const hint = callLLM(prompt);
    console.log(`\nHinter: Here's your hint - ${hint}`);
    return hint;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: string | null
  ): Promise<string> {
    if (execRes === null) {
      return "end";
    }

    // Send the hint to the guesser
    shared.guesserQueue.push(execRes);
    return "continue";
  }
}

// Guesser agent that tries to guess the word
class Guesser extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    if (shared.gameOver) {
      return null;
    }

    // Wait for a hint in the guesser queue
    while (shared.guesserQueue.length === 0) {
      await new Promise((resolve) => setTimeout(resolve, 100));
      if (shared.gameOver) return null;
    }

    const hint = shared.guesserQueue.shift();
    return {
      hint,
      pastGuesses: shared.pastGuesses,
    };
  }

  async exec(inputs: any): Promise<string | null> {
    if (inputs === null) {
      return null;
    }

    const { hint, pastGuesses } = inputs;
    const prompt = `Given hint: ${hint}, past wrong guesses: ${pastGuesses.join(
      ", "
    )}, make a new guess. Directly reply a single word:`;

    const guess = callLLM(prompt);
    console.log(`Guesser: I guess it's - ${guess}`);
    return guess;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: string | null
  ): Promise<string> {
    if (execRes === null) {
      return "end";
    }

    if (execRes.toLowerCase() === shared.targetWord.toLowerCase()) {
      console.log("Game Over - Correct guess!");
      shared.gameOver = true;
      shared.hinterQueue.push("GAME_OVER");
      return "end";
    }

    // Add to past guesses
    shared.pastGuesses.push(execRes);

    // Send the guess to the hinter
    shared.hinterQueue.push(execRes);
    return "continue";
  }
}

async function main() {
  // Set up game
  const shared: SharedStorage = {
    targetWord: "nostalgia",
    forbiddenWords: ["memory", "past", "remember", "feeling", "longing"],
    pastGuesses: [],
    hinterQueue: [],
    guesserQueue: [],
    gameOver: false,
  };

  console.log("Game starting!");
  console.log(`Target word: ${shared.targetWord}`);
  console.log(`Forbidden words: ${shared.forbiddenWords.join(", ")}`);

  // Initialize by sending empty guess to hinter
  shared.hinterQueue.push("");

  // Create agents
  const hinter = new Hinter();
  const guesser = new Guesser();

  // Set up flows
  hinter.on("continue", hinter);
  guesser.on("continue", guesser);

  const hinterFlow = new Flow(hinter);
  const guesserFlow = new Flow(guesser);

  // Run both agents
  await Promise.all([hinterFlow.run(shared), guesserFlow.run(shared)]);
}

main().catch(console.error);
```

The Output:

```
Game starting!
Target word: nostalgia
Forbidden words: memory, past, remember, feeling, longing

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Use at most 5 words.

Hinter: Here's your hint - Thinking of childhood summer days

[LLM PROMPT]: Given hint: Thinking of childhood summer days, past wrong guesses: , make a new guess. Directly reply a single word:
Guesser: I guess it's - popsicle

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Previous wrong guesses: popsicle
Make hint more specific.
Use at most 5 words.

Hinter: Here's your hint - When childhood cartoons make you emotional

[LLM PROMPT]: Given hint: When childhood cartoons make you emotional, past wrong guesses: popsicle, make a new guess. Directly reply a single word:
Guesser: I guess it's - nostalgic

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Previous wrong guesses: popsicle, nostalgic
Make hint more specific.
Use at most 5 words.

Hinter: Here's your hint - When old songs move you

[LLM PROMPT]: Given hint: When old songs move you, past wrong guesses: popsicle, nostalgic, make a new guess. Directly reply a single word:
Guesser: I guess it's - memories

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Previous wrong guesses: popsicle, nostalgic, memories
Make hint more specific.
Use at most 5 words.

Hinter: Here's your hint - That warm emotion about childhood

[LLM PROMPT]: Given hint: That warm emotion about childhood, past wrong guesses: popsicle, nostalgic, memories, make a new guess. Directly reply a single word:
Guesser: I guess it's - nostalgia
Game Over - Correct guess!
```
</file>

<file path="docs/design_pattern/rag.md">
---
layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` â€“ [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` â€“ [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` â€“ stores embeddings into a [vector database](../utility_function/vector.md).

```typescript
type SharedStore = {
  files?: string[];
  allChunks?: string[];
  allEmbeds?: number[][];
  index?: any;
};

class ChunkDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.files || [];
  }

  async exec(filepath: string): Promise<string[]> {
    const text = fs.readFileSync(filepath, "utf-8");
    // Simplified chunking for example
    const chunks: string[] = [];
    const size = 100;
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.substring(i, i + size));
    }
    return chunks;
  }

  async post(
    shared: SharedStore,
    _: string[],
    chunks: string[][]
  ): Promise<undefined> {
    shared.allChunks = chunks.flat();
    return undefined;
  }
}

class EmbedDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.allChunks || [];
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk);
  }

  async post(
    shared: SharedStore,
    _: string[],
    embeddings: number[][]
  ): Promise<undefined> {
    shared.allEmbeds = embeddings;
    return undefined;
  }
}

class StoreIndex extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<number[][]> {
    return shared.allEmbeds || [];
  }

  async exec(allEmbeds: number[][]): Promise<unknown> {
    return await createIndex(allEmbeds);
  }

  async post(
    shared: SharedStore,
    _: number[][],
    index: unknown
  ): Promise<undefined> {
    shared.index = index;
    return undefined;
  }
}

// Create indexing flow
const chunkNode = new ChunkDocs();
const embedNode = new EmbedDocs();
const storeNode = new StoreIndex();

chunkNode.next(embedNode).next(storeNode);
const offlineFlow = new Flow(chunkNode);
```

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` â€“ embeds the user's question.
2. `RetrieveDocs` â€“ retrieves top chunk from the index.
3. `GenerateAnswer` â€“ calls the LLM with the question + chunk to produce the final answer.

```typescript
type OnlineStore = SharedStore & {
  question?: string;
  qEmb?: number[];
  retrievedChunk?: string;
  answer?: string;
};

class EmbedQuery extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<string> {
    return shared.question || "";
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question);
  }

  async post(
    shared: OnlineStore,
    _: string,
    qEmb: number[]
  ): Promise<undefined> {
    shared.qEmb = qEmb;
    return undefined;
  }
}

class RetrieveDocs extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[number[], any, string[]]> {
    return [shared.qEmb || [], shared.index, shared.allChunks || []];
  }

  async exec([qEmb, index, chunks]: [
    number[],
    any,
    string[]
  ]): Promise<string> {
    const [ids] = await searchIndex(index, qEmb, { topK: 1 });
    return chunks[ids[0][0]];
  }

  async post(
    shared: OnlineStore,
    _: [number[], any, string[]],
    chunk: string
  ): Promise<undefined> {
    shared.retrievedChunk = chunk;
    return undefined;
  }
}

class GenerateAnswer extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[string, string]> {
    return [shared.question || "", shared.retrievedChunk || ""];
  }

  async exec([question, chunk]: [string, string]): Promise<string> {
    return await callLlm(`Question: ${question}\nContext: ${chunk}\nAnswer:`);
  }

  async post(
    shared: OnlineStore,
    _: [string, string],
    answer: string
  ): Promise<undefined> {
    shared.answer = answer;
    return undefined;
  }
}

// Create query flow
const embedQNode = new EmbedQuery();
const retrieveNode = new RetrieveDocs();
const generateNode = new GenerateAnswer();

embedQNode.next(retrieveNode).next(generateNode);
const onlineFlow = new Flow(embedQNode);
```

Usage example:

```typescript
const shared = {
  files: ["doc1.txt", "doc2.txt"], // any text files
};
await offlineFlow.run(shared);
```
</file>

<file path="docs/design_pattern/structure.md">
---
layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

## TypeScript Implementation

When using PocketFlow with structured output, follow these TypeScript patterns:

1. **Define Types** for your structured input/output
2. **Implement Validation** in your Node methods
3. **Use Type-Safe Operations** throughout your flow

### Example Text Summarization

````typescript
// Define types
type SummaryResult = {
  summary: string[];
};

type SharedStorage = {
  text?: string;
  result?: SummaryResult;
};

class SummarizeNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    return shared.text;
  }

  async exec(text: string | undefined): Promise<SummaryResult> {
    if (!text) return { summary: ["No text provided"] };

    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points

${text}

Output:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``;

    // Simulated LLM call
    const response =
      "```yaml\nsummary:\n  - First point\n  - Second insight\n  - Final conclusion\n```";

    // Parse YAML response
    const yamlStr = response.split("```yaml")[1].split("```")[0].trim();

    // Extract bullet points
    const result: SummaryResult = {
      summary: yamlStr
        .split("\n")
        .filter((line) => line.trim().startsWith("- "))
        .map((line) => line.trim().substring(2)),
    };

    // Validate
    if (!result.summary || !Array.isArray(result.summary)) {
      throw new Error("Invalid summary structure");
    }

    return result;
  }

  async post(
    shared: SharedStorage,
    _: string | undefined,
    result: SummaryResult
  ): Promise<string | undefined> {
    shared.result = result;
    return "default";
  }
}
````

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```
</file>

<file path="docs/design_pattern/workflow.md">
---
layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
> - You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.
>
> You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
> {: .best-practice }

### Example: Article Writing

```typescript
interface SharedState {
  topic?: string;
  outline?: string;
  draft?: string;
  final_article?: string;
}

// Helper function to simulate LLM call
async function callLLM(prompt: string): Promise<string> {
  return `Response to: ${prompt}`;
}

class GenerateOutline extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.topic || "";
  }

  async exec(topic: string): Promise<string> {
    return await callLLM(
      `Create a detailed outline for an article about ${topic}`
    );
  }

  async post(shared: SharedState, _: string, outline: string): Promise<string> {
    shared.outline = outline;
    return "default";
  }
}

class WriteSection extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.outline || "";
  }

  async exec(outline: string): Promise<string> {
    return await callLLM(`Write content based on this outline: ${outline}`);
  }

  async post(shared: SharedState, _: string, draft: string): Promise<string> {
    shared.draft = draft;
    return "default";
  }
}

class ReviewAndRefine extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.draft || "";
  }

  async exec(draft: string): Promise<string> {
    return await callLLM(`Review and improve this draft: ${draft}`);
  }

  async post(
    shared: SharedState,
    _: string,
    final: string
  ): Promise<undefined> {
    shared.final_article = final;
    return undefined;
  }
}

// Connect nodes in sequence
const outline = new GenerateOutline();
const write = new WriteSection();
const review = new ReviewAndRefine();

outline.next(write).next(review);

// Create and run flow
const writingFlow = new Flow(outline);
writingFlow.run({ topic: "AI Safety" });
```

For _dynamic cases_, consider using [Agents](./agent.md).
</file>

<file path="docs/utility_function/chunking.md">
---
layout: default
title: "Text Chunking"
parent: "Utility Function"
nav_order: 4
---

# Text Chunking

We recommend some implementations of commonly used text chunking approaches.

> Text Chunking is more a micro optimization, compared to the Flow Design.
>
> It's recommended to start with the Naive Chunking and optimize later.
> {: .best-practice }

---

## Example TypeScript Code Samples

### 1. Naive (Fixed-Size) Chunking

Splits text by a fixed number of characters, ignoring sentence or semantic boundaries.

```typescript
function fixedSizeChunk(text: string, chunkSize: number = 100): string[] {
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += chunkSize) {
    chunks.push(text.substring(i, i + chunkSize));
  }
  return chunks;
}
```

However, sentences are often cut awkwardly, losing coherence.

### 2. Sentence-Based Chunking

Using the popular [compromise](https://github.com/spencermountain/compromise) library (11.6k+ GitHub stars).

```typescript
import nlp from "compromise";

function sentenceBasedChunk(text: string, maxSentences: number = 2): string[] {
  // Parse the text into sentences
  const doc = nlp(text);
  const sentences = doc.sentences().out("array");

  // Group sentences into chunks
  const chunks: string[] = [];
  for (let i = 0; i < sentences.length; i += maxSentences) {
    chunks.push(sentences.slice(i, i + maxSentences).join(" "));
  }

  return chunks;
}
```

However, might not handle very long sentences or paragraphs well.

### 3. Other Chunking

- **Paragraph-Based**: Split text by paragraphs (e.g., newlines). Large paragraphs can create big chunks.

```typescript
function paragraphBasedChunk(text: string): string[] {
  // Split by double newlines (paragraphs)
  return text
    .split(/\n\s*\n/)
    .filter((paragraph) => paragraph.trim().length > 0);
}
```

- **Semantic**: Use embeddings or topic modeling to chunk by semantic boundaries.
- **Agentic**: Use an LLM to decide chunk boundaries based on context or meaning.
</file>

<file path="docs/utility_function/embedding.md">
---
layout: default
title: "Embedding"
parent: "Utility Function"
nav_order: 5
---

# Embedding

Below you will find an overview table of various text embedding APIs, along with example TypeScript code.

> Embedding is more a micro optimization, compared to the Flow Design.
>
> It's recommended to start with the most convenient one and optimize later.
> {: .best-practice }

| **API**              | **Free Tier**                           | **Pricing Model**                   | **Docs**                                                                                                                  |
| -------------------- | --------------------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI**           | ~$5 credit                              | ~$0.0001/1K tokens                  | [OpenAI Embeddings](https://platform.openai.com/docs/api-reference/embeddings)                                            |
| **Azure OpenAI**     | $200 credit                             | Same as OpenAI (~$0.0001/1K tokens) | [Azure OpenAI Embeddings](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?tabs=portal) |
| **Google Vertex AI** | $300 credit                             | ~$0.025 / million chars             | [Vertex AI Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)              |
| **AWS Bedrock**      | No free tier, but AWS credits may apply | ~$0.00002/1K tokens (Titan V2)      | [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/)                                                                    |
| **Cohere**           | Limited free tier                       | ~$0.0001/1K tokens                  | [Cohere Embeddings](https://docs.cohere.com/docs/cohere-embed)                                                            |
| **Hugging Face**     | ~$0.10 free compute monthly             | Pay per second of compute           | [HF Inference API](https://huggingface.co/docs/api-inference)                                                             |
| **Jina**             | 1M tokens free                          | Pay per token after                 | [Jina Embeddings](https://jina.ai/embeddings/)                                                                            |

## Example TypeScript Code

### 1. OpenAI

```typescript
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "YOUR_API_KEY",
});

async function getEmbedding(text: string) {
  const response = await openai.embeddings.create({
    model: "text-embedding-ada-002",
    input: text,
  });

  // Extract the embedding vector from the response
  const embedding = response.data[0].embedding;
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 2. Azure OpenAI

```typescript
import { AzureOpenAI } from "openai";
import {
  getBearerTokenProvider,
  DefaultAzureCredential,
} from "@azure/identity";

// Using Azure credentials (recommended)
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Or using API key directly
const client = new AzureOpenAI({
  apiKey: "YOUR_AZURE_API_KEY",
  endpoint: "https://YOUR_RESOURCE_NAME.openai.azure.com",
  apiVersion: "2023-05-15", // Update to the latest version
});

async function getEmbedding(text: string) {
  const response = await client.embeddings.create({
    model: "text-embedding-ada-002", // Or your deployment name
    input: text,
  });

  const embedding = response.data[0].embedding;
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 3. Google Vertex AI

```typescript
import { VertexAI } from "@google-cloud/vertexai";

// Initialize Vertex with your Google Cloud project and location
const vertex = new VertexAI({
  project: "YOUR_GCP_PROJECT_ID",
  location: "us-central1",
});

// Access embeddings model
const model = vertex.preview.getTextEmbeddingModel("textembedding-gecko@001");

async function getEmbedding(text: string) {
  const response = await model.getEmbeddings({
    texts: [text],
  });

  const embedding = response.embeddings[0].values;
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 4. AWS Bedrock

```typescript
import {
  BedrockRuntimeClient,
  InvokeModelCommand,
} from "@aws-sdk/client-bedrock-runtime";

const client = new BedrockRuntimeClient({
  region: "us-east-1", // Use your AWS region
});

async function getEmbedding(text: string) {
  const modelId = "amazon.titan-embed-text-v2:0";
  const input = {
    inputText: text,
    dimensions: 1536, // Optional: specify embedding dimensions
    normalize: true, // Optional: normalize embeddings
  };

  const command = new InvokeModelCommand({
    modelId: modelId,
    contentType: "application/json",
    accept: "application/json",
    body: JSON.stringify(input),
  });

  const response = await client.send(command);
  const responseBody = JSON.parse(new TextDecoder().decode(response.body));
  const embedding = responseBody.embedding;

  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 5. Cohere

```typescript
import { CohereClient } from "cohere-ai";

const cohere = new CohereClient({
  token: "YOUR_API_KEY",
});

async function getEmbedding(text: string) {
  const response = await cohere.embed({
    texts: [text],
    model: "embed-english-v3.0", // Use the latest model
    inputType: "search_query",
  });

  const embedding = response.embeddings[0];
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 6. Hugging Face

```typescript
import { InferenceClient } from "@huggingface/inference";

const hf = new InferenceClient({
  apiToken: "YOUR_HF_TOKEN",
});

async function getEmbedding(text: string) {
  const model = "sentence-transformers/all-MiniLM-L6-v2";

  const response = await hf.featureExtraction({
    model: model,
    inputs: text,
  });

  console.log(response);
  return response;
}

// Usage
getEmbedding("Hello world");
```

### 7. Jina

```typescript
import axios from "axios";

async function getEmbedding(text: string) {
  const url = "https://api.jina.ai/v1/embeddings";
  const headers = {
    Authorization: `Bearer YOUR_JINA_TOKEN`,
    "Content-Type": "application/json",
  };

  const payload = {
    model: "jina-embeddings-v3",
    input: [text],
    normalized: true,
  };

  const response = await axios.post(url, payload, { headers });
  const embedding = response.data.data[0].embedding;

  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```
</file>

<file path="docs/utility_function/index.md">
---
layout: default
title: "Utility Function"
nav_order: 4
has_children: true
---
</file>

<file path="docs/utility_function/llm.md">
---
layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1
---

# LLM Wrappers

Check out popular libraries like [LangChain](https://github.com/langchain-ai/langchainjs) (13.8k+ GitHub stars), [ModelFusion](https://github.com/vercel/modelfusion) (1.2k+ GitHub stars), or [Firebase GenKit](https://firebase.google.com/docs/genkit) for unified LLM interfaces.
Here, we provide some minimal example implementations:

1. OpenAI

   ```typescript
   import { OpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
     const r = await client.chat.completions.create({
       model: "gpt-4o",
       messages: [{ role: "user", content: prompt }],
     });
     return r.choices[0].message.content || "";
   }

   // Example usage
   callLlm("How are you?").then(console.log);
   ```

   > Store the API key in an environment variable like OPENAI_API_KEY for security.
   > {: .best-practice }

2. Claude (Anthropic)

   ```typescript
   import Anthropic from "@anthropic-ai/sdk";

   async function callLlm(prompt: string): Promise<string> {
     const client = new Anthropic({
       apiKey: "YOUR_API_KEY_HERE",
     });
     const response = await client.messages.create({
       model: "claude-3-7-sonnet-20250219",
       max_tokens: 3000,
       messages: [{ role: "user", content: prompt }],
     });
     return response.content[0].text;
   }
   ```

3. Google (Vertex AI)

   ```typescript
   import { VertexAI } from "@google-cloud/vertexai";

   async function callLlm(prompt: string): Promise<string> {
     const vertexAI = new VertexAI({
       project: "YOUR_PROJECT_ID",
       location: "us-central1",
     });

     const generativeModel = vertexAI.getGenerativeModel({
       model: "gemini-1.5-flash",
     });

     const response = await generativeModel.generateContent({
       contents: [{ role: "user", parts: [{ text: prompt }] }],
     });

     return response.response.candidates[0].content.parts[0].text;
   }
   ```

4. Azure (Azure OpenAI)

   ```typescript
   import { AzureOpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new AzureOpenAI({
       apiKey: "YOUR_API_KEY_HERE",
       azure: {
         apiVersion: "2023-05-15",
         endpoint: "https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
       },
     });

     const r = await client.chat.completions.create({
       model: "<YOUR_DEPLOYMENT_NAME>",
       messages: [{ role: "user", content: prompt }],
     });

     return r.choices[0].message.content || "";
   }
   ```

5. Ollama (Local LLM)

   ```typescript
   import ollama from "ollama";

   async function callLlm(prompt: string): Promise<string> {
     const response = await ollama.chat({
       model: "llama2",
       messages: [{ role: "user", content: prompt }],
     });
     return response.message.content;
   }
   ```

## Improvements

Feel free to enhance your `callLlm` function as needed. Here are examples:

- Handle chat history:

```typescript
interface Message {
  role: "user" | "assistant" | "system";
  content: string;
}

async function callLlm(messages: Message[]): Promise<string> {
  const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
  const r = await client.chat.completions.create({
    model: "gpt-4o",
    messages: messages,
  });
  return r.choices[0].message.content || "";
}
```

- Add in-memory caching

```typescript
import { memoize } from "lodash";

const callLlmMemoized = memoize(async (prompt: string): Promise<string> => {
  // Your implementation here
  return "";
});

async function callLlm(prompt: string, useCache = true): Promise<string> {
  if (useCache) {
    return callLlmMemoized(prompt);
  }
  // Call the underlying function directly
  return callLlmInternal(prompt);
}

class SummarizeNode {
  private curRetry = 0;

  async exec(text: string): Promise<string> {
    return callLlm(`Summarize: ${text}`, this.curRetry === 0);
  }
}
```

- Enable logging:

```typescript
async function callLlm(prompt: string): Promise<string> {
  console.info(`Prompt: ${prompt}`);
  // Your implementation here
  const response = ""; // Response from your implementation
  console.info(`Response: ${response}`);
  return response;
}
```
</file>

<file path="docs/utility_function/text_to_speech.md">
---
layout: default
title: "Text-to-Speech"
parent: "Utility Function"
nav_order: 7
---

# Text-to-Speech

| **Service**          | **Free Tier**       | **Pricing Model**                                            | **Docs**                                                                                  |
| -------------------- | ------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |
| **Amazon Polly**     | 5M std + 1M neural  | ~$4 /M (std), ~$16 /M (neural) after free tier               | [Polly Docs](https://aws.amazon.com/polly/)                                               |
| **Google Cloud TTS** | 4M std + 1M WaveNet | ~$4 /M (std), ~$16 /M (WaveNet) pay-as-you-go                | [Cloud TTS Docs](https://cloud.google.com/text-to-speech)                                 |
| **Azure TTS**        | 500K neural ongoing | ~$15 /M (neural), discount at higher volumes                 | [Azure TTS Docs](https://azure.microsoft.com/products/cognitive-services/text-to-speech/) |
| **IBM Watson TTS**   | 10K chars Lite plan | ~$0.02 /1K (i.e. ~$20 /M). Enterprise options available      | [IBM Watson Docs](https://www.ibm.com/cloud/watson-text-to-speech)                        |
| **ElevenLabs**       | 10K chars monthly   | From ~$5/mo (30K chars) up to $330/mo (2M chars). Enterprise | [ElevenLabs Docs](https://elevenlabs.io)                                                  |

## Example TypeScript Code

### Amazon Polly

```typescript
import { PollyClient, SynthesizeSpeechCommand } from "@aws-sdk/client-polly";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Create a Polly client
  const polly = new PollyClient({
    region: "us-east-1",
    credentials: {
      accessKeyId: "YOUR_AWS_ACCESS_KEY_ID",
      secretAccessKey: "YOUR_AWS_SECRET_ACCESS_KEY",
    },
  });

  // Set the parameters
  const params = {
    Text: "Hello from Polly!",
    OutputFormat: "mp3",
    VoiceId: "Joanna",
  };

  try {
    // Synthesize speech
    const command = new SynthesizeSpeechCommand(params);
    const response = await polly.send(command);

    // Convert AudioStream to Buffer and save to file
    if (response.AudioStream) {
      const audioBuffer = Buffer.from(
        await response.AudioStream.transformToByteArray()
      );
      writeFileSync("polly.mp3", audioBuffer);
      console.log("Audio content written to file: polly.mp3");
    }
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### Google Cloud TTS

```typescript
import { TextToSpeechClient } from "@google-cloud/text-to-speech";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Creates a client
  const client = new TextToSpeechClient();

  // The text to synthesize
  const text = "Hello from Google Cloud TTS!";

  // Construct the request
  const request = {
    input: { text: text },
    // Select the language and SSML voice gender
    voice: { languageCode: "en-US", ssmlGender: "NEUTRAL" },
    // Select the type of audio encoding
    audioConfig: { audioEncoding: "MP3" },
  };

  try {
    // Performs the text-to-speech request
    const [response] = await client.synthesizeSpeech(request);
    // Write the binary audio content to a local file
    writeFileSync("gcloud_tts.mp3", response.audioContent as Buffer);
    console.log("Audio content written to file: gcloud_tts.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### Azure TTS

```typescript
import {
  SpeechConfig,
  AudioConfig,
  SpeechSynthesizer,
} from "microsoft-cognitiveservices-speech-sdk";

async function synthesizeText() {
  // Create a speech configuration with subscription information
  const speechConfig = SpeechConfig.fromSubscription(
    "AZURE_KEY",
    "AZURE_REGION"
  );

  // Set speech synthesis output format
  speechConfig.speechSynthesisOutputFormat = 1; // 1 corresponds to Audio16Khz128KBitRateMonoMp3

  // Create an audio configuration for file output
  const audioConfig = AudioConfig.fromAudioFileOutput("azure_tts.mp3");

  // Create a speech synthesizer with the given configurations
  const synthesizer = new SpeechSynthesizer(speechConfig, audioConfig);

  try {
    // Synthesize text to speech
    const result = await new Promise((resolve, reject) => {
      synthesizer.speakTextAsync(
        "Hello from Azure TTS!",
        (result) => {
          synthesizer.close();
          resolve(result);
        },
        (error) => {
          synthesizer.close();
          reject(error);
        }
      );
    });

    console.log("Audio content written to file: azure_tts.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### IBM Watson TTS

```typescript
import { TextToSpeechV1 } from "ibm-watson/text-to-speech/v1";
import { IamAuthenticator } from "ibm-watson/auth";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Create a TextToSpeech client with authentication
  const textToSpeech = new TextToSpeechV1({
    authenticator: new IamAuthenticator({ apikey: "IBM_API_KEY" }),
    serviceUrl: "IBM_SERVICE_URL",
  });

  try {
    // Synthesize speech
    const params = {
      text: "Hello from IBM Watson!",
      voice: "en-US_AllisonV3Voice",
      accept: "audio/mp3",
    };

    const response = await textToSpeech.synthesize(params);
    const audio = response.result;

    // The wav header requires a file length, but this is unknown until after the header is already generated
    const repairedAudio = await textToSpeech.repairWavHeaderStream(audio);

    // Write audio to file
    writeFileSync("ibm_tts.mp3", repairedAudio);
    console.log("Audio content written to file: ibm_tts.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### ElevenLabs

```typescript
import { ElevenLabs } from "elevenlabs";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Initialize the ElevenLabs client
  const eleven = new ElevenLabs({
    apiKey: "ELEVENLABS_KEY",
  });

  try {
    // Generate speech
    const voiceId = "ELEVENLABS_VOICE_ID";
    const text = "Hello from ElevenLabs!";

    // Generate audio
    const audioResponse = await eleven.generate({
      voice: voiceId,
      text: text,
      model_id: "eleven_monolingual_v1",
      voice_settings: {
        stability: 0.75,
        similarity_boost: 0.75,
      },
    });

    // Convert to buffer and save to file
    const audioBuffer = Buffer.from(await audioResponse.arrayBuffer());
    writeFileSync("elevenlabs.mp3", audioBuffer);
    console.log("Audio content written to file: elevenlabs.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```
</file>

<file path="docs/utility_function/vector.md">
---
layout: default
title: "Vector Databases"
parent: "Utility Function"
nav_order: 6
---

# Vector Databases

Below is a table of the popular vector search solutions:

| **Tool**     | **Free Tier**  | **Pricing Model**        | **Docs**                               |
| ------------ | -------------- | ------------------------ | -------------------------------------- |
| **FAISS**    | N/A, self-host | Open-source              | [Faiss.ai](https://faiss.ai)           |
| **Pinecone** | 2GB free       | From $25/mo              | [pinecone.io](https://pinecone.io)     |
| **Qdrant**   | 1GB free cloud | Pay-as-you-go            | [qdrant.tech](https://qdrant.tech)     |
| **Weaviate** | 14-day sandbox | From $25/mo              | [weaviate.io](https://weaviate.io)     |
| **Milvus**   | 5GB free cloud | PAYG or $99/mo dedicated | [milvus.io](https://milvus.io)         |
| **Chroma**   | N/A, self-host | Free (Apache 2.0)        | [trychroma.com](https://trychroma.com) |
| **Redis**    | 30MB free      | From $5/mo               | [redis.io](https://redis.io)           |

---

## Example TypeScript Code

Below are basic usage snippets for each tool.

### FAISS

```typescript
import { IndexFlatL2 } from "faiss-node";

// Dimensionality of embeddings
const dimension = 128;

// Create a flat L2 index
const index = new IndexFlatL2(dimension);

// Create random vectors (using standard JS arrays)
const data: number[] = [];
for (let i = 0; i < 1000; i++) {
  for (let j = 0; j < dimension; j++) {
    data.push(Math.random());
  }
}

// Add vectors to the index
for (let i = 0; i < 1000; i++) {
  const vector = data.slice(i * dimension, (i + 1) * dimension);
  index.add(vector);
}

// Query
const query = Array(dimension)
  .fill(0)
  .map(() => Math.random());
const results = index.search(query, 5);

console.log("Distances:", results.distances);
console.log("Neighbors:", results.labels);
```

### Pinecone

```typescript
import { PineconeClient } from "@pinecone-database/pinecone";

// Define interface for your metadata (optional)
interface Metadata {
  type: string;
}

const init = async () => {
  // Initialize the client
  const pinecone = new PineconeClient();
  await pinecone.init({
    apiKey: "YOUR_API_KEY",
    environment: "YOUR_ENVIRONMENT",
  });

  const indexName = "my-index";

  // List indexes to check if it exists
  const indexes = await pinecone.listIndexes();

  // Create index if it doesn't exist
  if (!indexes.includes(indexName)) {
    await pinecone.createIndex({
      name: indexName,
      dimension: 128,
      metric: "cosine",
    });
  }

  // Connect to the index
  const index = pinecone.Index(indexName);

  // Upsert vectors
  await index.upsert({
    upsertRequest: {
      vectors: [
        {
          id: "id1",
          values: Array(128).fill(0.1),
          metadata: { type: "doc1" } as Metadata,
        },
        {
          id: "id2",
          values: Array(128).fill(0.2),
          metadata: { type: "doc2" } as Metadata,
        },
      ],
      namespace: "example-namespace",
    },
  });

  // Query
  const queryResult = await index.query({
    queryRequest: {
      vector: Array(128).fill(0.15),
      topK: 3,
      includeMetadata: true,
      namespace: "example-namespace",
    },
  });

  console.log(queryResult);
};

init();
```

### Qdrant

```typescript
import { QdrantClient } from "@qdrant/js-client-rest";

const init = async () => {
  const client = new QdrantClient({
    url: "https://YOUR-QDRANT-CLOUD-ENDPOINT",
    apiKey: "YOUR_API_KEY",
  });

  const collectionName = "my_collection";

  // Create or recreate collection
  await client.recreateCollection(collectionName, {
    vectors: {
      size: 128,
      distance: "Cosine",
    },
  });

  // Insert points
  await client.upsert(collectionName, {
    wait: true,
    points: [
      {
        id: "1",
        vector: Array(128).fill(0.1),
        payload: { type: "doc1" },
      },
      {
        id: "2",
        vector: Array(128).fill(0.2),
        payload: { type: "doc2" },
      },
    ],
  });

  // Search
  const searchResult = await client.search(collectionName, {
    vector: Array(128).fill(0.15),
    limit: 2,
  });

  console.log(searchResult);
};

init();
```

### Weaviate

```typescript
import weaviate from "weaviate-client";

const init = async () => {
  // Connect to Weaviate
  const client = weaviate.client({
    scheme: "https",
    host: "YOUR-WEAVIATE-CLOUD-ENDPOINT",
  });

  // Create schema
  const schema = {
    classes: [
      {
        class: "Article",
        vectorizer: "none",
      },
    ],
  };

  await client.schema.create(schema);

  // Add data
  await client.data
    .creator()
    .withClassName("Article")
    .withProperties({
      title: "Hello World",
      content: "Weaviate vector search",
    })
    .withVector(Array(128).fill(0.1))
    .do();

  // Query
  const result = await client.graphql
    .get()
    .withClassName("Article")
    .withFields(["title", "content"])
    .withNearVector({
      vector: Array(128).fill(0.15),
    })
    .withLimit(3)
    .do();

  console.log(result);
};

init();
```

### Milvus

```typescript
import { MilvusClient, DataType } from "@zilliz/milvus2-sdk-node";

const init = async () => {
  // Connect to Milvus
  const client = new MilvusClient("localhost:19530");

  // Wait for connection to be established
  await client.connectPromise;

  const collectionName = "MyCollection";

  // Create collection
  await client.createCollection({
    collection_name: collectionName,
    fields: [
      {
        name: "id",
        data_type: DataType.Int64,
        is_primary_key: true,
        autoID: true,
      },
      {
        name: "embedding",
        data_type: DataType.FloatVector,
        dim: 128,
      },
    ],
  });

  // Create random vectors
  const vectors = [];
  for (let i = 0; i < 10; i++) {
    vectors.push(
      Array(128)
        .fill(0)
        .map(() => Math.random())
    );
  }

  // Insert data
  await client.insert({
    collection_name: collectionName,
    fields_data: vectors.map((vector) => ({
      embedding: vector,
    })),
  });

  // Create index
  await client.createIndex({
    collection_name: collectionName,
    field_name: "embedding",
    extra_params: {
      index_type: "IVF_FLAT",
      metric_type: "L2",
      params: JSON.stringify({ nlist: 128 }),
    },
  });

  // Load collection to memory
  await client.loadCollection({
    collection_name: collectionName,
  });

  // Search
  const searchResult = await client.search({
    collection_name: collectionName,
    vector: Array(128)
      .fill(0)
      .map(() => Math.random()),
    search_params: {
      anns_field: "embedding",
      topk: 3,
      metric_type: "L2",
      params: JSON.stringify({ nprobe: 10 }),
    },
  });

  console.log(searchResult);
};

init();
```

### Chroma

```typescript
import { ChromaClient, Collection } from "chromadb";

const init = async () => {
  const client = new ChromaClient({
    path: "http://localhost:8000", // Default path if using chroma server
  });

  // Create or get collection
  const collection: Collection = await client.createCollection({
    name: "my_collection",
    // Optional metadata
    metadata: { description: "My test collection" },
  });

  // Add vectors
  await collection.add({
    ids: ["id1", "id2"],
    embeddings: [
      [0.1, 0.2, 0.3],
      [0.2, 0.2, 0.2],
    ],
    metadatas: [{ doc: "text1" }, { doc: "text2" }],
  });

  // Query
  const result = await collection.query({
    queryEmbeddings: [[0.15, 0.25, 0.3]],
    nResults: 2,
  });

  console.log(result);
};

init();
```

### Redis

```typescript
import { createClient } from "redis";
import { SchemaFieldTypes, VectorAlgorithms } from "@redis/search";

const init = async () => {
  // Connect to Redis
  const client = createClient({
    url: "redis://localhost:6379",
  });

  await client.connect();

  const indexName = "my_idx";

  // Create index for vector search
  try {
    await client.ft.create(
      indexName,
      {
        embedding: {
          type: SchemaFieldTypes.VECTOR,
          ALGORITHM: VectorAlgorithms.FLAT,
          TYPE: "FLOAT32",
          DIM: 128,
          DISTANCE_METRIC: "L2",
        },
      },
      {
        ON: "HASH",
      }
    );
  } catch (e) {
    console.log("Index might exist already");
  }

  // Create a Float32Array for the vectors
  const createVector = (value: number): Buffer => {
    const vector = new Float32Array(128).fill(value);
    return Buffer.from(vector.buffer);
  };

  // Insert
  await client.hSet("doc1", {
    embedding: createVector(0.1),
  });

  // Search
  const searchResults = await client.ft.search(
    indexName,
    "*=>[KNN 3 @embedding $BLOB AS dist]",
    {
      PARAMS: {
        BLOB: createVector(0.15),
      },
      RETURN: ["dist"],
      DIALECT: 2,
    }
  );

  console.log(searchResults);

  await client.quit();
};

init();
```
</file>

<file path="docs/utility_function/viz.md">
---
layout: default
title: "Viz and Debug"
parent: "Utility Function"
nav_order: 2
---

# Visualization and Debugging

Similar to LLM wrappers, we **don't** provide built-in visualization and debugging. Here, we recommend some _minimal_ (and incomplete) implementations These examples can serve as a starting point for your own tooling.

## 1. Visualization with Mermaid

### Using the Official Mermaid Library (Recommended)

The [mermaid library](https://github.com/mermaid-js/mermaid) is a very popular (76K+ stars) TypeScript library for creating diagrams from text. We recommend using it for visualization in production projects.

First, install the library:

```bash
npm install mermaid
# or
yarn add mermaid
# or
pnpm add mermaid
```

Then, you can use it to generate diagrams from your flow structure:

```typescript
import { BaseNode, Flow } from "../src/index";
import mermaid from "mermaid";

function generateFlowDiagram(start: BaseNode): string {
  // Configure mermaid
  mermaid.initialize({
    startOnLoad: false,
    theme: "default",
  });

  // Generate mermaid diagram definition
  const diagram = buildMermaidDefinition(start);

  // You can then render this diagram in a web context
  // or export it to SVG/PNG using mermaid's rendering capabilities
  return diagram;
}

// Helper function to build mermaid definition
function buildMermaidDefinition(start: BaseNode): string {
  // This is where we'll put our custom code to traverse the flow
  // and build a mermaid diagram definition
  // The implementation is similar to our custom approach below
  // [Implementation of custom traversal to generate mermaid diagram definition]
  // For a simple implementation example, see the custom approach below
  // The function returns a string containing the mermaid diagram definition
  // which can be rendered by the mermaid library
}
```

For rendering in a browser environment:

```typescript
// In a web context
const element = document.querySelector("#diagram");
mermaid.render("diagram-id", diagramDefinition, (svgCode) => {
  element.innerHTML = svgCode;
});
```

### Custom Implementation Alternative

If you prefer a custom lightweight implementation, this code recursively traverses the nested graph, assigns unique IDs to each node, and treats Flow nodes as subgraphs to generate Mermaid syntax for a hierarchical visualization:

{% raw %}

```typescript
import { BaseNode, Flow } from "../src/index";

type NodeIds = Map<BaseNode, string>;

function buildMermaid(start: BaseNode): string {
  const ids: NodeIds = new Map();
  const visited: Set<BaseNode> = new Set();
  const lines: string[] = ["graph LR"];
  let ctr = 1;

  function getId(node: BaseNode): string {
    if (ids.has(node)) {
      return ids.get(node)!;
    }
    const id = `N${ctr++}`;
    ids.set(node, id);
    return id;
  }

  function link(a: string, b: string): void {
    lines.push(`    ${a} --> ${b}`);
  }

  function walk(node: BaseNode, parent?: string): void {
    if (visited.has(node)) {
      if (parent) link(parent, getId(node));
      return;
    }

    visited.add(node);

    if (node instanceof Flow) {
      if (node.start && parent) {
        link(parent, getId(node.start));
      }

      lines.push(
        `\n    subgraph sub_flow_${getId(node)}[${node.constructor.name}]`
      );

      if (node.start) {
        walk(node.start);
      }

      // Get all successors from the node's internal map
      const successors = Array.from((node as any)._successors.entries());

      for (const [_, nextNode] of successors) {
        if (node.start) {
          walk(nextNode, getId(node.start));
        } else if (parent) {
          link(parent, getId(nextNode));
        } else {
          walk(nextNode);
        }
      }

      lines.push("    end\n");
    } else {
      const nodeId = getId(node);
      lines.push(`    ${nodeId}['${node.constructor.name}']`);

      if (parent) {
        link(parent, nodeId);
      }

      // Get all successors from the node's internal map
      const successors = Array.from((node as any)._successors.entries());

      for (const [_, nextNode] of successors) {
        walk(nextNode, nodeId);
      }
    }
  }

  walk(start);
  return lines.join("\n");
}
```

{% endraw %}

For example, suppose we have a complex Flow for data science:

```typescript
import { BaseNode, BatchNode, Node, Flow } from "../src/index";

class DataPrepBatchNode extends BatchNode {
  prep(shared: any): any[] {
    return [];
  }
}
class ValidateDataNode extends Node {}
class FeatureExtractionNode extends Node {}
class TrainModelNode extends Node {}
class EvaluateModelNode extends Node {}
class ModelFlow extends Flow {}
class DataScienceFlow extends Flow {}

const featureNode = new FeatureExtractionNode();
const trainNode = new TrainModelNode();
const evaluateNode = new EvaluateModelNode();
featureNode.next(trainNode);
trainNode.next(evaluateNode);
const modelFlow = new ModelFlow(featureNode);

const dataPrepNode = new DataPrepBatchNode();
const validateNode = new ValidateDataNode();
dataPrepNode.next(validateNode);
validateNode.next(modelFlow);
const dataScienceFlow = new DataScienceFlow(dataPrepNode);

const result = buildMermaid(dataScienceFlow);
```

The code generates a Mermaid diagram:

```mermaid
graph LR
    subgraph sub_flow_N1[DataScienceFlow]
    N2['DataPrepBatchNode']
    N3['ValidateDataNode']
    N2 --> N3
    N3 --> N4

    subgraph sub_flow_N5[ModelFlow]
    N4['FeatureExtractionNode']
    N6['TrainModelNode']
    N4 --> N6
    N7['EvaluateModelNode']
    N6 --> N7
    end

    end
```

## 2. Call Stack Debugging

There are several approaches to debug and trace execution in your flow application. Here are some recommended methods:

### Method 1: Using Decorators for Tracing (Recommended)

TypeScript decorators provide a clean way to add tracing functionality to your methods. This approach is more maintainable and flexible than manipulating the stack trace:

```typescript
import { BaseNode } from "../src/index";

// A simple tracing decorator that logs method execution
function trace(
  target: any,
  propertyKey: string,
  descriptor: PropertyDescriptor
) {
  const originalMethod = descriptor.value;

  descriptor.value = function (...args: any[]) {
    const nodeName = this.constructor.name;
    console.log(`Entering ${nodeName}.${propertyKey}`);

    try {
      const result = originalMethod.apply(this, args);
      console.log(`Exiting ${nodeName}.${propertyKey}`);
      return result;
    } catch (error) {
      console.error(`Error in ${nodeName}.${propertyKey}:`, error);
      throw error;
    }
  };

  return descriptor;
}

// Usage example
class MyNode extends BaseNode {
  @trace
  async prep(shared: any): Promise<any> {
    // Implementation
    return {};
  }

  @trace
  async exec(prepRes: any): Promise<any> {
    // Implementation
    return {};
  }

  @trace
  async post(
    shared: any,
    prepRes: any,
    execRes: any
  ): Promise<string | undefined> {
    // Implementation
    return "default";
  }
}
```

### Method 2: Stack Inspection

If you need to examine the call stack during runtime, you can implement a utility function that parses the Error stack trace:

```typescript
import { BaseNode } from "../src/index";

function getNodeCallStack(): string[] {
  const stack = new Error().stack || "";
  const stackLines = stack.split("\n").slice(1); // Skip the Error constructor line
  const nodeNames: string[] = [];
  const seenNodeIds = new Set<number>();

  // Regular expression to match the function names
  const functionNameRegex = /at\s+(\w+)\.(\w+)/;

  for (const line of stackLines) {
    const match = line.match(functionNameRegex);
    if (match) {
      // Try to find this.constructor.name in the stack frames
      // This is less precise than Python's inspect module, but can work
      // for basic stack tracing
      try {
        const instance = Function("return this")();
        if (instance instanceof BaseNode && !seenNodeIds.has(instance.nodeId)) {
          seenNodeIds.add(instance.nodeId);
          nodeNames.push(instance.constructor.name);
        }
      } catch (e) {
        // Unable to get instance from this context, continue
      }
    }
  }

  return nodeNames;
}
```

### Method 3: Using Existing Debugging Libraries

For production applications, consider using established debugging and tracing libraries:

```typescript
import debug from "debug"; // npm install debug
import { BaseNode } from "../src/index";

// Create debuggers for different aspects of your application
const prepDebug = debug("flow:prep");
const execDebug = debug("flow:exec");
const postDebug = debug("flow:post");

class DebuggableNode extends BaseNode {
  async prep(shared: any): Promise<any> {
    prepDebug(`${this.constructor.name} prep started with shared:`, shared);
    const result = await super.prep(shared);
    prepDebug(`${this.constructor.name} prep completed with result:`, result);
    return result;
  }

  async exec(prepRes: any): Promise<any> {
    execDebug(`${this.constructor.name} exec started with:`, prepRes);
    const result = await super.exec(prepRes);
    execDebug(`${this.constructor.name} exec completed with:`, result);
    return result;
  }

  async post(
    shared: any,
    prepRes: any,
    execRes: any
  ): Promise<string | undefined> {
    postDebug(`${this.constructor.name} post started`);
    const result = await super.post(shared, prepRes, execRes);
    postDebug(`${this.constructor.name} post completed with action:`, result);
    return result;
  }
}
```

To enable debugging, run your application with the DEBUG environment variable:

```bash
# Enable all flow debugging
DEBUG=flow:* node your-app.js

# Enable only specific parts
DEBUG=flow:prep,flow:post node your-app.js
```

### Example with Custom EvaluateModelNode

Here's a complete example using our original approach:

```typescript
import { BaseNode, BatchNode, Node, Flow } from "../src/index";

class DataPrepBatchNode extends BatchNode {
  prep(shared: any): any[] {
    return [];
  }
}
class ValidateDataNode extends Node {}
class FeatureExtractionNode extends Node {}
class TrainModelNode extends Node {}
class EvaluateModelNode extends Node {
  prep(shared: any): void {
    const stack = getNodeCallStack();
    console.log("Call stack:", stack);
  }
}
class ModelFlow extends Flow {}
class DataScienceFlow extends Flow {}

const featureNode = new FeatureExtractionNode();
const trainNode = new TrainModelNode();
const evaluateNode = new EvaluateModelNode();
featureNode.next(trainNode);
trainNode.next(evaluateNode);
const modelFlow = new ModelFlow(featureNode);

const dataPrepNode = new DataPrepBatchNode();
const validateNode = new ValidateDataNode();
dataPrepNode.next(validateNode);
validateNode.next(modelFlow);
const dataScienceFlow = new DataScienceFlow(dataPrepNode);

dataScienceFlow.run({});
```

The output would be: `Call stack: ['EvaluateModelNode', 'ModelFlow', 'DataScienceFlow']`

> Note: Each debugging approach has its strengths and limitations. The decorator-based approach offers the cleanest integration but requires TypeScript configuration with decorator support. The debug library approach is ideal for production systems with configurable logging levels.
</file>

<file path="docs/utility_function/websearch.md">
---
layout: default
title: "Web Search"
parent: "Utility Function"
nav_order: 3
---

# Web Search

We recommend some implementations of commonly used web search tools.

| **API**                           | **Free Tier**                                       | **Pricing Model**                                   | **Docs**                                                                    |
| --------------------------------- | --------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------- |
| **Google Custom Search JSON API** | 100 queries/day free                                | $5 per 1000 queries.                                | [Link](https://developers.google.com/custom-search/v1/overview)             |
| **Bing Web Search API**           | 1,000 queries/month                                 | $15â€“$25 per 1,000 queries.                          | [Link](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/) |
| **DuckDuckGo Instant Answer**     | Completely free (Instant Answers only, **no URLs**) | No paid plans; usage unlimited, but data is limited | [Link](https://duckduckgo.com/api)                                          |
| **Brave Search API**              | 2,000 queries/month free                            | $3 per 1k queries for Base, $5 per 1k for Pro       | [Link](https://brave.com/search/api/)                                       |
| **SerpApi**                       | 100 searches/month free                             | Start at $75/month for 5,000 searches               | [Link](https://serpapi.com/)                                                |

## Example TypeScript Code

### 1. Google Custom Search JSON API

```typescript
// Google doesn't provide an official SDK for JavaScript/TypeScript
// This example uses the googleapis package which provides a more SDK-like experience

import { google } from "googleapis";

interface GoogleSearchResult {
  kind: string;
  url: {
    type: string;
    template: string;
  };
  queries: {
    request: any[];
    nextPage?: any[];
  };
  context: {
    title: string;
  };
  searchInformation: {
    searchTime: number;
    formattedSearchTime: string;
    totalResults: string;
    formattedTotalResults: string;
  };
  items: Array<{
    kind: string;
    title: string;
    htmlTitle: string;
    link: string;
    displayLink: string;
    snippet: string;
    htmlSnippet: string;
    formattedUrl: string;
    htmlFormattedUrl: string;
    pagemap?: Record<string, any>;
  }>;
}

/**
 * Perform a Google Custom Search using the Google APIs client library
 */
async function googleSearch(
  query: string,
  apiKey: string,
  cxId: string,
  start: number = 1
): Promise<GoogleSearchResult> {
  // Initialize the Custom Search API service
  const customsearch = google.customsearch("v1");

  try {
    // Make the search request
    const response = await customsearch.cse.list({
      auth: apiKey,
      cx: cxId,
      q: query,
      start,
    });

    return response.data as GoogleSearchResult;
  } catch (error) {
    console.error("Error performing Google Custom Search:", error);
    throw error;
  }
}

// Example usage
const API_KEY = "YOUR_API_KEY";
const CX_ID = "YOUR_CX_ID";

googleSearch("example query", API_KEY, CX_ID)
  .then((results) => {
    console.log(`Total Results: ${results.searchInformation.totalResults}`);
    results.items.forEach((item) => {
      console.log(`Title: ${item.title}`);
      console.log(`Link: ${item.link}`);
      console.log(`Snippet: ${item.snippet}`);
      console.log("---");
    });
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 2. Bing Web Search API

```typescript
// Using the official Microsoft Bing Web Search SDK
import * as BingWebSearchAPI from "@azure/cognitiveservices-websearch";
import { CognitiveServicesCredentials } from "@azure/ms-rest-azure-js";

/**
 * Perform a Bing Web Search using the official Microsoft SDK
 */
async function bingSearch(
  query: string,
  subscriptionKey: string,
  count: number = 10,
  offset: number = 0,
  market: string = "en-US"
) {
  // Create a Cognitive Services credentials instance with your subscription key
  const credentials = new CognitiveServicesCredentials(subscriptionKey);

  // Create the search client
  const client = new BingWebSearchAPI.WebSearchClient(credentials);

  try {
    // Make the search request
    const searchResponse = await client.web.search(query, {
      count,
      offset,
      market,
      responseFilter: ["Webpages", "News", "Videos", "RelatedSearches"],
    });

    return searchResponse;
  } catch (error) {
    console.error("Error performing Bing Web Search:", error);
    throw error;
  }
}

// Example usage
const SUBSCRIPTION_KEY = "YOUR_BING_API_KEY";

bingSearch("example query", SUBSCRIPTION_KEY)
  .then((results) => {
    if (results.webPages) {
      console.log(`Total Results: ${results.webPages.totalEstimatedMatches}`);
      results.webPages.value.forEach((item) => {
        console.log(`Title: ${item.name}`);
        console.log(`URL: ${item.url}`);
        console.log(`Snippet: ${item.snippet}`);
        console.log("---");
      });
    } else {
      console.log("No web results found");
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 3. DuckDuckGo Instant Answer API

```typescript
// Using duck-duck-scrape, a community SDK for DuckDuckGo
import { DuckDuckGo } from "duck-duck-scrape";

interface DuckDuckGoResultItem {
  title: string;
  url: string;
  description: string;
}

/**
 * Search DuckDuckGo using the duck-duck-scrape library
 * This provides more functionality than the Instant Answer API alone
 */
async function duckDuckGoSearch(query: string, maxResults: number = 10) {
  const ddg = new DuckDuckGo();

  try {
    // Get instant answers and search results
    const response = await ddg.search(query, {
      safeSearch: "moderate",
      time: "y", // past year
      region: "us-en",
    });

    return {
      // Get instant answer if available
      abstractText: response.abstractText || "",
      abstractSource: response.abstractSource || "",
      abstractUrl: response.abstractUrl || "",

      // Get search results
      results: response.results.slice(0, maxResults).map((result) => ({
        title: result.title,
        url: result.url,
        description: result.description,
      })),

      // Get related topics
      relatedTopics: (response.relatedTopics || [])
        .slice(0, 5)
        .map((topic) => ({
          text: topic.text,
          url: topic.url,
        })),
    };
  } catch (error) {
    console.error("Error performing DuckDuckGo search:", error);
    throw error;
  }
}

// Example usage
duckDuckGoSearch("climate change")
  .then((results) => {
    if (results.abstractText) {
      console.log(`Abstract: ${results.abstractText}`);
      console.log(`Source: ${results.abstractSource} (${results.abstractUrl})`);
    }

    if (results.results.length > 0) {
      console.log("\nResults:");
      results.results.forEach((result, index) => {
        console.log(`${index + 1}. ${result.title}`);
        console.log(`   URL: ${result.url}`);
        console.log(`   ${result.description}`);
        console.log("---");
      });
    }

    if (results.relatedTopics.length > 0) {
      console.log("\nRelated Topics:");
      results.relatedTopics.forEach((topic) => {
        console.log(`- ${topic.text} (${topic.url})`);
      });
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 4. Brave Search API

```typescript
// Using brave-search TypeScript SDK
import { BraveSearch } from "brave-search";

/**
 * Perform a Brave Search using the brave-search SDK
 */
async function braveSearchWithSDK(
  query: string,
  apiKey: string,
  count: number = 10,
  offset: number = 0
) {
  // Initialize the Brave Search client
  const braveClient = new BraveSearch(apiKey);

  try {
    // Make the search request
    const results = await braveClient.search({
      q: query,
      count,
      offset,
    });

    return results;
  } catch (error) {
    console.error("Error performing Brave search:", error);
    throw error;
  }
}

// Example usage
const BRAVE_API_TOKEN = "YOUR_BRAVE_API_TOKEN";

braveSearchWithSDK("renewable energy", BRAVE_API_TOKEN)
  .then((results) => {
    console.log("Search Results:");

    // Access the web results
    if (results.web && results.web.results) {
      results.web.results.forEach((result, index) => {
        console.log(`${index + 1}. ${result.title}`);
        console.log(`   URL: ${result.url}`);
        console.log(`   ${result.description}`);
        console.log("---");
      });
    }

    // Check if automatic summarization is available (Pro plan feature)
    if (results.goggles) {
      console.log("Automatic Summarization:");
      console.log(results.goggles.content);
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 5. SerpApi

```typescript
// Using the official SerpApi SDK
import { getJson } from "serpapi";

interface SerpApiResult {
  search_metadata: {
    id: string;
    status: string;
    json_endpoint: string;
    created_at: string;
    processed_at: string;
    google_url: string;
    raw_html_file: string;
    total_time_taken: number;
  };
  search_parameters: {
    engine: string;
    q: string;
    location_requested: string;
    location_used: string;
    google_domain: string;
    hl: string;
    gl: string;
    device: string;
  };
  search_information: {
    organic_results_state: string;
    query_displayed: string;
    total_results: number;
    time_taken_displayed: number;
  };
  organic_results: Array<{
    position: number;
    title: string;
    link: string;
    displayed_link: string;
    snippet: string;
    snippet_highlighted_words: string[];
    sitelinks?: any;
    about_this_result?: any;
    cached_page_link?: string;
    related_pages_link?: string;
  }>;
  related_questions?: any[];
  related_searches?: any[];
  pagination?: any;
  knowledge_graph?: any;
  answer_box?: any;
}

/**
 * Perform a search using the SerpApi SDK
 */
async function serpApiSearch(
  query: string,
  apiKey: string,
  location: string = "United States",
  num: number = 10,
  start: number = 0
): Promise<SerpApiResult> {
  try {
    // Make the search request using the SDK
    return (await getJson({
      engine: "google",
      api_key: apiKey,
      q: query,
      location,
      num,
      start,
    })) as SerpApiResult;
  } catch (error) {
    console.error("Error performing SerpApi search:", error);
    throw error;
  }
}

// Example usage
const SERPAPI_KEY = "YOUR_SERPAPI_KEY";

serpApiSearch("electric vehicles", SERPAPI_KEY)
  .then((response) => {
    console.log(`Total Results: ${response.search_information.total_results}`);
    console.log("Organic Results:");

    response.organic_results.forEach((result, index) => {
      console.log(`${index + 1}. ${result.title}`);
      console.log(`   URL: ${result.link}`);
      console.log(`   ${result.snippet}`);
      console.log("---");
    });

    if (response.pagination) {
      console.log("Pagination available for more results");
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```
</file>

<file path="docs/_config.yml">
# Basic site settings
title: PocketFlow.js
tagline: A 100-line LLM framework
description: Minimalist LLM Framework in 100 Lines, Enabling LLMs to Program Themselves

# Theme settings
remote_theme: just-the-docs/just-the-docs

# Navigation
nav_sort: case_sensitive

# Aux links (shown in upper right)
aux_links:
  "View on GitHub":
    - "//github.com/the-pocket/PocketFlow"

# Color scheme
color_scheme: light

# Author settings
author:
  name: Zachary Huang
  url: https://www.columbia.edu/~zh2408/
  twitter: ZacharyHuang12

# Mermaid settings
mermaid:
  version: "9.1.3" # Pick the version you want
  # Default configuration
  config: |
    directionLR

# Callouts settings
callouts:
  warning:
    title: Warning
    color: red
  note:
    title: Note
    color: blue
  best-practice:
    title: Best Practice
    color: green

# The custom navigation
nav:
  - Home: index.md # Link to your main docs index
  - GitHub: "https://github.com/the-pocket/PocketFlow"
  - Discord: "https://discord.gg/hUHHE9Sa6T"
</file>

<file path="docs/guide.md">
---
layout: default
title: "Agentic Coding"
---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agents involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
> {: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps             |   Human    |     AI     | Comment                                                                                        |
| :---------------- | :--------: | :--------: | :--------------------------------------------------------------------------------------------- |
| 1. Requirements   |  â˜…â˜…â˜… High  |  â˜…â˜†â˜† Low   | Humans understand the requirements and context.                                                |
| 2. Flow           | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans specify the high-level design, and the AI fills in the details.                         |
| 3. Utilities      | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Node           |  â˜…â˜†â˜† Low   |  â˜…â˜…â˜… High  | The AI helps design the node types and data handling based on the flow.                        |
| 5. Implementation |  â˜…â˜†â˜† Low   |  â˜…â˜…â˜… High  | The AI implements the flow based on the design.                                                |
| 6. Optimization   | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans evaluate the results, and the AI helps optimize.                                        |
| 7. Reliability    |  â˜…â˜†â˜† Low   |  â˜…â˜…â˜… High  | The AI writes test cases and addresses corner cases.                                           |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit.

   - Understand AI systems' strengths and limitations:
     - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
     - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
     - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
   - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
   - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.

   - Identify applicable design patterns (e.g., [Map Reduce](./design_pattern/mapreduce.md), [Agent](./design_pattern/agent.md), [RAG](./design_pattern/rag.md)).
     - For each node in the flow, start with a high-level one-line description of what it does.
     - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
     - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
     - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
   - Outline the flow and draw it in a mermaid diagram. For example:

     ```mermaid
     flowchart LR
         start[Start] --> batch[Batch]
         batch --> check[Check]
         check -->|OK| process
         check -->|Error| fix[Fix]
         fix --> check

         subgraph process[Process]
           step1[Step 1] --> step2[Step 2]
         end

         process --> endNode[End]
     ```

   - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
     > {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.

   - Think of your AI system as the brain. It needs a bodyâ€”these _external utility functions_â€”to interact with the real world:
       <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

     - Reading inputs (e.g., retrieving Slack messages, reading emails)
     - Writing outputs (e.g., generating reports, sending emails)
     - Using external tools (e.g., calling LLMs, searching the web)
     - **NOTE**: _LLM-based tasks_ (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are _core functions_ internal in the AI system.

   - For each utility function, implement it and write a simple test.
   - Document their input/output, as well as why they are necessary. For example:
     - `name`: `getEmbedding` (`src/utils/getEmbedding.ts`)
     - `input`: `string`
     - `output`: a vector of 3072 numbers
     - `necessity`: Used by the second node to embed text
   - Example utility implementation:

     ```typescript
     // src/utils/callLlm.ts
     import { OpenAI } from "openai";

     export async function callLlm(prompt: string): Promise<string> {
       const client = new OpenAI({
         apiKey: process.env.OPENAI_API_KEY,
       });

       const response = await client.chat.completions.create({
         model: "gpt-4o",
         messages: [{ role: "user", content: prompt }],
       });

       return response.choices[0].message.content || "";
     }
     ```

   - > **Sometimes, design Utilies before Flow:** For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
     > {: .best-practice }

4. **Node Design**: Plan how each node will read and write data, and use utility functions.

   - One core design principle for PocketFlow is to use a [shared store](./core_abstraction/communication.md), so start with a shared store design:

     - For simple systems, use an in-memory object.
     - For more complex systems or when persistence is required, use a database.
     - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
     - Example shared store design:

       ```typescript
       interface SharedStore {
         user: {
           id: string;
           context: {
             weather: { temp: number; condition: string };
             location: string;
           };
         };
         results: Record<string, unknown>;
       }

       const shared: SharedStore = {
         user: {
           id: "user123",
           context: {
             weather: { temp: 72, condition: "sunny" },
             location: "San Francisco",
           },
         },
         results: {}, // Empty object to store outputs
       };
       ```

   - For each [Node](./core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Node (or BatchNode, or ParallelBatchNode)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function
     - `post`: Write "embedding" to the shared store

5. **Implementation**: Implement the initial nodes and flows based on the design.

   - ðŸŽ‰ If you've reached this step, humans have finished the design. Now _Agentic Coding_ begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Avoid `try` logic so you can quickly identify any weak points in the system.
   - Add logging throughout the code to facilitate debugging.

6. **Optimization**:

   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:

     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3â€“6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     > {: .best-practice }

7. **Reliability**
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `maxRetries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts
â”‚   â”œâ”€â”€ nodes.ts
â”‚   â”œâ”€â”€ flow.ts
â”‚   â”œâ”€â”€ types.ts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ callLlm.ts
â”‚       â””â”€â”€ searchWeb.ts
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ design.md
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

- **`docs/design.md`**: Contains project documentation for each step above. This should be _high-level_ and _no-code_.
- **`src/types.ts`**: Contains shared type definitions and interfaces used throughout the project.
- **`src/utils/`**: Contains all utility functions.
  - It's recommended to dedicate one TypeScript file to each API call, for example `callLlm.ts` or `searchWeb.ts`.
  - Each file should export functions that can be imported elsewhere in the project
  - Include test cases for each utility function using `utilityFunctionName.test.ts`
- **`src/nodes.ts`**: Contains all the node definitions.

  ```typescript
  // src/types.ts
  export interface QASharedStore {
    question?: string;
    answer?: string;
  }
  ```

  ```typescript
  // src/nodes.ts
  import { Node } from "pocketflow";
  import { callLlm } from "./utils/callLlm";
  import { QASharedStore } from "./types";
  import PromptSync from "prompt-sync";

  const prompt = PromptSync();

  export class GetQuestionNode extends Node<QASharedStore> {
    async exec(): Promise<string> {
      // Get question directly from user input
      const userQuestion = prompt("Enter your question: ") || "";
      return userQuestion;
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the user's question
      shared.question = execRes;
      return "default"; // Go to the next node
    }
  }

  export class AnswerNode extends Node<QASharedStore> {
    async prep(shared: QASharedStore): Promise<string> {
      // Read question from shared
      return shared.question || "";
    }

    async exec(question: string): Promise<string> {
      // Call LLM to get the answer
      return await callLlm(question);
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the answer in shared
      shared.answer = execRes;
      return undefined;
    }
  }
  ```

- **`src/flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.

  ```typescript
  // src/flow.ts
  import { Flow } from "pocketflow";
  import { GetQuestionNode, AnswerNode } from "./nodes";
  import { QASharedStore } from "./types";

  export function createQaFlow(): Flow {
    // Create nodes
    const getQuestionNode = new GetQuestionNode();
    const answerNode = new AnswerNode();

    // Connect nodes in sequence
    getQuestionNode.next(answerNode);

    // Create flow starting with input node
    return new Flow<QASharedStore>(getQuestionNode);
  }
  ```

- **`src/index.ts`**: Serves as the project's entry point.

  ```typescript
  // src/index.ts
  import { createQaFlow } from "./flow";
  import { QASharedStore } from "./types";

  // Example main function
  async function main(): Promise<void> {
    const shared: QASharedStore = {
      question: undefined, // Will be populated by GetQuestionNode from user input
      answer: undefined, // Will be populated by AnswerNode
    };

    // Create the flow and run it
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    console.log(`Question: ${shared.question}`);
    console.log(`Answer: ${shared.answer}`);
  }

  // Run the main function
  main().catch(console.error);
  ```

- **`package.json`**: Contains project metadata and dependencies.

- **`tsconfig.json`**: Contains TypeScript compiler configuration.
</file>

<file path="docs/index.md">
---
layout: default
title: "Home"
nav_order: 1
---

# PocketFlow.js

A [100-line](https://github.com/The-Pocket/PocketFlow-Typescript/blob/main/src/index.ts) minimalist LLM framework for _Agents, Task Decomposition, RAG, etc_.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworksâ€”([Multi-](./design_pattern/multi_agent.html))[Agents](./design_pattern/agent.html), [Workflow](./design_pattern/workflow.html), [RAG](./design_pattern/rag.html), and more.
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](./core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](./core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](./core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](./core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [(Advanced) Parallel](./core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="700"/>
</div>

## Design Pattern

From there, itâ€™s easy to implement popular design patterns:

- [Agent](./design_pattern/agent.md) autonomously makes decisions.
- [Workflow](./design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](./design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](./design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](./design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](./design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="700"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer _examples_â€”please _implement your own_:

- [LLM Wrapper](./utility_function/llm.md)
- [Viz and Debug](./utility_function/viz.md)
- [Web Search](./utility_function/websearch.md)
- [Chunking](./utility_function/chunking.md)
- [Embedding](./utility_function/embedding.md)
- [Vector Databases](./utility_function/vector.md)
- [Text-to-Speech](./utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a _bad practice_ for vendor-specific APIs in a general framework:

- _API Volatility_: Frequent changes lead to heavy maintenance for hardcoded APIs.
- _Flexibility_: You may want to switch vendors, use fine-tuned models, or run them locally.
- _Optimizations_: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps?

Check out [Agentic Coding Guidance](./guide.md), the fastest way to develop LLM projects with PocketFlow.js!
</file>

<file path="src/index.ts">
type NonIterableObject = Partial<Record<string, unknown>> & { [Symbol.iterator]?: never }; type Action = string;
class BaseNode<S = unknown, P extends NonIterableObject = NonIterableObject> {
  protected _params: P = {} as P; protected _successors: Map<Action, BaseNode> = new Map();
  protected async _exec(prepRes: unknown): Promise<unknown> { return await this.exec(prepRes); }
  async prep(shared: S): Promise<unknown> { return undefined; }
  async exec(prepRes: unknown): Promise<unknown> { return undefined; }
  async post(shared: S, prepRes: unknown, execRes: unknown): Promise<Action | undefined> { return undefined; }
  async _run(shared: S): Promise<Action | undefined> {
    const p = await this.prep(shared), e = await this._exec(p); return await this.post(shared, p, e);
  }
  async run(shared: S): Promise<Action | undefined> {
    if (this._successors.size > 0) console.warn("Node won't run successors. Use Flow.");
    return await this._run(shared);
  }
  setParams(params: P): this { this._params = params; return this; }
  next<T extends BaseNode>(node: T): T { this.on("default", node); return node; }
  on(action: Action, node: BaseNode): this {
    if (this._successors.has(action)) console.warn(`Overwriting successor for action '${action}'`);
    this._successors.set(action, node); return this;
  }
  getNextNode(action: Action = "default"): BaseNode | undefined {
    const nextAction = action || 'default', next = this._successors.get(nextAction)
    if (!next && this._successors.size > 0)
      console.warn(`Flow ends: '${nextAction}' not found in [${Array.from(this._successors.keys())}]`)
    return next
  }
  clone(): this {
    const clonedNode = Object.create(Object.getPrototypeOf(this)); Object.assign(clonedNode, this);
    clonedNode._params = { ...this._params }; clonedNode._successors = new Map(this._successors);
    return clonedNode;
  }
}
class Node<S = unknown, P extends NonIterableObject = NonIterableObject> extends BaseNode<S, P> {
  maxRetries: number; wait: number; currentRetry: number = 0;
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(); this.maxRetries = maxRetries; this.wait = wait;
  }
  async execFallback(prepRes: unknown, error: Error): Promise<unknown> { throw error; }
  async _exec(prepRes: unknown): Promise<unknown> {
    for (this.currentRetry = 0; this.currentRetry < this.maxRetries; this.currentRetry++) {
      try { return await this.exec(prepRes); } 
      catch (e) {
        if (this.currentRetry === this.maxRetries - 1) return await this.execFallback(prepRes, e as Error);
        if (this.wait > 0) await new Promise(resolve => setTimeout(resolve, this.wait * 1000));
      }
    }
    return undefined;
  }
}
class BatchNode<S = unknown, P extends NonIterableObject = NonIterableObject> extends Node<S, P> {
  async _exec(items: unknown[]): Promise<unknown[]> {
    if (!items || !Array.isArray(items)) return [];
    const results = []; for (const item of items) results.push(await super._exec(item)); return results;
  }
}
class ParallelBatchNode<S = unknown, P extends NonIterableObject = NonIterableObject> extends Node<S, P> {
  async _exec(items: unknown[]): Promise<unknown[]> {
    if (!items || !Array.isArray(items)) return []
    return Promise.all(items.map((item) => super._exec(item)))
  }
}
class Flow<S = unknown, P extends NonIterableObject = NonIterableObject> extends BaseNode<S, P> {
  start: BaseNode;
  constructor(start: BaseNode) { super(); this.start = start; }
  protected async _orchestrate(shared: S, params?: P): Promise<void> {
    let current: BaseNode | undefined = this.start.clone();
    const p = params || this._params;
    while (current) {
      current.setParams(p); const action = await current._run(shared);
      current = current.getNextNode(action); current = current?.clone();
    }
  }
  async _run(shared: S): Promise<Action | undefined> {
    const pr = await this.prep(shared); await this._orchestrate(shared);
    return await this.post(shared, pr, undefined);
  }
  async exec(prepRes: unknown): Promise<unknown> { throw new Error("Flow can't exec."); }
}
class BatchFlow<S = unknown, P extends NonIterableObject = NonIterableObject, NP extends NonIterableObject[] = NonIterableObject[]> extends Flow<S, P> {
  async _run(shared: S): Promise<Action | undefined> {
    const batchParams = await this.prep(shared);
    for (const bp of batchParams) {
      const mergedParams = { ...this._params, ...bp };
      await this._orchestrate(shared, mergedParams);
    }
    return await this.post(shared, batchParams, undefined);
  }
  async prep(shared: S): Promise<NP> { const empty: readonly NonIterableObject[] = []; return empty as NP; }
}
class ParallelBatchFlow<S = unknown, P extends NonIterableObject = NonIterableObject, NP extends NonIterableObject[] = NonIterableObject[]> extends BatchFlow<S, P, NP> {
  async _run(shared: S): Promise<Action | undefined> {
    const batchParams = await this.prep(shared);
    await Promise.all(batchParams.map(bp => {
      const mergedParams = { ...this._params, ...bp };
      return this._orchestrate(shared, mergedParams);
    }));
    return await this.post(shared, batchParams, undefined);
  }
}
export { BaseNode, Node, BatchNode, ParallelBatchNode, Flow, BatchFlow, ParallelBatchFlow };
</file>

<file path="tests/batch-flow.test.ts">
// tests/async-batch-flow.test.ts
import { Node, BatchFlow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  inputData?: Record<string, number>;
  results?: Record<string, number>;
  intermediateResults?: Record<string, number>;
};

// Parameters type
type BatchParams = {
  key: string;
  multiplier?: number;
};

class AsyncDataProcessNode extends Node<SharedStorage, BatchParams> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number> {
    const key = this._params.key;
    const data = shared.inputData?.[key] ?? 0;

    if (!shared.results) {
      shared.results = {};
    }

    shared.results[key] = data;
    return data;
  }

  async exec(prepRes: number): Promise<number> {
    return prepRes; // Just return the prep result as-is
  }

  async post(
    shared: SharedStorage,
    prepRes: number,
    execRes: number
  ): Promise<string | undefined> {
    await new Promise((resolve) => setTimeout(resolve, 10)); // Simulate async work
    const key = this._params.key;

    if (!shared.results) {
      shared.results = {};
    }

    shared.results[key] = execRes * 2; // Double the value
    return 'processed';
  }
}

class AsyncErrorNode extends Node<SharedStorage, BatchParams> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<any> {
    return undefined;
  }

  async exec(prepRes: any): Promise<any> {
    return undefined;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: any
  ): Promise<string | undefined> {
    const key = this._params.key;
    if (key === 'errorKey') {
      throw new Error(`Async error processing key: ${key}`);
    }
    return 'processed';
  }
}

describe('BatchFlow Tests', () => {
  let processNode: AsyncDataProcessNode;

  beforeEach(() => {
    processNode = new AsyncDataProcessNode();
  });

  test('basic async batch processing', async () => {
    class SimpleTestBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }
    }

    const shared: SharedStorage = {
      inputData: {
        a: 1,
        b: 2,
        c: 3,
      },
    };

    const flow = new SimpleTestBatchFlow(processNode);
    await flow.run(shared);

    expect(shared.results).toEqual({
      a: 2, // 1 * 2
      b: 4, // 2 * 2
      c: 6, // 3 * 2
    });
  });

  test('empty async batch', async () => {
    class EmptyTestBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        // Initialize results as an empty object
        if (!shared.results) {
          shared.results = {};
        }
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }

      // Ensure post is called even if batch is empty
      async post(
        shared: SharedStorage,
        prepRes: BatchParams[],
        execRes: any
      ): Promise<string | undefined> {
        if (!shared.results) {
          shared.results = {};
        }
        return undefined;
      }
    }

    const shared: SharedStorage = {
      inputData: {},
    };

    const flow = new EmptyTestBatchFlow(processNode);
    await flow.run(shared);

    expect(shared.results).toEqual({});
  });

  test('async error handling', async () => {
    class ErrorTestBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }
    }

    const shared: SharedStorage = {
      inputData: {
        normalKey: 1,
        errorKey: 2,
        anotherKey: 3,
      },
    };

    const flow = new ErrorTestBatchFlow(new AsyncErrorNode());

    await expect(async () => {
      await flow.run(shared);
    }).rejects.toThrow('Async error processing key: errorKey');
  });

  test('nested async flow', async () => {
    class AsyncInnerNode extends Node<SharedStorage, BatchParams> {
      async prep(shared: SharedStorage): Promise<any> {
        return undefined;
      }

      async exec(prepRes: any): Promise<any> {
        return undefined;
      }

      async post(
        shared: SharedStorage,
        prepRes: any,
        execRes: any
      ): Promise<string | undefined> {
        const key = this._params.key;

        if (!shared.intermediateResults) {
          shared.intermediateResults = {};
        }

        // Safely access inputData
        const inputValue = shared.inputData?.[key] ?? 0;
        shared.intermediateResults[key] = inputValue + 1;

        await new Promise((resolve) => setTimeout(resolve, 10));
        return 'next';
      }
    }

    class AsyncOuterNode extends Node<SharedStorage, BatchParams> {
      async prep(shared: SharedStorage): Promise<any> {
        return undefined;
      }

      async exec(prepRes: any): Promise<any> {
        return undefined;
      }

      async post(
        shared: SharedStorage,
        prepRes: any,
        execRes: any
      ): Promise<string | undefined> {
        const key = this._params.key;

        if (!shared.results) {
          shared.results = {};
        }

        if (!shared.intermediateResults) {
          shared.intermediateResults = {};
        }

        shared.results[key] = shared.intermediateResults[key] * 2;
        await new Promise((resolve) => setTimeout(resolve, 10));
        return 'done';
      }
    }

    class NestedBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }
    }

    // Create inner flow
    const innerNode = new AsyncInnerNode();
    const outerNode = new AsyncOuterNode();
    innerNode.on('next', outerNode);

    const shared: SharedStorage = {
      inputData: {
        x: 1,
        y: 2,
      },
    };

    const flow = new NestedBatchFlow(innerNode);
    await flow.run(shared);

    expect(shared.results).toEqual({
      x: 4, // (1 + 1) * 2
      y: 6, // (2 + 1) * 2
    });
  });

  test('custom async parameters', async () => {
    class CustomParamNode extends Node<SharedStorage, BatchParams> {
      async prep(shared: SharedStorage): Promise<any> {
        return undefined;
      }

      async exec(prepRes: any): Promise<any> {
        return undefined;
      }

      async post(
        shared: SharedStorage,
        prepRes: any,
        execRes: any
      ): Promise<string | undefined> {
        const key = this._params.key;
        const multiplier = this._params.multiplier || 1;

        await new Promise((resolve) => setTimeout(resolve, 10));

        if (!shared.results) {
          shared.results = {};
        }

        // Safely access inputData with default value
        const inputValue = shared.inputData?.[key] ?? 0;
        shared.results[key] = inputValue * multiplier;

        return 'done';
      }
    }

    class CustomParamBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k, i) => ({
          key: k,
          multiplier: i + 1,
        }));
      }
    }

    const shared: SharedStorage = {
      inputData: {
        a: 1,
        b: 2,
        c: 3,
      },
    };

    const flow = new CustomParamBatchFlow(new CustomParamNode());
    await flow.run(shared);

    expect(shared.results).toEqual({
      a: 1 * 1, // first item, multiplier = 1
      b: 2 * 2, // second item, multiplier = 2
      c: 3 * 3, // third item, multiplier = 3
    });
  });
});
</file>

<file path="tests/batch-node.test.ts">
// tests/batch-node.test.ts
import { Node, BatchNode, Flow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  inputArray?: number[];
  chunkResults?: number[];
  total?: number;
  result?: string;
  parallelResults?: number[];
  completionOrder?: number[];
};

class AsyncArrayChunkNode extends BatchNode<SharedStorage> {
  private chunkSize: number;

  constructor(
    chunkSize: number = 10,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
    this.chunkSize = chunkSize;
  }

  async prep(shared: SharedStorage): Promise<number[][]> {
    // Get array from shared storage and split into chunks
    const array = shared.inputArray || [];
    const chunks: number[][] = [];

    for (let start = 0; start < array.length; start += this.chunkSize) {
      const end = Math.min(start + this.chunkSize, array.length);
      chunks.push(array.slice(start, end));
    }

    return chunks;
  }

  async exec(chunk: number[]): Promise<number> {
    // Simulate async processing of each chunk
    await new Promise((resolve) => setTimeout(resolve, 10));
    return chunk.reduce((sum, value) => sum + value, 0);
  }

  async post(
    shared: SharedStorage,
    prepRes: number[][],
    execRes: number[]
  ): Promise<string | undefined> {
    // Store chunk results in shared storage
    shared.chunkResults = execRes;
    return 'processed';
  }
}

class AsyncSumReduceNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    // Get chunk results from shared storage
    return shared.chunkResults || [];
  }

  async exec(chunkResults: number[]): Promise<number> {
    // Simulate async processing
    await new Promise((resolve) => setTimeout(resolve, 10));
    return chunkResults.reduce((sum, value) => sum + value, 0);
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number
  ): Promise<string | undefined> {
    // Store the total in shared storage
    shared.total = execRes;
    return 'reduced';
  }
}

// Create an error-throwing node for testing error handling
class ErrorBatchNode extends BatchNode<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    return shared.inputArray || [];
  }

  async exec(item: number): Promise<number> {
    if (item === 2) {
      throw new Error('Error processing item 2');
    }
    return item;
  }
}

describe('BatchNode Tests', () => {
  test('array chunking', async () => {
    // Test that the array is correctly split into chunks and processed asynchronously
    const shared: SharedStorage = {
      inputArray: Array.from({ length: 25 }, (_, i) => i), // [0,1,2,...,24]
    };

    const chunkNode = new AsyncArrayChunkNode(10);
    await chunkNode.run(shared);

    expect(shared.chunkResults).toEqual([45, 145, 110]); // Sum of chunks [0-9], [10-19], [20-24]
  });

  test('async map-reduce sum', async () => {
    // Test a complete async map-reduce pipeline that sums a large array
    const array = Array.from({ length: 100 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0); // 4950

    const shared: SharedStorage = {
      inputArray: array,
    };

    // Create nodes
    const chunkNode = new AsyncArrayChunkNode(10);
    const reduceNode = new AsyncSumReduceNode();

    // Connect nodes
    chunkNode.on('processed', reduceNode);

    // Create and run pipeline
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('uneven chunks', async () => {
    // Test that the async map-reduce works correctly with array lengths
    // that don't divide evenly by chunkSize
    const array = Array.from({ length: 25 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0); // 300

    const shared: SharedStorage = {
      inputArray: array,
    };

    const chunkNode = new AsyncArrayChunkNode(10);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('custom chunk size', async () => {
    // Test that the async map-reduce works with different chunk sizes
    const array = Array.from({ length: 100 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0);

    const shared: SharedStorage = {
      inputArray: array,
    };

    // Use chunkSize=15 instead of default 10
    const chunkNode = new AsyncArrayChunkNode(15);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('single element chunks', async () => {
    // Test extreme case where chunkSize=1
    const array = Array.from({ length: 5 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0);

    const shared: SharedStorage = {
      inputArray: array,
    };

    const chunkNode = new AsyncArrayChunkNode(1);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('empty array', async () => {
    // Test edge case of empty input array
    const shared: SharedStorage = {
      inputArray: [],
    };

    const chunkNode = new AsyncArrayChunkNode(10);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(0);
  });

  test('error handling', async () => {
    // Test error handling in async batch processing
    const shared: SharedStorage = {
      inputArray: [1, 2, 3],
    };

    const errorNode = new ErrorBatchNode();

    await expect(async () => {
      await errorNode.run(shared);
    }).rejects.toThrow('Error processing item 2');
  });

  test('retry mechanism', async () => {
    // Test the retry mechanism with a node that fails intermittently
    let attempts = 0;

    class RetryTestNode extends BatchNode<SharedStorage> {
      constructor() {
        super(3, 0.01); // 3 retries with 10ms wait time
      }

      async prep(shared: SharedStorage): Promise<number[]> {
        return [1];
      }

      async exec(item: number): Promise<string> {
        attempts++;
        if (attempts < 3) {
          throw new Error(`Failure on attempt ${attempts}`);
        }
        return `Success on attempt ${attempts}`;
      }

      async post(
        shared: SharedStorage,
        prepRes: number[],
        execRes: string[]
      ): Promise<string | undefined> {
        shared.result = execRes[0];
        return undefined;
      }
    }

    const shared: SharedStorage = {};
    const retryNode = new RetryTestNode();

    await retryNode.run(shared);

    expect(attempts).toBe(3);
    expect(shared.result).toBe('Success on attempt 3');
  });

  test('parallel batch processing', async () => {
    // Test that ParallelBatchNode processes items in parallel
    let completed: number[] = [];

    // Import ParallelBatchNode
    const { ParallelBatchNode } = require('../src/index');

    class ParallelProcessingNode extends ParallelBatchNode<SharedStorage> {
      constructor() {
        super(1, 0);
      }

      async prep(shared: SharedStorage): Promise<number[]> {
        return [1, 2, 3, 4, 5];
      }

      async exec(item: number): Promise<number> {
        // Items with higher values will complete first
        const delay = 100 - item * 20;
        await new Promise((resolve) => setTimeout(resolve, delay));
        completed.push(item);
        return item;
      }

      async post(
        shared: SharedStorage,
        prepRes: number[],
        execRes: number[]
      ): Promise<string | undefined> {
        shared.parallelResults = execRes;
        shared.completionOrder = [...completed];
        return undefined;
      }
    }

    const shared: SharedStorage = {};
    const parallelNode = new ParallelProcessingNode();

    await parallelNode.run(shared);

    // Check that results contain all processed items
    expect(shared.parallelResults?.sort()).toEqual([1, 2, 3, 4, 5]);

    // Check that items were not necessarily processed in order
    // Higher numbered items should complete before lower ones due to the delay logic
    expect(shared.completionOrder).not.toEqual([1, 2, 3, 4, 5]);
  });
});
</file>

<file path="tests/core-abstraction-examples.test.ts">
// tests/core_abstraction_examples.test.ts
import { BaseNode, Node, BatchNode, ParallelBatchNode, Flow, BatchFlow, ParallelBatchFlow } from '../src/index';

// Define shared storage types
type SharedStorage = {
  data?: string;
  summary?: string;
  files?: string[];
  results?: any[];
  current?: number;
  expenses?: Array<{id: string; amount: number; status?: string}>;
  // For nested flows example
  payments?: Array<{id: string; status: string}>;
  inventory?: Array<{id: string; status: string}>;
  shipping?: Array<{id: string; status: string}>;
  orderComplete?: boolean;
};

// 1. Simple Node Example (SummarizeFile from node.md)
class SummarizeFile extends Node<SharedStorage, FileParams> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    return shared.data;
  }

  async exec(prepRes: string | undefined): Promise<string> {
    if (!prepRes) {
      return "Empty file content";
    }
    // Simulate LLM call
    return `Summary: ${prepRes.substring(0, 10)}...`;
  }

  async execFallback(prepRes: string | undefined, error: Error): Promise<string> {
    // Provide a simple fallback
    return "There was an error processing your request.";
  }

  async post(shared: SharedStorage, prepRes: string | undefined, execRes: string): Promise<string | undefined> {
    shared.summary = execRes;
    return undefined; // "default" action
  }
}

// 2. Expense Approval Flow example (from flow.md)
class ReviewExpense extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    // Select the first pending expense
    return shared.expenses?.find(e => !e.status);
  }

  async post(shared: SharedStorage, prepRes: any): Promise<string> {
    if (!prepRes) return "finished";
    
    // Based on amount, determine approval path
    const expense = prepRes;
    if (expense.amount <= 100) {
      expense.status = "approved";
      return "approved";
    } else if (expense.amount > 500) {
      expense.status = "rejected";
      return "rejected";
    } else {
      expense.status = "needs_revision";
      return "needs_revision";
    }
  }
}

class ReviseExpense extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    // Find expense that needs revision
    return shared.expenses?.find(e => e.status === "needs_revision");
  }

  async post(shared: SharedStorage, prepRes: any): Promise<string> {
    if (!prepRes) return "finished";
    
    // Reduce the expense amount to make it more likely to be approved
    const expense = prepRes;
    expense.amount = 75; // Revised to a lower amount
    expense.status = undefined; // Reset status for re-review
    
    return "default"; // Go back to review
  }
}

class ProcessPayment extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    // Find approved expenses to process
    return shared.expenses?.find(e => e.status === "approved");
  }

  async post(shared: SharedStorage, prepRes: any): Promise<string> {
    if (!prepRes) return "finished";
    
    // Mark expense as paid
    const expense = prepRes;
    expense.status = "paid";
    
    return "default"; // Continue to finish
  }
}

class FinishProcess extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    // Simply mark the process as done and return
    return "finished";
  }
}

// 3. BatchNode example (MapSummaries from batch.md)
class MapSummaries extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // Suppose we have a big file; chunk it
    const content = shared.data || "";
    const chunks: string[] = [];
    const chunkSize = 10;

    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize));
    }

    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    // Process each chunk
    return `Chunk summary: ${chunk.substring(0, 3)}...`;
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    // Combine summaries
    shared.summary = execRes.join("\n");
    return "default";
  }
}

// 4. BatchFlow example (SummarizeAllFiles from batch.md)
type FileParams = {
  filename: string;
};

class LoadFile extends Node<SharedStorage, FileParams> {
  async prep(shared: SharedStorage): Promise<string> {
    // Simulate loading a file
    const filename = this._params.filename;
    return `Content of ${filename}`;
  }

  async post(shared: SharedStorage, prepRes: string): Promise<string | undefined> {
    shared.data = prepRes;
    return undefined;
  }
}

class SummarizeAllFiles extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    return (shared.files || []).map((filename) => ({ filename }));
  }
}

// 5. ParallelBatchNode example (TextSummarizer from parallel.md)
class TextSummarizer extends ParallelBatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // For testing, we'll create an array of texts
    return shared.data?.split('\n') || [];
  }

  async exec(text: string): Promise<string> {
    // Simulate LLM call
    return `Summary of: ${text.substring(0, 5)}...`;
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.results = execRes;
    return "default";
  }
}

// 6. Nested Flow example (Order Processing Pipeline from flow.md)
// Payment Flow nodes
class ValidatePayment extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (!shared.payments) shared.payments = [];
    shared.payments.push({ id: "payment1", status: "validated" });
    return "default";
  }
}

class ProcessPaymentFlow extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.payments) {
      shared.payments.forEach(p => {
        if (p.status === "validated") p.status = "processed";
      });
    }
    return "default";
  }
}

class PaymentConfirmation extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.payments) {
      shared.payments.forEach(p => {
        if (p.status === "processed") p.status = "confirmed";
      });
    }
    return "default";
  }
}

// Inventory Flow nodes
class CheckStock extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (!shared.inventory) shared.inventory = [];
    shared.inventory.push({ id: "inventory1", status: "checked" });
    return "default";
  }
}

class ReserveItems extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.inventory) {
      shared.inventory.forEach(i => {
        if (i.status === "checked") i.status = "reserved";
      });
    }
    return "default";
  }
}

class UpdateInventory extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.inventory) {
      shared.inventory.forEach(i => {
        if (i.status === "reserved") i.status = "updated";
      });
    }
    return "default";
  }
}

// Shipping Flow nodes
class CreateLabel extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (!shared.shipping) shared.shipping = [];
    shared.shipping.push({ id: "shipping1", status: "labeled" });
    return "default";
  }
}

class AssignCarrier extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.shipping) {
      shared.shipping.forEach(s => {
        if (s.status === "labeled") s.status = "assigned";
      });
    }
    return "default";
  }
}

class SchedulePickup extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.shipping) {
      shared.shipping.forEach(s => {
        if (s.status === "assigned") s.status = "scheduled";
      });
    }
    
    // Order complete
    shared.orderComplete = true;
    return "default";
  }
}

describe('Core Abstraction Examples', () => {
  // 1. Basic Node tests
  describe('Node Example: SummarizeFile', () => {
    test('should summarize file content', async () => {
      const shared: SharedStorage = { data: "This is a test document that needs to be summarized." };
      const summarizeNode = new SummarizeFile(3); // maxRetries = 3
      await summarizeNode.run(shared);
      
      expect(shared.summary).toBe("Summary: This is a ...");
    });
    
    test('should handle empty content', async () => {
      const shared: SharedStorage = { data: "" };
      const summarizeNode = new SummarizeFile();
      await summarizeNode.run(shared);
      
      expect(shared.summary).toBe("Empty file content");
    });
  });
  
  // 2. Flow and branching tests
  describe('Flow Example: Expense Approval', () => {
    test('should approve small expenses', async () => {
      const shared: SharedStorage = {
        expenses: [{ id: "exp1", amount: 50 }]
      };
      
      const review = new ReviewExpense();
      const payment = new ProcessPayment();
      const finish = new FinishProcess();
      
      review.on("approved", payment);
      payment.next(finish);
      
      const flow = new Flow(review);
      await flow.run(shared);
      
      expect(shared.expenses?.[0].status).toBe("paid");
    });
    
    test('should reject large expenses', async () => {
      const shared: SharedStorage = {
        expenses: [{ id: "exp2", amount: 1000 }]
      };
      
      const review = new ReviewExpense();
      const payment = new ProcessPayment();
      const finish = new FinishProcess();
      
      review.on("rejected", finish);
      
      const flow = new Flow(review);
      await flow.run(shared);
      
      expect(shared.expenses?.[0].status).toBe("rejected");
    });
    
    test('should handle expense revision and then approval', async () => {
      const shared: SharedStorage = {
        expenses: [{ id: "exp3", amount: 200 }]
      };
      
      const review = new ReviewExpense();
      const revise = new ReviseExpense();
      const payment = new ProcessPayment();
      const finish = new FinishProcess();
      
      // Set up the flow connections
      review.on("approved", payment);
      review.on("needs_revision", revise);
      review.on("rejected", finish);
      
      revise.next(review);
      payment.next(finish);
      
      const flow = new Flow(review);
      await flow.run(shared);
      
      expect(shared.expenses?.[0].status).toBe("paid");
      expect(shared.expenses?.[0].amount).toBe(75); // revised amount
    });
  });
  
  // 3. BatchNode tests
  describe('BatchNode Example: MapSummaries', () => {
    test('should process text in chunks', async () => {
      const shared: SharedStorage = {
        data: "This is a very long document that needs to be processed in chunks."
      };
      
      const mapSummaries = new MapSummaries();
      await mapSummaries.run(shared);
      
      expect(shared.summary).toContain("Chunk summary: Thi");
      expect(shared.summary?.split('\n').length).toBeGreaterThan(1);
    });
  });
  
  // 4. BatchFlow tests
  describe('BatchFlow Example: SummarizeAllFiles', () => {
    test('should process multiple files', async () => {
      const shared: SharedStorage = {
        files: ["file1.txt", "file2.txt", "file3.txt"]
      };
      
      const loadFile = new LoadFile();
      const summarizeFile = new SummarizeFile();
      
      loadFile.next(summarizeFile);
      
      const summarizeAllFiles = new SummarizeAllFiles(new Flow(loadFile));
      await summarizeAllFiles.run(shared);
      
      // The last file's summary should be in shared.summary
      expect(shared.summary).toBe("Summary: Content of...");
    });
  });
  
  // 5. ParallelBatchNode tests
  describe('ParallelBatchNode Example: TextSummarizer', () => {
    test('should process multiple texts in parallel', async () => {
      const shared: SharedStorage = {
        data: "Text 1\nText 2\nText 3\nText 4"
      };
      
      const textSummarizer = new TextSummarizer();
      await textSummarizer.run(shared);
      
      expect(shared.results?.length).toBe(4);
      expect(shared.results?.[0]).toBe("Summary of: Text ...");
      expect(shared.results?.[1]).toBe("Summary of: Text ...");
    });
  });
  
  // 6. Nested Flow tests
  describe('Nested Flow Example: Order Processing Pipeline', () => {
    test('should process a complete order through multiple flows', async () => {
      const shared: SharedStorage = {};
      
      // Build Payment Flow
      const validatePayment = new ValidatePayment();
      const processPayment = new ProcessPaymentFlow();
      const paymentConfirmation = new PaymentConfirmation();
      
      validatePayment.next(processPayment).next(paymentConfirmation);
      const paymentFlow = new Flow(validatePayment);
      
      // Build Inventory Flow
      const checkStock = new CheckStock();
      const reserveItems = new ReserveItems();
      const updateInventory = new UpdateInventory();
      
      checkStock.next(reserveItems).next(updateInventory);
      const inventoryFlow = new Flow(checkStock);
      
      // Build Shipping Flow
      const createLabel = new CreateLabel();
      const assignCarrier = new AssignCarrier();
      const schedulePickup = new SchedulePickup();
      
      createLabel.next(assignCarrier).next(schedulePickup);
      const shippingFlow = new Flow(createLabel);
      
      // Connect the flows
      paymentFlow.next(inventoryFlow).next(shippingFlow);
      
      // Create and run the master flow
      const orderPipeline = new Flow(paymentFlow);
      await orderPipeline.run(shared);
      
      // Check results
      expect(shared.payments?.[0].status).toBe("confirmed");
      expect(shared.inventory?.[0].status).toBe("updated");
      expect(shared.shipping?.[0].status).toBe("scheduled");
      expect(shared.orderComplete).toBe(true);
    });
  });
});
</file>

<file path="tests/fallback.test.ts">
// tests/fallback.test.ts
import { Node, Flow } from '../src/index';

// Define a shared storage type
type SharedStorage = {
  results?: Array<{
    attempts: number;
    result: string;
  }>;
  finalResult?: any;
};

class FallbackNode extends Node<SharedStorage> {
  private shouldFail: boolean;
  private attemptCount: number = 0;
  
  constructor(shouldFail: boolean = true, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.shouldFail = shouldFail;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    this.attemptCount++;
    if (this.shouldFail) {
      throw new Error("Intentional failure");
    }
    return "success";
  }
  
  async execFallback(prepResult: null, error: Error): Promise<string> {
    return "fallback";
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: this.attemptCount,
      result: execResult
    });
    return undefined;
  }
}

class AsyncFallbackNode extends Node<SharedStorage> {
  private shouldFail: boolean;
  private attemptCount: number = 0;
  
  constructor(shouldFail: boolean = true, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.shouldFail = shouldFail;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    this.attemptCount++;
    if (this.shouldFail) {
      throw new Error("Intentional async failure");
    }
    return "success";
  }
  
  async execFallback(prepResult: null, error: Error): Promise<string> {
    // Simulate async work
    await new Promise(resolve => setTimeout(resolve, 10));
    return "async_fallback";
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: this.attemptCount,
      result: execResult
    });
    return undefined;
  }
}

// Changed to extend Node instead of BaseNode
class ResultNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }
  
  async prep(shared: SharedStorage): Promise<any> {
    return shared.results || [];
  }
  
  async exec(prepResult: any): Promise<any> {
    return prepResult;
  }
  
  async post(shared: SharedStorage, prepResult: any, execResult: any): Promise<string | undefined> {
    shared.finalResult = execResult;
    return undefined;
  }
}

// Changed to extend Node instead of BaseNode
class NoFallbackNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    throw new Error("Test error");
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({ attempts: 1, result: execResult });
    return execResult;
  }
}

// New class to demonstrate retry with eventual success
class EventualSuccessNode extends Node<SharedStorage> {
  private succeedAfterAttempts: number;
  private attemptCount: number = 0;
  
  constructor(succeedAfterAttempts: number = 2, maxRetries: number = 3, wait: number = 0.01) {
    super(maxRetries, wait);
    this.succeedAfterAttempts = succeedAfterAttempts;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    this.attemptCount++;
    if (this.attemptCount < this.succeedAfterAttempts) {
      throw new Error(`Fail on attempt ${this.attemptCount}`);
    }
    return `success_after_${this.attemptCount}_attempts`;
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: this.attemptCount,
      result: execResult
    });
    return undefined;
  }
}

// New class to demonstrate customized error handling
class CustomErrorHandlerNode extends Node<SharedStorage> {
  private errorType: string;
  
  constructor(errorType: string = "standard", maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.errorType = errorType;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    throw new Error(this.errorType);
  }
  
  async execFallback(prepResult: null, error: Error): Promise<string> {
    // Custom error handling based on error type
    if (error.message === "network") {
      return "network_error_handled";
    } else if (error.message === "timeout") {
      return "timeout_error_handled";
    } else {
      return "generic_error_handled";
    }
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: 1,
      result: execResult
    });
    return undefined;
  }
}

describe('Fallback Functionality Tests with Node', () => {
  test('successful execution', async () => {
    // Test that execFallback is not called when execution succeeds
    const shared: SharedStorage = {};
    const node = new FallbackNode(false);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(1);
    expect(shared.results?.[0].result).toBe("success");
  });

  test('fallback after failure', async () => {
    // Test that execFallback is called after all retries are exhausted
    const shared: SharedStorage = {};
    const node = new AsyncFallbackNode(true, 2);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(2);
    expect(shared.results?.[0].result).toBe("async_fallback");
  });

  test('fallback in flow', async () => {
    // Test that fallback works within a Flow
    const shared: SharedStorage = {};
    const fallbackNode = new FallbackNode(true, 1);
    const resultNode = new ResultNode();
    
    fallbackNode.next(resultNode);
    
    const flow = new Flow(fallbackNode);
    await flow.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].result).toBe("fallback");
    expect(shared.finalResult).toEqual([{ attempts: 1, result: 'fallback' }]);
  });

  test('no fallback implementation', async () => {
    // Test that without overriding execFallback, Node will rethrow the error
    const shared: SharedStorage = {};
    const node = new NoFallbackNode();
    
    await expect(async () => {
      await node.run(shared);
    }).rejects.toThrow("Test error");
  });

  test('retry before fallback', async () => {
    // Test that retries are attempted before calling fallback
    const shared: SharedStorage = {};
    const node = new AsyncFallbackNode(true, 3);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(3);
    expect(shared.results?.[0].result).toBe("async_fallback");
  });
  
  test('eventual success after retries', async () => {
    // Test node that succeeds after multiple attempts
    const shared: SharedStorage = {};
    const node = new EventualSuccessNode(2, 3);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(2);
    expect(shared.results?.[0].result).toBe("success_after_2_attempts");
  });
  
  test('custom error handling based on error type', async () => {
    // Test custom fallback logic based on error type
    const shared1: SharedStorage = {};
    const node1 = new CustomErrorHandlerNode("network");
    await node1.run(shared1);
    expect(shared1.results?.[0].result).toBe("network_error_handled");
    
    const shared2: SharedStorage = {};
    const node2 = new CustomErrorHandlerNode("timeout");
    await node2.run(shared2);
    expect(shared2.results?.[0].result).toBe("timeout_error_handled");
    
    const shared3: SharedStorage = {};
    const node3 = new CustomErrorHandlerNode("other");
    await node3.run(shared3);
    expect(shared3.results?.[0].result).toBe("generic_error_handled");
  });
  
  test('flow with mixed retry patterns', async () => {
    // Test complex flow with different retry patterns
    const shared: SharedStorage = {};
    
    const node1 = new FallbackNode(true, 1);
    const node2 = new AsyncFallbackNode(false, 2);
    const node3 = new EventualSuccessNode(2, 3);
    const resultNode = new ResultNode();
    
    node1.next(node2);
    node2.next(node3);
    node3.next(resultNode);
    
    const flow = new Flow(node1);
    await flow.run(shared);
    
    expect(shared.results?.length).toBe(3);
    expect(shared.results?.[0].result).toBe("fallback");
    expect(shared.results?.[1].result).toBe("success");
    expect(shared.results?.[2].result).toBe("success_after_2_attempts");
  });
});
</file>

<file path="tests/flow-basic.test.ts">
// tests/flow_basic.test.ts
import { Node, Flow } from '../src/index';

// Define a shared storage type
type SharedStorage = {
  current?: number;
  execResult?: any;
};

class NumberNode extends Node<SharedStorage, Record<string, unknown>> {
  constructor(
    private number: number,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    shared.current = this.number;
  }
}

class AddNode extends Node<SharedStorage> {
  constructor(
    private number: number,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current += this.number;
    }
  }
}

class MultiplyNode extends Node<SharedStorage> {
  constructor(
    private number: number,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current *= this.number;
    }
  }
}

class CheckPositiveNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async post(shared: SharedStorage): Promise<string> {
    if (shared.current !== undefined && shared.current >= 0) {
      return 'positive';
    } else {
      return 'negative';
    }
  }
}

class NoOpNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(): Promise<void> {
    // Do nothing, just pass
  }
}

// New class to demonstrate Node's retry capabilities
class FlakyNode extends Node<SharedStorage> {
  private attemptCount = 0;

  constructor(
    private failUntilAttempt: number,
    maxRetries: number = 3,
    wait: number = 0.1
  ) {
    super(maxRetries, wait);
  }

  async exec(): Promise<any> {
    this.attemptCount++;

    if (this.attemptCount < this.failUntilAttempt) {
      throw new Error(`Attempt ${this.attemptCount} failed`);
    }

    return `Success on attempt ${this.attemptCount}`;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: any
  ): Promise<string> {
    shared.execResult = execRes;
    return 'default';
  }
}

// New class to demonstrate using exec method more explicitly
class ExecNode extends Node<SharedStorage> {
  constructor(
    private operation: string,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number> {
    // Return the current value for processing in exec
    return shared.current || 0;
  }

  async exec(currentValue: number): Promise<number> {
    switch (this.operation) {
      case 'square':
        return currentValue * currentValue;
      case 'double':
        return currentValue * 2;
      case 'negate':
        return -currentValue;
      default:
        return currentValue;
    }
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: any
  ): Promise<string> {
    shared.current = execRes;
    return 'default';
  }
}

describe('Pocket Flow Tests with Node', () => {
  test('single number', async () => {
    const shared: SharedStorage = {};
    const start = new NumberNode(5);
    const pipeline = new Flow(start);
    await pipeline.run(shared);
    expect(shared.current).toBe(5);
  });

  test('sequence with chaining', async () => {
    /**
     * Test a simple linear pipeline:
     * NumberNode(5) -> AddNode(3) -> MultiplyNode(2)
     *
     * Expected result:
     * (5 + 3) * 2 = 16
     */
    const shared: SharedStorage = {};
    const n1 = new NumberNode(5);
    const n2 = new AddNode(3);
    const n3 = new MultiplyNode(2);

    // Chain them in sequence using method chaining
    n1.next(n2).next(n3);

    const pipeline = new Flow(n1);
    await pipeline.run(shared);

    expect(shared.current).toBe(16);
  });

  test('branching positive', async () => {
    /**
     * Test a branching pipeline with positive route:
     * start = NumberNode(5)
     * check = CheckPositiveNode()
     * if 'positive' -> AddNode(10)
     * if 'negative' -> AddNode(-20)
     */
    const shared: SharedStorage = {};
    const start = new NumberNode(5);
    const check = new CheckPositiveNode();
    const addIfPositive = new AddNode(10);
    const addIfNegative = new AddNode(-20);

    // Setup with chaining
    start
      .next(check)
      .on('positive', addIfPositive)
      .on('negative', addIfNegative);

    const pipeline = new Flow(start);
    await pipeline.run(shared);

    expect(shared.current).toBe(15);
  });

  test('negative branch', async () => {
    /**
     * Same branching pipeline, but starting with -5.
     * Final result: (-5) + (-20) = -25.
     */
    const shared: SharedStorage = {};
    const start = new NumberNode(-5);
    const check = new CheckPositiveNode();
    const addIfPositive = new AddNode(10);
    const addIfNegative = new AddNode(-20);

    // Build the flow with chaining
    start
      .next(check)
      .on('positive', addIfPositive)
      .on('negative', addIfNegative);

    const pipeline = new Flow(start);
    await pipeline.run(shared);

    expect(shared.current).toBe(-25);
  });

  test('cycle until negative', async () => {
    /**
     * Demonstrate a cyclical pipeline:
     * Start with 10, check if positive -> subtract 3, then go back to check.
     * Repeat until the number becomes negative.
     */
    const shared: SharedStorage = {};
    const n1 = new NumberNode(10);
    const check = new CheckPositiveNode();
    const subtract3 = new AddNode(-3);
    const noOp = new NoOpNode();

    // Build the cycle with chaining
    n1.next(check).on('positive', subtract3).on('negative', noOp);

    subtract3.next(check);

    const pipeline = new Flow(n1);
    await pipeline.run(shared);

    // final result should be -2: (10 -> 7 -> 4 -> 1 -> -2)
    expect(shared.current).toBe(-2);
  });

  // New tests demonstrating Node features

  test('retry functionality', async () => {
    const shared: SharedStorage = {};

    // This node will fail on the first attempt but succeed on the second
    const flakyNode = new FlakyNode(2, 3, 0.01);

    const pipeline = new Flow(flakyNode);
    await pipeline.run(shared);

    // Check that we got a success message indicating it was the second attempt
    expect(shared.execResult).toBe('Success on attempt 2');
  });

  test('retry with fallback', async () => {
    const shared: SharedStorage = {};

    // This node will always fail (requires 5 attempts, but we only allow 2)
    const flakyNode = new FlakyNode(5, 2, 0.01);

    // Override the execFallback method to handle the failure
    flakyNode.execFallback = async (
      prepRes: any,
      error: Error
    ): Promise<any> => {
      return 'Fallback executed due to failure';
    };

    const pipeline = new Flow(flakyNode);
    await pipeline.run(shared);

    // Check that we got the fallback result
    expect(shared.execResult).toBe('Fallback executed due to failure');
  });

  test('exec method processing', async () => {
    const shared: SharedStorage = { current: 5 };

    const squareNode = new ExecNode('square');
    const doubleNode = new ExecNode('double');
    const negateNode = new ExecNode('negate');

    squareNode.next(doubleNode).next(negateNode);

    const pipeline = new Flow(squareNode);
    await pipeline.run(shared);

    // 5 â†’ square â†’ 25 â†’ double â†’ 50 â†’ negate â†’ -50
    expect(shared.current).toBe(-50);
  });
});
</file>

<file path="tests/flow-composition.test.ts">
// tests/flow_composition.test.ts
import { Node, Flow } from '../src/index';

// Define a shared storage type
type SharedStorage = {
  current?: number;
};

class NumberNode extends Node<SharedStorage> {
  constructor(private number: number, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    shared.current = this.number;
  }
}

class AddNode extends Node<SharedStorage> {
  constructor(private number: number, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current += this.number;
    }
  }
}

class MultiplyNode extends Node<SharedStorage> {
  constructor(private number: number, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current *= this.number;
    }
  }
}

describe('Flow Composition Tests with Node', () => {
  test('flow as node', async () => {
    /**
     * 1) Create a Flow (f1) starting with NumberNode(5), then AddNode(10), then MultiplyNode(2).
     * 2) Create a second Flow (f2) whose start is f1.
     * 3) Create a wrapper Flow (f3) that contains f2 to ensure proper execution.
     * Expected final result in shared.current: (5 + 10) * 2 = 30.
     */
    const shared: SharedStorage = {};
    
    // Inner flow f1
    const numberNode = new NumberNode(5);
    const addNode = new AddNode(10);
    const multiplyNode = new MultiplyNode(2);
    
    numberNode.next(addNode);
    addNode.next(multiplyNode);
    
    const f1 = new Flow(numberNode);
    
    // f2 starts with f1
    const f2 = new Flow(f1);
    
    // Wrapper flow f3 to ensure proper execution
    const f3 = new Flow(f2);
    await f3.run(shared);
    
    expect(shared.current).toBe(30);
  });
  
  test('nested flow', async () => {
    /**
     * Demonstrates nested flows with proper wrapping:
     * inner_flow: NumberNode(5) -> AddNode(3)
     * middle_flow: starts with inner_flow -> MultiplyNode(4)
     * wrapper_flow: contains middle_flow to ensure proper execution
     * Expected final result: (5 + 3) * 4 = 32.
     */
    const shared: SharedStorage = {};
    
    // Build the inner flow
    const numberNode = new NumberNode(5);
    const addNode = new AddNode(3);
    numberNode.next(addNode);
    const innerFlow = new Flow(numberNode);
    
    // Build the middle flow, whose start is the inner flow
    const multiplyNode = new MultiplyNode(4);
    innerFlow.next(multiplyNode);
    const middleFlow = new Flow(innerFlow);
    
    // Wrapper flow to ensure proper execution
    const wrapperFlow = new Flow(middleFlow);
    await wrapperFlow.run(shared);
    
    expect(shared.current).toBe(32);
  });
  
  test('flow chaining flows', async () => {
    /**
     * Demonstrates chaining two flows with proper wrapping:
     * flow1: NumberNode(10) -> AddNode(10) # final = 20
     * flow2: MultiplyNode(2) # final = 40
     * wrapper_flow: contains both flow1 and flow2 to ensure proper execution
     * Expected final result: (10 + 10) * 2 = 40.
     */
    const shared: SharedStorage = {};
    
    // flow1
    const numberNode = new NumberNode(10);
    const addNode = new AddNode(10);
    numberNode.next(addNode);
    const flow1 = new Flow(numberNode);
    
    // flow2
    const multiplyNode = new MultiplyNode(2);
    const flow2 = new Flow(multiplyNode);
    
    // Chain flow1 to flow2
    flow1.next(flow2);
    
    // Wrapper flow to ensure proper execution
    const wrapperFlow = new Flow(flow1);
    await wrapperFlow.run(shared);
    
    expect(shared.current).toBe(40);
  });
  
  test('flow with retry handling', async () => {
    /**
     * Demonstrates retry capabilities in flow composition:
     * Using nodes with retry logic in a flow structure
     */
    const shared: SharedStorage = {};
    
    // Create a faulty NumberNode that will fail once before succeeding
    class FaultyNumberNode extends Node<SharedStorage> {
      private attempts = 0;
      
      constructor(private number: number, maxRetries: number = 2, wait: number = 0.01) {
        super(maxRetries, wait);
      }
      
      async prep(shared: SharedStorage): Promise<number> {
        return this.number;
      }
      
      async exec(number: number): Promise<number> {
        this.attempts++;
        if (this.attempts === 1) {
          throw new Error("Simulated failure on first attempt");
        }
        return number;
      }
      
      async post(shared: SharedStorage, prepRes: any, execRes: any): Promise<string> {
        shared.current = execRes;
        return "default";
      }
    }
    
    // Create a flow with the faulty node followed by standard nodes
    const faultyNumberNode = new FaultyNumberNode(5);
    const addNode = new AddNode(10);
    const multiplyNode = new MultiplyNode(2);
    
    faultyNumberNode.next(addNode);
    addNode.next(multiplyNode);
    
    const flow = new Flow(faultyNumberNode);
    await flow.run(shared);
    
    // Despite the initial failure, the retry mechanism should allow
    // the flow to complete successfully: (5 + 10) * 2 = 30
    expect(shared.current).toBe(30);
  });
  
  test('nested flows with mixed retry configurations', async () => {
    /**
     * Demonstrates nesting flows with different retry configurations
     */
    const shared: SharedStorage = {};
    
    // Inner flow with a node that has high retry count
    const numberNode = new NumberNode(5, 5, 0.01); // 5 retries
    const addNode = new AddNode(3, 2, 0.01);      // 2 retries
    numberNode.next(addNode);
    const innerFlow = new Flow(numberNode);
    
    // Middle flow with a node that has medium retry count
    const multiplyNode = new MultiplyNode(4, 3, 0.01); // 3 retries
    innerFlow.next(multiplyNode);
    const middleFlow = new Flow(innerFlow);
    
    // Wrapper flow
    const wrapperFlow = new Flow(middleFlow);
    await wrapperFlow.run(shared);
    
    // The result should be the same: (5 + 3) * 4 = 32
    expect(shared.current).toBe(32);
  });
});
</file>

<file path="tests/mapreduce-pattern.test.ts">
// tests/mapreduce-pattern.test.ts
import { BatchNode, Node, Flow, ParallelBatchNode } from '../src/index';

// Mock utility function to simulate LLM calls
async function callLLM(prompt: string): Promise<string> {
  // In a real implementation, this would call an actual LLM
  return `Summary for: ${prompt.slice(0, 20)}...`;
}

// Define shared storage type for MapReduce pattern
type MapReduceSharedStorage = {
  files?: Record<string, string>;
  file_summaries?: Record<string, string>;
  all_files_summary?: string;
  text_to_process?: string;
  text_chunks?: string[];
  processed_chunks?: string[];
  final_result?: string;
};

// Document Summarization Example (Sequential)
class SummarizeAllFiles extends BatchNode<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<[string, string][]> {
    const files = shared.files || {};
    return Object.entries(files);
  }

  async exec(one_file: [string, string]): Promise<[string, string]> {
    const [filename, file_content] = one_file;
    const summary_text = await callLLM(
      `Summarize the following file:\n${file_content}`
    );
    return [filename, summary_text];
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: [string, string][],
    execRes: [string, string][]
  ): Promise<string | undefined> {
    shared.file_summaries = Object.fromEntries(execRes);
    return "summarized";
  }
}

class CombineSummaries extends Node<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<Record<string, string>> {
    return shared.file_summaries || {};
  }

  async exec(file_summaries: Record<string, string>): Promise<string> {
    // Format as: "File1: summary\nFile2: summary...\n"
    const text_list: string[] = [];
    for (const [fname, summ] of Object.entries(file_summaries)) {
      text_list.push(`${fname} summary:\n${summ}\n`);
    }
    const big_text = text_list.join("\n---\n");

    return await callLLM(
      `Combine these file summaries into one final summary:\n${big_text}`
    );
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: Record<string, string>,
    final_summary: string
  ): Promise<string | undefined> {
    shared.all_files_summary = final_summary;
    return "combined";
  }
}

// Generic MapReduce Example with Parallel Processing
class MapChunks extends ParallelBatchNode<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<string[]> {
    // Split text into chunks
    const text = shared.text_to_process || "";
    const chunkSize = 10;
    const chunks: string[] = [];
    
    for (let i = 0; i < text.length; i += chunkSize) {
      chunks.push(text.slice(i, i + chunkSize));
    }
    
    shared.text_chunks = chunks;
    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    // Process each chunk (map phase)
    // In a real application, this could be any transformation
    return chunk.toUpperCase();
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.processed_chunks = execRes;
    return "mapped";
  }
}

class ReduceResults extends Node<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<string[]> {
    return shared.processed_chunks || [];
  }

  async exec(processedChunks: string[]): Promise<string> {
    // Combine processed chunks (reduce phase)
    // In a real application, this could be any aggregation function
    return processedChunks.join(" + ");
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: string[],
    result: string
  ): Promise<string | undefined> {
    shared.final_result = result;
    return "reduced";
  }
}

// Tests for the MapReduce pattern
describe('MapReduce Pattern Tests', () => {
  // Test Document Summarization Example
  test('Document Summarization MapReduce', async () => {
    // Create and connect nodes
    const batchNode = new SummarizeAllFiles();
    const combineNode = new CombineSummaries();
    
    batchNode.on("summarized", combineNode);
    
    const flow = new Flow(batchNode);
    
    // Prepare test data
    const shared: MapReduceSharedStorage = {
      files: {
        "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
        "file2.txt": "Some other interesting text ...",
        "file3.txt": "Yet another file with some content to summarize..."
      }
    };
    
    // Run the flow
    await flow.run(shared);
    
    // Verify results
    expect(shared.file_summaries).toBeDefined();
    expect(Object.keys(shared.file_summaries || {}).length).toBe(3);
    expect(shared.all_files_summary).toBeDefined();
    expect(typeof shared.all_files_summary).toBe('string');
  });
  
  // Test Generic MapReduce with ParallelBatchNode
  test('Parallel Text Processing MapReduce', async () => {
    // Create and connect nodes
    const mapNode = new MapChunks();
    const reduceNode = new ReduceResults();
    
    mapNode.on("mapped", reduceNode);
    
    const flow = new Flow(mapNode);
    
    // Prepare test data
    const shared: MapReduceSharedStorage = {
      text_to_process: "This is a longer text that will be processed in parallel using the MapReduce pattern."
    };
    
    // Run the flow
    await flow.run(shared);
    
    // Verify results
    expect(shared.text_chunks).toBeDefined();
    expect(shared.text_chunks?.length).toBeGreaterThan(0);
    expect(shared.processed_chunks).toBeDefined();
    expect(shared.processed_chunks?.length).toBe(shared.text_chunks?.length);
    expect(shared.final_result).toBeDefined();
    expect(typeof shared.final_result).toBe('string');
    
    // Verify the content is actually transformed
    expect(shared.processed_chunks?.every(chunk => 
      chunk === chunk.toUpperCase()
    )).toBe(true);
  });
  
  // Test changing chunk size affects parallel processing
  test('Varying Chunk Size in MapReduce', async () => {
    // This test demonstrates how chunk size affects the MapReduce process
    
    // Create a custom MapChunks class with configurable chunk size
    class ConfigurableMapChunks extends ParallelBatchNode<MapReduceSharedStorage> {
      private chunkSize: number;
      
      constructor(chunkSize: number) {
        super();
        this.chunkSize = chunkSize;
      }
      
      async prep(shared: MapReduceSharedStorage): Promise<string[]> {
        const text = shared.text_to_process || "";
        const chunks: string[] = [];
        
        for (let i = 0; i < text.length; i += this.chunkSize) {
          chunks.push(text.slice(i, i + this.chunkSize));
        }
        
        shared.text_chunks = chunks;
        return chunks;
      }
      
      async exec(chunk: string): Promise<string> {
        return chunk.toUpperCase();
      }
      
      async post(
        shared: MapReduceSharedStorage,
        prepRes: string[],
        execRes: string[]
      ): Promise<string | undefined> {
        shared.processed_chunks = execRes;
        return "mapped";
      }
    }
    
    // Test with small chunk size
    const mapNodeSmall = new ConfigurableMapChunks(5);
    const reduceNodeSmall = new ReduceResults();
    mapNodeSmall.on("mapped", reduceNodeSmall);
    const flowSmall = new Flow(mapNodeSmall);
    
    // Test with larger chunk size
    const mapNodeLarge = new ConfigurableMapChunks(20);
    const reduceNodeLarge = new ReduceResults();
    mapNodeLarge.on("mapped", reduceNodeLarge);
    const flowLarge = new Flow(mapNodeLarge);
    
    // Same input for both flows
    const text = "This is a test text for demonstrating chunk size effects.";
    
    // Run with small chunks
    const sharedSmall: MapReduceSharedStorage = {
      text_to_process: text
    };
    await flowSmall.run(sharedSmall);
    
    // Run with large chunks
    const sharedLarge: MapReduceSharedStorage = {
      text_to_process: text
    };
    await flowLarge.run(sharedLarge);
    
    // Verify different chunk counts
    if (sharedSmall.text_chunks && sharedLarge.text_chunks) {
      expect(sharedSmall.text_chunks.length).toBeGreaterThan(sharedLarge.text_chunks.length);
    }
    
    // Verify end results are identical despite different chunking
    expect(sharedSmall.final_result?.replace(/\s\+\s/g, '')).toBe(
      sharedLarge.final_result?.replace(/\s\+\s/g, '')
    );
  });
});
</file>

<file path="tests/multi-agent-pattern.test.ts">
// tests/multi-agent-pattern.test.ts
import { Node, Flow } from '../src/index';

// Define shared storage with message queue for basic agent communication
type MessageQueueSharedStorage = {
  messages: string[];
  processedMessages: string[];
  processing?: boolean;
};

// Mock utility function to simulate LLM calls
function mockLLM(prompt: string): string {
  // Simple mock LLM that responds based on the prompt
  if (prompt.includes('Generate hint')) {
    return 'This is a hint: Something cold on a stick';
  } else if (prompt.includes('Guess')) {
    return 'popsicle';
  }
  return `Response to: ${prompt.substring(0, 20)}...`;
}

// Basic Agent Communication Example
class ListenerAgent extends Node<MessageQueueSharedStorage> {
  async prep(shared: MessageQueueSharedStorage): Promise<string | undefined> {
    // Check if there are messages to process
    if (shared.messages.length === 0) {
      return undefined;
    }
    // Get the next message
    return shared.messages.shift();
  }

  async exec(message: string | undefined): Promise<string | undefined> {
    if (!message) {
      return undefined;
    }
    
    // Process the message (in real implementation, this could call an LLM)
    const response = `Processed: ${message}`;
    return response;
  }

  async post(
    shared: MessageQueueSharedStorage,
    prepRes: string | undefined,
    execRes: string | undefined
  ): Promise<string> {
    if (execRes) {
      // Store the processed message
      shared.processedMessages.push(execRes);
    }
    
    if (shared.messages.length === 0) {
      // Add a small delay to avoid tight loop CPU consumption in real implementation
      return "finished";
    }
    
    // Continue processing messages
    return "continue";
  }
}

// Taboo Game Example
// Define shared storage for the game
type TabooGameSharedStorage = {
  targetWord: string;
  forbiddenWords: string[];
  pastGuesses: string[];
  hinterQueue: string[];
  guesserQueue: string[];
  gameOver: boolean;
  maxRounds: number;
  currentRound: number;
  isCorrectGuess: boolean;
};

// Hinter agent that provides clues
class Hinter extends Node<TabooGameSharedStorage> {
  async prep(shared: TabooGameSharedStorage): Promise<any> {
    if (shared.gameOver) {
      return null;
    }

    // In test, we'll simulate waiting for a message by checking if it's our turn
    if (shared.hinterQueue.length === 0) {
      return null;
    }

    const message = shared.hinterQueue.shift();
    
    return {
      target: shared.targetWord,
      forbidden: shared.forbiddenWords,
      pastGuesses: shared.pastGuesses,
      message
    };
  }

  async exec(input: any): Promise<string | null> {
    if (!input) return null;
    
    // Generate a hint using mock LLM
    const prompt = `Generate hint for word "${input.target}" without using forbidden words: ${input.forbidden.join(', ')}`;
    const hint = mockLLM(prompt);
    return hint;
  }

  async post(
    shared: TabooGameSharedStorage,
    prepRes: any,
    hint: string | null
  ): Promise<string | undefined> {
    if (!hint) {
      if (shared.gameOver) {
        return "finished";
      }
      return "continue_hinter";
    }
    
    // Send hint to guesser
    shared.guesserQueue.push(hint);
    shared.currentRound++;
    
    return "continue_hinter";
  }
}

// Guesser agent that tries to guess the target word
class Guesser extends Node<TabooGameSharedStorage> {
  async prep(shared: TabooGameSharedStorage): Promise<string | null> {
    if (shared.gameOver) {
      return null;
    }

    // Wait for a hint from the hinter
    if (shared.guesserQueue.length === 0) {
      return null;
    }

    return shared.guesserQueue.shift() || null;
  }

  async exec(hint: string | null): Promise<string | null> {
    if (!hint) return null;
    
    // Generate a guess using mock LLM
    const prompt = `Guess the word based on the hint: ${hint}`;
    const guess = mockLLM(prompt);
    return guess;
  }

  async post(
    shared: TabooGameSharedStorage,
    hint: string | null,
    guess: string | null
  ): Promise<string | undefined> {
    if (!guess) {
      if (shared.gameOver) {
        return "finished";
      }
      return "continue_guesser";
    }
    
    // Record the guess
    shared.pastGuesses.push(guess);
    
    // Check if the guess is correct
    if (guess.toLowerCase() === shared.targetWord.toLowerCase()) {
      shared.isCorrectGuess = true;
      shared.gameOver = true;
      return "finished";
    }
    
    // Check if we've reached maximum rounds
    if (shared.currentRound >= shared.maxRounds) {
      shared.gameOver = true;
      return "finished";
    }
    
    // Send message to hinter for next round
    shared.hinterQueue.push("next_hint");
    
    return "continue_guesser";
  }
}

// Tests for Multi-Agent pattern
describe('Multi-Agent Pattern Tests', () => {
  // Test basic agent message queue
  test('Basic Agent Message Queue', async () => {
    // Create agent node
    const agent = new ListenerAgent();
    agent.on("continue", agent); // Connect to self to continue processing
    
    // Create flow
    const flow = new Flow(agent);
    
    // Create shared storage with messages
    const shared: MessageQueueSharedStorage = {
      messages: [
        "System status: all systems operational",
        "Memory usage: normal",
        "Network connectivity: stable",
        "Processing load: optimal",
      ],
      processedMessages: []
    };
    
    // Run the flow
    await flow.run(shared);
    
    // Verify results
    expect(shared.messages.length).toBe(0);
    expect(shared.processedMessages.length).toBe(4);
    expect(shared.processedMessages[0]).toBe("Processed: System status: all systems operational");
  });
  
  // Test Taboo game multi-agent interaction
  test('Taboo Game Multi-Agent Interaction', async () => {
    // Create the agents
    const hinter = new Hinter();
    const guesser = new Guesser();
    
    // Connect agents
    hinter.on("continue_hinter", hinter);
    guesser.on("continue_guesser", guesser);
    
    // Create shared game state
    const shared: TabooGameSharedStorage = {
      targetWord: "popsicle",
      forbiddenWords: ["ice", "cream", "frozen", "stick", "summer"],
      pastGuesses: [],
      hinterQueue: ["start_game"], // Initial message to start the game
      guesserQueue: [],
      gameOver: false,
      maxRounds: 3,
      currentRound: 0,
      isCorrectGuess: false
    };
    
    // Create flows
    const hinterFlow = new Flow(hinter);
    const guesserFlow = new Flow(guesser);
    
    // Run both flows concurrently to simulate multi-agent interaction
    const hinterPromise = hinterFlow.run(shared);
    const guesserPromise = guesserFlow.run(shared);
    
    // Wait for both to finish
    await Promise.all([hinterPromise, guesserPromise]);
    
    // Verify results
    expect(shared.gameOver).toBe(true);
    expect(shared.pastGuesses.length).toBeGreaterThan(0);
    expect(shared.isCorrectGuess).toBe(true);
  });
  
  // Test changing agent behavior with different parameters
  test('Configurable Agent Behavior', async () => {
    // Create a configurable agent that can be adjusted for testing
    class ConfigurableAgent extends Node<MessageQueueSharedStorage> {
      private processingDelay: number;
      
      constructor(processingDelay: number = 0) {
        super();
        this.processingDelay = processingDelay;
      }
      
      async prep(shared: MessageQueueSharedStorage): Promise<string | undefined> {
        if (shared.messages.length === 0) {
          return undefined;
        }
        return shared.messages.shift();
      }
      
      async exec(message: string | undefined): Promise<string | undefined> {
        if (!message) {
          return undefined;
        }
        
        // Simulate processing time
        if (this.processingDelay > 0) {
          await new Promise(resolve => setTimeout(resolve, this.processingDelay));
        }
        
        return `Processed with ${this.processingDelay}ms delay: ${message}`;
      }
      
      async post(
        shared: MessageQueueSharedStorage,
        prepRes: string | undefined,
        execRes: string | undefined
      ): Promise<string> {
        if (execRes) {
          shared.processedMessages.push(execRes);
        }
        
        if (shared.messages.length === 0) {
          return "finished";
        }
        return "continue";
      }
    }
    
    // Test with fast agent
    const fastAgent = new ConfigurableAgent(0);
    fastAgent.on("continue", fastAgent);
    const fastFlow = new Flow(fastAgent);
    
    const fastShared: MessageQueueSharedStorage = {
      messages: ["Message 1", "Message 2", "Message 3"],
      processedMessages: []
    };
    
    await fastFlow.run(fastShared);
    
    // Test with slow agent
    const slowAgent = new ConfigurableAgent(10);
    slowAgent.on("continue", slowAgent);
    const slowFlow = new Flow(slowAgent);
    
    const slowShared: MessageQueueSharedStorage = {
      messages: ["Message 1", "Message 2", "Message 3"],
      processedMessages: []
    };
    
    await slowFlow.run(slowShared);
    
    // Verify both processed all messages
    expect(fastShared.processedMessages.length).toBe(3);
    expect(slowShared.processedMessages.length).toBe(3);
    
    // Verify processing indicators in the output
    expect(fastShared.processedMessages[0]).toContain("Processed with 0ms delay");
    expect(slowShared.processedMessages[0]).toContain("Processed with 10ms delay");
  });
});
</file>

<file path="tests/parallel-batch-flow.test.ts">
// tests/parallel-batch-flow.test.ts
import { Node, ParallelBatchNode, Flow, ParallelBatchFlow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  batches?: number[][];
  processedNumbers?: Record<number, number[]>;
  total?: number;
};

class AsyncParallelNumberProcessor extends ParallelBatchNode<
  SharedStorage,
  { batchId: number }
> {
  private delay: number;

  constructor(delay: number = 0.1, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.delay = delay;
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    const batchId = this._params.batchId;
    return shared.batches?.[batchId] || [];
  }

  async exec(number: number): Promise<number> {
    // Simulate async processing
    await new Promise((resolve) => setTimeout(resolve, this.delay * 1000));
    return number * 2;
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number[]
  ): Promise<string | undefined> {
    if (!shared.processedNumbers) {
      shared.processedNumbers = {};
    }
    shared.processedNumbers[this._params.batchId] = execRes;
    return 'processed';
  }
}

class AsyncAggregatorNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    // Combine all batch results in order
    const allResults: number[] = [];
    const processed = shared.processedNumbers || {};

    for (let i = 0; i < Object.keys(processed).length; i++) {
      allResults.push(...processed[i]);
    }

    return allResults;
  }

  async exec(prepResult: number[]): Promise<number> {
    await new Promise((resolve) => setTimeout(resolve, 10));
    return prepResult.reduce((sum, val) => sum + val, 0);
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number
  ): Promise<string | undefined> {
    shared.total = execRes;
    return 'aggregated';
  }
}

// Custom ParallelBatchFlow that processes batches based on batchId
class TestParallelBatchFlow extends ParallelBatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, any>[]> {
    return (shared.batches || []).map((_, i) => ({ batchId: i }));
  }
}

describe('ParallelBatchFlow Tests', () => {
  test('parallel batch flow', async () => {
    /**
     * Test basic parallel batch processing flow with batch IDs
     */
    const shared: SharedStorage = {
      batches: [
        [1, 2, 3], // batchId: 0
        [4, 5, 6], // batchId: 1
        [7, 8, 9], // batchId: 2
      ],
    };

    const processor = new AsyncParallelNumberProcessor(0.1);
    const aggregator = new AsyncAggregatorNode();

    processor.on('processed', aggregator);
    const flow = new TestParallelBatchFlow(processor);

    const startTime = Date.now();
    await flow.run(shared);
    const executionTime = (Date.now() - startTime) / 1000;

    // Verify each batch was processed correctly
    const expectedBatchResults = {
      0: [2, 4, 6], // [1,2,3] * 2
      1: [8, 10, 12], // [4,5,6] * 2
      2: [14, 16, 18], // [7,8,9] * 2
    };

    expect(shared.processedNumbers).toEqual(expectedBatchResults);

    // Verify total
    const expectedTotal = shared
      .batches!.flat()
      .reduce((sum, num) => sum + num * 2, 0);
    expect(shared.total).toBe(expectedTotal);

    // Verify parallel execution
    expect(executionTime).toBeLessThan(0.2);
  });

  test('error handling', async () => {
    /**
     * Test error handling in parallel batch flow
     */
    class ErrorProcessor extends AsyncParallelNumberProcessor {
      async exec(item: number): Promise<number> {
        if (item === 2) {
          throw new Error(`Error processing item ${item}`);
        }
        return item;
      }
    }

    const shared: SharedStorage = {
      batches: [
        [1, 2, 3], // Contains error-triggering value
        [4, 5, 6],
      ],
    };

    const processor = new ErrorProcessor();
    const flow = new TestParallelBatchFlow(processor);

    await expect(async () => {
      await flow.run(shared);
    }).rejects.toThrow('Error processing item 2');
  });

  test('multiple batch sizes', async () => {
    /**
     * Test parallel batch flow with varying batch sizes
     */
    const shared: SharedStorage = {
      batches: [
        [1], // batchId: 0
        [2, 3, 4], // batchId: 1
        [5, 6], // batchId: 2
        [7, 8, 9, 10], // batchId: 3
      ],
    };

    const processor = new AsyncParallelNumberProcessor(0.05);
    const aggregator = new AsyncAggregatorNode();

    processor.on('processed', aggregator);
    const flow = new TestParallelBatchFlow(processor);

    await flow.run(shared);

    // Verify each batch was processed correctly
    const expectedBatchResults = {
      0: [2], // [1] * 2
      1: [4, 6, 8], // [2,3,4] * 2
      2: [10, 12], // [5,6] * 2
      3: [14, 16, 18, 20], // [7,8,9,10] * 2
    };

    expect(shared.processedNumbers).toEqual(expectedBatchResults);

    // Verify total
    const expectedTotal = shared
      .batches!.flat()
      .reduce((sum, num) => sum + num * 2, 0);
    expect(shared.total).toBe(expectedTotal);
  });
});
</file>

<file path="tests/parallel-batch-node.test.ts">
// tests/parallel-batch-node.test.ts
import { ParallelBatchNode, Flow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  inputNumbers?: number[];
  processedNumbers?: number[];
  executionOrder?: number[];
  finalResults?: number[];
};

class AsyncParallelNumberProcessor extends ParallelBatchNode<SharedStorage> {
  private delay: number;

  constructor(delay: number = 0.1, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.delay = delay;
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    return shared.inputNumbers || [];
  }

  async exec(number: number): Promise<number> {
    await new Promise((resolve) => setTimeout(resolve, this.delay * 1000));
    return number * 2;
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number[]
  ): Promise<string | undefined> {
    shared.processedNumbers = execRes;
    return 'processed';
  }
}

class ErrorProcessor extends ParallelBatchNode<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    return shared.inputNumbers || [];
  }

  async exec(item: number): Promise<number> {
    if (item === 2) {
      throw new Error(`Error processing item ${item}`);
    }
    return item;
  }
}

class OrderTrackingProcessor extends ParallelBatchNode<SharedStorage> {
  private executionOrder: number[] = [];

  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    this.executionOrder = [];
    shared.executionOrder = this.executionOrder;
    return shared.inputNumbers || [];
  }

  async exec(item: number): Promise<number> {
    const delay = item % 2 === 0 ? 0.1 : 0.05;
    await new Promise((resolve) => setTimeout(resolve, delay * 1000));
    this.executionOrder.push(item);
    return item;
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number[]
  ): Promise<string | undefined> {
    shared.executionOrder = this.executionOrder;
    return undefined;
  }
}

describe('AsyncParallelBatchNode Tests', () => {
  test('parallel processing', async () => {
    // Test that numbers are processed in parallel by measuring execution time
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: 5 }, (_, i) => i),
    };

    const processor = new AsyncParallelNumberProcessor(0.1);

    // Record start time
    const startTime = Date.now();
    await processor.run(shared);
    const endTime = Date.now();

    // Check results
    const expected = [0, 2, 4, 6, 8]; // Each number doubled
    expect(shared.processedNumbers).toEqual(expected);

    // Since processing is parallel, total time should be approximately
    // equal to the delay of a single operation, not delay * number_of_items
    const executionTime = endTime - startTime;
    expect(executionTime).toBeLessThan(200); // Should be around 100ms plus minimal overhead
  });

  test('empty input', async () => {
    // Test processing of empty input
    const shared: SharedStorage = {
      inputNumbers: [],
    };

    const processor = new AsyncParallelNumberProcessor();
    await processor.run(shared);

    expect(shared.processedNumbers).toEqual([]);
  });

  test('single item', async () => {
    // Test processing of a single item
    const shared: SharedStorage = {
      inputNumbers: [42],
    };

    const processor = new AsyncParallelNumberProcessor();
    await processor.run(shared);

    expect(shared.processedNumbers).toEqual([84]);
  });

  test('large batch', async () => {
    // Test processing of a large batch of numbers
    const inputSize = 100;
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: inputSize }, (_, i) => i),
    };

    const processor = new AsyncParallelNumberProcessor(0.01);
    await processor.run(shared);

    const expected = Array.from({ length: inputSize }, (_, i) => i * 2);
    expect(shared.processedNumbers).toEqual(expected);
  });

  test('error handling', async () => {
    // Test error handling during parallel processing
    const shared: SharedStorage = {
      inputNumbers: [1, 2, 3],
    };

    const processor = new ErrorProcessor();

    await expect(async () => {
      await processor.run(shared);
    }).rejects.toThrow('Error processing item 2');
  });

  test('concurrent execution', async () => {
    // Test that tasks are actually running concurrently by tracking execution order
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: 4 }, (_, i) => i), // [0, 1, 2, 3]
    };

    const processor = new OrderTrackingProcessor();
    await processor.run(shared);

    // Odd numbers should finish before even numbers due to shorter delay
    expect(shared.executionOrder?.indexOf(1)).toBeLessThan(
      shared.executionOrder?.indexOf(0) as number
    );
    expect(shared.executionOrder?.indexOf(3)).toBeLessThan(
      shared.executionOrder?.indexOf(2) as number
    );
  });

  test('integration with Flow', async () => {
    // Test integration with Flow
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: 5 }, (_, i) => i),
    };

    class ProcessResultsNode extends ParallelBatchNode<SharedStorage> {
      async prep(shared: SharedStorage): Promise<number[]> {
        return shared.processedNumbers || [];
      }

      async exec(num: number): Promise<number> {
        return num + 1;
      }

      async post(
        shared: SharedStorage,
        prepRes: number[],
        execRes: number[]
      ): Promise<string | undefined> {
        shared.finalResults = execRes;
        return 'completed';
      }
    }

    const processor = new AsyncParallelNumberProcessor();
    const resultsProcessor = new ProcessResultsNode();

    processor.on('processed', resultsProcessor);

    const pipeline = new Flow(processor);
    await pipeline.run(shared);

    // Each number should be doubled and then incremented
    const expected = [1, 3, 5, 7, 9];
    expect(shared.finalResults).toEqual(expected);
  });
});
</file>

<file path="tests/qa-pattern.test.ts">
// tests/qa-pattern.test.ts
import { Node, Flow } from '../src/index';

// Mock the prompt function since we're in a test environment
const mockUserInput = "What is PocketFlow?";
global.prompt = jest.fn().mockImplementation(() => mockUserInput);

// Mock utility function for LLM calls
async function callLlm(question: string): Promise<string> {
  // Simple mock LLM call that returns a predefined answer based on the question
  if (question.includes("PocketFlow")) {
    return "PocketFlow is a TypeScript library for building reliable AI pipelines with a focus on composition and reusability.";
  }
  return "I don't know the answer to that question.";
}

// Define the shared store type as shown in the guide
interface QASharedStore {
  question?: string;
  answer?: string;
  [key: string]: unknown;
}

// Implement the GetQuestionNode from the guide
class GetQuestionNode extends Node<QASharedStore> {
  async exec(_: unknown): Promise<string> {
    // Get question directly from user input
    const userQuestion = prompt("Enter your question: ") || "";
    return userQuestion;
  }

  async post(
    shared: QASharedStore,
    _: unknown,
    execRes: string
  ): Promise<string | undefined> {
    // Store the user's question
    shared.question = execRes;
    return "default"; // Go to the next node
  }
}

// Implement the AnswerNode from the guide
class AnswerNode extends Node<QASharedStore> {
  async prep(shared: QASharedStore): Promise<string> {
    // Read question from shared
    return shared.question || "";
  }

  async exec(question: string): Promise<string> {
    // Call LLM to get the answer
    return await callLlm(question);
  }

  async post(
    shared: QASharedStore,
    _: unknown,
    execRes: string
  ): Promise<string | undefined> {
    // Store the answer in shared
    shared.answer = execRes;
    return undefined;
  }
}

// Create a function to set up the QA flow
function createQaFlow(): Flow {
  // Create nodes
  const getQuestionNode = new GetQuestionNode();
  const answerNode = new AnswerNode();

  // Connect nodes in sequence
  getQuestionNode.next(answerNode);

  // Create flow starting with input node
  return new Flow(getQuestionNode);
}

// Tests for the QA pattern
describe('QA Pattern Tests', () => {
  // Test the basic QA flow
  test('Basic QA Flow with mocked user input', async () => {
    // Create shared store
    const shared: QASharedStore = {
      question: undefined,
      answer: undefined,
    };
    
    // Create and run the flow
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    
    // Verify results
    expect(shared.question).toBe(mockUserInput);
    expect(shared.answer).toBe("PocketFlow is a TypeScript library for building reliable AI pipelines with a focus on composition and reusability.");
  });
  
  // Test with a different question (simulating a different user input)
  test('QA Flow with unknown question', async () => {
    // Change the mock implementation for this test
    global.prompt = jest.fn().mockImplementation(() => "What is the meaning of life?");
    
    const shared: QASharedStore = {
      question: undefined,
      answer: undefined,
    };
    
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    
    expect(shared.question).toBe("What is the meaning of life?");
    expect(shared.answer).toBe("I don't know the answer to that question.");
  });
  
  // Test error handling (missing question)
  test('QA Flow with missing question', async () => {
    // Mock a null or empty response
    global.prompt = jest.fn().mockImplementation(() => "");
    
    const shared: QASharedStore = {
      question: undefined,
      answer: undefined,
    };
    
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    
    expect(shared.question).toBe("");
    expect(shared.answer).toBe("I don't know the answer to that question.");
  });
});
</file>

<file path="tests/rag-pattern.test.ts">
// tests/rag-pattern.test.ts
import { BaseNode, Node, BatchNode, Flow } from '../src/index';

// Mock utility functions to simulate real operations
async function getEmbedding(text: string): Promise<number[]> {
  // Simple mock embedding - converts string to vector of character codes
  // In real applications, this would call an embedding API
  return Array.from(text.substring(0, 5)).map(char => char.charCodeAt(0));
}

async function createIndex(embeddings: number[][]): Promise<{ embeddings: number[][] }> {
  // Simple mock index creation
  return { embeddings };
}

async function searchIndex(index: { embeddings: number[][] }, queryEmbedding: number[], options: { topK: number }): Promise<[number[][], number[][]]> {
  // Mock search function that returns indices and distances
  // In real applications, this would do vector similarity search
  const similarities = index.embeddings.map((emb, idx) => {
    // Simple dot product as similarity
    const similarity = emb.reduce((sum, val, i) => sum + val * (queryEmbedding[i] || 0), 0);
    return [idx, similarity];
  });
  
  // Sort by similarity (descending)
  similarities.sort((a, b) => b[1] - a[1]);
  
  // Return top-k indices and distances
  const topK = Math.min(options.topK, similarities.length);
  const indices = [similarities.slice(0, topK).map(s => s[0])];
  const distances = [similarities.slice(0, topK).map(s => s[1])];
  
  return [indices, distances];
}

async function callLlm(prompt: string): Promise<string> {
  // Simple mock LLM call
  return `Answer based on: ${prompt.substring(0, 30)}...`;
}

// Define shared storage type for RAG pattern
type RAGSharedStorage = {
  files?: string[];
  allChunks?: string[];
  allEmbeds?: number[][];
  index?: any;
  question?: string;
  qEmb?: number[];
  retrievedChunk?: string;
  answer?: string;
};

// Stage 1: Offline Indexing Nodes
class ChunkDocs extends BatchNode<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<string[]> {
    return shared.files || [];
  }

  async exec(filepath: string): Promise<string[]> {
    // Mock file reading - in real usage, you would read actual files
    const text = `This is mock content for ${filepath}. It contains some sample text for testing the RAG pattern.`;
    
    // Chunk by 20 chars each
    const chunks: string[] = [];
    const size = 20;
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.substring(i, i + size));
    }
    return chunks;
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: string[],
    execResList: string[][]
  ): Promise<string | undefined> {
    // Flatten chunks from all files
    const allChunks: string[] = [];
    for (const chunkList of execResList) {
      allChunks.push(...chunkList);
    }
    shared.allChunks = allChunks;
    return undefined;
  }
}

class EmbedDocs extends BatchNode<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<string[]> {
    return shared.allChunks || [];
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: string[],
    execResList: number[][]
  ): Promise<string | undefined> {
    shared.allEmbeds = execResList;
    return undefined;
  }
}

class StoreIndex extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<number[][]> {
    return shared.allEmbeds || [];
  }

  async exec(allEmbeds: number[][]): Promise<unknown> {
    return await createIndex(allEmbeds);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: number[][],
    index: unknown
  ): Promise<string | undefined> {
    shared.index = index;
    return undefined;
  }
}

// Stage 2: Online Query & Answer Nodes
class EmbedQuery extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<string> {
    return shared.question || '';
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: string,
    qEmb: number[]
  ): Promise<string | undefined> {
    shared.qEmb = qEmb;
    return undefined;
  }
}

class RetrieveDocs extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<[number[], unknown, string[]]> {
    return [shared.qEmb || [], shared.index || {}, shared.allChunks || []];
  }

  async exec(inputs: [number[], unknown, string[]]): Promise<string> {
    const [qEmb, index, chunks] = inputs;
    const [I, D] = await searchIndex(index as { embeddings: number[][] }, qEmb, { topK: 1 });
    const bestId = I[0][0];
    const relevantChunk = chunks[bestId];
    return relevantChunk;
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: [number[], unknown, string[]],
    relevantChunk: string
  ): Promise<string | undefined> {
    shared.retrievedChunk = relevantChunk;
    return undefined;
  }
}

class GenerateAnswer extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<[string, string]> {
    return [shared.question || '', shared.retrievedChunk || ''];
  }

  async exec(inputs: [string, string]): Promise<string> {
    const [question, chunk] = inputs;
    const prompt = `Question: ${question}\nContext: ${chunk}\nAnswer:`;
    return await callLlm(prompt);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: [string, string],
    answer: string
  ): Promise<string | undefined> {
    shared.answer = answer;
    return undefined;
  }
}

// Tests for the RAG pattern
describe('RAG Pattern Tests', () => {
  // Test the offline indexing flow
  test('Offline Indexing Flow', async () => {
    // Create and connect nodes
    const chunkNode = new ChunkDocs();
    const embedNode = new EmbedDocs();
    const storeNode = new StoreIndex();
    
    chunkNode.next(embedNode);
    embedNode.next(storeNode);
    
    const offlineFlow = new Flow(chunkNode);
    
    // Prepare test data
    const shared: RAGSharedStorage = {
      files: ['doc1.txt', 'doc2.txt'],
    };
    
    // Run the flow
    await offlineFlow.run(shared);
    
    // Verify results
    expect(shared.allChunks).toBeDefined();
    expect(shared.allChunks?.length).toBeGreaterThan(0);
    expect(shared.allEmbeds).toBeDefined();
    expect(shared.allEmbeds?.length).toBe(shared.allChunks?.length);
    expect(shared.index).toBeDefined();
  });
  
  // Test the online query and answer flow
  test('Online Query & Answer Flow', async () => {
    // First run the offline indexing to prepare the data
    const chunkNode = new ChunkDocs();
    const embedNode = new EmbedDocs();
    const storeNode = new StoreIndex();
    
    chunkNode.next(embedNode);
    embedNode.next(storeNode);
    
    const offlineFlow = new Flow(chunkNode);
    
    const shared: RAGSharedStorage = {
      files: ['doc1.txt', 'doc2.txt'],
    };
    
    await offlineFlow.run(shared);
    
    // Now create and run the online flow
    const embedQNode = new EmbedQuery();
    const retrieveNode = new RetrieveDocs();
    const generateNode = new GenerateAnswer();
    
    embedQNode.next(retrieveNode);
    retrieveNode.next(generateNode);
    
    const onlineFlow = new Flow(embedQNode);
    
    // Set the question
    shared.question = 'What is the content about?';
    
    // Run the flow
    await onlineFlow.run(shared);
    
    // Verify results
    expect(shared.qEmb).toBeDefined();
    expect(shared.retrievedChunk).toBeDefined();
    expect(shared.answer).toBeDefined();
    expect(typeof shared.answer).toBe('string');
  });
  
  // Test the complete RAG pipeline
  test('Complete RAG Pipeline', async () => {
    // Create a combined flow for both offline and online stages
    
    // Offline stage nodes
    const chunkNode = new ChunkDocs();
    const embedNode = new EmbedDocs();
    const storeNode = new StoreIndex();
    
    // Online stage nodes
    const embedQNode = new EmbedQuery();
    const retrieveNode = new RetrieveDocs();
    const generateNode = new GenerateAnswer();
    
    // Connect offline stage
    chunkNode.next(embedNode);
    embedNode.next(storeNode);
    
    // Connect online stage
    storeNode.next(embedQNode);
    embedQNode.next(retrieveNode);
    retrieveNode.next(generateNode);
    
    // Create flow
    const fullRagFlow = new Flow(chunkNode);
    
    // Prepare test data
    const shared: RAGSharedStorage = {
      files: ['doc1.txt', 'doc2.txt'],
      question: 'What is the content about?',
    };
    
    // Run the full flow
    await fullRagFlow.run(shared);
    
    // Verify results
    expect(shared.allChunks).toBeDefined();
    expect(shared.allEmbeds).toBeDefined();
    expect(shared.index).toBeDefined();
    expect(shared.qEmb).toBeDefined();
    expect(shared.retrievedChunk).toBeDefined();
    expect(shared.answer).toBeDefined();
    expect(typeof shared.answer).toBe('string');
  });
});
</file>

<file path=".cursorrules">
================================================
File: docs/guide.md
================================================

---

layout: default
title: "Agentic Coding"

---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agents involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
> {: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps             |   Human    |     AI     | Comment                                                                                        |
| :---------------- | :--------: | :--------: | :--------------------------------------------------------------------------------------------- |
| 1. Requirements   |  â˜…â˜…â˜… High  |  â˜…â˜†â˜† Low   | Humans understand the requirements and context.                                                |
| 2. Flow           | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans specify the high-level design, and the AI fills in the details.                         |
| 3. Utilities      | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Node           |  â˜…â˜†â˜† Low   |  â˜…â˜…â˜… High  | The AI helps design the node types and data handling based on the flow.                        |
| 5. Implementation |  â˜…â˜†â˜† Low   |  â˜…â˜…â˜… High  | The AI implements the flow based on the design.                                                |
| 6. Optimization   | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans evaluate the results, and the AI helps optimize.                                        |
| 7. Reliability    |  â˜…â˜†â˜† Low   |  â˜…â˜…â˜… High  | The AI writes test cases and addresses corner cases.                                           |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit.

   - Understand AI systems' strengths and limitations:
     - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
     - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
     - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
   - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
   - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.

   - Identify applicable design patterns (e.g., [Map Reduce](./design_pattern/mapreduce.md), [Agent](./design_pattern/agent.md), [RAG](./design_pattern/rag.md)).
     - For each node in the flow, start with a high-level one-line description of what it does.
     - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
     - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
     - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
   - Outline the flow and draw it in a mermaid diagram. For example:

     ```mermaid
     flowchart LR
         start[Start] --> batch[Batch]
         batch --> check[Check]
         check -->|OK| process
         check -->|Error| fix[Fix]
         fix --> check

         subgraph process[Process]
           step1[Step 1] --> step2[Step 2]
         end

         process --> endNode[End]
     ```

   - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
     > {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.

   - Think of your AI system as the brain. It needs a bodyâ€”these _external utility functions_â€”to interact with the real world:
       <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

     - Reading inputs (e.g., retrieving Slack messages, reading emails)
     - Writing outputs (e.g., generating reports, sending emails)
     - Using external tools (e.g., calling LLMs, searching the web)
     - **NOTE**: _LLM-based tasks_ (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are _core functions_ internal in the AI system.

   - For each utility function, implement it and write a simple test.
   - Document their input/output, as well as why they are necessary. For example:
     - `name`: `getEmbedding` (`src/utils/getEmbedding.ts`)
     - `input`: `string`
     - `output`: a vector of 3072 numbers
     - `necessity`: Used by the second node to embed text
   - Example utility implementation:

     ```typescript
     // src/utils/callLlm.ts
     import { OpenAI } from "openai";

     export async function callLlm(prompt: string): Promise<string> {
       const client = new OpenAI({
         apiKey: process.env.OPENAI_API_KEY,
       });

       const response = await client.chat.completions.create({
         model: "gpt-4o",
         messages: [{ role: "user", content: prompt }],
       });

       return response.choices[0].message.content || "";
     }
     ```

   - > **Sometimes, design Utilies before Flow:** For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
     > {: .best-practice }

4. **Node Design**: Plan how each node will read and write data, and use utility functions.

   - One core design principle for PocketFlow is to use a [shared store](./core_abstraction/communication.md), so start with a shared store design:

     - For simple systems, use an in-memory object.
     - For more complex systems or when persistence is required, use a database.
     - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
     - Example shared store design:

       ```typescript
       interface SharedStore {
         user: {
           id: string;
           context: {
             weather: { temp: number; condition: string };
             location: string;
           };
         };
         results: Record<string, unknown>;
       }

       const shared: SharedStore = {
         user: {
           id: "user123",
           context: {
             weather: { temp: 72, condition: "sunny" },
             location: "San Francisco",
           },
         },
         results: {}, // Empty object to store outputs
       };
       ```

   - For each [Node](./core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Node (or BatchNode, or ParallelBatchNode)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function
     - `post`: Write "embedding" to the shared store

5. **Implementation**: Implement the initial nodes and flows based on the design.

   - ðŸŽ‰ If you've reached this step, humans have finished the design. Now _Agentic Coding_ begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Avoid `try` logic so you can quickly identify any weak points in the system.
   - Add logging throughout the code to facilitate debugging.

6. **Optimization**:

   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:

     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3â€“6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     > {: .best-practice }

7. **Reliability**
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `maxRetries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts
â”‚   â”œâ”€â”€ nodes.ts
â”‚   â”œâ”€â”€ flow.ts
â”‚   â”œâ”€â”€ types.ts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ callLlm.ts
â”‚       â””â”€â”€ searchWeb.ts
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ design.md
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

- **`docs/design.md`**: Contains project documentation for each step above. This should be _high-level_ and _no-code_.
- **`src/types.ts`**: Contains shared type definitions and interfaces used throughout the project.
- **`src/utils/`**: Contains all utility functions.
  - It's recommended to dedicate one TypeScript file to each API call, for example `callLlm.ts` or `searchWeb.ts`.
  - Each file should export functions that can be imported elsewhere in the project
  - Include test cases for each utility function using `utilityFunctionName.test.ts`
- **`src/nodes.ts`**: Contains all the node definitions.

  ```typescript
  // src/types.ts
  export interface QASharedStore {
    question?: string;
    answer?: string;
  }
  ```

  ```typescript
  // src/nodes.ts
  import { Node } from "pocketflow";
  import { callLlm } from "./utils/callLlm";
  import { QASharedStore } from "./types";
  import PromptSync from "prompt-sync";

  const prompt = PromptSync();

  export class GetQuestionNode extends Node<QASharedStore> {
    async exec(): Promise<string> {
      // Get question directly from user input
      const userQuestion = prompt("Enter your question: ") || "";
      return userQuestion;
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the user's question
      shared.question = execRes;
      return "default"; // Go to the next node
    }
  }

  export class AnswerNode extends Node<QASharedStore> {
    async prep(shared: QASharedStore): Promise<string> {
      // Read question from shared
      return shared.question || "";
    }

    async exec(question: string): Promise<string> {
      // Call LLM to get the answer
      return await callLlm(question);
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the answer in shared
      shared.answer = execRes;
      return undefined;
    }
  }
  ```

- **`src/flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.

  ```typescript
  // src/flow.ts
  import { Flow } from "pocketflow";
  import { GetQuestionNode, AnswerNode } from "./nodes";
  import { QASharedStore } from "./types";

  export function createQaFlow(): Flow {
    // Create nodes
    const getQuestionNode = new GetQuestionNode();
    const answerNode = new AnswerNode();

    // Connect nodes in sequence
    getQuestionNode.next(answerNode);

    // Create flow starting with input node
    return new Flow<QASharedStore>(getQuestionNode);
  }
  ```

- **`src/index.ts`**: Serves as the project's entry point.

  ```typescript
  // src/index.ts
  import { createQaFlow } from "./flow";
  import { QASharedStore } from "./types";

  // Example main function
  async function main(): Promise<void> {
    const shared: QASharedStore = {
      question: undefined, // Will be populated by GetQuestionNode from user input
      answer: undefined, // Will be populated by AnswerNode
    };

    // Create the flow and run it
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    console.log(`Question: ${shared.question}`);
    console.log(`Answer: ${shared.answer}`);
  }

  // Run the main function
  main().catch(console.error);
  ```

- **`package.json`**: Contains project metadata and dependencies.

- **`tsconfig.json`**: Contains TypeScript compiler configuration.

================================================
File: docs/index.md
================================================

---

layout: default
title: "Home"
nav_order: 1

---

---
layout: default
title: "Home"
nav_order: 1
---

# PocketFlow.js

A [100-line](https://github.com/The-Pocket/PocketFlow-Typescript/blob/main/src/index.ts) minimalist LLM framework for _Agents, Task Decomposition, RAG, etc_.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworksâ€”([Multi-](./design_pattern/multi_agent.html))[Agents](./design_pattern/agent.html), [Workflow](./design_pattern/workflow.html), [RAG](./design_pattern/rag.html), and more.
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](./core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](./core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](./core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](./core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [(Advanced) Parallel](./core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="700"/>
</div>

## Design Pattern

From there, itâ€™s easy to implement popular design patterns:

- [Agent](./design_pattern/agent.md) autonomously makes decisions.
- [Workflow](./design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](./design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](./design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](./design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](./design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="700"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer _examples_â€”please _implement your own_:

- [LLM Wrapper](./utility_function/llm.md)
- [Viz and Debug](./utility_function/viz.md)
- [Web Search](./utility_function/websearch.md)
- [Chunking](./utility_function/chunking.md)
- [Embedding](./utility_function/embedding.md)
- [Vector Databases](./utility_function/vector.md)
- [Text-to-Speech](./utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a _bad practice_ for vendor-specific APIs in a general framework:

- _API Volatility_: Frequent changes lead to heavy maintenance for hardcoded APIs.
- _Flexibility_: You may want to switch vendors, use fine-tuned models, or run them locally.
- _Optimizations_: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps?

Check out [Agentic Coding Guidance](./guide.md), the fastest way to develop LLM projects with PocketFlow.js!

================================================
File: docs/core_abstraction/batch.md
================================================

---

layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4

---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:

- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **array** of items to process.
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prepRes, execResList)`**: after all items are processed, receives a **list** of results (`execResList`) and returns an **Action**.

### Example: Summarize a Large File

```typescript
type SharedStorage = {
  data: string;
  summary?: string;
};

class MapSummaries extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // Chunk content into manageable pieces
    const content = shared.data;
    const chunks: string[] = [];
    const chunkSize = 10000;

    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize));
    }

    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    _: string[],
    summaries: string[]
  ): Promise<string> {
    shared.summary = summaries.join("\n");
    return "default";
  }
}

// Usage
const flow = new Flow(new MapSummaries());
await flow.run({ data: "very long text content..." });
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Example: Summarize Many Files

```typescript
type SharedStorage = {
  files: string[];
};

type FileParams = {
  filename: string;
};

class SummarizeAllFiles extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    return shared.files.map((filename) => ({ filename }));
  }
}

// Create a per-file summarization flow
const summarizeFile = new SummarizeFile();
const summarizeAllFiles = new SummarizeAllFiles(summarizeFile);

await summarizeAllFiles.run({ files: ["file1.txt", "file2.txt"] });
```

### Under the Hood

1. `prep(shared)` returns a list of param objectsâ€”e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each object and:
   - Merges it with the BatchFlow's own `params`
   - Calls `flow.run(shared)` using the merged result
3. This means the sub-Flow runs **repeatedly**, once for every param object.

---

## 3. Nested Batches

You can nest BatchFlows to handle hierarchical data processing:

```typescript
type DirectoryParams = {
  directory: string;
};

type FileParams = DirectoryParams & {
  filename: string;
};

class FileBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    const directory = this._params.directory;
    const files = await getFilesInDirectory(directory).filter((f) =>
      f.endsWith(".txt")
    );

    return files.map((filename) => ({
      directory, // Pass on directory from parent
      filename, // Add filename for this batch item
    }));
  }
}

class DirectoryBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<DirectoryParams[]> {
    return ["/path/to/dirA", "/path/to/dirB"].map((directory) => ({
      directory,
    }));
  }
}

// Process all files in all directories
const processingNode = new ProcessingNode();
const fileFlow = new FileBatchFlow(processingNode);
const dirFlow = new DirectoryBatchFlow(fileFlow);
await dirFlow.run({});
```

================================================
File: docs/core_abstraction/communication.md
================================================

---

layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3

---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)**

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate _Data Schema_ from _Compute Logic_! This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](./batch.md).
     > {: .best-practice }

2. **Params (only for [Batch](./batch.md))**
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:

```typescript
interface SharedStore {
  data: Record<string, unknown>;
  summary: Record<string, unknown>;
  config: Record<string, unknown>;
  // ...other properties
}

const shared: SharedStore = { data: {}, summary: {}, config: {} /* ... */ };
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```typescript
interface SharedStore {
  data: string;
  summary: string;
}

class LoadData extends Node<SharedStore> {
  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write data to shared store
    shared.data = "Some text content";
    return "default";
  }
}

class Summarize extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<unknown> {
    // We read data from shared store
    return shared.data;
  }

  async exec(prepRes: unknown): Promise<unknown> {
    // Call LLM to summarize
    const prompt = `Summarize: ${prepRes}`;
    const summary = await callLlm(prompt);
    return summary;
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write summary to shared store
    shared.summary = execRes as string;
    return "default";
  }
}

const loadData = new LoadData();
const summarize = new Summarize();
loadData.next(summarize);
const flow = new Flow(loadData);

const shared: SharedStore = { data: "", summary: "" };
flow.run(shared);
```

Here:

- `LoadData` writes to `shared.data`.
- `Summarize` reads from `shared.data`, summarizes, and writes to `shared.summary`.

---

## 2. Params

**Params** let you store _per-Node_ or _per-Flow_ config that doesn't need to live in the shared store. They are:

- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `setParams()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow.
>
> If you need to set child node params, see [Batch](./batch.md).
> {: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```typescript
interface SharedStore {
  data: Record<string, string>;
  summary: Record<string, string>;
}

interface SummarizeParams {
  filename: string;
}

// 1) Create a Node that uses params
class SummarizeFile extends Node<SharedStore, SummarizeParams> {
  async prep(shared: SharedStore): Promise<unknown> {
    // Access the node's param
    const filename = this._params.filename;
    return shared.data[filename] || "";
  }

  async exec(prepRes: unknown): Promise<unknown> {
    const prompt = `Summarize: ${prepRes}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    const filename = this._params.filename;
    shared.summary[filename] = execRes as string;
    return "default";
  }
}

// 2) Set params
const node = new SummarizeFile();

// 3) Set Node params directly (for testing)
node.setParams({ filename: "doc1.txt" });
node.run(shared);

// 4) Create Flow
const flow = new Flow<SharedStore>(node);

// 5) Set Flow params (overwrites node params)
flow.setParams({ filename: "doc2.txt" });
flow.run(shared); // The node summarizes doc2, not doc1
```

================================================
File: docs/core_abstraction/flow.md
================================================

---

layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2

---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `nodeA.next(nodeB)`
   This means if `nodeA.post()` returns `"default"`, go to `nodeB`.
   (Equivalent to `nodeA.on("default", nodeB)`)

2. **Named action transition**: `nodeA.on("actionName", nodeB)`
   This means if `nodeA.post()` returns `"actionName"`, go to `nodeB`.

It's possible to create loops, branching, or multi-step flows.

## 2. Method Chaining

The transition methods support **chaining** for more concise flow creation:

### Chaining `on()` Methods

The `on()` method returns the current node, so you can chain multiple action definitions:

```typescript
// All transitions from the same node
nodeA.on("approved", nodeB).on("rejected", nodeC).on("needs_review", nodeD);

// Equivalent to:
nodeA.on("approved", nodeB);
nodeA.on("rejected", nodeC);
nodeA.on("needs_review", nodeD);
```

### Chaining `next()` Methods

The `next()` method returns the target node, allowing you to create linear sequences in a single expression:

```typescript
// Creates a linear A â†’ B â†’ C â†’ D sequence
nodeA.next(nodeB).next(nodeC).next(nodeD);

// Equivalent to:
nodeA.next(nodeB);
nodeB.next(nodeC);
nodeC.next(nodeD);
```

### Combining Chain Types

You can combine both chaining styles for complex flows:

```typescript
nodeA
  .on("action1", nodeB.next(nodeC).next(nodeD))
  .on("action2", nodeE.on("success", nodeF).on("failure", nodeG));
```

## 3. Creating a Flow

A **Flow** begins with a **start** node. You call `const flow = new Flow(someNode)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```typescript
nodeA.next(nodeB);
const flow = new Flow(nodeA);
flow.run(shared);
```

- When you run the flow, it executes `nodeA`.
- Suppose `nodeA.post()` returns `"default"`.
- The flow then sees `"default"` Action is linked to `nodeB` and runs `nodeB`.
- `nodeB.post()` returns `"default"` but we didn't define a successor for `nodeB`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```typescript
// Define the flow connections
review.on("approved", payment); // If approved, process payment
review.on("needs_revision", revise); // If needs changes, go to revision
review.on("rejected", finish); // If rejected, finish the process

revise.next(review); // After revision, go back for another review
payment.next(finish); // After payment, finish the process

const flow = new Flow(review);
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action.
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
>
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
> {: .warning }

## 4. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.
2. Combine multiple smaller Flows into a larger Flow for reuse.
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `undefined` for `execRes` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```typescript
// Create a sub-flow
nodeA.next(nodeB);
const subflow = new Flow(nodeA);

// Connect it to another node
subflow.next(nodeC);

// Create the parent flow
const parentFlow = new Flow(subflow);
```

When `parentFlow.run()` executes:

1. It starts `subflow`
2. `subflow` runs through its nodes (`nodeA->nodeB`)
3. After `subflow` completes, execution continues to `nodeC`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation);
const paymentFlow = new Flow(validatePayment);

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory);
const inventoryFlow = new Flow(checkStock);

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup);
const shippingFlow = new Flow(createLabel);

// Connect the flows into a main order pipeline
paymentFlow.next(inventoryFlow).next(shippingFlow);

// Create the master flow
const orderPipeline = new Flow(paymentFlow);

// Run the entire pipeline
orderPipeline.run(sharedData);
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

================================================
File: docs/core_abstraction/node.md
================================================

---

layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1

---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`

   - **Read and preprocess data** from `shared` store.
   - Examples: _query DB, read files, or serialize data into a string_.
   - Return `prepRes`, which is used by `exec()` and `post()`.

2. `exec(prepRes)`

   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: _(mostly) LLM calls, remote APIs, tool use_.
   - âš ï¸ This shall be only for compute and **NOT** access `shared`.
   - âš ï¸ If retries enabled, ensure idempotent implementation.
   - Return `execRes`, which is passed to `post()`.

3. `post(shared, prepRes, execRes)`
   - **Postprocess and write data** back to `shared`.
   - Examples: _update DB, change states, log results_.
   - **Decide the next action** by returning a _string_ (`action = "default"` if _None_).

> **Why 3 steps?** To enforce the principle of _separation of concerns_. The data storage and data processing are operated separately.
>
> All steps are _optional_. E.g., you can only implement `prep` and `post` if you just need to process data.
> {: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting).
  `wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```typescript
const myNode = new SummarizeFile(3, 10); // maxRetries = 3, wait = 10 seconds
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `maxRetries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `this.currentRetry`.

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```typescript
execFallback(prepRes: unknown, error: Error): unknown {
  return "There was an error processing your request.";
}
```

By default, it just re-raises the exception.

### Example: Summarize file

```typescript
type SharedStore = {
  data: string;
  summary?: string;
};

class SummarizeFile extends Node<SharedStore> {
  prep(shared: SharedStore): string {
    return shared.data;
  }

  exec(content: string): string {
    if (!content) return "Empty file content";

    const prompt = `Summarize this text in 10 words: ${content}`;
    return callLlm(prompt);
  }

  execFallback(_: string, error: Error): string {
    return "There was an error processing your request.";
  }

  post(shared: SharedStore, _: string, summary: string): string | undefined {
    shared.summary = summary;
    return undefined; // "default" action
  }
}

// Example usage
const node = new SummarizeFile(3); // maxRetries = 3
const shared: SharedStore = { data: "Long text to summarize..." };
const action = node.run(shared);

console.log("Action:", action);
console.log("Summary:", shared.summary);
```

================================================
File: docs/core_abstraction/parallel.md
================================================

---

layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6

---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple operations **concurrently**â€”for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute.

> Parallel nodes and flows excel at overlapping I/O-bound workâ€”like LLM calls, database queries, API requests, or file I/O. TypeScript's Promise-based implementation allows for truly concurrent execution of asynchronous operations.
> {: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
>
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism.
>
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
>   {: .best-practice }

## ParallelBatchNode

Like **BatchNode**, but runs operations in **parallel** using Promise.all():

```typescript
class TextSummarizer extends ParallelBatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // e.g., multiple texts
    return shared.texts || [];
  }

  async exec(text: string): Promise<string> {
    const prompt = `Summarize: ${text}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.summaries = execRes;
    return "default";
  }
}

const node = new TextSummarizer();
const flow = new Flow(node);
```

## ParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using Promise.all():

```typescript
class SummarizeMultipleFiles extends ParallelBatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, any>[]> {
    return (shared.files || []).map((f) => ({ filename: f }));
  }
}

const subFlow = new Flow(new LoadAndSummarizeFile());
const parallelFlow = new SummarizeMultipleFiles(subFlow);
await parallelFlow.run(shared);
```

================================================
File: docs/design_pattern/agent.md
================================================

---

layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1

---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodesâ€”and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide actionâ€”for example:

```typescript
`
### CONTEXT
Task: ${task}
Previous Actions: ${prevActions}
Current State: ${state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters: query (str)

[2] answer
  Description: Conclude based on the results
  Parameters: result (str)

### NEXT ACTION
Decide the next action based on the current context.
Return your response in YAML format:

\`\`\`yaml
thinking: <reasoning process>
action: <action_name>
parameters: <parameters>
\`\`\``;
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md).

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actionsâ€”avoiding overlap like separate `read_databases` or `read_csvs`.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks instead of all at once.
- **Overview-zoom-in:** First provide high-level structure, then allow drilling into details.
- **Parameterized/Programmable:** Enable parameterized or programmable actions.
- **Backtracking:** Let the agent undo the last step instead of restarting entirely.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
interface SharedState {
  query?: string;
  context?: Array<{ term: string; result: string }>;
  search_term?: string;
  answer?: string;
}

class DecideAction extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    const context = shared.context
      ? JSON.stringify(shared.context)
      : "No previous search";
    return [shared.query || "", context];
  }

  async exec([query, context]: [string, string]): Promise<any> {
    const prompt = `
Given input: ${query}
Previous search results: ${context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
\`\`\`yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
\`\`\``;
    const resp = await callLlm(prompt);
    const yamlStr = resp.split("```yaml")[1].split("```")[0].trim();
    return yaml.load(yamlStr);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    result: any
  ): Promise<string> {
    if (result.action === "search") {
      shared.search_term = result.search_term;
    }
    return result.action;
  }
}

class SearchWeb extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.search_term || "";
  }

  async exec(searchTerm: string): Promise<string> {
    return await searchWeb(searchTerm);
  }

  async post(shared: SharedState, _: string, execRes: string): Promise<string> {
    shared.context = [
      ...(shared.context || []),
      { term: shared.search_term || "", result: execRes },
    ];
    return "decide";
  }
}

class DirectAnswer extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    return [
      shared.query || "",
      shared.context ? JSON.stringify(shared.context) : "",
    ];
  }

  async exec([query, context]: [string, string]): Promise<string> {
    return await callLlm(`Context: ${context}\nAnswer: ${query}`);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    execRes: string
  ): Promise<undefined> {
    shared.answer = execRes;
    return undefined;
  }
}

// Connect nodes
const decide = new DecideAction();
const search = new SearchWeb();
const answer = new DirectAnswer();

decide.on("search", search);
decide.on("answer", answer);
search.on("decide", decide); // Loop back

const flow = new Flow(decide);
await flow.run({ query: "Who won the Nobel Prize in Physics 2024?" });
````

================================================
File: docs/design_pattern/mapreduce.md
================================================

---

layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4

---

# Map Reduce

MapReduce is a design pattern suitable when you have either:

- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```typescript
type SharedStorage = {
  files?: Record<string, string>;
  file_summaries?: Record<string, string>;
  all_files_summary?: string;
};

class SummarizeAllFiles extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<[string, string][]> {
    return Object.entries(shared.files || {}); // [["file1.txt", "aaa..."], ["file2.txt", "bbb..."], ...]
  }

  async exec([filename, content]: [string, string]): Promise<[string, string]> {
    const summary = await callLLM(`Summarize the following file:\n${content}`);
    return [filename, summary];
  }

  async post(
    shared: SharedStorage,
    _: [string, string][],
    summaries: [string, string][]
  ): Promise<string> {
    shared.file_summaries = Object.fromEntries(summaries);
    return "summarized";
  }
}

class CombineSummaries extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, string>> {
    return shared.file_summaries || {};
  }

  async exec(summaries: Record<string, string>): Promise<string> {
    const text_list = Object.entries(summaries).map(
      ([fname, summ]) => `${fname} summary:\n${summ}\n`
    );

    return await callLLM(
      `Combine these file summaries into one final summary:\n${text_list.join(
        "\n---\n"
      )}`
    );
  }

  async post(
    shared: SharedStorage,
    _: Record<string, string>,
    finalSummary: string
  ): Promise<string> {
    shared.all_files_summary = finalSummary;
    return "combined";
  }
}

// Create and connect flow
const batchNode = new SummarizeAllFiles();
const combineNode = new CombineSummaries();
batchNode.on("summarized", combineNode);

// Run the flow with test data
const flow = new Flow(batchNode);
flow.run({
  files: {
    "file1.txt":
      "Alice was beginning to get very tired of sitting by her sister...",
    "file2.txt": "Some other interesting text ...",
  },
});
```

> **Performance Tip**: The example above works sequentially. You can speed up the map phase by using `ParallelBatchNode` instead of `BatchNode`. See [(Advanced) Parallel](../core_abstraction/parallel.md) for more details.
> {: .note }

================================================
File: docs/design_pattern/rag.md
================================================

---

layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3

---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` â€“ [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` â€“ [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` â€“ stores embeddings into a [vector database](../utility_function/vector.md).

```typescript
type SharedStore = {
  files?: string[];
  allChunks?: string[];
  allEmbeds?: number[][];
  index?: any;
};

class ChunkDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.files || [];
  }

  async exec(filepath: string): Promise<string[]> {
    const text = fs.readFileSync(filepath, "utf-8");
    // Simplified chunking for example
    const chunks: string[] = [];
    const size = 100;
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.substring(i, i + size));
    }
    return chunks;
  }

  async post(
    shared: SharedStore,
    _: string[],
    chunks: string[][]
  ): Promise<undefined> {
    shared.allChunks = chunks.flat();
    return undefined;
  }
}

class EmbedDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.allChunks || [];
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk);
  }

  async post(
    shared: SharedStore,
    _: string[],
    embeddings: number[][]
  ): Promise<undefined> {
    shared.allEmbeds = embeddings;
    return undefined;
  }
}

class StoreIndex extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<number[][]> {
    return shared.allEmbeds || [];
  }

  async exec(allEmbeds: number[][]): Promise<unknown> {
    return await createIndex(allEmbeds);
  }

  async post(
    shared: SharedStore,
    _: number[][],
    index: unknown
  ): Promise<undefined> {
    shared.index = index;
    return undefined;
  }
}

// Create indexing flow
const chunkNode = new ChunkDocs();
const embedNode = new EmbedDocs();
const storeNode = new StoreIndex();

chunkNode.next(embedNode).next(storeNode);
const offlineFlow = new Flow(chunkNode);
```

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` â€“ embeds the user's question.
2. `RetrieveDocs` â€“ retrieves top chunk from the index.
3. `GenerateAnswer` â€“ calls the LLM with the question + chunk to produce the final answer.

```typescript
type OnlineStore = SharedStore & {
  question?: string;
  qEmb?: number[];
  retrievedChunk?: string;
  answer?: string;
};

class EmbedQuery extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<string> {
    return shared.question || "";
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question);
  }

  async post(
    shared: OnlineStore,
    _: string,
    qEmb: number[]
  ): Promise<undefined> {
    shared.qEmb = qEmb;
    return undefined;
  }
}

class RetrieveDocs extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[number[], any, string[]]> {
    return [shared.qEmb || [], shared.index, shared.allChunks || []];
  }

  async exec([qEmb, index, chunks]: [
    number[],
    any,
    string[]
  ]): Promise<string> {
    const [ids] = await searchIndex(index, qEmb, { topK: 1 });
    return chunks[ids[0][0]];
  }

  async post(
    shared: OnlineStore,
    _: [number[], any, string[]],
    chunk: string
  ): Promise<undefined> {
    shared.retrievedChunk = chunk;
    return undefined;
  }
}

class GenerateAnswer extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[string, string]> {
    return [shared.question || "", shared.retrievedChunk || ""];
  }

  async exec([question, chunk]: [string, string]): Promise<string> {
    return await callLlm(`Question: ${question}\nContext: ${chunk}\nAnswer:`);
  }

  async post(
    shared: OnlineStore,
    _: [string, string],
    answer: string
  ): Promise<undefined> {
    shared.answer = answer;
    return undefined;
  }
}

// Create query flow
const embedQNode = new EmbedQuery();
const retrieveNode = new RetrieveDocs();
const generateNode = new GenerateAnswer();

embedQNode.next(retrieveNode).next(generateNode);
const onlineFlow = new Flow(embedQNode);
```

Usage example:

```typescript
const shared = {
  files: ["doc1.txt", "doc2.txt"], // any text files
};
await offlineFlow.run(shared);
```

================================================
File: docs/design_pattern/structure.md
================================================

---

layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5

---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

## TypeScript Implementation

When using PocketFlow with structured output, follow these TypeScript patterns:

1. **Define Types** for your structured input/output
2. **Implement Validation** in your Node methods
3. **Use Type-Safe Operations** throughout your flow

### Example Text Summarization

````typescript
// Define types
type SummaryResult = {
  summary: string[];
};

type SharedStorage = {
  text?: string;
  result?: SummaryResult;
};

class SummarizeNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    return shared.text;
  }

  async exec(text: string | undefined): Promise<SummaryResult> {
    if (!text) return { summary: ["No text provided"] };

    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points

${text}

Output:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``;

    // Simulated LLM call
    const response =
      "```yaml\nsummary:\n  - First point\n  - Second insight\n  - Final conclusion\n```";

    // Parse YAML response
    const yamlStr = response.split("```yaml")[1].split("```")[0].trim();

    // Extract bullet points
    const result: SummaryResult = {
      summary: yamlStr
        .split("\n")
        .filter((line) => line.trim().startsWith("- "))
        .map((line) => line.trim().substring(2)),
    };

    // Validate
    if (!result.summary || !Array.isArray(result.summary)) {
      throw new Error("Invalid summary structure");
    }

    return result;
  }

  async post(
    shared: SharedStorage,
    _: string | undefined,
    result: SummaryResult
  ): Promise<string | undefined> {
    shared.result = result;
    return "default";
  }
}
````

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

================================================
File: docs/design_pattern/workflow.md
================================================

---

layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2

---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
> - You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.
>
> You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
> {: .best-practice }

### Example: Article Writing

```typescript
interface SharedState {
  topic?: string;
  outline?: string;
  draft?: string;
  final_article?: string;
}

// Helper function to simulate LLM call
async function callLLM(prompt: string): Promise<string> {
  return `Response to: ${prompt}`;
}

class GenerateOutline extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.topic || "";
  }

  async exec(topic: string): Promise<string> {
    return await callLLM(
      `Create a detailed outline for an article about ${topic}`
    );
  }

  async post(shared: SharedState, _: string, outline: string): Promise<string> {
    shared.outline = outline;
    return "default";
  }
}

class WriteSection extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.outline || "";
  }

  async exec(outline: string): Promise<string> {
    return await callLLM(`Write content based on this outline: ${outline}`);
  }

  async post(shared: SharedState, _: string, draft: string): Promise<string> {
    shared.draft = draft;
    return "default";
  }
}

class ReviewAndRefine extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.draft || "";
  }

  async exec(draft: string): Promise<string> {
    return await callLLM(`Review and improve this draft: ${draft}`);
  }

  async post(
    shared: SharedState,
    _: string,
    final: string
  ): Promise<undefined> {
    shared.final_article = final;
    return undefined;
  }
}

// Connect nodes in sequence
const outline = new GenerateOutline();
const write = new WriteSection();
const review = new ReviewAndRefine();

outline.next(write).next(review);

// Create and run flow
const writingFlow = new Flow(outline);
writingFlow.run({ topic: "AI Safety" });
```

For _dynamic cases_, consider using [Agents](./agent.md).

================================================
File: docs/utility_function/llm.md
================================================

---

layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1

---

# LLM Wrappers

Check out popular libraries like [LangChain](https://github.com/langchain-ai/langchainjs) (13.8k+ GitHub stars), [ModelFusion](https://github.com/vercel/modelfusion) (1.2k+ GitHub stars), or [Firebase GenKit](https://firebase.google.com/docs/genkit) for unified LLM interfaces.
Here, we provide some minimal example implementations:

1. OpenAI

   ```typescript
   import { OpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
     const r = await client.chat.completions.create({
       model: "gpt-4o",
       messages: [{ role: "user", content: prompt }],
     });
     return r.choices[0].message.content || "";
   }

   // Example usage
   callLlm("How are you?").then(console.log);
   ```

   > Store the API key in an environment variable like OPENAI_API_KEY for security.
   > {: .best-practice }

2. Claude (Anthropic)

   ```typescript
   import Anthropic from "@anthropic-ai/sdk";

   async function callLlm(prompt: string): Promise<string> {
     const client = new Anthropic({
       apiKey: "YOUR_API_KEY_HERE",
     });
     const response = await client.messages.create({
       model: "claude-3-7-sonnet-20250219",
       max_tokens: 3000,
       messages: [{ role: "user", content: prompt }],
     });
     return response.content[0].text;
   }
   ```

3. Google (Vertex AI)

   ```typescript
   import { VertexAI } from "@google-cloud/vertexai";

   async function callLlm(prompt: string): Promise<string> {
     const vertexAI = new VertexAI({
       project: "YOUR_PROJECT_ID",
       location: "us-central1",
     });

     const generativeModel = vertexAI.getGenerativeModel({
       model: "gemini-1.5-flash",
     });

     const response = await generativeModel.generateContent({
       contents: [{ role: "user", parts: [{ text: prompt }] }],
     });

     return response.response.candidates[0].content.parts[0].text;
   }
   ```

4. Azure (Azure OpenAI)

   ```typescript
   import { AzureOpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new AzureOpenAI({
       apiKey: "YOUR_API_KEY_HERE",
       azure: {
         apiVersion: "2023-05-15",
         endpoint: "https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
       },
     });

     const r = await client.chat.completions.create({
       model: "<YOUR_DEPLOYMENT_NAME>",
       messages: [{ role: "user", content: prompt }],
     });

     return r.choices[0].message.content || "";
   }
   ```

5. Ollama (Local LLM)

   ```typescript
   import ollama from "ollama";

   async function callLlm(prompt: string): Promise<string> {
     const response = await ollama.chat({
       model: "llama2",
       messages: [{ role: "user", content: prompt }],
     });
     return response.message.content;
   }
   ```

## Improvements

Feel free to enhance your `callLlm` function as needed. Here are examples:

- Handle chat history:

```typescript
interface Message {
  role: "user" | "assistant" | "system";
  content: string;
}

async function callLlm(messages: Message[]): Promise<string> {
  const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
  const r = await client.chat.completions.create({
    model: "gpt-4o",
    messages: messages,
  });
  return r.choices[0].message.content || "";
}
```

- Add in-memory caching

```typescript
import { memoize } from "lodash";

const callLlmMemoized = memoize(async (prompt: string): Promise<string> => {
  // Your implementation here
  return "";
});

async function callLlm(prompt: string, useCache = true): Promise<string> {
  if (useCache) {
    return callLlmMemoized(prompt);
  }
  // Call the underlying function directly
  return callLlmInternal(prompt);
}

class SummarizeNode {
  private curRetry = 0;

  async exec(text: string): Promise<string> {
    return callLlm(`Summarize: ${text}`, this.curRetry === 0);
  }
}
```

- Enable logging:

```typescript
async function callLlm(prompt: string): Promise<string> {
  console.info(`Prompt: ${prompt}`);
  // Your implementation here
  const response = ""; // Response from your implementation
  console.info(`Response: ${response}`);
  return response;
}
```
</file>

<file path=".gitignore">
# OS generated files
.DS_Store
.DS_Store?
._*
.direnv
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db


# IDE specific files
.idea/
.vscode/
*.swp
*.swo
*~

# Node
node_modules/
npm-debug.log
yarn-debug.log
yarn-error.log
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
venv/
ENV/

# Logs and databases
*.log
*.sql
*.sqlite

# Build output
dist/
build/
out/

# Coverage reports
coverage/
.coverage
.coverage.*
htmlcov/

# Misc
*.bak
*.tmp
*.temp


test.ipynb
.pytest_cache/
</file>

<file path=".npmignore">
src/
tests/
jest.config.js
tsconfig.json
.npmignore
</file>

<file path="CODE_OF_CONDUCT.md">
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

- Demonstrating empathy and kindness toward other people
- Being respectful of differing opinions, viewpoints, and experiences
- Giving and gracefully accepting constructive feedback
- Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
- Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

- The use of sexualized language or imagery, and sexual attention or
  advances of any kind
- Trolling, insulting or derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or email
  address, without their explicit permission
- Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Project maintainers are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies
</file>

<file path="CONTRIBUTING.md">
# Contributing to Pocket Flow TypeScript

Thank you for your interest in contributing to Pocket Flow! This document provides guidelines and instructions for contributing to this project.

## Table of Contents

- [Code of Conduct](#code-of-conduct)
- [Getting Started](#getting-started)
- [Development Workflow](#development-workflow)
- [Pull Request Process](#pull-request-process)
- [Coding Standards](#coding-standards)
- [Testing](#testing)
- [Documentation](#documentation)
- [Community](#community)

## Code of Conduct

By participating in this project, you agree to abide by the [Code of Conduct](CODE_OF_CONDUCT.md). Please read it to understand the expectations for all contributors.

## Getting Started

### Prerequisites

- Node.js (recommended version: 18.x or later)
- npm (recommended version: 8.x or later)
- Git

### Setup Development Environment

1. Fork the repository on GitHub
2. Clone your fork:
   ```bash
   git clone https://github.com/YOUR_USERNAME/PocketFlow-Typescript.git
   cd PocketFlow-Typescript
   ```
3. Add the original repository as upstream:
   ```bash
   git remote add upstream https://github.com/The-Pocket/PocketFlow-Typescript.git
   ```
4. Install dependencies:
   ```bash
   npm install
   ```

## Development Workflow

1. Create a new branch for your feature or bugfix:

   ```bash
   git checkout -b feature/your-feature-name
   # or
   git checkout -b fix/issue-you-are-fixing
   ```

2. Make your changes

3. Run tests to ensure your changes don't break existing functionality:

   ```bash
   npm test
   ```

4. Update documentation if necessary

5. Commit your changes with a descriptive commit message:

   ```bash
   git commit -m "Description of changes"
   ```

6. Push your branch to your fork:

   ```bash
   git push origin feature/your-feature-name
   ```

7. Create a Pull Request from your fork to the original repository

## Pull Request Process

1. Ensure your PR has a clear title and description
2. Link any relevant issues with "Fixes #issue-number" in the PR description
3. Make sure all tests pass
4. Request a code review from maintainers
5. Address any feedback from code reviews
6. Once approved, a maintainer will merge your PR

## Coding Standards

- Follow the established code style in the project
- Use TypeScript for type safety
- Keep functions small and focused on a single responsibility
- Use meaningful variable and function names
- Comment your code when necessary, but prefer self-documenting code
- Avoid any unnecessary dependencies

## Testing

- Write tests for new features and bug fixes
- Ensure all tests pass before submitting a PR
- Aim for high test coverage
- Follow existing testing patterns

## Documentation

- Update relevant documentation for any new features
- Include JSDoc comments for public APIs
- Keep documentation up-to-date with code changes
- Consider adding examples for complex features

## Community

- Join our [Discord](https://discord.gg/hUHHE9Sa6T) for discussions and questions
- Be respectful and helpful to other community members
- Share knowledge and help others learn

Thank you for contributing to Pocket Flow!
</file>

<file path="jest.config.js">
module.exports = {
    preset: 'ts-jest',
    testEnvironment: 'node',
    testMatch: ['**/tests/**/*.test.ts'],
  };
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Victor Duarte and Zachary Huang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.json">
{
  "name": "pocketflow",
  "description": "A minimalist LLM framework",
  "version": "1.0.4",
  "main": "./dist/index.js",
  "module": "./dist/index.mjs",
  "types": "./dist/index.d.ts",
  "engines": {
    "node": ">=18.0.0"
  },
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "import": "./dist/index.mjs",
      "require": "./dist/index.js"
    }
  },
  "files": [
    "dist"
  ],
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/The-Pocket/PocketFlow-Typescript.git"
  },
  "keywords": [
    "pocketflow",
    "typescript",
    "llm",
    "ai",
    "framework",
    "workflow",
    "minimalist"
  ],
  "scripts": {
    "build": "tsup",
    "test": "jest",
    "test:watch": "jest --watch",
    "prepublishOnly": "npm run build"
  },
  "devDependencies": {
    "@types/jest": "^29.5.14",
    "jest": "^29.7.0",
    "ts-jest": "^29.3.0",
    "tsup": "^8.4.0",
    "typescript": "^5.8.2"
  }
}
</file>

<file path="README.md">
<div align="center">
  <img src="https://raw.githubusercontent.com/The-Pocket/.github/main/assets/title.png" width="600"/>
</div>

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
<a href="https://discord.gg/hUHHE9Sa6T">
<img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

# PocketFlow.js

PocketFlow.js is a TypeScript port of the original [Python version](https://github.com/The-Pocket/PocketFlow) - a minimalist LLM framework.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Documentation](#documentation)
- [Testing](#testing)
- [Contributing](#contributing)
- [Community](#community)
- [License](#license)

## Features

- **Lightweight**: Zero bloat, zero dependencies, zero vendor lock-in.

- **Expressive**: Everything you loveâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), and more.

- **[Agentic Coding](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Let AI Agents (e.g., Cursor AI) build Agentsâ€”10x productivity boost!

## Installation

```bash
npm install pocketflow
```

Alternatively, you can simply copy the [source code](src/index.ts) directly into your project.

## Quick Start

Run the following command to create a new PocketFlow project:

```bash
npx create-pocketflow
```

Use cursor/windsurf/any other LLM builtin IDE to open the project.  
You can type the following prompt to the agent to confirm the project is setup correctly:

```
Help me describe briefly about PocketFlow.js
```

Simply start typing your prompt, and the AI agent will build the project for you.  
Here's a simple example:

```
I want to create an application that can write novel:

1. User can enter a novel title
2. It will generate a outline of the novel
3. It will generate a chapter based on the outline
4. It will save the chapter to ./output/title_name.md

First, read the requirements carefully.
Then, start with design.md first. Stop there until further instructions.
```

Once you have the design, and you have no questions, start the implementation by simply typing:

```
Start implementing the design.
```

## Documentation

- Check out the [official documentation](https://the-pocket.github.io/PocketFlow/) for comprehensive guides and examples. The TypeScript version is still under development, so some features may not be available.
- For an in-depth design explanation, read our [design essay](https://github.com/The-Pocket/.github/blob/main/profile/pocketflow.md)

## Testing

To run tests locally:

```bash
# Install dependencies
npm install

# Run tests
npm test
```

## Contributing

We welcome contributions from the community! Here's how you can help:

### Code of Conduct

Please read and follow our [Code of Conduct](CODE_OF_CONDUCT.md) to foster an inclusive community.

### CI/CD Workflow

We use GitHub Actions for continuous integration and deployment:

- **CI Workflow**: Automatically runs tests and builds the project on each push and pull request to the main branch.
- **Code Quality**: Checks TypeScript compilation to ensure code quality.
- **Release**: Publishes the package to npm when a new release is created.

Note: To publish to npm, maintainers need to configure the `NPM_TOKEN` secret in the repository settings.

### How to Contribute

1. **Fork the Repository**

   - Create your own fork of the repo

2. **Create a Branch**

   - Create a feature branch (`git checkout -b feature/amazing-feature`)
   - For bug fixes, use (`git checkout -b fix/bug-description`)

3. **Make Your Changes**

   - Follow the code style and conventions
   - Add or update tests as needed
   - Keep your changes focused and related to a single issue

4. **Test Your Changes**

   - Ensure all tests pass with `npm test`
   - Add new tests if appropriate

5. **Commit Your Changes**

   - Use clear and descriptive commit messages
   - Reference issue numbers in commit messages when applicable

6. **Submit a Pull Request**
   - Provide a clear description of the changes
   - Link any related issues
   - Answer any questions or feedback during review

### Creating a CursorRule

To create a CursorRule to make AI agents work more effectively on the codebase:

1. Visit [gitingest.com](https://gitingest.com/)
2. Paste the link to the docs folder (e.g., https://github.com/The-Pocket/PocketFlow-Typescript/tree/main/docs) to generate content
3. Remove the following from the generated result:
   - All utility function files except for llm
   - The design_pattern/multi_agent.md file
   - All \_config.yaml and index.md files, except for docs/index.md
4. Save the result as a CursorRule to help AI agents understand the codebase structure better

### Development Setup

```bash
# Clone your forked repository
git clone https://github.com/yourusername/PocketFlow-Typescript.git
cd PocketFlow-Typescript

# Install dependencies
npm install

# Run tests
npm test
```

### Reporting Bugs

When reporting bugs, please include:

- A clear, descriptive title
- Detailed steps to reproduce the issue
- Expected and actual behavior
- Environment information (OS, Node.js version, etc.)
- Any additional context or screenshots

## Community

- Join our [Discord server](https://discord.gg/hUHHE9Sa6T) for discussions and support
- Follow us on [GitHub](https://github.com/The-Pocket)

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="tsconfig.json">
{
    "compilerOptions": {
      "target": "es2018",
      "module": "commonjs",
      "declaration": true,
      "outDir": "./dist",
      "strict": true,
      "esModuleInterop": true,
      "skipLibCheck": true,
      "forceConsistentCasingInFileNames": true,
      "rootDir": "./src"
    },
    "include": ["src/**/*"],
    "exclude": ["node_modules", "dist", "tests"]
  }
</file>

<file path="tsup.config.ts">
import { defineConfig } from 'tsup';

export default defineConfig({
  entry: ['src/index.ts'],
  format: ['cjs', 'esm'],
  dts: true,
  splitting: false,
  sourcemap: true,
  clean: true,
  outDir: 'dist',
  outExtension({ format }) {
    return {
      js: format === 'cjs' ? '.js' : '.mjs',
    };
  }
});
</file>

</files>
</file>

<file path=".docs/repomix-output-The-Pocket-PocketFlow.xml">
This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    core_abstraction/
      async.mdc
      batch.mdc
      communication.mdc
      flow.mdc
      node.mdc
      parallel.mdc
    design_pattern/
      agent.mdc
      mapreduce.mdc
      multi_agent.mdc
      rag.mdc
      structure.mdc
      workflow.mdc
    utility_function/
      chunking.mdc
      embedding.mdc
      llm.mdc
      text_to_speech.mdc
      vector.mdc
      viz.mdc
      websearch.mdc
    guide_for_pocketflow.mdc
cookbook/
  data/
    PaulGrahamEssaysLarge/
      addiction.txt
      aord.txt
      apple.txt
      avg.txt
      before.txt
  pocketflow-a2a/
    common/
      client/
        __init__.py
        card_resolver.py
        client.py
      server/
        __init__.py
        server.py
        task_manager.py
        utils.py
      utils/
        in_memory_cache.py
        push_notification_auth.py
      types.py
    a2a_client.py
    a2a_server.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    task_manager.py
    utils.py
  pocketflow-agent/
    demo.ipynb
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    utils.py
  pocketflow-async-basic/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    utils.py
  pocketflow-batch/
    translations/
      README_CHINESE.md
      README_FRENCH.md
      README_GERMAN.md
      README_JAPANESE.md
      README_KOREAN.md
      README_PORTUGUESE.md
      README_RUSSIAN.md
      README_SPANISH.md
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-batch-flow/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-batch-node/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-chat/
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-chat-guardrail/
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-chat-memory/
    utils/
      __init__.py
      call_llm.py
      get_embedding.py
      vector_index.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-cli-hitl/
    docs/
      design.md
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-code-generator/
    doc/
      design.md
    utils/
      call_llm.py
      code_executor.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-communication/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-fastapi-background/
    docs/
      design.md
    static/
      index.html
      progress.html
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-fastapi-hitl/
    docs/
      design.md
    static/
      style.css
    templates/
      index.html
    utils/
      process_task.py
    flow.py
    nodes.py
    README.md
    requirements.txt
    server.py
  pocketflow-fastapi-websocket/
    docs/
      design.md
    static/
      index.html
    utils/
      __init__.py
      stream_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-flow/
    flow.py
    main.py
    README.md
    requirements.txt
  pocketflow-google-calendar/
    utils/
      google_calendar.py
    .env.exemplo
    .gitignore
    main.py
    nodes.py
    Pipfile
    README.md
  pocketflow-gradio-hitl/
    utils/
      call_llm.py
      call_mock_api.py
      conversation.py
      format_chat_history.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-hello-world/
    docs/
      design.md
    utils/
      call_llm.py
    flow.py
    main.py
    README.md
  pocketflow-llm-streaming/
    main.py
    README.md
    utils.py
  pocketflow-majority-vote/
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-map-reduce/
    data/
      resume1.txt
      resume2.txt
      resume3.txt
      resume4.txt
      resume5.txt
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    utils.py
  pocketflow-mcp/
    main.py
    README.md
    requirements.txt
    simple_server.py
    utils.py
  pocketflow-multi-agent/
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-nested-batch/
    school/
      class_a/
        student1.txt
        student2.txt
      class_b/
        student3.txt
        student4.txt
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-node/
    utils/
      call_llm.py
    flow.py
    main.py
    README.md
    requirements.txt
  pocketflow-parallel-batch/
    translations/
      README_CHINESE.md
      README_FRENCH.md
      README_GERMAN.md
      README_JAPANESE.md
      README_KOREAN.md
      README_PORTUGUESE.md
      README_RUSSIAN.md
      README_SPANISH.md
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-parallel-batch-flow/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-rag/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    utils.py
  pocketflow-streamlit-fsm/
    docs/
      design.md
    utils/
      generate_image.py
    app.py
    flow.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-structured-output/
    data.txt
    main.py
    README.md
    requirements.txt
    utils.py
  pocketflow-supervisor/
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    utils.py
  pocketflow-tao/
    flow.py
    main.py
    nodes.py
    README.md
    utils.py
  pocketflow-text2sql/
    docs/
      design.md
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    populate_db.py
    README.md
    requirements.txt
  pocketflow-thinking/
    design.md
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
    utils.py
  pocketflow-tool-crawler/
    tools/
      crawler.py
      parser.py
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-tool-database/
    tools/
      database.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-tool-embeddings/
    tools/
      embeddings.py
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-tool-pdf-vision/
    tools/
      pdf.py
      vision.py
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-tool-search/
    tools/
      parser.py
      search.py
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-tracing/
    examples/
      async_example.py
      basic_example.py
    tracing/
      __init__.py
      config.py
      core.py
      decorator.py
    utils/
      __init__.py
      setup.py
    .env.example
    README.md
    requirements.txt
    setup.py
    test_tracing.py
  pocketflow-visualization/
    viz/
      flow_visualization.html
      flow_visualization.json
    async_flow.py
    async_loop_flow.py
    README.md
    visualize.py
  pocketflow-voice-chat/
    docs/
      design.md
    utils/
      audio_utils.py
      call_llm.py
      speech_to_text.py
      text_to_speech.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow-workflow/
    utils/
      call_llm.py
    flow.py
    main.py
    nodes.py
    README.md
    requirements.txt
  pocketflow_demo.ipynb
  README.md
docs/
  _includes/
    footer_custom.html
  core_abstraction/
    async.md
    batch.md
    communication.md
    flow.md
    index.md
    node.md
    parallel.md
  design_pattern/
    agent.md
    index.md
    mapreduce.md
    multi_agent.md
    rag.md
    structure.md
    workflow.md
  utility_function/
    chunking.md
    embedding.md
    index.md
    llm.md
    text_to_speech.md
    vector.md
    viz.md
    websearch.md
  _config.yml
  guide.md
  index.md
pocketflow/
  __init__.py
  __init__.pyi
tests/
  test_async_batch_flow.py
  test_async_batch_node.py
  test_async_flow.py
  test_async_parallel_batch_flow.py
  test_async_parallel_batch_node.py
  test_batch_flow.py
  test_batch_node.py
  test_fall_back.py
  test_flow_basic.py
  test_flow_composition.py
utils/
  update_pocketflow_mdc.py
.cursorrules
.gitignore
LICENSE
README.md
setup.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/core_abstraction/async.mdc">
---
description: Guidelines for using PocketFlow, Core Abstraction, (Advanced) Async
globs: 
alwaysApply: false
---
# (Advanced) Async

**Async** Nodes implement `prep_async()`, `exec_async()`, `exec_fallback_async()`, and/or `post_async()`. This is useful for:

1. **prep_async()**: For *fetching/reading data (files, APIs, DB)* in an I/O-friendly way.
2. **exec_async()**: Typically used for async LLM calls.
3. **post_async()**: For *awaiting user feedback*, *coordinating across multi-agents* or any additional async steps after `exec_async()`.

**Note**: `AsyncNode` must be wrapped in `AsyncFlow`. `AsyncFlow` can also include regular (sync) nodes.

### Example

```python
class SummarizeThenVerify(AsyncNode):
    async def prep_async(self, shared):
        # Example: read a file asynchronously
        doc_text = await read_file_async(shared["doc_path"])
        return doc_text

    async def exec_async(self, prep_res):
        # Example: async LLM call
        summary = await call_llm_async(f"Summarize: {prep_res}")
        return summary

    async def post_async(self, shared, prep_res, exec_res):
        # Example: wait for user feedback
        decision = await gather_user_feedback(exec_res)
        if decision == "approve":
            shared["summary"] = exec_res
            return "approve"
        return "deny"

summarize_node = SummarizeThenVerify()
final_node = Finalize()

# Define transitions
summarize_node - "approve" >> final_node
summarize_node - "deny"    >> summarize_node  # retry

flow = AsyncFlow(start=summarize_node)

async def main():
    shared = {"doc_path": "document.txt"}
    await flow.run_async(shared)
    print("Final Summary:", shared.get("summary"))

asyncio.run(main())
```
</file>

<file path=".cursor/rules/core_abstraction/batch.mdc">
---
description: Guidelines for using PocketFlow, Core Abstraction, Batch
globs: 
alwaysApply: false
---
# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:
- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.


### Example: Summarize a Large File

```python
class MapSummaries(BatchNode):
    def prep(self, shared):
        # Suppose we have a big file; chunk it
        content = shared["data"]
        chunk_size = 10000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        return chunks

    def exec(self, chunk):
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res_list):
        combined = "\n".join(exec_res_list)
        shared["summary"] = combined
        return "default"

map_summaries = MapSummaries()
flow = Flow(start=map_summaries)
flow.run(shared)
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Key Differences from BatchNode

**Important**: Unlike BatchNode, which processes items and modifies the shared store:

1. BatchFlow returns **parameters to pass to the child Flow**, not data to process
2. These parameters are accessed in child nodes via `self.params`, not from the shared store
3. Each child Flow runs independently with a different set of parameters
4. Child nodes can be regular Nodes, not BatchNodes (the batching happens at the Flow level)

### Example: Summarize Many Files

```python
class SummarizeAllFiles(BatchFlow):
    def prep(self, shared):
        # IMPORTANT: Return a list of param dictionaries (not data for processing)
        filenames = list(shared["data"].keys())  # e.g., ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

# Child node that accesses filename from params, not shared store
class LoadFile(Node):
    def prep(self, shared):
        # Access filename from params (not from shared)
        filename = self.params["filename"]  # Important! Use self.params, not shared
        return filename
        
    def exec(self, filename):
        with open(filename, 'r') as f:
            return f.read()
            
    def post(self, shared, prep_res, exec_res):
        # Store file content in shared
        shared["current_file_content"] = exec_res
        return "default"

# Summarize node that works on the currently loaded file
class Summarize(Node):
    def prep(self, shared):
        return shared["current_file_content"]
        
    def exec(self, content):
        prompt = f"Summarize this file in 50 words: {content}"
        return call_llm(prompt)
        
    def post(self, shared, prep_res, exec_res):
        # Store summary in shared, indexed by current filename
        filename = self.params["filename"]  # Again, using params
        if "summaries" not in shared:
            shared["summaries"] = {}
        shared["summaries"][filename] = exec_res
        return "default"

# Create a per-file flow
load_file = LoadFile()
summarize = Summarize()
load_file >> summarize
summarize_file = Flow(start=load_file)

# Wrap in a BatchFlow to process all files
summarize_all_files = SummarizeAllFiles(start=summarize_file)
summarize_all_files.run(shared)
```

### Under the Hood
1. `prep(shared)` in the BatchFlow returns a list of param dictsâ€”e.g., `[{"filename": "file1.txt"}, {"filename": "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlow's own `params` (if any): `{**batch_flow.params, **dict_from_prep}`
   - It calls `flow.run(shared)` using the merged parameters
   - **IMPORTANT**: These parameters are passed to the child Flow's nodes via `self.params`, NOT via the shared store
3. This means the sub-Flow is run **repeatedly**, once for every param dict, with each node in the flow accessing the parameters via `self.params`.

---

## 3. Nested or Multi-Level Batches

You can nest a **BatchFlow** in another **BatchFlow**. For instance:
- **Outer** batch: returns a list of directory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parentâ€™s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

```python

class FileBatchFlow(BatchFlow):
    def prep(self, shared):
        # Access directory from params (set by parent)
        directory = self.params["directory"]
        # e.g., files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(BatchFlow):
    def prep(self, shared):
        directories = [ "/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# The actual processing node
class ProcessFile(Node):
    def prep(self, shared):
        # Access both directory and filename from params
        directory = self.params["directory"]  # From outer batch
        filename = self.params["filename"]    # From inner batch
        full_path = os.path.join(directory, filename)
        return full_path
        
    def exec(self, full_path):
        # Process the file...
        return f"Processed {full_path}"
        
    def post(self, shared, prep_res, exec_res):
        # Store results, perhaps indexed by path
        if "results" not in shared:
            shared["results"] = {}
        shared["results"][prep_res] = exec_res
        return "default"

# Set up the nested batch structure
process_node = ProcessFile()
inner_flow = FileBatchFlow(start=process_node)
outer_flow = DirectoryBatchFlow(start=inner_flow)

# Run it
outer_flow.run(shared)
```
</file>

<file path=".cursor/rules/core_abstraction/communication.mdc">
---
description: Guidelines for using PocketFlow, Core Abstraction, Communication
globs: 
alwaysApply: false
---
# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)** 

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).  
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
     
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate *Data Schema* from *Compute Logic*!  This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](mdc:./batch.md).
     {: .best-practice }

2. **Params (only for [Batch](mdc:./batch.md))** 
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:
```python
shared = {"data": {}, "summary": {}, "config": {...}, ...}
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```python
class LoadData(Node):
    def post(self, shared, prep_res, exec_res):
        # We write data to shared store
        shared["data"] = "Some text content"
        return None

class Summarize(Node):
    def prep(self, shared):
        # We read data from shared store
        return shared["data"]

    def exec(self, prep_res):
        # Call LLM to summarize
        prompt = f"Summarize: {prep_res}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res):
        # We write summary to shared store
        shared["summary"] = exec_res
        return "default"

load_data = LoadData()
summarize = Summarize()
load_data >> summarize
flow = Flow(start=load_data)

shared = {}
flow.run(shared)
```

Here:
- `LoadData` writes to `shared["data"]`.
- `Summarize` reads from `shared["data"]`, summarizes, and writes to `shared["summary"]`.

---

## 2. Params

**Params** let you store *per-Node* or *per-Flow* config that doesn't need to live in the shared store. They are:
- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `set_params()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow. 
> 
> If you need to set child node params, see [Batch](mdc:./batch.md).
{: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```python
# 1) Create a Node that uses params
class SummarizeFile(Node):
    def prep(self, shared):
        # Access the node's param
        filename = self.params["filename"]
        return shared["data"].get(filename, "")

    def exec(self, prep_res):
        prompt = f"Summarize: {prep_res}"
        return call_llm(prompt)

    def post(self, shared, prep_res, exec_res):
        filename = self.params["filename"]
        shared["summary"][filename] = exec_res
        return "default"

# 2) Set params
node = SummarizeFile()

# 3) Set Node params directly (for testing)
node.set_params({"filename": "doc1.txt"})
node.run(shared)

# 4) Create Flow
flow = Flow(start=node)

# 5) Set Flow params (overwrites node params)
flow.set_params({"filename": "doc2.txt"})
flow.run(shared)  # The node summarizes doc2, not doc1
```
</file>

<file path=".cursor/rules/core_abstraction/flow.mdc">
---
description: Guidelines for using PocketFlow, Core Abstraction, Flow
globs: 
alwaysApply: false
---
# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `node_a >> node_b`
  This means if `node_a.post()` returns `"default"`, go to `node_b`. 
  (Equivalent to `node_a - "default" >> node_b`)

2. **Named action transition**: `node_a - "action_name" >> node_b`
  This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

It's possible to create loops, branching, or multi-step flows.

## 2. Creating a Flow

A **Flow** begins with a **start** node. You call `Flow(start=some_node)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```python
node_a >> node_b
flow = Flow(start=node_a)
flow.run(shared)
```

- When you run the flow, it executes `node_a`.  
- Suppose `node_a.post()` returns `"default"`.  
- The flow then sees `"default"` Action is linked to `node_b` and runs `node_b`.  
- `node_b.post()` returns `"default"` but we didn't define `node_b >> something_else`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision 
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```python
# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

flow = Flow(start=review)
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action. 
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
> 
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
{: .warning }

## 3. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.  
2. Combine multiple smaller Flows into a larger Flow for reuse.  
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `None` for `exec_res` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```python
# Create a sub-flow
node_a >> node_b
subflow = Flow(start=node_a)

# Connect it to another node
subflow >> node_c

# Create the parent flow
parent_flow = Flow(start=subflow)
```

When `parent_flow.run()` executes:
1. It starts `subflow`
2. `subflow` runs through its nodes (`node_a->node_b`)
3. After `subflow` completes, execution continues to `node_c`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```python
# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow

# Create the master flow
order_pipeline = Flow(start=payment_flow)

# Run the entire pipeline
order_pipeline.run(shared_data)
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```
</file>

<file path=".cursor/rules/core_abstraction/node.mdc">
---
description: Guidelines for using PocketFlow, Core Abstraction, Node
globs: 
alwaysApply: false
---
# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:



1. `prep(shared)`
   - **Read and preprocess data** from `shared` store. 
   - Examples: *query DB, read files, or serialize data into a string*.
   - Return `prep_res`, which is used by `exec()` and `post()`.

2. `exec(prep_res)`
   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: *(mostly) LLM calls, remote APIs, tool use*.
   - âš ï¸ This shall be only for compute and **NOT** access `shared`.
   - âš ï¸ If retries enabled, ensure idempotent implementation.
   - âš ï¸ Defer exception handling to the Node's built-in retry mechanism.
   - Return `exec_res`, which is passed to `post()`.

3. `post(shared, prep_res, exec_res)`
   - **Postprocess and write data** back to `shared`.
   - Examples: *update DB, change states, log results*.
   - **Decide the next action** by returning a *string* (`action = "default"` if *None*).

> **Why 3 steps?** To enforce the principle of *separation of concerns*. The data storage and data processing are operated separately.
>
> All steps are *optional*. E.g., you can only implement `prep` and `post` if you just need to process data.
{: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting). 
`wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```python 
my_node = SummarizeFile(max_retries=3, wait=10)
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `max_retries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `self.cur_retry`.

```python 
class RetryNode(Node):
    def exec(self, prep_res):
        print(f"Retry {self.cur_retry} times")
        raise Exception("Failed")
```

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```python 
def exec_fallback(self, prep_res, exc):
    raise exc
```

By default, it just re-raises exception. But you can return a fallback result instead, which becomes the `exec_res` passed to `post()`.

### Example: Summarize file

```python 
class SummarizeFile(Node):
    def prep(self, shared):
        return shared["data"]

    def exec(self, prep_res):
        if not prep_res:
            return "Empty file content"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    def exec_fallback(self, prep_res, exc):
        # Provide a simple fallback instead of crashing
        return "There was an error processing your request."

    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # Return "default" by not returning

summarize_node = SummarizeFile(max_retries=3)

# node.run() calls prep->exec->post
# If exec() fails, it retries up to 3 times before calling exec_fallback()
action_result = summarize_node.run(shared)

print("Action returned:", action_result)  # "default"
print("Summary stored:", shared["summary"])
```
</file>

<file path=".cursor/rules/core_abstraction/parallel.mdc">
---
description: Guidelines for using PocketFlow, Core Abstraction, (Advanced) Parallel
globs: 
alwaysApply: false
---
# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple **Async** Nodes and Flows  **concurrently**â€”for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute. 

> Because of Pythonâ€™s GIL, parallel nodes and flows canâ€™t truly parallelize CPU-bound tasks (e.g., heavy numerical computations). However, they excel at overlapping I/O-bound workâ€”like LLM calls, database queries, API requests, or file I/O.
{: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
> 
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism (e.g., semaphores or sleep intervals).
> 
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
{: .best-practice }

## AsyncParallelBatchNode

Like **AsyncBatchNode**, but run `exec_async()` in **parallel**:

```python
class ParallelSummaries(AsyncParallelBatchNode):
    async def prep_async(self, shared):
        # e.g., multiple texts
        return shared["texts"]

    async def exec_async(self, text):
        prompt = f"Summarize: {text}"
        return await call_llm_async(prompt)

    async def post_async(self, shared, prep_res, exec_res_list):
        shared["summary"] = "\n\n".join(exec_res_list)
        return "default"

node = ParallelSummaries()
flow = AsyncFlow(start=node)
```

## AsyncParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using different parameters:

```python
class SummarizeMultipleFiles(AsyncParallelBatchFlow):
    async def prep_async(self, shared):
        return [{"filename": f} for f in shared["files"]]

sub_flow = AsyncFlow(start=LoadAndSummarizeFile())
parallel_flow = SummarizeMultipleFiles(start=sub_flow)
await parallel_flow.run_async(shared)
```
</file>

<file path=".cursor/rules/design_pattern/agent.mdc">
---
description: Guidelines for using PocketFlow, Design Pattern, Agent
globs: 
alwaysApply: false
---
# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.



## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.  
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodesâ€”and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide actionâ€”for example:

```python
f"""
### CONTEXT
Task: {task_description}
Previous Actions: {previous_actions}
Current State: {current_state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Conclude based on the results
  Parameters:
    - result (str): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

```yaml
thinking: |
    
action: 
parameters:
    : 
```"""
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide *relevant, minimal context.* For example, rather than including an entire chat history, retrieve the most relevant via [RAG](mdc:./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide *a well-structured and unambiguous* set of actionsâ€”avoiding overlap like separate `read_databases` or  `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:
1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

```python
class DecideAction(Node):
    def prep(self, shared):
        context = shared.get("context", "No previous search")
        query = shared["query"]
        return query, context
        
    def exec(self, inputs):
        query, context = inputs
        prompt = f"""
Given input: {query}
Previous search results: {context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
```yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
```"""
        resp = call_llm(prompt)
        yaml_str = resp.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        assert isinstance(result, dict)
        assert "action" in result
        assert "reason" in result
        assert result["action"] in ["search", "answer"]
        if result["action"] == "search":
            assert "search_term" in result
        
        return result

    def post(self, shared, prep_res, exec_res):
        if exec_res["action"] == "search":
            shared["search_term"] = exec_res["search_term"]
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        return shared["search_term"]
        
    def exec(self, search_term):
        return search_web(search_term)
    
    def post(self, shared, prep_res, exec_res):
        prev_searches = shared.get("context", [])
        shared["context"] = prev_searches + [
            {"term": shared["search_term"], "result": exec_res}
        ]
        return "decide"
        
class DirectAnswer(Node):
    def prep(self, shared):
        return shared["query"], shared.get("context", "")
        
    def exec(self, inputs):
        query, context = inputs
        return call_llm(f"Context: {context}\nAnswer: {query}")

    def post(self, shared, prep_res, exec_res):
       print(f"Answer: {exec_res}")
       shared["answer"] = exec_res

# Connect nodes
decide = DecideAction()
search = SearchWeb()
answer = DirectAnswer()

decide - "search" >> search
decide - "answer" >> answer
search - "decide" >> decide  # Loop back

flow = Flow(start=decide)
flow.run({"query": "Who won the Nobel Prize in Physics 2024?"})
```
</file>

<file path=".cursor/rules/design_pattern/mapreduce.mdc">
---
description: Guidelines for using PocketFlow, Design Pattern, Map Reduce
globs: 
alwaysApply: false
---
# Map Reduce

MapReduce is a design pattern suitable when you have either:
- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts. 



You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```python
class SummarizeAllFiles(BatchNode):
    def prep(self, shared):
        files_dict = shared["files"]  # e.g. 10 files
        return list(files_dict.items())  # [("file1.txt", "aaa..."), ("file2.txt", "bbb..."), ...]

    def exec(self, one_file):
        filename, file_content = one_file
        summary_text = call_llm(f"Summarize the following file:\n{file_content}")
        return (filename, summary_text)

    def post(self, shared, prep_res, exec_res_list):
        shared["file_summaries"] = dict(exec_res_list)

class CombineSummaries(Node):
    def prep(self, shared):
        return shared["file_summaries"]

    def exec(self, file_summaries):
        # format as: "File1: summary\nFile2: summary...\n"
        text_list = []
        for fname, summ in file_summaries.items():
            text_list.append(f"{fname} summary:\n{summ}\n")
        big_text = "\n---\n".join(text_list)

        return call_llm(f"Combine these file summaries into one final summary:\n{big_text}")

    def post(self, shared, prep_res, final_summary):
        shared["all_files_summary"] = final_summary

batch_node = SummarizeAllFiles()
combine_node = CombineSummaries()
batch_node >> combine_node

flow = Flow(start=batch_node)

shared = {
    "files": {
        "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
        "file2.txt": "Some other interesting text ...",
        # ...
    }
}
flow.run(shared)
print("Individual Summaries:", shared["file_summaries"])
print("\nFinal Summary:\n", shared["all_files_summary"])
```

> **Performance Tip**: The example above works sequentially. You can speed up the map phase by running it in parallel. See [(Advanced) Parallel](../core_abstraction/parallel.md) for more details.
{: .note }
</file>

<file path=".cursor/rules/design_pattern/multi_agent.mdc">
---
description: Guidelines for using PocketFlow, Design Pattern, (Advanced) Multi-Agents
globs: 
alwaysApply: false
---
# (Advanced) Multi-Agents

Multiple [Agents](mdc:./flow.md) can work together by handling subtasks and communicating the progress. 
Communication between agents is typically implemented using message queues in shared storage.

> Most of time, you don't need Multi-Agents. Start with a simple solution first.
{: .best-practice }

### Example Agent Communication: Message Queue

Here's a simple example showing how to implement agent communication using `asyncio.Queue`. 
The agent listens for messages, processes them, and continues listening:

```python
class AgentNode(AsyncNode):
    async def prep_async(self, _):
        message_queue = self.params["messages"]
        message = await message_queue.get()
        print(f"Agent received: {message}")
        return message

# Create node and flow
agent = AgentNode()
agent >> agent  # connect to self
flow = AsyncFlow(start=agent)

# Create heartbeat sender
async def send_system_messages(message_queue):
    counter = 0
    messages = [
        "System status: all systems operational",
        "Memory usage: normal",
        "Network connectivity: stable",
        "Processing load: optimal"
    ]
    
    while True:
        message = f"{messages[counter % len(messages)]} | timestamp_{counter}"
        await message_queue.put(message)
        counter += 1
        await asyncio.sleep(1)

async def main():
    message_queue = asyncio.Queue()
    shared = {}
    flow.set_params({"messages": message_queue})
    
    # Run both coroutines
    await asyncio.gather(
        flow.run_async(shared),
        send_system_messages(message_queue)
    )
    
asyncio.run(main())
```

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
```

### Interactive Multi-Agent Example: Taboo Game

Here's a more complex example where two agents play the word-guessing game Taboo. 
One agent provides hints while avoiding forbidden words, and another agent tries to guess the target word:

```python
class AsyncHinter(AsyncNode):
    async def prep_async(self, shared):
        guess = await shared["hinter_queue"].get()
        if guess == "GAME_OVER":
            return None
        return shared["target_word"], shared["forbidden_words"], shared.get("past_guesses", [])

    async def exec_async(self, inputs):
        if inputs is None:
            return None
        target, forbidden, past_guesses = inputs
        prompt = f"Generate hint for '{target}'\nForbidden words: {forbidden}"
        if past_guesses:
            prompt += f"\nPrevious wrong guesses: {past_guesses}\nMake hint more specific."
        prompt += "\nUse at most 5 words."
        
        hint = call_llm(prompt)
        print(f"\nHinter: Here's your hint - {hint}")
        return hint

    async def post_async(self, shared, prep_res, exec_res):
        if exec_res is None:
            return "end"
        await shared["guesser_queue"].put(exec_res)
        return "continue"

class AsyncGuesser(AsyncNode):
    async def prep_async(self, shared):
        hint = await shared["guesser_queue"].get()
        return hint, shared.get("past_guesses", [])

    async def exec_async(self, inputs):
        hint, past_guesses = inputs
        prompt = f"Given hint: {hint}, past wrong guesses: {past_guesses}, make a new guess. Directly reply a single word:"
        guess = call_llm(prompt)
        print(f"Guesser: I guess it's - {guess}")
        return guess

    async def post_async(self, shared, prep_res, exec_res):
        if exec_res.lower() == shared["target_word"].lower():
            print("Game Over - Correct guess!")
            await shared["hinter_queue"].put("GAME_OVER")
            return "end"
            
        if "past_guesses" not in shared:
            shared["past_guesses"] = []
        shared["past_guesses"].append(exec_res)
        
        await shared["hinter_queue"].put(exec_res)
        return "continue"

async def main():
    # Set up game
    shared = {
        "target_word": "nostalgia",
        "forbidden_words": ["memory", "past", "remember", "feeling", "longing"],
        "hinter_queue": asyncio.Queue(),
        "guesser_queue": asyncio.Queue()
    }
    
    print("Game starting!")
    print(f"Target word: {shared['target_word']}")
    print(f"Forbidden words: {shared['forbidden_words']}")

    # Initialize by sending empty guess to hinter
    await shared["hinter_queue"].put("")

    # Create nodes and flows
    hinter = AsyncHinter()
    guesser = AsyncGuesser()

    # Set up flows
    hinter_flow = AsyncFlow(start=hinter)
    guesser_flow = AsyncFlow(start=guesser)

    # Connect nodes to themselves
    hinter - "continue" >> hinter
    guesser - "continue" >> guesser

    # Run both agents concurrently
    await asyncio.gather(
        hinter_flow.run_async(shared),
        guesser_flow.run_async(shared)
    )

asyncio.run(main())
```

The Output:

```
Game starting!
Target word: nostalgia
Forbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']

Hinter: Here's your hint - Thinking of childhood summer days
Guesser: I guess it's - popsicle

Hinter: Here's your hint - When childhood cartoons make you emotional
Guesser: I guess it's - nostalgic

Hinter: Here's your hint - When old songs move you
Guesser: I guess it's - memories

Hinter: Here's your hint - That warm emotion about childhood
Guesser: I guess it's - nostalgia
Game Over - Correct guess!
```
</file>

<file path=".cursor/rules/design_pattern/rag.mdc">
---
description: Guidelines for using PocketFlow, Design Pattern, RAG
globs: 
alwaysApply: false
---
# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:



1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---
## Stage 1: Offline Indexing

We create three Nodes:
1. `ChunkDocs` â€“ [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` â€“ [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` â€“ stores embeddings into a [vector database](../utility_function/vector.md).

```python
class ChunkDocs(BatchNode):
    def prep(self, shared):
        # A list of file paths in shared["files"]. We process each file.
        return shared["files"]

    def exec(self, filepath):
        # read file content. In real usage, do error handling.
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        # chunk by 100 chars each
        chunks = []
        size = 100
        for i in range(0, len(text), size):
            chunks.append(text[i : i + size])
        return chunks
    
    def post(self, shared, prep_res, exec_res_list):
        # exec_res_list is a list of chunk-lists, one per file.
        # flatten them all into a single list of chunks.
        all_chunks = []
        for chunk_list in exec_res_list:
            all_chunks.extend(chunk_list)
        shared["all_chunks"] = all_chunks

class EmbedDocs(BatchNode):
    def prep(self, shared):
        return shared["all_chunks"]

    def exec(self, chunk):
        return get_embedding(chunk)

    def post(self, shared, prep_res, exec_res_list):
        # Store the list of embeddings.
        shared["all_embeds"] = exec_res_list
        print(f"Total embeddings: {len(exec_res_list)}")

class StoreIndex(Node):
    def prep(self, shared):
        # We'll read all embeds from shared.
        return shared["all_embeds"]

    def exec(self, all_embeds):
        # Create a vector index (faiss or other DB in real usage).
        index = create_index(all_embeds)
        return index

    def post(self, shared, prep_res, index):
        shared["index"] = index

# Wire them in sequence
chunk_node = ChunkDocs()
embed_node = EmbedDocs()
store_node = StoreIndex()

chunk_node >> embed_node >> store_node

OfflineFlow = Flow(start=chunk_node)
```

Usage example:

```python
shared = {
    "files": ["doc1.txt", "doc2.txt"],  # any text files
}
OfflineFlow.run(shared)
```

---
## Stage 2: Online Query & Answer

We have 3 nodes:
1. `EmbedQuery` â€“ embeds the userâ€™s question.
2. `RetrieveDocs` â€“ retrieves top chunk from the index.
3. `GenerateAnswer` â€“ calls the LLM with the question + chunk to produce the final answer.

```python
class EmbedQuery(Node):
    def prep(self, shared):
        return shared["question"]

    def exec(self, question):
        return get_embedding(question)

    def post(self, shared, prep_res, q_emb):
        shared["q_emb"] = q_emb

class RetrieveDocs(Node):
    def prep(self, shared):
        # We'll need the query embedding, plus the offline index/chunks
        return shared["q_emb"], shared["index"], shared["all_chunks"]

    def exec(self, inputs):
        q_emb, index, chunks = inputs
        I, D = search_index(index, q_emb, top_k=1)
        best_id = I[0][0]
        relevant_chunk = chunks[best_id]
        return relevant_chunk

    def post(self, shared, prep_res, relevant_chunk):
        shared["retrieved_chunk"] = relevant_chunk
        print("Retrieved chunk:", relevant_chunk[:60], "...")

class GenerateAnswer(Node):
    def prep(self, shared):
        return shared["question"], shared["retrieved_chunk"]

    def exec(self, inputs):
        question, chunk = inputs
        prompt = f"Question: {question}\nContext: {chunk}\nAnswer:"
        return call_llm(prompt)

    def post(self, shared, prep_res, answer):
        shared["answer"] = answer
        print("Answer:", answer)

embed_qnode = EmbedQuery()
retrieve_node = RetrieveDocs()
generate_node = GenerateAnswer()

embed_qnode >> retrieve_node >> generate_node
OnlineFlow = Flow(start=embed_qnode)
```

Usage example:

```python
# Suppose we already ran OfflineFlow and have:
# shared["all_chunks"], shared["index"], etc.
shared["question"] = "Why do people like cats?"

OnlineFlow.run(shared)
# final answer in shared["answer"]
```
</file>

<file path=".cursor/rules/design_pattern/structure.mdc">
---
description: Guidelines for using PocketFlow, Design Pattern, Structured Output
globs: 
alwaysApply: false
---
# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:
- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information 

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:
1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

```python
class SummarizeNode(Node):
    def exec(self, prep_res):
        # Suppose `prep_res` is the text to summarize.
        prompt = f"""
Please summarize the following text as YAML, with exactly 3 bullet points

{prep_res}

Now, output:
```yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()

        import yaml
        structured_result = yaml.safe_load(yaml_str)

        assert "summary" in structured_result
        assert isinstance(structured_result["summary"], list)

        return structured_result
```

> Besides using `assert` statements, another popular way to validate schemas is [Pydantic](https://github.com/pydantic/pydantic)
{: .note }

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**  

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**  

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotesâ€”just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.
</file>

<file path=".cursor/rules/design_pattern/workflow.mdc">
---
description: Guidelines for using PocketFlow, Design Pattern, Workflow
globs: 
alwaysApply: false
---
# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.



> - You don't want to make each task **too coarse**, because it may be *too complex for one LLM call*.
> - You don't want to make each task **too granular**, because then *the LLM call doesn't have enough context* and results are *not consistent across nodes*.
> 
> You usually need multiple *iterations* to find the *sweet spot*. If the task has too many *edge cases*, consider using [Agents](mdc:./agent.md).
{: .best-practice }

### Example: Article Writing

```python
class GenerateOutline(Node):
    def prep(self, shared): return shared["topic"]
    def exec(self, topic): return call_llm(f"Create a detailed outline for an article about {topic}")
    def post(self, shared, prep_res, exec_res): shared["outline"] = exec_res

class WriteSection(Node):
    def prep(self, shared): return shared["outline"]
    def exec(self, outline): return call_llm(f"Write content based on this outline: {outline}")
    def post(self, shared, prep_res, exec_res): shared["draft"] = exec_res

class ReviewAndRefine(Node):
    def prep(self, shared): return shared["draft"]
    def exec(self, draft): return call_llm(f"Review and improve this draft: {draft}")
    def post(self, shared, prep_res, exec_res): shared["final_article"] = exec_res

# Connect nodes
outline = GenerateOutline()
write = WriteSection()
review = ReviewAndRefine()

outline >> write >> review

# Create and run flow
writing_flow = Flow(start=outline)
shared = {"topic": "AI Safety"}
writing_flow.run(shared)
```

For *dynamic cases*, consider using [Agents](mdc:./agent.md).
</file>

<file path=".cursor/rules/utility_function/chunking.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, Text Chunking
globs: 
alwaysApply: false
---
# Text Chunking

We recommend some implementations of commonly used text chunking approaches.


> Text Chunking is more a micro optimization, compared to the Flow Design.
> 
> It's recommended to start with the Naive Chunking and optimize later.
{: .best-practice }

---

## Example Python Code Samples

### 1. Naive (Fixed-Size) Chunking
Splits text by a fixed number of words, ignoring sentence or semantic boundaries.

```python
def fixed_size_chunk(text, chunk_size=100):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i : i + chunk_size])
    return chunks
```

However, sentences are often cut awkwardly, losing coherence.

### 2. Sentence-Based Chunking

```python
import nltk

def sentence_based_chunk(text, max_sentences=2):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    for i in range(0, len(sentences), max_sentences):
        chunks.append(" ".join(sentences[i : i + max_sentences]))
    return chunks
```

However, might not handle very long sentences or paragraphs well.

### 3. Other Chunking

- **Paragraph-Based**: Split text by paragraphs (e.g., newlines). Large paragraphs can create big chunks.
- **Semantic**: Use embeddings or topic modeling to chunk by semantic boundaries.
- **Agentic**: Use an LLM to decide chunk boundaries based on context or meaning.
</file>

<file path=".cursor/rules/utility_function/embedding.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, Embedding
globs: 
alwaysApply: false
---
# Embedding

Below you will find an overview table of various text embedding APIs, along with example Python code.

>  Embedding is more a micro optimization, compared to the Flow Design.
> 
> It's recommended to start with the most convenient one and optimize later.
{: .best-practice }


| **API** | **Free Tier** | **Pricing Model** | **Docs** |
| --- | --- | --- | --- |
| **OpenAI** | ~$5 credit | ~$0.0001/1K tokens | [OpenAI Embeddings](https://platform.openai.com/docs/api-reference/embeddings) |
| **Azure OpenAI** | $200 credit | Same as OpenAI (~$0.0001/1K tokens) | [Azure OpenAI Embeddings](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?tabs=portal) |
| **Google Vertex AI** | $300 credit | ~$0.025 / million chars | [Vertex AI Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) |
| **AWS Bedrock** | No free tier, but AWS credits may apply | ~$0.00002/1K tokens (Titan V2) | [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/) |
| **Cohere** | Limited free tier | ~$0.0001/1K tokens | [Cohere Embeddings](https://docs.cohere.com/docs/cohere-embed) |
| **Hugging Face** | ~$0.10 free compute monthly | Pay per second of compute | [HF Inference API](https://huggingface.co/docs/api-inference) |
| **Jina** | 1M tokens free | Pay per token after | [Jina Embeddings](https://jina.ai/embeddings/) |

## Example Python Code

### 1. OpenAI
```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")
response = client.embeddings.create(
    model="text-embedding-ada-002",
    input=text
)
    
# Extract the embedding vector from the response
embedding = response.data[0].embedding
embedding = np.array(embedding, dtype=np.float32)
print(embedding)
```

### 2. Azure OpenAI
```python
import openai

openai.api_type = "azure"
openai.api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com"
openai.api_version = "2023-03-15-preview"
openai.api_key = "YOUR_AZURE_API_KEY"

resp = openai.Embedding.create(engine="ada-embedding", input="Hello world")
vec = resp["data"][0]["embedding"]
print(vec)
```

### 3. Google Vertex AI
```python
from vertexai.preview.language_models import TextEmbeddingModel
import vertexai

vertexai.init(project="YOUR_GCP_PROJECT_ID", location="us-central1")
model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")

emb = model.get_embeddings(["Hello world"])
print(emb[0])
```

### 4. AWS Bedrock
```python
import boto3, json

client = boto3.client("bedrock-runtime", region_name="us-east-1")
body = {"inputText": "Hello world"}
resp = client.invoke_model(modelId="amazon.titan-embed-text-v2:0", contentType="application/json", body=json.dumps(body))
resp_body = json.loads(resp["body"].read())
vec = resp_body["embedding"]
print(vec)
```

### 5. Cohere
```python
import cohere

co = cohere.Client("YOUR_API_KEY")
resp = co.embed(texts=["Hello world"])
vec = resp.embeddings[0]
print(vec)
```

### 6. Hugging Face
```python
import requests

API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
HEADERS = {"Authorization": "Bearer YOUR_HF_TOKEN"}

res = requests.post(API_URL, headers=HEADERS, json={"inputs": "Hello world"})
vec = res.json()[0]
print(vec)
```

### 7. Jina
```python
import requests

url = "https://api.jina.ai/v2/embed"
headers = {"Authorization": "Bearer YOUR_JINA_TOKEN"}
payload = {"data": ["Hello world"], "model": "jina-embeddings-v3"}
res = requests.post(url, headers=headers, json=payload)
vec = res.json()["data"][0]["embedding"]
print(vec)
```
</file>

<file path=".cursor/rules/utility_function/llm.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, LLM Wrapper
globs: 
alwaysApply: false
---
# LLM Wrappers

Check out libraries like [litellm](https://github.com/BerriAI/litellm). 
Here, we provide some minimal example implementations:

1. OpenAI
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_API_KEY_HERE")
        r = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content

    # Example usage
    call_llm("How are you?")
    ```
    > Store the API key in an environment variable like OPENAI_API_KEY for security.
    {: .best-practice }

2. Claude (Anthropic)
    ```python
    def call_llm(prompt):
        from anthropic import Anthropic
        client = Anthropic(api_key="YOUR_API_KEY_HERE")
        r = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=3000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return r.content[0].text
    ```

3. Google (Generative AI Studio / PaLM API)
    ```python
    def call_llm(prompt):
    from google import genai
    client = genai.Client(api_key='GEMINI_API_KEY')
        response = client.models.generate_content(
        model='gemini-2.0-flash-001',
        contents=prompt
    )
    return response.text
    ```

4. Azure (Azure OpenAI)
    ```python
    def call_llm(prompt):
        from openai import AzureOpenAI
        client = AzureOpenAI(
            azure_endpoint="https://.openai.azure.com/",
            api_key="YOUR_API_KEY_HERE",
            api_version="2023-05-15"
        )
        r = client.chat.completions.create(
            model="",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```

5. Ollama (Local LLM)
    ```python
    def call_llm(prompt):
        from ollama import chat
        response = chat(
            model="llama2",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.message.content
    ```
    
6. DeepSeek
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_DEEPSEEK_API_KEY", base_url="https://api.deepseek.com")
        r = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```


## Improvements
Feel free to enhance your `call_llm` function as needed. Here are examples:

- Handle chat history:

```python
def call_llm(messages):
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return r.choices[0].message.content
```

- Add in-memory caching 

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def call_llm(prompt):
    # Your implementation here
    pass
```

> âš ï¸ Caching conflicts with Node retries, as retries yield the same result.
>
> To address this, you could use cached results only if not retried.
{: .warning }


```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_call(prompt):
    pass

def call_llm(prompt, use_cache):
    if use_cache:
        return cached_call(prompt)
    # Call the underlying function directly
    return cached_call.__wrapped__(prompt)

class SummarizeNode(Node):
    def exec(self, text):
        return call_llm(f"Summarize: {text}", self.cur_retry==0)
```

- Enable logging:

```python
def call_llm(prompt):
    import logging
    logging.info(f"Prompt: {prompt}")
    response = ... # Your implementation here
    logging.info(f"Response: {response}")
    return response
```
</file>

<file path=".cursor/rules/utility_function/text_to_speech.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, Text-to-Speech
globs: 
alwaysApply: false
---
# Text-to-Speech

| **Service**          | **Free Tier**         | **Pricing Model**                                            | **Docs**                                                            |
|----------------------|-----------------------|--------------------------------------------------------------|---------------------------------------------------------------------|
| **Amazon Polly**     | 5M std + 1M neural   | ~$4 /M (std), ~$16 /M (neural) after free tier               | [Polly Docs](https://aws.amazon.com/polly/)                         |
| **Google Cloud TTS** | 4M std + 1M WaveNet  | ~$4 /M (std), ~$16 /M (WaveNet) pay-as-you-go                | [Cloud TTS Docs](https://cloud.google.com/text-to-speech)           |
| **Azure TTS**        | 500K neural ongoing  | ~$15 /M (neural), discount at higher volumes                 | [Azure TTS Docs](https://azure.microsoft.com/products/cognitive-services/text-to-speech/) |
| **IBM Watson TTS**   | 10K chars Lite plan  | ~$0.02 /1K (i.e. ~$20 /M). Enterprise options available       | [IBM Watson Docs](https://www.ibm.com/cloud/watson-text-to-speech)   |
| **ElevenLabs**       | 10K chars monthly    | From ~$5/mo (30K chars) up to $330/mo (2M chars). Enterprise  | [ElevenLabs Docs](https://elevenlabs.io)                            |

## Example Python Code

### Amazon Polly
```python
import boto3

polly = boto3.client("polly", region_name="us-east-1",
                     aws_access_key_id="YOUR_AWS_ACCESS_KEY_ID",
                     aws_secret_access_key="YOUR_AWS_SECRET_ACCESS_KEY")

resp = polly.synthesize_speech(
    Text="Hello from Polly!",
    OutputFormat="mp3",
    VoiceId="Joanna"
)

with open("polly.mp3", "wb") as f:
    f.write(resp["AudioStream"].read())
```

### Google Cloud TTS
```python
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()
input_text = texttospeech.SynthesisInput(text="Hello from Google Cloud TTS!")
voice = texttospeech.VoiceSelectionParams(language_code="en-US")
audio_cfg = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

resp = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_cfg)

with open("gcloud_tts.mp3", "wb") as f:
    f.write(resp.audio_content)
```

### Azure TTS
```python
import azure.cognitiveservices.speech as speechsdk

speech_config = speechsdk.SpeechConfig(
    subscription="AZURE_KEY", region="AZURE_REGION")
audio_cfg = speechsdk.audio.AudioConfig(filename="azure_tts.wav")

synthesizer = speechsdk.SpeechSynthesizer(
    speech_config=speech_config,
    audio_config=audio_cfg
)

synthesizer.speak_text_async("Hello from Azure TTS!").get()
```

### IBM Watson TTS
```python
from ibm_watson import TextToSpeechV1
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator

auth = IAMAuthenticator("IBM_API_KEY")
service = TextToSpeechV1(authenticator=auth)
service.set_service_url("IBM_SERVICE_URL")

resp = service.synthesize(
    "Hello from IBM Watson!",
    voice="en-US_AllisonV3Voice",
    accept="audio/mp3"
).get_result()

with open("ibm_tts.mp3", "wb") as f:
    f.write(resp.content)
```

### ElevenLabs
```python
import requests

api_key = "ELEVENLABS_KEY"
voice_id = "ELEVENLABS_VOICE"
url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
headers = {"xi-api-key": api_key, "Content-Type": "application/json"}

json_data = {
    "text": "Hello from ElevenLabs!",
    "voice_settings": {"stability": 0.75, "similarity_boost": 0.75}
}

resp = requests.post(url, headers=headers, json=json_data)

with open("elevenlabs.mp3", "wb") as f:
    f.write(resp.content)
```
</file>

<file path=".cursor/rules/utility_function/vector.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, Vector Databases
globs: 
alwaysApply: false
---
# Vector Databases


Below is a  table of the popular vector search solutions:

| **Tool** | **Free Tier** | **Pricing Model** | **Docs** |
| --- | --- | --- | --- |
| **FAISS** | N/A, self-host | Open-source | [Faiss.ai](https://faiss.ai) |
| **Pinecone** | 2GB free | From $25/mo | [pinecone.io](https://pinecone.io) |
| **Qdrant** | 1GB free cloud | Pay-as-you-go | [qdrant.tech](https://qdrant.tech) |
| **Weaviate** | 14-day sandbox | From $25/mo | [weaviate.io](https://weaviate.io) |
| **Milvus** | 5GB free cloud | PAYG or $99/mo dedicated | [milvus.io](https://milvus.io) |
| **Chroma** | N/A, self-host | Free (Apache 2.0) | [trychroma.com](https://trychroma.com) |
| **Redis** | 30MB free | From $5/mo | [redis.io](https://redis.io) |

---
## Example Python Code

Below are basic usage snippets for each tool.

### FAISS
```python
import faiss
import numpy as np

# Dimensionality of embeddings
d = 128

# Create a flat L2 index
index = faiss.IndexFlatL2(d)

# Random vectors
data = np.random.random((1000, d)).astype('float32')
index.add(data)

# Query
query = np.random.random((1, d)).astype('float32')
D, I = index.search(query, k=5)

print("Distances:", D)
print("Neighbors:", I)
```

### Pinecone
```python
import pinecone

pinecone.init(api_key="YOUR_API_KEY", environment="YOUR_ENV")

index_name = "my-index"

# Create the index if it doesn't exist
if index_name not in pinecone.list_indexes():
    pinecone.create_index(name=index_name, dimension=128)

# Connect
index = pinecone.Index(index_name)

# Upsert
vectors = [
    ("id1", [0.1]*128),
    ("id2", [0.2]*128)
]
index.upsert(vectors)

# Query
response = index.query([[0.15]*128], top_k=3)
print(response)
```

### Qdrant
```python
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct

client = qdrant_client.QdrantClient(
    url="https://YOUR-QDRANT-CLOUD-ENDPOINT",
    api_key="YOUR_API_KEY"
)

collection = "my_collection"
client.recreate_collection(
    collection_name=collection,
    vectors_config=VectorParams(size=128, distance=Distance.COSINE)
)

points = [
    PointStruct(id=1, vector=[0.1]*128, payload={"type": "doc1"}),
    PointStruct(id=2, vector=[0.2]*128, payload={"type": "doc2"}),
]

client.upsert(collection_name=collection, points=points)

results = client.search(
    collection_name=collection,
    query_vector=[0.15]*128,
    limit=2
)
print(results)
```

### Weaviate
```python
import weaviate

client = weaviate.Client("https://YOUR-WEAVIATE-CLOUD-ENDPOINT")

schema = {
    "classes": [
        {
            "class": "Article",
            "vectorizer": "none"
        }
    ]
}
client.schema.create(schema)

obj = {
    "title": "Hello World",
    "content": "Weaviate vector search"
}
client.data_object.create(obj, "Article", vector=[0.1]*128)

resp = (
    client.query
    .get("Article", ["title", "content"])
    .with_near_vector({"vector": [0.15]*128})
    .with_limit(3)
    .do()
)
print(resp)
```

### Milvus
```python
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection
import numpy as np

connections.connect(alias="default", host="localhost", port="19530")

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
]
schema = CollectionSchema(fields)
collection = Collection("MyCollection", schema)

emb = np.random.rand(10, 128).astype('float32')
ids = list(range(10))
collection.insert([ids, emb])

index_params = {
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128},
    "metric_type": "L2"
}
collection.create_index("embedding", index_params)
collection.load()

query_emb = np.random.rand(1, 128).astype('float32')
results = collection.search(query_emb, "embedding", param={"nprobe": 10}, limit=3)
print(results)
```

### Chroma
```python
import chromadb
from chromadb.config import Settings

client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_data"
))

coll = client.create_collection("my_collection")

vectors = [[0.1, 0.2, 0.3], [0.2, 0.2, 0.2]]
metas = [{"doc": "text1"}, {"doc": "text2"}]
ids = ["id1", "id2"]
coll.add(embeddings=vectors, metadatas=metas, ids=ids)

res = coll.query(query_embeddings=[[0.15, 0.25, 0.3]], n_results=2)
print(res)
```

### Redis
```python
import redis
import struct

r = redis.Redis(host="localhost", port=6379)

# Create index
r.execute_command(
    "FT.CREATE", "my_idx", "ON", "HASH",
    "SCHEMA", "embedding", "VECTOR", "FLAT", "6",
    "TYPE", "FLOAT32", "DIM", "128",
    "DISTANCE_METRIC", "L2"
)

# Insert
vec = struct.pack('128f', *[0.1]*128)
r.hset("doc1", mapping={"embedding": vec})

# Search
qvec = struct.pack('128f', *[0.15]*128)
q = "*=>[KNN 3 @embedding $BLOB AS dist]"
res = r.ft("my_idx").search(q, query_params={"BLOB": qvec})
print(res.docs)
```
</file>

<file path=".cursor/rules/utility_function/viz.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, Viz and Debug
globs: 
alwaysApply: false
---
# Visualization and Debugging

Similar to LLM wrappers, we **don't** provide built-in visualization and debugging. Here, we recommend some *minimal* (and incomplete) implementations These examples can serve as a starting point for your own tooling.

## 1. Visualization with Mermaid

This code recursively traverses the nested graph, assigns unique IDs to each node, and treats Flow nodes as subgraphs to generate Mermaid syntax for a hierarchical visualization.

{% raw %}
```python
def build_mermaid(start):
    ids, visited, lines = {}, set(), ["graph LR"]
    ctr = 1
    def get_id(n):
        nonlocal ctr
        return ids[n] if n in ids else (ids.setdefault(n, f"N{ctr}"), (ctr := ctr + 1))[0]
    def link(a, b):
        lines.append(f"    {a} --> {b}")
    def walk(node, parent=None):
        if node in visited:
            return parent and link(parent, get_id(node))
        visited.add(node)
        if isinstance(node, Flow):
            node.start_node and parent and link(parent, get_id(node.start_node))
            lines.append(f"\n    subgraph sub_flow_{get_id(node)}[{type(node).__name__}]")
            node.start_node and walk(node.start_node)
            for nxt in node.successors.values():
                node.start_node and walk(nxt, get_id(node.start_node)) or (parent and link(parent, get_id(nxt))) or walk(nxt)
            lines.append("    end\n")
        else:
            lines.append(f"    {(nid := get_id(node))}['{type(node).__name__}']")
            parent and link(parent, nid)
            [walk(nxt, nid) for nxt in node.successors.values()]
    walk(start)
    return "\n".join(lines)
```
{% endraw %}


For example, suppose we have a complex Flow for data science:

```python
class DataPrepBatchNode(BatchNode):
    def prep(self,shared): return []
class ValidateDataNode(Node): pass
class FeatureExtractionNode(Node): pass
class TrainModelNode(Node): pass
class EvaluateModelNode(Node): pass
class ModelFlow(Flow): pass
class DataScienceFlow(Flow):pass

feature_node = FeatureExtractionNode()
train_node = TrainModelNode()
evaluate_node = EvaluateModelNode()
feature_node >> train_node >> evaluate_node
model_flow = ModelFlow(start=feature_node)
data_prep_node = DataPrepBatchNode()
validate_node = ValidateDataNode()
data_prep_node >> validate_node >> model_flow
data_science_flow = DataScienceFlow(start=data_prep_node)
result = build_mermaid(start=data_science_flow)
```

The code generates a Mermaid diagram:

```mermaid
graph LR
    subgraph sub_flow_N1[DataScienceFlow]
    N2['DataPrepBatchNode']
    N3['ValidateDataNode']
    N2 --> N3
    N3 --> N4

    subgraph sub_flow_N5[ModelFlow]
    N4['FeatureExtractionNode']
    N6['TrainModelNode']
    N4 --> N6
    N7['EvaluateModelNode']
    N6 --> N7
    end

    end
```

For visualization based on d3.js, check out [the cookbook](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-visualization).

## 2. Call Stack Debugging

It would be useful to print the Node call stacks for debugging. This can be achieved by inspecting the runtime call stack:

```python
import inspect

def get_node_call_stack():
    stack = inspect.stack()
    node_names = []
    seen_ids = set()
    for frame_info in stack[1:]:
        local_vars = frame_info.frame.f_locals
        if 'self' in local_vars:
            caller_self = local_vars['self']
            if isinstance(caller_self, BaseNode) and id(caller_self) not in seen_ids:
                seen_ids.add(id(caller_self))
                node_names.append(type(caller_self).__name__)
    return node_names
```

For example, suppose we have a complex Flow for data science:

```python
class DataPrepBatchNode(BatchNode): 
    def prep(self, shared): return []
class ValidateDataNode(Node): pass
class FeatureExtractionNode(Node): pass
class TrainModelNode(Node): pass
class EvaluateModelNode(Node): 
    def prep(self, shared):
        stack = get_node_call_stack()
        print("Call stack:", stack)
class ModelFlow(Flow): pass
class DataScienceFlow(Flow):pass

feature_node = FeatureExtractionNode()
train_node = TrainModelNode()
evaluate_node = EvaluateModelNode()
feature_node >> train_node >> evaluate_node
model_flow = ModelFlow(start=feature_node)
data_prep_node = DataPrepBatchNode()
validate_node = ValidateDataNode()
data_prep_node >> validate_node >> model_flow
data_science_flow = DataScienceFlow(start=data_prep_node)
data_science_flow.run({})
```

The output would be: `Call stack: ['EvaluateModelNode', 'ModelFlow', 'DataScienceFlow']`

For a more complete implementation, check out [the cookbook](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-tracing).
</file>

<file path=".cursor/rules/utility_function/websearch.mdc">
---
description: Guidelines for using PocketFlow, Utility Function, Web Search
globs: 
alwaysApply: false
---
# Web Search

We recommend some implementations of commonly used web search tools.

| **API**                         | **Free Tier**                                | **Pricing Model**                                              | **Docs**                                                  |
|---------------------------------|-----------------------------------------------|-----------------------------------------------------------------|------------------------------------------------------------------------|
| **Google Custom Search JSON API** | 100 queries/day free       | $5 per 1000 queries.           | [Link](https://developers.google.com/custom-search/v1/overview)        |
| **Bing Web Search API**         | 1,000 queries/month               | $15â€“$25 per 1,000 queries. | [Link](https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/) |
| **DuckDuckGo Instant Answer**   | Completely free (Instant Answers only, **no URLs**) | No paid plans; usage unlimited, but data is limited             | [Link](https://duckduckgo.com/api)                                     |
| **Brave Search API**         | 2,000 queries/month free | $3 per 1k queries for Base, $5 per 1k for Pro | [Link](https://brave.com/search/api/)                                  |
| **SerpApi**              | 100 searches/month free            | Start at $75/month for 5,000 searches| [Link](https://serpapi.com/)                                             |
| **RapidAPI**           | Many  options    | Many  options             | [Link](https://rapidapi.com/search?term=search&sortBy=ByRelevance)      |

## Example Python Code

### 1. Google Custom Search JSON API
```python
import requests

API_KEY = "YOUR_API_KEY"
CX_ID = "YOUR_CX_ID"
query = "example"

url = "https://www.googleapis.com/customsearch/v1"
params = {
    "key": API_KEY,
    "cx": CX_ID,
    "q": query
}

response = requests.get(url, params=params)
results = response.json()
print(results)
```

### 2. Bing Web Search API
```python
import requests

SUBSCRIPTION_KEY = "YOUR_BING_API_KEY"
query = "example"

url = "https://api.bing.microsoft.com/v7.0/search"
headers = {"Ocp-Apim-Subscription-Key": SUBSCRIPTION_KEY}
params = {"q": query}

response = requests.get(url, headers=headers, params=params)
results = response.json()
print(results)
```

### 3. DuckDuckGo Instant Answer
```python
import requests

query = "example"
url = "https://api.duckduckgo.com/"
params = {
    "q": query,
    "format": "json"
}

response = requests.get(url, params=params)
results = response.json()
print(results)
```

### 4. Brave Search API
```python
import requests

SUBSCRIPTION_TOKEN = "YOUR_BRAVE_API_TOKEN"
query = "example"

url = "https://api.search.brave.com/res/v1/web/search"
headers = {
    "X-Subscription-Token": SUBSCRIPTION_TOKEN
}
params = {
    "q": query
}

response = requests.get(url, headers=headers, params=params)
results = response.json()
print(results)
```

### 5. SerpApi
```python
import requests

API_KEY = "YOUR_SERPAPI_KEY"
query = "example"

url = "https://serpapi.com/search"
params = {
    "engine": "google",
    "q": query,
    "api_key": API_KEY
}

response = requests.get(url, params=params)
results = response.json()
print(results)
```
</file>

<file path=".cursor/rules/guide_for_pocketflow.mdc">
---
description: Guidelines for using PocketFlow, Agentic Coding
globs: **/*.py
alwaysApply: true
---
# DOCUMENTATION FIRST POLICY

**CRITICAL INSTRUCTION**: When implementing a Pocket Flow app:

1. **ALWAYS REQUEST MDC FILES FIRST** - Before writing any code, request and review all relevant MDC documentation files. This doc provides an explaination of the documents.
2. **UNDERSTAND THE FRAMEWORK** - Gain comprehensive understanding of the Pocket Flow framework from documentation
3. **AVOID ASSUMPTION-DRIVEN DEVELOPMENT** - Do not base your implementation on assumptions or guesswork. Even if the human didn't explicitly mention pocket flow in their request, if the code you are editing is using pocket flow, you should request relevant docs to help you understand best practice as well before editing.

**VERIFICATION**: Begin each implementation with a brief summary of the documentation you've reviewed to inform your approach.

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agent involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
{: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps                  | Human      | AI        | Comment                                                                 |
|:-----------------------|:----------:|:---------:|:------------------------------------------------------------------------|
| 1. Requirements | â˜…â˜…â˜… High  | â˜…â˜†â˜† Low   | Humans understand the requirements and context.                    |
| 2. Flow          | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium |  Humans specify the high-level design, and the AI fills in the details. |
| 3. Utilities   | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Data          | â˜…â˜†â˜† Low    | â˜…â˜…â˜… High   | AI designs the data schema, and humans verify.                            |
| 5. Node          | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  | The AI helps design the node based on the flow.          |
| 6. Implementation      | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI implements the flow based on the design. |
| 7. Optimization        | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans evaluate the results, and the AI helps optimize. |
| 8. Reliability         | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI writes test cases and addresses corner cases.     |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit. 
    - Understand AI systems' strengths and limitations:
      - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
      - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
      - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
    - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
    - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.
    - Identify applicable design patterns (e.g., [Map Reduce], [Agent], [RAG]).
      - For each node in the flow, start with a high-level one-line description of what it does.
      - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
      - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
      - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
    - Outline the flow and draw it in a mermaid diagram. For example:
      ```mermaid
      flowchart LR
          start[Start] --> batch[Batch]
          batch --> check[Check]
          check -->|OK| process
          check -->|Error| fix[Fix]
          fix --> check
          
          subgraph process[Process]
            step1[Step 1] --> step2[Step 2]
          end
          
          process --> endNode[End]
      ```
    - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
      {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.
    - Think of your AI system as the brain. It needs a bodyâ€”these *external utility functions*â€”to interact with the real world:
        

        - Reading inputs (e.g., retrieving Slack messages, reading emails)
        - Writing outputs (e.g., generating reports, sending emails)
        - Using external tools (e.g., calling LLMs, searching the web)
        - **NOTE**: *LLM-based tasks* (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are *core functions* internal in the AI system.
    - For each utility function, implement it and write a simple test.
    - Document their input/output, as well as why they are necessary. For example:
      - `name`: `get_embedding` (`utils/get_embedding.py`)
      - `input`: `str`
      - `output`: a vector of 3072 floats
      - `necessity`: Used by the second node to embed text
    - Example utility implementation:
      ```python
      # utils/call_llm.py
      from openai import OpenAI

      def call_llm(prompt):    
          client = OpenAI(api_key="YOUR_API_KEY_HERE")
          r = client.chat.completions.create(
              model="gpt-4o",
              messages=[{"role": "user", "content": prompt}]
          )
          return r.choices[0].message.content
          
      if __name__ == "__main__":
          prompt = "What is the meaning of life?"
          print(call_llm(prompt))
      ```
    - > **Sometimes, design Utilities before Flow:**  For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
      {: .best-practice }
    - > **Avoid Exception Handling in Utilities**: If a utility function is called from a Node's `exec()` method, avoid using `try...except` blocks within the utility. Let the Node's built-in retry mechanism handle failures.
      {: .warning }

4. **Data Design**: Design the shared store that nodes will use to communicate.
   - One core design principle for PocketFlow is to use a well-designed [shared store]â€”a data contract that all nodes agree upon to retrieve and store data.
      - For simple systems, use an in-memory dictionary.
      - For more complex systems or when persistence is required, use a database.
      - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
      - Example shared store design:
        ```python
        shared = {
            "user": {
                "id": "user123",
                "context": {                # Another nested dict
                    "weather": {"temp": 72, "condition": "sunny"},
                    "location": "San Francisco"
                }
            },
            "results": {}                   # Empty dict to store outputs
        }
        ```

5. **Node Design**: Plan how each node will read and write data, and use utility functions.
   - For each [Node], describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Regular (or Batch, or Async)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function. **Avoid exception handling here**; let the Node's retry mechanism manage failures.
     - `post`: Write "embedding" to the shared store

6. **Implementation**: Implement the initial nodes and flows based on the design.
   - ðŸŽ‰ If you've reached this step, humans have finished the design. Now *Agentic Coding* begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Leverage the built-in [Node] retry and fallback mechanisms to handle failures gracefully. This helps you quickly identify weak points in the system.
   - Add logging throughout the code to facilitate debugging.

7. **Optimization**:
   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:
     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3â€“6 hundreds of times.
     >
     > 
     {: .best-practice }

8. **Reliability**  
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `max_retries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my_project/
â”œâ”€â”€ main.py
â”œâ”€â”€ nodes.py
â”œâ”€â”€ flow.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ call_llm.py
â”‚   â””â”€â”€ search_web.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs/
    â””â”€â”€ design.md
```

- **`docs/design.md`**: Contains project documentation for each step above. This should be *high-level* and *no-code*.
- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one Python file to each API call, for example `call_llm.py` or `search_web.py`.
  - Each file should also include a `main()` function to try that API call
- **`nodes.py`**: Contains all the node definitions.
  ```python
  # nodes.py
  from pocketflow import Node
  from utils.call_llm import call_llm

  class GetQuestionNode(Node):
      def exec(self, _):
          # Get question directly from user input
          user_question = input("Enter your question: ")
          return user_question
      
      def post(self, shared, prep_res, exec_res):
          # Store the user's question
          shared["question"] = exec_res
          return "default"  # Go to the next node

  class AnswerNode(Node):
      def prep(self, shared):
          # Read question from shared
          return shared["question"]
      
      def exec(self, question):
          # Call LLM to get the answer
          return call_llm(question)
      
      def post(self, shared, prep_res, exec_res):
          # Store the answer in shared
          shared["answer"] = exec_res
  ```
- **`flow.py`**: Implements functions that create flows by importing node definitions and connecting them.
  ```python
  # flow.py
  from pocketflow import Flow
  from nodes import GetQuestionNode, AnswerNode

  def create_qa_flow():
      """Create and return a question-answering flow."""
      # Create nodes
      get_question_node = GetQuestionNode()
      answer_node = AnswerNode()
      
      # Connect nodes in sequence
      get_question_node >> answer_node
      
      # Create flow starting with input node
      return Flow(start=get_question_node)
  ```
- **`main.py`**: Serves as the project's entry point.
  ```python
  # main.py
  from flow import create_qa_flow

  # Example main function
  # Please replace this with your own main function
  def main():
      shared = {
          "question": None,  # Will be populated by GetQuestionNode from user input
          "answer": None     # Will be populated by AnswerNode
      }

      # Create the flow and run it
      qa_flow = create_qa_flow()
      qa_flow.run(shared)
      print(f"Question: {shared['question']}")
      print(f"Answer: {shared['answer']}")

  if __name__ == "__main__":
      main()
  ```


# Pocket Flow

A [100-line](https://github.com/the-pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalist LLM framework for *Agents, Task Decomposition, RAG, etc*.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworksâ€”([Multi-])[Agents], [Workflow], [RAG], and more.  
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.




## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node] handles simple (LLM) tasks.
- [Flow] connects nodes through **Actions** (labeled edges).
- [Shared Store] enables communication between nodes within flows.
- [Batch] nodes/flows allow for data-intensive tasks.
- [Async] nodes/flows allow waiting for asynchronous tasks.
- [(Advanced) Parallel] nodes/flows handle I/O-bound tasks.



## Design Pattern

From there, itâ€™s easy to implement popular design patterns:

- [Agent] autonomously makes decisions.
- [Workflow] chains multiple tasks into pipelines.
- [RAG] integrates data retrieval with generation.
- [Map Reduce] splits data tasks into Map and Reduce steps.
- [Structured Output] formats outputs consistently.
- [(Advanced) Multi-Agents] coordinate multiple agents.



## Utility Function

We **do not** provide built-in utilities. Instead, we offer *examples*â€”please *implement your own*:

- [LLM Wrapper]
- [Viz and Debug]
- [Web Search]
- [Chunking]
- [Embedding]
- [Vector Databases]
- [Text-to-Speech]

**Why not built-in?**: I believe it's a *bad practice* for vendor-specific APIs in a general framework:
- *API Volatility*: Frequent changes lead to heavy maintenance for hardcoded APIs.
- *Flexibility*: You may want to switch vendors, use fine-tuned models, or run them locally.
- *Optimizations*: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps? 

Check out [Agentic Coding Guidance], the fastest way to develop LLM projects with Pocket Flow!
</file>

<file path="cookbook/data/PaulGrahamEssaysLarge/addiction.txt">
July 2010What hard liquor, cigarettes, heroin, and crack have in common is
that they're all more concentrated forms of less addictive predecessors.
Most if not all the things we describe as addictive are.  And the
scary thing is, the process that created them is accelerating.We wouldn't want to stop it.  It's the same process that cures
diseases: technological progress.  Technological progress means
making things do more of what we want.  When the thing we want is
something we want to want, we consider technological progress good.
If some new technique makes solar cells x% more efficient, that
seems strictly better.  When progress concentrates something we
don't want to wantâ€”when it transforms opium into heroinâ€”it seems
bad.  But it's the same process at work.
[1]No one doubts this process is accelerating, which means increasing
numbers of things we like will be transformed into things we like
too much.
[2]As far as I know there's no word for something we like too much.
The closest is the colloquial sense of "addictive." That usage has
become increasingly common during my lifetime.  And it's clear why:
there are an increasing number of things we need it for.  At the
extreme end of the spectrum are crack and meth.  Food has been
transformed by a combination of factory farming and innovations in
food processing into something with way more immediate bang for the
buck, and you can see the results in any town in America.  Checkers
and solitaire have been replaced by World of Warcraft and FarmVille.
TV has become much more engaging, and even so it can't compete with Facebook.The world is more addictive than it was 40 years ago.   And unless
the forms of technological progress that produced these things are
subject to different laws than technological progress in general,
the world will get more addictive in the next 40 years than it did
in the last 40.The next 40 years will bring us some wonderful things.  I don't
mean to imply they're all to be avoided.  Alcohol is a dangerous
drug, but I'd rather live in a world with wine than one without.
Most people can coexist with alcohol; but you have to be careful.
More things we like will mean more things we have to be careful
about.Most people won't, unfortunately.  Which means that as the world
becomes more addictive, the two senses in which one can live a
normal life will be driven ever further apart.  One sense of "normal"
is statistically normal: what everyone else does.  The other is the
sense we mean when we talk about the normal operating range of a
piece of machinery: what works best.These two senses are already quite far apart.  Already someone
trying to live well would seem eccentrically abstemious in most of
the US.  That phenomenon is only going to become more pronounced.
You can probably take it as a rule of thumb from now on that if
people don't think you're weird, you're living badly.Societies eventually develop antibodies to addictive new things.
I've seen that happen with cigarettes.  When cigarettes first
appeared, they spread the way an infectious disease spreads through
a previously isolated population.  Smoking rapidly became a
(statistically) normal thing.  There were ashtrays everywhere.  We
had ashtrays in our house when I was a kid, even though neither of
my parents smoked.  You had to for guests.As knowledge spread about the dangers of smoking, customs changed.
In the last 20 years, smoking has been transformed from something
that seemed totally normal into a rather seedy habit: from something
movie stars did in publicity shots to something small huddles of
addicts do outside the doors of office buildings.  A lot of the
change was due to legislation, of course, but the legislation
couldn't have happened if customs hadn't already changed.It took a while thoughâ€”on the order of 100 years.  And unless the
rate at which social antibodies evolve can increase to match the
accelerating rate at which technological progress throws off new
addictions, we'll be increasingly unable to rely on customs to
protect us.
[3]
Unless we want to be canaries in the coal mine
of each new addictionâ€”the people whose sad example becomes a
lesson to future generationsâ€”we'll have to figure out for ourselves
what to avoid and how.  It will actually become a reasonable strategy
(or a more reasonable strategy) to suspect 
everything new.In fact, even that won't be enough.  We'll have to worry not just
about new things, but also about existing things becoming more
addictive.  That's what bit me.  I've avoided most addictions, but
the Internet got me because it became addictive while I was using
it.
[4]Most people I know have problems with Internet addiction.  We're
all trying to figure out our own customs for getting free of it.
That's why I don't have an iPhone, for example; the last thing I
want is for the Internet to follow me out into the world.
[5]
My latest trick is taking long hikes.  I used to think running was a
better form of exercise than hiking because it took less time.  Now
the slowness of hiking seems an advantage, because the longer I
spend on the trail, the longer I have to think without interruption.Sounds pretty eccentric, doesn't it?  It always will when you're
trying to solve problems where there are no customs yet to guide
you.  Maybe I can't plead Occam's razor; maybe I'm simply eccentric.
But if I'm right about the acceleration of addictiveness, then this
kind of lonely squirming to avoid it will increasingly be the fate
of anyone who wants to get things done.  We'll increasingly be
defined by what we say no to.
Notes[1]
Could you restrict technological progress to areas where you
wanted it?  Only in a limited way, without becoming a police state.
And even then your restrictions would have undesirable side effects.
"Good" and "bad" technological progress aren't sharply differentiated,
so you'd find you couldn't slow the latter without also slowing the
former.  And in any case, as Prohibition and the "war on drugs"
show, bans often do more harm than good.[2]
Technology has always been accelerating.  By Paleolithic
standards, technology evolved at a blistering pace in the Neolithic
period.[3]
Unless we mass produce social customs.  I suspect the recent
resurgence of evangelical Christianity in the US is partly a reaction
to drugs.  In desperation people reach for the sledgehammer; if
their kids won't listen to them, maybe they'll listen to God.  But
that solution has broader consequences than just getting kids to
say no to drugs.  You end up saying no to 
science as well.
I worry we may be heading for a future in which only a few people
plot their own itinerary through no-land, while everyone else books
a package tour.  Or worse still, has one booked for them by the
government.[4]
People commonly use the word "procrastination" to describe
what they do on the Internet.  It seems to me too mild to describe
what's happening as merely not-doing-work.  We don't call it
procrastination when someone gets drunk instead of working.[5]
Several people have told me they like the iPad because it
lets them bring the Internet into situations where a laptop would
be too conspicuous.  In other words, it's a hip flask.  (This is
true of the iPhone too, of course, but this advantage isn't as
obvious because it reads as a phone, and everyone's used to those.)Thanks to Sam Altman, Patrick Collison, Jessica Livingston, and
Robert Morris for reading drafts of this.
</file>

<file path="cookbook/data/PaulGrahamEssaysLarge/aord.txt">
October 2015When I talk to a startup that's been operating for more than 8 or
9 months, the first thing I want to know is almost always the same.
Assuming their expenses remain constant and their revenue growth
is what it has been over the last several months, do they make it to
profitability on the money they have left?  Or to put it more
dramatically, by default do they live or die?The startling thing is how often the founders themselves don't know.
Half the founders I talk to don't know whether they're default alive
or default dead.If you're among that number, Trevor Blackwell has made a handy
calculator you can use to find out.The reason I want to know first whether a startup is default alive
or default dead is that the rest of the conversation depends on the
answer.  If the company is default alive, we can talk about ambitious
new things they could do.  If it's default dead, we probably need
to talk about how to save it.  We know the current trajectory ends
badly.  How can they get off that trajectory?Why do so few founders know whether they're default alive or default
dead?  Mainly, I think, because they're not used to asking that.
It's not a question that makes sense to ask early on, any more than
it makes sense to ask a 3 year old how he plans to support
himself.  But as the company grows older, the question switches from
meaningless to critical.  That kind of switch often takes people
by surprise.I propose the following solution: instead of starting to ask too
late whether you're default alive or default dead, start asking too
early.  It's hard to say precisely when the question switches
polarity.  But it's probably not that dangerous to start worrying
too early that you're default dead, whereas it's very dangerous to
start worrying too late.The reason is a phenomenon I wrote about earlier: the
fatal pinch.
The fatal pinch is default dead + slow growth + not enough
time to fix it.  And the way founders end up in it is by not realizing
that's where they're headed.There is another reason founders don't ask themselves whether they're
default alive or default dead: they assume it will be easy to raise
more money.  But that assumption is often false, and worse still, the
more you depend on it, the falser it becomes.Maybe it will help to separate facts from hopes. Instead of thinking
of the future with vague optimism, explicitly separate the components.
Say "We're default dead, but we're counting on investors to save
us." Maybe as you say that, it will set off the same alarms in your
head that it does in mine.  And if you set off the alarms sufficiently
early, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors
saving you.  As a rule their interest is a function of
growth.  If you have steep revenue growth, say over 5x a year, you
can start to count on investors being interested even if you're not
profitable.
[1]
But investors are so fickle that you can never
do more than start to count on them.  Sometimes something about your
business will spook investors even if your growth is great.  So no
matter how good your growth is, you can never safely treat fundraising
as more than a plan A. You should always have a plan B as well: you
should know (as in write down) precisely what you'll need to do to
survive if you can't raise more money, and precisely when you'll 
have to switch to plan B if plan A isn't working.In any case, growing fast versus operating cheaply is far from the
sharp dichotomy many founders assume it to be.  In practice there
is surprisingly little connection between how much a startup spends
and how fast it grows.  When a startup grows fast, it's usually
because the product hits a nerve, in the sense of hitting some big
need straight on.  When a startup spends a lot, it's usually because
the product is expensive to develop or sell, or simply because
they're wasteful.If you're paying attention, you'll be asking at this point not just
how to avoid the fatal pinch, but how to avoid being default dead.
That one is easy: don't hire too fast.  Hiring too fast is by far
the biggest killer of startups that raise money.
[2]Founders tell themselves they need to hire in order to grow.  But
most err on the side of overestimating this need rather than
underestimating it.  Why?  Partly because there's so much work to
do.  Naive founders think that if they can just hire enough
people, it will all get done.  Partly because successful startups have
lots of employees, so it seems like that's what one does in order
to be successful.  In fact the large staffs of successful startups
are probably more the effect of growth than the cause.  And
partly because when founders have slow growth they don't want to
face what is usually the real reason: the product is not appealing
enough.Plus founders who've just raised money are often encouraged to
overhire by the VCs who funded them.  Kill-or-cure strategies are
optimal for VCs because they're protected by the portfolio effect.
VCs want to blow you up, in one sense of the phrase or the other.
But as a founder your incentives are different.  You want above all
to survive.
[3]Here's a common way startups die.  They make something moderately
appealing and have decent initial growth. They raise their first
round fairly easily, because the founders seem smart and the idea
sounds plausible. But because the product is only moderately
appealing, growth is ok but not great.  The founders convince
themselves that hiring a bunch of people is the way to boost growth.
Their investors agree.  But (because the product is only moderately
appealing) the growth never comes.  Now they're rapidly running out
of runway.  They hope further investment will save them. But because
they have high expenses and slow growth, they're now unappealing
to investors. They're unable to raise more, and the company dies.What the company should have done is address the fundamental problem:
that the product is only moderately appealing.  Hiring people is
rarely the way to fix that.  More often than not it makes it harder.
At this early stage, the product needs to evolve more than to be
"built out," and that's usually easier with fewer people.
[4]Asking whether you're default alive or default dead may save you
from this.  Maybe the alarm bells it sets off will counteract the
forces that push you to overhire.  Instead you'll be compelled to
seek growth in other ways. For example, by doing
things that don't scale, or by redesigning the product in the
way only founders can.
And for many if not most startups, these paths to growth will be
the ones that actually work.Airbnb waited 4 months after raising money at the end of YÂ Combinator
before they hired their first employee.  In the meantime the founders
were terribly overworked.  But they were overworked evolving Airbnb
into the astonishingly successful organism it is now.Notes[1]
Steep usage growth will also interest investors.  Revenue
will ultimately be a constant multiple of usage, so x% usage growth
predicts x% revenue growth.  But in practice investors discount
merely predicted revenue, so if you're measuring usage you need a
higher growth rate to impress investors.[2]
Startups that don't raise money are saved from hiring too
fast because they can't afford to. But that doesn't mean you should
avoid raising money in order to avoid this problem, any more than
that total abstinence is the only way to avoid becoming an alcoholic.[3]
I would not be surprised if VCs' tendency to push founders
to overhire is not even in their own interest.  They don't know how
many of the companies that get killed by overspending might have
done well if they'd survived.  My guess is a significant number.[4]
After reading a draft, Sam Altman wrote:"I think you should make the hiring point more strongly.  I think
it's roughly correct to say that YC's most successful companies
have never been the fastest to hire, and one of the marks of a great
founder is being able to resist this urge."Paul Buchheit adds:"A related problem that I see a lot is premature scalingâ€”founders
take a small business that isn't really working (bad unit economics,
typically) and then scale it up because they want impressive growth
numbers. This is similar to over-hiring in that it makes the business
much harder to fix once it's big, plus they are bleeding cash really
fast."
Thanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston,
and Geoff Ralston for reading drafts of this.
</file>

<file path="cookbook/data/PaulGrahamEssaysLarge/apple.txt">
Want to start a startup?  Get funded by
Y Combinator.




November 2009I don't think Apple realizes how badly the App Store approval process
is broken.  Or rather, I don't think they realize how much it matters
that it's broken.The way Apple runs the App Store has harmed their reputation with
programmers more than anything else they've ever done. 
Their reputation with programmers used to be great.
It used to be the most common complaint you heard
about Apple was that their fans admired them too uncritically.
The App Store has changed that.  Now a lot of programmers
have started to see Apple as evil.How much of the goodwill Apple once had with programmers have they
lost over the App Store?  A third?  Half?  And that's just so far.
The App Store is an ongoing karma leak.* * *How did Apple get into this mess?  Their fundamental problem is
that they don't understand software.They treat iPhone apps the way they treat the music they sell through
iTunes.  Apple is the channel; they own the user; if you want to
reach users, you do it on their terms. The record labels agreed,
reluctantly.  But this model doesn't work for software.  It doesn't
work for an intermediary to own the user.  The software business
learned that in the early 1980s, when companies like VisiCorp showed
that although the words "software" and "publisher" fit together,
the underlying concepts don't.  Software isn't like music or books.
It's too complicated for a third party to act as an intermediary
between developer and user.   And yet that's what Apple is trying
to be with the App Store: a software publisher.  And a particularly
overreaching one at that, with fussy tastes and a rigidly enforced
house style.If software publishing didn't work in 1980, it works even less now
that software development has evolved from a small number of big
releases to a constant stream of small ones.  But Apple doesn't
understand that either.  Their model of product development derives
from hardware.  They work on something till they think it's finished,
then they release it.  You have to do that with hardware, but because
software is so easy to change, its design can benefit from evolution.
The standard way to develop applications now is to launch fast and
iterate.  Which means it's a disaster to have long, random delays
each time you release a new version.Apparently Apple's attitude is that developers should be more careful
when they submit a new version to the App Store.  They would say
that.  But powerful as they are, they're not powerful enough to
turn back the evolution of technology.  Programmers don't use
launch-fast-and-iterate out of laziness.  They use it because it
yields the best results.  By obstructing that process, Apple is
making them do bad work, and programmers hate that as much as Apple
would.How would Apple like it if when they discovered a serious bug in
OSÂ X, instead of releasing a software update immediately, they had
to submit their code to an intermediary who sat on it for a month
and then rejected it because it contained an icon they didn't like?By breaking software development, Apple gets the opposite of what
they intended: the version of an app currently available in the App
Store tends to be an old and buggy one.  One developer told me:

  As a result of their process, the App Store is full of half-baked
  applications. I make a new version almost every day that I release
  to beta users. The version on the App Store feels old and crappy.
  I'm sure that a lot of developers feel this way: One emotion is
  "I'm not really proud about what's in the App Store", and it's
  combined with the emotion "Really, it's Apple's fault."

Another wrote:

  I believe that they think their approval process helps users by
  ensuring quality.  In reality, bugs like ours get through all the
  time and then it can take 4-8 weeks to get that bug fix approved,
  leaving users to think that iPhone apps sometimes just don't work.
  Worse for Apple, these apps work just fine on other platforms
  that have immediate approval processes.

Actually I suppose Apple has a third misconception: that all the
complaints about App Store approvals are not a serious problem.
They must hear developers complaining.  But partners and suppliers
are always complaining.  It would be a bad sign if they weren't;
it would mean you were being too easy on them.  Meanwhile the iPhone
is selling better than ever.  So why do they need to fix anything?They get away with maltreating developers, in the short term, because
they make such great hardware.  I just bought a new 27" iMac a
couple days ago.  It's fabulous.  The screen's too shiny, and the
disk is surprisingly loud, but it's so beautiful that you can't
make yourself care.So I bought it, but I bought it, for the first time, with misgivings.
I felt the way I'd feel buying something made in a country with a
bad human rights record.  That was new.  In the past when I bought
things from Apple it was an unalloyed pleasure.  Oh boy!  They make
such great stuff.  This time it felt like a Faustian bargain.  They
make such great stuff, but they're such assholes.  Do I really want
to support this company?* * *Should Apple care what people like me think?  What difference does
it make if they alienate a small minority of their users?There are a couple reasons they should care.  One is that these
users are the people they want as employees.  If your company seems
evil, the best programmers won't work for you.  That hurt Microsoft
a lot starting in the 90s.  Programmers started to feel sheepish
about working there.  It seemed like selling out.  When people from
Microsoft were talking to other programmers and they mentioned where
they worked, there were a lot of self-deprecating jokes about having
gone over to the dark side.  But the real problem for Microsoft
wasn't the embarrassment of the people they hired.  It was the
people they never got.  And you know who got them?  Google and
Apple.  If Microsoft was the Empire, they were the Rebel Alliance.
And it's largely because they got more of the best people that
Google and Apple are doing so much better than Microsoft today.Why are programmers so fussy about their employers' morals?  Partly
because they can afford to be.  The best programmers can work
wherever they want.  They don't have to work for a company they
have qualms about.But the other reason programmers are fussy, I think, is that evil
begets stupidity.  An organization that wins by exercising power
starts to lose the ability to win by doing better work.  And it's
not fun for a smart person to work in a place where the best ideas
aren't the ones that win.  I think the reason Google embraced "Don't
be evil" so eagerly was not so much to impress the outside world
as to inoculate themselves against arrogance.
[1]That has worked for Google so far.  They've become more
bureaucratic, but otherwise they seem to have held true to their
original principles. With Apple that seems less the case.  When you
look at the famous 
1984 ad 
now, it's easier to imagine Apple as the
dictator on the screen than the woman with the hammer.
[2]
In fact, if you read the dictator's speech it sounds uncannily like a
prophecy of the App Store.

  We have triumphed over the unprincipled dissemination of facts.We have created, for the first time in all history, a garden of
  pure ideology, where each worker may bloom secure from the pests
  of contradictory and confusing truths.

The other reason Apple should care what programmers think of them
is that when you sell a platform, developers make or break you.  If
anyone should know this, Apple should.  VisiCalc made the Apple II.And programmers build applications for the platforms they use.  Most
applicationsâ€”most startups, probablyâ€”grow out of personal projects.
Apple itself did.  Apple made microcomputers because that's what
Steve Wozniak wanted for himself.  He couldn't have afforded a
minicomputer. 
[3]
 Microsoft likewise started out making interpreters
for little microcomputers because
Bill Gates and Paul Allen were interested in using them.  It's a
rare startup that doesn't build something the founders use.The main reason there are so many iPhone apps is that so many programmers
have iPhones.  They may know, because they read it in an article,
that Blackberry has such and such market share.  But in practice
it's as if RIM didn't exist. If they're going to build something,
they want to be able to use it themselves, and that means building
an iPhone app.So programmers continue to develop iPhone apps, even though Apple
continues to maltreat them.  They're like someone stuck in an abusive
relationship.  They're so attracted to the iPhone that they can't
leave.  But they're looking for a way out.  One wrote:

  While I did enjoy developing for the iPhone, the control they
  place on the App Store does not give me the drive to develop
  applications as I would like. In fact I don't intend to make any
  more iPhone applications unless absolutely necessary.
[4]

Can anything break this cycle?  No device I've seen so far could.
Palm and RIM haven't a hope.  The only credible contender is Android.
But Android is an orphan; Google doesn't really care about it, not
the way Apple cares about the iPhone.  Apple cares about the iPhone
the way Google cares about search.* * *Is the future of handheld devices one locked down by Apple?  It's
a worrying prospect.  It would be a bummer to have another grim
monoculture like we had in the 1990s.  In 1995, writing software
for end users was effectively identical with writing Windows
applications.  Our horror at that prospect was the single biggest
thing that drove us to start building web apps.At least we know now what it would take to break Apple's lock.
You'd have to get iPhones out of programmers' hands.  If programmers
used some other device for mobile web access, they'd start to develop
apps for that instead.How could you make a device programmers liked better than the iPhone?
It's unlikely you could make something better designed.  Apple
leaves no room there.  So this alternative device probably couldn't
win on general appeal.  It would have to win by virtue of some
appeal it had to programmers specifically.One way to appeal to programmers is with software.  If you
could think of an application programmers had to have, but that
would be impossible in the circumscribed world of the iPhone, 
you could presumably get them to switch.That would definitely happen if programmers started to use handhelds
as development machinesâ€”if handhelds displaced laptops the
way laptops displaced desktops.  You need more control of a development
machine than Apple will let you have over an iPhone.Could anyone make a device that you'd carry around in your pocket
like a phone, and yet would also work as a development machine?
It's hard to imagine what it would look like.  But I've learned
never to say never about technology.  A phone-sized device that
would work as a development machine is no more miraculous by present
standards than the iPhone itself would have seemed by the standards
of 1995.My current development machine is a MacBook Air, which I use with
an external monitor and keyboard in my office, and by itself when
traveling.  If there was a version half the size I'd prefer it.
That still wouldn't be small enough to carry around everywhere like
a phone, but we're within a factor of 4 or so.  Surely that gap is
bridgeable.  In fact, let's make it an
RFS. Wanted: 
Woman with hammer.Notes[1]
When Google adopted "Don't be evil," they were still so small
that no one would have expected them to be, yet.
[2]
The dictator in the 1984 ad isn't Microsoft, incidentally;
it's IBM.  IBM seemed a lot more frightening in those days, but
they were friendlier to developers than Apple is now.[3]
He couldn't even afford a monitor.  That's why the Apple
I used a TV as a monitor.[4]
Several people I talked to mentioned how much they liked the
iPhone SDK.  The problem is not Apple's products but their policies.
Fortunately policies are software; Apple can change them instantly
if they want to.  Handy that, isn't it?Thanks to Sam Altman, Trevor Blackwell, Ross Boucher, 
James Bracy, Gabor Cselle,
Patrick Collison, Jason Freedman, John Gruber, Joe Hewitt, Jessica Livingston,
Robert Morris, Teng Siong Ong, Nikhil Pandit, Savraj Singh, and Jared Tame for reading drafts of this.
</file>

<file path="cookbook/data/PaulGrahamEssaysLarge/avg.txt">
Want to start a startup?  Get funded by
Y Combinator.




April 2001, rev. April 2003(This article is derived from a talk given at the 2001 Franz
Developer Symposium.)
In the summer of 1995, my friend Robert Morris and I
started a startup called 
Viaweb.  
Our plan was to write
software that would let end users build online stores.
What was novel about this software, at the time, was
that it ran on our server, using ordinary Web pages
as the interface.A lot of people could have been having this idea at the
same time, of course, but as far as I know, Viaweb was
the first Web-based application.  It seemed such
a novel idea to us that we named the company after it:
Viaweb, because our software worked via the Web,
instead of running on your desktop computer.Another unusual thing about this software was that it
was written primarily in a programming language called
Lisp. It was one of the first big end-user
applications to be written in Lisp, which up till then
had been used mostly in universities and research labs. [1]The Secret WeaponEric Raymond has written an essay called "How to Become a Hacker,"
and in it, among other things, he tells would-be hackers what
languages they should learn.  He suggests starting with Python and
Java, because they are easy to learn.  The serious hacker will also
want to learn C, in order to hack Unix, and Perl for system
administration and cgi scripts.  Finally, the truly serious hacker
should consider learning Lisp:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

This is the same argument you tend to hear for learning Latin.  It
won't get you a job, except perhaps as a classics professor, but
it will improve your mind, and make you a better writer in languages
you do want to use, like English.But wait a minute.  This metaphor doesn't stretch that far.  The
reason Latin won't get you a job is that no one speaks it.  If you
write in Latin, no one can understand you.  But Lisp is a computer
language, and computers speak whatever language you, the programmer,
tell them to.So if Lisp makes you a better programmer, like he says, why wouldn't
you want to use it? If a painter were offered a brush that would
make him a better painter, it seems to me that he would want to
use it in all his paintings, wouldn't he? I'm not trying to make
fun of Eric Raymond here.  On the whole, his advice is good.  What
he says about Lisp is pretty much the conventional wisdom.  But
there is a contradiction in the conventional wisdom:  Lisp will
make you a better programmer, and yet you won't use it.Why not?  Programming languages are just tools, after all.  If Lisp
really does yield better programs, you should use it.  And if it
doesn't, then who needs it?This is not just a theoretical question.  Software is a very
competitive business, prone to natural monopolies.  A company that
gets software written faster and better will, all other things
being equal, put its competitors out of business.  And when you're
starting a startup, you feel this very keenly.  Startups tend to
be an all or nothing proposition.  You either get rich, or you get
nothing.  In a startup, if you bet on the wrong technology, your
competitors will crush you.Robert and I both knew Lisp well, and we couldn't see any reason
not to trust our instincts and go with Lisp.  We knew that everyone
else was writing their software in C++ or Perl.  But we also knew
that that didn't mean anything.  If you chose technology that way,
you'd be running Windows.  When you choose technology, you have to
ignore what other people are doing, and consider only what will
work the best.This is especially true in a startup.  In a big company, you can
do what all the other big companies are doing.  But a startup can't
do what all the other startups do.  I don't think a lot of people
realize this, even in startups.The average big company grows at about ten percent a year.  So if
you're running a big company and you do everything the way the
average big company does it, you can expect to do as well as the
average big company-- that is, to grow about ten percent a year.The same thing will happen if you're running a startup, of course.
If you do everything the way the average startup does it, you should
expect average performance.  The problem here is, average performance
means that you'll go out of business.  The survival rate for startups
is way less than fifty percent.  So if you're running a startup,
you had better be doing something odd.  If not, you're in trouble.Back in 1995, we knew something that I don't think our competitors
understood, and few understand even now:  when you're writing
software that only has to run on your own servers, you can use
any language you want.  When you're writing desktop software,
there's a strong bias toward writing applications in the same
language as the operating system.  Ten years ago, writing applications
meant writing applications in C.  But with Web-based software,
especially when you have the source code of both the language and
the operating system, you can use whatever language you want.This new freedom is a double-edged sword, however.  Now that you
can use any language, you have to think about which one to use.
Companies that try to pretend nothing has changed risk finding that
their competitors do not.If you can use any language, which do you use?  We chose Lisp.
For one thing, it was obvious that rapid development would be
important in this market.  We were all starting from scratch, so
a company that could get new features done before its competitors
would have a big advantage.  We knew Lisp was a really good language
for writing software quickly, and server-based applications magnify
the effect of rapid development, because you can release software
the minute it's done.If other companies didn't want to use Lisp, so much the better.
It might give us a technological edge, and we needed all the help
we could get.  When we started Viaweb, we had no experience in
business.  We didn't know anything about marketing, or hiring
people, or raising money, or getting customers.  Neither of us had
ever even had what you would call a real job.  The only thing we
were good at was writing software.  We hoped that would save us.
Any advantage we could get in the software department, we would
take.So you could say that using Lisp was an experiment.  Our hypothesis
was that if we wrote our software in Lisp, we'd be able to get
features done faster than our competitors, and also to do things
in our software that they couldn't do.  And because Lisp was so
high-level, we wouldn't need a big development team, so our costs
would be lower.  If this were so, we could offer a better product
for less money, and still make a profit.  We would end up getting
all the users, and our competitors would get none, and eventually
go out of business.  That was what we hoped would happen, anyway.What were the results of this experiment?  Somewhat surprisingly,
it worked.  We eventually had many competitors, on the order of
twenty to thirty of them, but none of their software could compete
with ours.  We had a wysiwyg online store builder that ran on the
server and yet felt like a desktop application.  Our competitors
had cgi scripts.  And we were always far ahead of them in features.
Sometimes, in desperation, competitors would try to introduce
features that we didn't have.  But with Lisp our development cycle
was so fast that we could sometimes duplicate a new feature within
a day or two of a competitor announcing it in a press release.  By
the time journalists covering the press release got round to calling
us, we would have the new feature too.It must have seemed to our competitors that we had some kind of
secret weapon-- that we were decoding their Enigma traffic or
something.  In fact we did have a secret weapon, but it was simpler
than they realized.  No one was leaking news of their features to
us.   We were just able to develop software faster than anyone
thought possible.When I was about nine I happened to get hold of a copy of The Day
of the Jackal, by Frederick Forsyth.  The main character is an
assassin who is hired to kill the president of France.  The assassin
has to get past the police to get up to an apartment that overlooks
the president's route.  He walks right by them, dressed up as an
old man on crutches, and they never suspect him.Our secret weapon was similar.  We wrote our software in a weird
AI language, with a bizarre syntax full of parentheses.  For years
it had annoyed me to hear Lisp described that way.  But now it
worked to our advantage.  In business, there is nothing more valuable
than a technical advantage your competitors don't understand.  In
business, as in war, surprise is worth as much as force.And so, I'm a little embarrassed to say, I never said anything
publicly about Lisp while we were working on Viaweb.  We never
mentioned it to the press, and if you searched for Lisp on our Web
site, all you'd find were the titles of two books in my bio.  This
was no accident.  A startup should give its competitors as little
information as possible.  If they didn't know what language our
software was written in, or didn't care, I wanted to keep it that
way.[2]The people who understood our technology best were the customers.
They didn't care what language Viaweb was written in either, but
they noticed that it worked really well.  It let them build great
looking online stores literally in minutes.  And so, by word of
mouth mostly, we got more and more users.  By the end of 1996 we
had about 70 stores online.  At the end of 1997 we had 500.  Six
months later, when Yahoo bought us, we had 1070 users.  Today, as
Yahoo Store, this software continues to dominate its market.  It's
one of the more profitable pieces of Yahoo, and the stores built
with it are the foundation of Yahoo Shopping.  I left Yahoo in
1999, so I don't know exactly how many users they have now, but
the last I heard there were about 20,000.
The Blub ParadoxWhat's so great about Lisp?  And if Lisp is so great, why doesn't
everyone use it?  These sound like rhetorical questions, but actually
they have straightforward answers.  Lisp is so great not because
of some magic quality visible only to devotees, but because it is
simply the most powerful language available.  And the reason everyone
doesn't use it is that programming languages are not merely
technologies, but habits of mind as well, and nothing changes
slower.  Of course, both these answers need explaining.I'll begin with a shockingly controversial statement:  programming
languages vary in power.Few would dispute, at least, that high level languages are more
powerful than machine language.  Most programmers today would agree
that you do not, ordinarily, want to program in machine language.
Instead, you should program in a high-level language, and have a
compiler translate it into machine language for you.  This idea is
even built into the hardware now: since the 1980s, instruction sets
have been designed for compilers rather than human programmers.Everyone knows it's a mistake to write your whole program by hand
in machine language.  What's less often understood is that there
is a more general principle here: that if you have a choice of
several languages, it is, all other things being equal, a mistake
to program in anything but the most powerful one. [3]There are many exceptions to this rule.  If you're writing a program
that has to work very closely with a program written in a certain
language, it might be a good idea to write the new program in the
same language.  If you're writing a program that only has to do
something very simple, like number crunching or bit manipulation,
you may as well use a less abstract language, especially since it
may be slightly faster.  And if you're writing a short, throwaway
program, you may be better off just using whatever language has
the best library functions for the task.  But in general, for
application software, you want to be using the most powerful
(reasonably efficient) language you can get, and using anything
else is a mistake, of exactly the same kind, though possibly in a
lesser degree, as programming in machine language.You can see that machine language is very low level.  But, at least
as a kind of social convention, high-level languages are often all
treated as equivalent.  They're not.  Technically the term "high-level
language" doesn't mean anything very definite.  There's no dividing
line with machine languages on one side and all the high-level
languages on the other.  Languages fall along a continuum [4] of
abstractness, from the most powerful all the way down to machine
languages, which themselves vary in power.Consider Cobol.  Cobol is a high-level language, in the sense that
it gets compiled into machine language.  Would anyone seriously
argue that Cobol is equivalent in power to, say, Python?  It's
probably closer to machine language than Python.Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures
got added to the language.  Most Perl hackers would agree that Perl
5 is more powerful than Perl 4.  But once you've admitted that,
you've admitted that one high level language can be more powerful
than another.  And it follows inexorably that, except in special
cases, you ought to use the most powerful you can get.This idea is rarely followed to its conclusion, though.  After a
certain age, programmers rarely switch languages voluntarily.
Whatever language people happen to be used to, they tend to consider
just good enough.Programmers get very attached to their favorite languages, and I
don't want to hurt anyone's feelings, so to explain this point I'm
going to use a hypothetical language called Blub.  Blub falls right
in the middle of the abstractness continuum.  It is not the most
powerful language, but it is more powerful than Cobol or machine
language.And in fact, our hypothetical Blub programmer wouldn't use either
of them.  Of course he wouldn't program in machine language.  That's
what compilers are for.  And as for Cobol, he doesn't know how
anyone can get anything done with it.  It doesn't even have x (Blub
feature of your choice).As long as our hypothetical Blub programmer is looking down the
power continuum, he knows he's looking down.  Languages less powerful
than Blub are obviously less powerful, because they're missing some
feature he's used to.  But when our hypothetical Blub programmer
looks in the other direction, up the power continuum, he doesn't
realize he's looking up.  What he sees are merely weird languages.
He probably considers them about equivalent in power to Blub, but
with all this other hairy stuff thrown in as well.  Blub is good
enough for him, because he thinks in Blub.When we switch to the point of view of a programmer using any of
the languages higher up the power continuum, however, we find that
he in turn looks down upon Blub.  How can you get anything done in
Blub? It doesn't even have y.By induction, the only programmers in a position to see all the
differences in power between the various languages are those who
understand the most powerful one.  (This is probably what Eric
Raymond meant about Lisp making you a better programmer.) You can't
trust the opinions of the others, because of the Blub paradox:
they're satisfied with whatever language they happen to use, because
it dictates the way they think about programs.I know this from my own experience, as a high school kid writing
programs in Basic.  That language didn't even support recursion.
It's hard to imagine writing programs without using recursion, but
I didn't miss it at the time.  I thought in Basic.  And I was a
whiz at it.  Master of all I surveyed.The five languages that Eric Raymond recommends to hackers fall at
various points on the power continuum.  Where they fall relative
to one another is a sensitive topic.  What I will say is that I
think Lisp is at the top.  And to support this claim I'll tell you
about one of the things I find missing when I look at the other
four languages.  How can you get anything done in them, I think,
without macros? [5]Many languages have something called a macro.  But Lisp macros are
unique.  And believe it or not, what they do is related to the
parentheses.  The designers of Lisp didn't put all those parentheses
in the language just to be different.  To the Blub programmer, Lisp
code looks weird.  But those parentheses are there for a reason.
They are the outward evidence of a fundamental difference between
Lisp and other languages.Lisp code is made out of Lisp data objects.  And not in the trivial
sense that the source files contain characters, and strings are
one of the data types supported by the language.  Lisp code, after
it's read by the parser, is made of data structures that you can
traverse.If you understand how compilers work, what's really going on is
not so much that Lisp has a strange syntax as that Lisp has no
syntax.  You write programs in the parse trees that get generated
within the compiler when other languages are parsed.  But these
parse trees are fully accessible to your programs.  You can write
programs that manipulate them.  In Lisp, these programs are called
macros.  They are programs that write programs.Programs that write programs?  When would you ever want to do that?
Not very often, if you think in Cobol.  All the time, if you think
in Lisp.  It would be convenient here if I could give an example
of a powerful macro, and say there! how about that?  But if I did,
it would just look like gibberish to someone who didn't know Lisp;
there isn't room here to explain everything you'd need to know to
understand what it meant.  In 
Ansi Common Lisp I tried to move
things along as fast as I could, and even so I didn't get to macros
until page 160.But I think I can give a kind of argument that might be convincing.
The source code of the Viaweb editor was probably about 20-25%
macros.  Macros are harder to write than ordinary Lisp functions,
and it's considered to be bad style to use them when they're not
necessary.  So every macro in that code is there because it has to
be.  What that means is that at least 20-25% of the code in this
program is doing things that you can't easily do in any other
language.  However skeptical the Blub programmer might be about my
claims for the mysterious powers of Lisp, this ought to make him
curious.  We weren't writing this code for our own amusement.  We
were a tiny startup, programming as hard as we could in order to
put technical barriers between us and our competitors.A suspicious person might begin to wonder if there was some
correlation here.  A big chunk of our code was doing things that
are very hard to do in other languages.  The resulting software
did things our competitors' software couldn't do.  Maybe there was
some kind of connection.  I encourage you to follow that thread.
There may be more to that old man hobbling along on his crutches
than meets the eye.Aikido for StartupsBut I don't expect to convince anyone 
(over 25) 
to go out and learn
Lisp.  The purpose of this article is not to change anyone's mind,
but to reassure people already interested in using Lisp-- people
who know that Lisp is a powerful language, but worry because it
isn't widely used.  In a competitive situation, that's an advantage.
Lisp's power is multiplied by the fact that your competitors don't
get it.If you think of using Lisp in a startup, you shouldn't worry that
it isn't widely understood.  You should hope that it stays that
way. And it's likely to.  It's the nature of programming languages
to make most people satisfied with whatever they currently use.
Computer hardware changes so much faster than personal habits that
programming practice is usually ten to twenty years behind the
processor.  At places like MIT they were writing programs in
high-level languages in the early 1960s, but many companies continued
to write code in machine language well into the 1980s.  I bet a
lot of people continued to write machine language until the processor,
like a bartender eager to close up and go home, finally kicked them
out by switching to a risc instruction set.Ordinarily technology changes fast.  But programming languages are
different: programming languages are not just technology, but what
programmers think in.  They're half technology and half religion.[6]
And so the median language, meaning whatever language the median
programmer uses, moves as slow as an iceberg.  Garbage collection,
introduced by Lisp in about 1960, is now widely considered to be
a good thing.  Runtime typing, ditto, is growing in popularity.
Lexical closures, introduced by Lisp in the early 1970s, are now,
just barely, on the radar screen.  Macros, introduced by Lisp in the
mid 1960s, are still terra incognita.Obviously, the median language has enormous momentum.  I'm not
proposing that you can fight this powerful force.  What I'm proposing
is exactly the opposite: that, like a practitioner of Aikido, you
can use it against your opponents.If you work for a big company, this may not be easy.  You will have
a hard time convincing the pointy-haired boss to let you build
things in Lisp, when he has just read in the paper that some other
language is poised, like Ada was twenty years ago, to take over
the world.  But if you work for a startup that doesn't have
pointy-haired bosses yet, you can, like we did, turn the Blub
paradox to your advantage:  you can use technology that your
competitors, glued immovably to the median language, will never be
able to match.If you ever do find yourself working for a startup, here's a handy
tip for evaluating competitors.  Read their job listings.  Everything
else on their site may be stock photos or the prose equivalent,
but the job listings have to be specific about what they want, or
they'll get the wrong candidates.During the years we worked on Viaweb I read a lot of job descriptions.
A new competitor seemed to emerge out of the woodwork every month
or so.  The first thing I would do, after checking to see if they
had a live online demo, was look at their job listings.  After a
couple years of this I could tell which companies to worry about
and which not to.  The more of an IT flavor the job descriptions
had, the less dangerous the company was.  The safest kind were the
ones that wanted Oracle experience.  You never had to worry about
those.  You were also safe if they said they wanted C++ or Java
developers.  If they wanted Perl or Python programmers, that would
be a bit frightening-- that's starting to sound like a company
where the technical side, at least, is run by real hackers.  If I
had ever seen a job posting looking for Lisp hackers, I would have
been really worried.
Notes[1] Viaweb at first had two parts: the editor, written in Lisp,
which people used to build their sites, and the ordering system,
written in C, which handled orders.  The first version was mostly
Lisp, because the ordering system was small.  Later we added two
more modules, an image generator written in C, and a back-office
manager written mostly in Perl.In January 2003, Yahoo released a new version of the editor 
written in C++ and Perl.  It's hard to say whether the program is no
longer written in Lisp, though, because to translate this program
into C++ they literally had to write a Lisp interpreter: the source
files of all the page-generating templates are still, as far as I
know,  Lisp code.  (See Greenspun's Tenth Rule.)[2] Robert Morris says that I didn't need to be secretive, because
even if our competitors had known we were using Lisp, they wouldn't
have understood why:  "If they were that smart they'd already be
programming in Lisp."[3] All languages are equally powerful in the sense of being Turing
equivalent, but that's not the sense of the word programmers care
about. (No one wants to program a Turing machine.)  The kind of
power programmers care about may not be formally definable, but
one way to explain it would be to say that it refers to features
you could only get in the less powerful language by writing an
interpreter for the more powerful language in it. If language A
has an operator for removing spaces from strings and language B
doesn't, that probably doesn't make A more powerful, because you
can probably write a subroutine to do it in B.  But if A supports,
say, recursion, and B doesn't, that's not likely to be something
you can fix by writing library functions.[4] Note to nerds: or possibly a lattice, narrowing toward the top;
it's not the shape that matters here but the idea that there is at
least a partial order.[5] It is a bit misleading to treat macros as a separate feature.
In practice their usefulness is greatly enhanced by other Lisp
features like lexical closures and rest parameters.[6] As a result, comparisons of programming languages either take
the form of religious wars or undergraduate textbooks so determinedly
neutral that they're really works of anthropology.  People who
value their peace, or want tenure, avoid the topic.  But the question
is only half a religious one; there is something there worth
studying, especially if you want to design new languages.
</file>

<file path="cookbook/data/PaulGrahamEssaysLarge/before.txt">
Want to start a startup?  Get funded by
Y Combinator.




October 2014(This essay is derived from a guest lecture in Sam Altman's startup class at
Stanford.  It's intended for college students, but much of it is
applicable to potential founders at other ages.)One of the advantages of having kids is that when you have to give
advice, you can ask yourself "what would I tell my own kids?"  My
kids are little, but I can imagine what I'd tell them about startups
if they were in college, and that's what I'm going to tell you.Startups are very counterintuitive.  I'm not sure why.  Maybe it's
just because knowledge about them hasn't permeated our culture yet.
But whatever the reason, starting a startup is a task where you
can't always trust your instincts.It's like skiing in that way.  When you first try skiing and you
want to slow down, your instinct is to lean back.  But if you lean
back on skis you fly down the hill out of control.  So part of
learning to ski is learning to suppress that impulse.  Eventually
you get new habits, but at first it takes a conscious effort.  At
first there's a list of things you're trying to remember as you
start down the hill.Startups are as unnatural as skiing, so there's a similar list for
startups. Here I'm going to give you the first part of it â€” the things
to remember if you want to prepare yourself to start a startup.
CounterintuitiveThe first item on it is the fact I already mentioned: that startups
are so weird that if you trust your instincts, you'll make a lot
of mistakes.  If you know nothing more than this, you may at least
pause before making them.When I was running Y Combinator I used to joke that our function
was to tell founders things they would ignore.  It's really true.
Batch after batch, the YC partners warn founders about mistakes
they're about to make, and the founders ignore them, and then come
back a year later and say "I wish we'd listened."Why do the founders ignore the partners' advice?  Well, that's the
thing about counterintuitive ideas: they contradict your intuitions.
They seem wrong.  So of course your first impulse is to disregard
them.  And in fact my joking description is not merely the curse
of Y Combinator but part of its raison d'etre. If founders' instincts
already gave them the right answers, they wouldn't need us.  You
only need other people to give you advice that surprises you. That's
why there are a lot of ski instructors and not many running
instructors.
[1]You can, however, trust your instincts about people.  And in fact
one of the most common mistakes young founders make is not to
do that enough.  They get involved with people who seem impressive,
but about whom they feel some misgivings personally.  Later when
things blow up they say "I knew there was something off about him,
but I ignored it because he seemed so impressive."If you're thinking about getting involved with someone â€” as a
cofounder, an employee, an investor, or an acquirer â€” and you
have misgivings about them, trust your gut.  If someone seems
slippery, or bogus, or a jerk, don't ignore it.This is one case where it pays to be self-indulgent. Work with
people you genuinely like, and you've known long enough to be sure.
ExpertiseThe second counterintuitive point is that it's not that important
to know a lot about startups.  The way to succeed in a startup is
not to be an expert on startups, but to be an expert on your users
and the problem you're solving for them.
Mark Zuckerberg didn't succeed because he was an expert on startups.
He succeeded despite being a complete noob at startups, because he
understood his users really well.If you don't know anything about, say, how to raise an angel round,
don't feel bad on that account.  That sort of thing you can learn
when you need to, and forget after you've done it.In fact, I worry it's not merely unnecessary to learn in great
detail about the mechanics of startups, but possibly somewhat
dangerous.  If I met an undergrad who knew all about convertible
notes and employee agreements and (God forbid) class FF stock, I
wouldn't think "here is someone who is way ahead of their peers."
It would set off alarms.  Because another of the characteristic
mistakes of young founders is to go through the motions of starting
a startup.  They make up some plausible-sounding idea, raise money
at a good valuation, rent a cool office, hire a bunch of people.
From the outside that seems like what startups do.  But the next
step after rent a cool office and hire a bunch of people is: gradually
realize how completely fucked they are, because while imitating all
the outward forms of a startup they have neglected the one thing
that's actually essential: making something people want.
GameWe saw this happen so often that we made up a name for it: playing
house.  Eventually I realized why it was happening.  The reason
young founders go through the motions of starting a startup is
because that's what they've been trained to do for their whole lives
up to that point.  Think about what you have to do to get into
college, for example.  Extracurricular activities, check.  Even in
college classes most of the work is as artificial as running laps.I'm not attacking the educational system for being this way. There
will always be a certain amount of fakeness in the work you do when
you're being taught something, and if you measure their performance
it's inevitable that people will exploit the difference to the point
where much of what you're measuring is artifacts of the fakeness.I confess I did it myself in college. I found that in a lot of
classes there might only be 20 or 30 ideas that were the right shape
to make good exam questions.  The way I studied for exams in these
classes was not (except incidentally) to master the material taught
in the class, but to make a list of potential exam questions and
work out the answers in advance. When I walked into the final, the
main thing I'd be feeling was curiosity about which of my questions
would turn up on the exam.  It was like a game.It's not surprising that after being trained for their whole lives
to play such games, young founders' first impulse on starting a
startup is to try to figure out the tricks for winning at this new
game. Since fundraising appears to be the measure of success for
startups (another classic noob mistake), they always want to know what the
tricks are for convincing investors.  We tell them the best way to
convince investors is to make a startup
that's actually doing well, meaning growing fast, and then simply
tell investors so.  Then they want to know what the tricks are for
growing fast.  And we have to tell them the best way to do that is
simply to make something people want.So many of the conversations YC partners have with young founders
begin with the founder asking "How do we..." and the partner replying
"Just..."Why do the founders always make things so complicated?  The reason,
I realized, is that they're looking for the trick.So this is the third counterintuitive thing to remember about
startups: starting a startup is where gaming the system stops
working.  Gaming the system may continue to work if you go to work
for a big company. Depending on how broken the company is, you can
succeed by sucking up to the right people, giving the impression
of productivity, and so on. 
[2]
But that doesn't work with startups.
There is no boss to trick, only users, and all users care about is
whether your product does what they want. Startups are as impersonal
as physics.  You have to make something people want, and you prosper
only to the extent you do.The dangerous thing is, faking does work to some degree on investors.
If you're super good at sounding like you know what you're talking
about, you can fool investors for at least one and perhaps even two
rounds of funding.  But it's not in your interest to.  The company
is ultimately doomed.  All you're doing is wasting your own time
riding it down.So stop looking for the trick. There are tricks in startups, as
there are in any domain, but they are an order of magnitude less
important than solving the real problem. A founder who knows nothing
about fundraising but has made something users love will have an
easier time raising money than one who knows every trick in the
book but has a flat usage graph. And more importantly, the founder
who has made something users love is the one who will go on to
succeed after raising the money.Though in a sense it's bad news in that you're deprived of one of
your most powerful weapons, I think it's exciting that gaming the
system stops working when you start a startup.  It's exciting that
there even exist parts of the world where you win by doing good
work.  Imagine how depressing the world would be if it were all
like school and big companies, where you either have to spend a lot
of time on bullshit things or lose to people who do.
[3]
I would
have been delighted if I'd realized in college that there were parts
of the real world where gaming the system mattered less than others,
and a few where it hardly mattered at all.  But there are, and this
variation is one of the most important things to consider when
you're thinking about your future.  How do you win in each type of
work, and what would you like to win by doing?
[4]
All-ConsumingThat brings us to our fourth counterintuitive point: startups are
all-consuming.  If you start a startup, it will take over your life
to a degree you cannot imagine.  And if your startup succeeds, it
will take over your life for a long time: for several years at the
very least, maybe for a decade, maybe for the rest of your working
life.  So there is a real opportunity cost here.Larry Page may seem to have an enviable life, but there are aspects
of it that are unenviable.  Basically at 25 he started running as
fast as he could and it must seem to him that he hasn't stopped to
catch his breath since.  Every day new shit happens in the Google
empire that only the CEO can deal with, and he, as CEO, has to deal
with it.  If he goes on vacation for even a week, a whole week's
backlog of shit accumulates.  And he has to bear this uncomplainingly,
partly because as the company's daddy he can never show fear or
weakness, and partly because billionaires get less than zero sympathy
if they talk about having difficult lives.  Which has the strange
side effect that the difficulty of being a successful startup founder
is concealed from almost everyone except those who've done it.Y Combinator has now funded several companies that can be called
big successes, and in every single case the founders say the same
thing.  It never gets any easier.  The nature of the problems change.
You're worrying about construction delays at your London office
instead of the broken air conditioner in your studio apartment.
But the total volume of worry never decreases; if anything it
increases.Starting a successful startup is similar to having kids in that
it's like a button you push that changes your life irrevocably.
And while it's truly wonderful having kids, there are a lot of
things that are easier to do before you have them than after.  Many
of which will make you a better parent when you do have kids. And
since you can delay pushing the button for a while, most people in
rich countries do.Yet when it comes to startups, a lot of people seem to think they're
supposed to start them while they're still in college.  Are you
crazy?  And what are the universities thinking?  They go out of
their way to ensure their students are well supplied with contraceptives,
and yet they're setting up entrepreneurship programs and startup
incubators left and right.To be fair, the universities have their hand forced here.  A lot
of incoming students are interested in startups.  Universities are,
at least de facto, expected to prepare them for their careers.  So
students who want to start startups hope universities can teach
them about startups.  And whether universities can do this or not,
there's some pressure to claim they can, lest they lose applicants
to other universities that do.Can universities teach students about startups?  Yes and no.  They
can teach students about startups, but as I explained before, this
is not what you need to know.  What you need to learn about are the
needs of your own users, and you can't do that until you actually
start the company.
[5]
So starting a startup is intrinsically
something you can only really learn by doing it.  And it's impossible
to do that in college, for the reason I just explained: startups
take over your life.  You can't start a startup for real as a
student, because if you start a startup for real you're not a student
anymore. You may be nominally a student for a bit, but you won't even
be that for long.
[6]Given this dichotomy, which of the two paths should you take?  Be
a real student and not start a startup, or start a real startup and
not be a student?  I can answer that one for you. Do not start a
startup in college.  How to start a startup is just a subset of a
bigger problem you're trying to solve: how to have a good life.
And though starting a startup can be part of a good life for a lot
of ambitious people, age 20 is not the optimal time to do it.
Starting a startup is like a brutally fast depth-first search.  Most
people should still be searching breadth-first at 20.You can do things in your early 20s that you can't do as well before
or after, like plunge deeply into projects on a whim and travel
super cheaply with no sense of a deadline.  For unambitious people,
this sort of thing is the dreaded "failure to launch," but for the
ambitious ones it can be an incomparably valuable sort of exploration.
If you start a startup at 20 and you're sufficiently successful,
you'll never get to do it.
[7]Mark Zuckerberg will never get to bum around a foreign country.  He
can do other things most people can't, like charter jets to fly him
to foreign countries. But success has taken a lot of the serendipity
out of his life. Facebook is running him as much as he's running
Facebook. And while it can be very cool to be in the grip of a
project you consider your life's work, there are advantages to
serendipity too, especially early in life.  Among other things it
gives you more options to choose your life's work from.There's not even a tradeoff here. You're not sacrificing anything
if you forgo starting a startup at 20, because you're more likely
to succeed if you wait.  In the unlikely case that you're 20 and
one of your side projects takes off like Facebook did, you'll face
a choice of running with it or not, and it may be reasonable to run
with it.  But the usual way startups take off is for the founders
to make them take off, and it's gratuitously
stupid to do that at 20.
TryShould you do it at any age?  I realize I've made startups sound
pretty hard.  If I haven't, let me try again: starting a startup
is really hard.  What if it's too hard?  How can you tell if you're
up to this challenge?The answer is the fifth counterintuitive point: you can't tell. Your
life so far may have given you some idea what your prospects might
be if you tried to become a mathematician, or a professional football
player.  But unless you've had a very strange life you haven't done
much that was like being a startup founder.
Starting a startup will change you a lot.  So what you're trying
to estimate is not just what you are, but what you could grow into,
and who can do that?For the past 9 years it was my job to predict whether people would
have what it took to start successful startups.  It was easy to
tell how smart they were, and most people reading this will be over
that threshold.  The hard part was predicting how tough and ambitious they would become.  There
may be no one who has more experience at trying to predict that,
so I can tell you how much an expert can know about it, and the
answer is: not much.  I learned to keep a completely open mind about
which of the startups in each batch would turn out to be the stars.The founders sometimes think they know. Some arrive feeling sure
they will ace Y Combinator just as they've aced every one of the (few,
artificial, easy) tests they've faced in life so far.  Others arrive
wondering how they got in, and hoping YC doesn't discover whatever
mistake caused it to accept them.  But there is little correlation
between founders' initial attitudes and how well their companies
do.I've read that the same is true in the military â€” that the
swaggering recruits are no more likely to turn out to be really
tough than the quiet ones. And probably for the same reason: that
the tests involved are so different from the ones in their previous
lives.If you're absolutely terrified of starting a startup, you probably
shouldn't do it.  But if you're merely unsure whether you're up to
it, the only way to find out is to try.  Just not now.
IdeasSo if you want to start a startup one day, what should you do in
college?  There are only two things you need initially: an idea and
cofounders.  And the m.o. for getting both is the same.  Which leads
to our sixth and last counterintuitive point: that the way to get
startup ideas is not to try to think of startup ideas.I've written a whole essay on this,
so I won't repeat it all here.  But the short version is that if
you make a conscious effort to think of startup ideas, the ideas
you come up with will not merely be bad, but bad and plausible-sounding,
meaning you'll waste a lot of time on them before realizing they're
bad.The way to come up with good startup ideas is to take a step back.
Instead of making a conscious effort to think of startup ideas,
turn your mind into the type that startup ideas form in without any
conscious effort.  In fact, so unconsciously that you don't even
realize at first that they're startup ideas.This is not only possible, it's how Apple, Yahoo, Google, and
Facebook all got started.  None of these companies were even meant
to be companies at first.  They were all just side projects.  The
best startups almost have to start as side projects, because great
ideas tend to be such outliers that your conscious mind would reject
them as ideas for companies.Ok, so how do you turn your mind into the type that startup ideas
form in unconsciously?  (1) Learn a lot about things that matter,
then (2) work on problems that interest you (3) with people you
like and respect.  The third part, incidentally, is how you get
cofounders at the same time as the idea.The first time I wrote that paragraph, instead of "learn a lot about
things that matter," I wrote "become good at some technology." But
that prescription, though sufficient, is too narrow.  What was
special about Brian Chesky and Joe Gebbia was not that they were
experts in technology.  They were good at design, and perhaps even
more importantly, they were good at organizing groups and making
projects happen.  So you don't have to work on technology per se,
so long as you work on problems demanding enough to stretch you.What kind of problems are those?  That is very hard to answer in
the general case.  History is full of examples of young people who
were working on important problems that no
one else at the time thought were important, and in particular
that their parents didn't think were important.  On the other hand,
history is even fuller of examples of parents who thought their
kids were wasting their time and who were right.  So how do you
know when you're working on real stuff?
[8]I know how I know.  Real problems are interesting, and I am
self-indulgent in the sense that I always want to work on interesting
things, even if no one else cares about them (in fact, especially
if no one else cares about them), and find it very hard to make
myself work on boring things, even if they're supposed to be
important.My life is full of case after case where I worked on something just
because it seemed interesting, and it turned out later to be useful
in some worldly way.  Y
Combinator itself was something I only did because it seemed
interesting. So I seem to have some sort of internal compass that
helps me out.  But I don't know what other people have in their
heads. Maybe if I think more about this I can come up with heuristics
for recognizing genuinely interesting problems, but for the moment
the best I can offer is the hopelessly question-begging advice that
if you have a taste for genuinely interesting problems, indulging
it energetically is the best way to prepare yourself for a startup.
And indeed, probably also the best way to live.
[9]But although I can't explain in the general case what counts as an
interesting problem, I can tell you about a large subset of them.
If you think of technology as something that's spreading like a
sort of fractal stain, every moving point on the edge represents
an interesting problem.  So one guaranteed way to turn your mind
into the type that has good startup ideas is to get yourself to the
leading edge of some technology â€” to cause yourself, as Paul
Buchheit put it, to "live in the future." When you reach that point,
ideas that will seem to other people uncannily prescient will seem
obvious to you.  You may not realize they're startup ideas, but
you'll know they're something that ought to exist.For example, back at Harvard in the mid 90s a fellow grad student
of my friends Robert and Trevor wrote his own voice over IP software.
He didn't mean it to be a startup, and he never tried to turn it
into one.  He just wanted to talk to his girlfriend in Taiwan without
paying for long distance calls, and since he was an expert on
networks it seemed obvious to him that the way to do it was turn
the sound into packets and ship it over the Internet. He never did
any more with his software than talk to his girlfriend, but this
is exactly the way the best startups get started.So strangely enough the optimal thing to do in college if you want
to be a successful startup founder is not some sort of new, vocational
version of college focused on "entrepreneurship." It's the classic
version of college as education for its own sake. If you want to
start a startup after college, what you should do in college is
learn powerful things.  And if you have genuine intellectual
curiosity, that's what you'll naturally tend to do if you just
follow your own inclinations.
[10]The component of entrepreneurship that really matters is domain
expertise.  The way to become Larry Page was to become an expert
on search. And the way to become an expert on search was to be
driven by genuine curiosity, not some ulterior motive.At its best, starting a startup is merely an ulterior motive for
curiosity.  And you'll do it best if you introduce the ulterior
motive toward the end of the process.So here is the ultimate advice for young would-be startup founders,
boiled down to two words: just learn.
Notes[1]
Some founders listen more than others, and this tends to be a
predictor of success. One of the things I
remember about the Airbnbs during YC is how intently they listened.[2]
In fact, this is one of the reasons startups are possible.  If
big companies weren't plagued by internal inefficiencies, they'd
be proportionately more effective, leaving less room for startups.[3]
In a startup you have to spend a lot of time on schleps, but this sort of work is merely
unglamorous, not bogus.[4]
What should you do if your true calling is gaming the system?
Management consulting.[5]
The company may not be incorporated, but if you start to get
significant numbers of users, you've started it, whether you realize
it yet or not.[6]
It shouldn't be that surprising that colleges can't teach
students how to be good startup founders, because they can't teach
them how to be good employees either.The way universities "teach" students how to be employees is to
hand off the task to companies via internship programs.  But you
couldn't do the equivalent thing for startups, because by definition
if the students did well they would never come back.[7]
Charles Darwin was 22 when he received an invitation to travel
aboard the HMS Beagle as a naturalist.  It was only because he was
otherwise unoccupied, to a degree that alarmed his family, that he
could accept it. And yet if he hadn't we probably would not know
his name.[8]
Parents can sometimes be especially conservative in this
department.  There are some whose definition of important problems
includes only those on the critical path to med school.[9]
I did manage to think of a heuristic for detecting whether you
have a taste for interesting ideas: whether you find known boring
ideas intolerable.  Could you endure studying literary theory, or
working in middle management at a large company?[10]
In fact, if your goal is to start a startup, you can stick
even more closely to the ideal of a liberal education than past
generations have. Back when students focused mainly on getting a
job after college, they thought at least a little about how the
courses they took might look to an employer.  And perhaps even
worse, they might shy away from taking a difficult class lest they
get a low grade, which would harm their all-important GPA.  Good
news: users don't care what your GPA
was.  And I've never heard of investors caring either.  Y Combinator
certainly never asks what classes you took in college or what grades
you got in them.
Thanks to Sam Altman, Paul Buchheit, John Collison, Patrick
Collison, Jessica Livingston, Robert Morris, Geoff Ralston, and
Fred Wilson for reading drafts of this.
</file>

<file path="cookbook/pocketflow-a2a/common/client/__init__.py">
from .client import A2AClient
from .card_resolver import A2ACardResolver

__all__ = ["A2AClient", "A2ACardResolver"]
</file>

<file path="cookbook/pocketflow-a2a/common/client/card_resolver.py">
import httpx
from common.types import (
    AgentCard,
    A2AClientJSONError,
)
import json


class A2ACardResolver:
    def __init__(self, base_url, agent_card_path="/.well-known/agent.json"):
        self.base_url = base_url.rstrip("/")
        self.agent_card_path = agent_card_path.lstrip("/")

    def get_agent_card(self) -> AgentCard:
        with httpx.Client() as client:
            response = client.get(self.base_url + "/" + self.agent_card_path)
            response.raise_for_status()
            try:
                return AgentCard(**response.json())
            except json.JSONDecodeError as e:
                raise A2AClientJSONError(str(e)) from e
</file>

<file path="cookbook/pocketflow-a2a/common/client/client.py">
import httpx
from httpx_sse import connect_sse
from typing import Any, AsyncIterable
from common.types import (
    AgentCard,
    GetTaskRequest,
    SendTaskRequest,
    SendTaskResponse,
    JSONRPCRequest,
    JSONRPCResponse,
    JSONRPCError,
    GetTaskResponse,
    CancelTaskResponse,
    CancelTaskRequest,
    SetTaskPushNotificationRequest,
    SetTaskPushNotificationResponse,
    GetTaskPushNotificationRequest,
    GetTaskPushNotificationResponse,
    A2AClientHTTPError,
    A2AClientJSONError,
    SendTaskStreamingRequest,
    SendTaskStreamingResponse,
    Task,
    TaskPushNotificationConfig,
    TaskStatusUpdateEvent,
    TaskArtifactUpdateEvent,
)
import json
import logging

# Configure a logger specific to the client
logger = logging.getLogger("A2AClient")

class A2AClientError(Exception):
    """Base class for A2A client errors"""
    def __init__(self, message):
        super().__init__(message)

class RpcError(Exception):
    code: int
    data: Any = None
    def __init__(self, code: int, message: str, data: Any = None):
        super().__init__(message)
        self.name = "RpcError"
        self.code = code
        self.data = data

class A2AClient:
    def __init__(self, agent_card: AgentCard = None, url: str = None):
        if agent_card:
            self.url = agent_card.url.rstrip("/")
        elif url:
            self.url = url.rstrip("/")
        else:
            raise ValueError("Must provide either agent_card or url")
        self.fetchImpl = httpx.AsyncClient(timeout=None)

    def _generateRequestId(self):
        import time
        return int(time.time() * 1000)

    async def _send_request(self, request: JSONRPCRequest) -> dict[str, Any]:
        req_id = request.id
        req_method = request.method
        req_dump = request.model_dump(exclude_none=True)

        logger.info(f"-> Sending Request (ID: {req_id}, Method: {req_method}):\n{json.dumps(req_dump, indent=2)}")

        try:
            response = await self.fetchImpl.post(
                self.url, json=req_dump, timeout=60.0
            )
            logger.info(f"<- Received HTTP Status {response.status_code} for Request (ID: {req_id})")
            response_text = await response.aread()
            logger.debug(f"Raw Response Body (ID: {req_id}):\n{response_text.decode('utf-8', errors='replace')}")

            response.raise_for_status()

            try:
                json_response = json.loads(response_text)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to decode JSON response (ID: {req_id}): {e}")
                raise A2AClientJSONError(f"Failed to decode JSON: {e}") from e

            if "error" in json_response and json_response["error"] is not None:
                rpc_error = json_response["error"]
                logger.warning(f"<- Received JSON-RPC Error (ID: {req_id}): Code={rpc_error.get('code')}, Msg='{rpc_error.get('message')}'")
                raise RpcError(rpc_error.get("code", -32000), rpc_error.get("message", "Unknown RPC Error"), rpc_error.get("data"))

            logger.info(f"<- Received Success Response (ID: {req_id}):\n{json.dumps(json_response, indent=2)}")
            return json_response

        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error for Request (ID: {req_id}): {e.response.status_code} - {e.request.url}")
            error_body = await e.response.aread()
            raise A2AClientHTTPError(e.response.status_code, f"{e}. Body: {error_body.decode('utf-8', errors='replace')}") from e
        except httpx.RequestError as e:
            logger.error(f"Request Error for (ID: {req_id}): {e}")
            raise A2AClientError(f"Network or request error: {e}") from e
        except RpcError:
             raise
        except Exception as e:
             logger.error(f"Unexpected error during request (ID: {req_id}): {e}", exc_info=True)
             raise A2AClientError(f"Unexpected error: {e}") from e

    async def send_task(self, payload: dict[str, Any]) -> SendTaskResponse:
        request = SendTaskRequest(params=payload)
        response_dict = await self._send_request(request)
        return SendTaskResponse(**response_dict)

    async def send_task_streaming(
        self, payload: dict[str, Any]
    ) -> AsyncIterable[SendTaskStreamingResponse]:
        request = SendTaskStreamingRequest(params=payload)
        req_id = request.id
        req_dump = request.model_dump(exclude_none=True)

        logger.info(f"-> Sending Streaming Request (ID: {req_id}, Method: {request.method}):\n{json.dumps(req_dump, indent=2)}")

        try:
            async with self.fetchImpl.stream("POST", self.url, json=req_dump, timeout=None) as response:
                 logger.info(f"<- Received HTTP Status {response.status_code} for Streaming Request (ID: {req_id})")
                 response.raise_for_status()

                 buffer = ""
                 async for line in response.aiter_lines():
                     if not line:
                         if buffer.startswith("data:"):
                             data_str = buffer[len("data:"):].strip()
                             logger.debug(f"Received SSE Data Line (ID: {req_id}): {data_str}")
                             try:
                                 sse_data_dict = json.loads(data_str)
                                 yield SendTaskStreamingResponse(**sse_data_dict)
                             except json.JSONDecodeError as e:
                                 logger.error(f"Failed to decode SSE JSON (ID: {req_id}): {e}. Data: '{data_str}'")
                             except Exception as e:
                                 logger.error(f"Error processing SSE data (ID: {req_id}): {e}. Data: '{data_str}'", exc_info=True)
                         elif buffer:
                             logger.debug(f"Received non-data SSE line (ID: {req_id}): {buffer}")
                         buffer = ""
                     else:
                         buffer += line + "\n"

                 if buffer:
                     logger.warning(f"SSE stream ended with partial data in buffer (ID: {req_id}): {buffer}")

                 logger.info(f"SSE Stream ended for request ID: {req_id}")

        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP Error during streaming connection (ID: {req_id}): {e.response.status_code} - {e.request.url}")
            error_body = await e.response.aread()
            raise A2AClientHTTPError(e.response.status_code, f"{e}. Body: {error_body.decode('utf-8', errors='replace')}") from e
        except httpx.RequestError as e:
             logger.error(f"Request Error during streaming (ID: {req_id}): {e}")
             raise A2AClientError(f"Network or request error during streaming: {e}") from e
        except Exception as e:
            logger.error(f"Unexpected error during streaming (ID: {req_id}): {e}", exc_info=True)
            raise A2AClientError(f"Unexpected streaming error: {e}") from e

    async def get_task(self, payload: dict[str, Any]) -> GetTaskResponse:
        request = GetTaskRequest(params=payload)
        response_dict = await self._send_request(request)
        return GetTaskResponse(**response_dict)

    async def cancel_task(self, payload: dict[str, Any]) -> CancelTaskResponse:
        request = CancelTaskRequest(params=payload)
        response_dict = await self._send_request(request)
        return CancelTaskResponse(**response_dict)

    async def set_task_callback(
        self, payload: dict[str, Any]
    ) -> SetTaskPushNotificationResponse:
        request = SetTaskPushNotificationRequest(params=payload)
        response_dict = await self._send_request(request)
        return SetTaskPushNotificationResponse(**response_dict)

    async def get_task_callback(
        self, payload: dict[str, Any]
    ) -> GetTaskPushNotificationResponse:
        request = GetTaskPushNotificationRequest(params=payload)
        response_dict = await self._send_request(request)
        return GetTaskPushNotificationResponse(**response_dict)
</file>

<file path="cookbook/pocketflow-a2a/common/server/__init__.py">
from .server import A2AServer
from .task_manager import TaskManager, InMemoryTaskManager

__all__ = ["A2AServer", "TaskManager", "InMemoryTaskManager"]
</file>

<file path="cookbook/pocketflow-a2a/common/server/server.py">
from starlette.applications import Starlette
from starlette.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from starlette.requests import Request
from common.types import (
    A2ARequest,
    JSONRPCResponse,
    InvalidRequestError,
    JSONParseError,
    GetTaskRequest,
    CancelTaskRequest,
    SendTaskRequest,
    SetTaskPushNotificationRequest,
    GetTaskPushNotificationRequest,
    InternalError,
    AgentCard,
    TaskResubscriptionRequest,
    SendTaskStreamingRequest,
    Message,
)
from pydantic import ValidationError
import json
from typing import AsyncIterable, Any
from common.server.task_manager import TaskManager

import logging

# Configure a logger specific to the server
logger = logging.getLogger("A2AServer")


class A2AServer:
    def __init__(
        self,
        host="0.0.0.0",
        port=5000,
        endpoint="/",
        agent_card: AgentCard = None,
        task_manager: TaskManager = None,
    ):
        self.host = host
        self.port = port
        self.endpoint = endpoint
        self.task_manager = task_manager
        self.agent_card = agent_card
        self.app = Starlette()
        self.app.add_route(self.endpoint, self._process_request, methods=["POST"])
        self.app.add_route(
            "/.well-known/agent.json", self._get_agent_card, methods=["GET"]
        )

    def start(self):
        if self.agent_card is None:
            raise ValueError("agent_card is not defined")

        if self.task_manager is None:
            raise ValueError("request_handler is not defined")

        import uvicorn

        # Basic logging config moved to __main__.py for application-level control
        uvicorn.run(self.app, host=self.host, port=self.port)

    def _get_agent_card(self, request: Request) -> JSONResponse:
        logger.info("Serving Agent Card request")
        return JSONResponse(self.agent_card.model_dump(exclude_none=True))

    async def _process_request(self, request: Request):
        request_id_for_log = "N/A"  # Default if parsing fails early
        raw_body = b""
        try:
            # Log raw body first
            raw_body = await request.body()
            body = json.loads(raw_body)  # Attempt parsing
            request_id_for_log = body.get("id", "N/A")  # Get ID if possible
            logger.info(f"<- Received Request (ID: {request_id_for_log}):\n{json.dumps(body, indent=2)}")

            json_rpc_request = A2ARequest.validate_python(body)

            # Route based on method (same as before)
            if isinstance(json_rpc_request, GetTaskRequest):
                result = await self.task_manager.on_get_task(json_rpc_request)
            elif isinstance(json_rpc_request, SendTaskRequest):
                result = await self.task_manager.on_send_task(json_rpc_request)
            elif isinstance(json_rpc_request, SendTaskStreamingRequest):
                result = await self.task_manager.on_send_task_subscribe(
                    json_rpc_request
                )
            elif isinstance(json_rpc_request, CancelTaskRequest):
                result = await self.task_manager.on_cancel_task(json_rpc_request)
            elif isinstance(json_rpc_request, SetTaskPushNotificationRequest):
                result = await self.task_manager.on_set_task_push_notification(json_rpc_request)
            elif isinstance(json_rpc_request, GetTaskPushNotificationRequest):
                result = await self.task_manager.on_get_task_push_notification(json_rpc_request)
            elif isinstance(json_rpc_request, TaskResubscriptionRequest):
                result = await self.task_manager.on_resubscribe_to_task(
                    json_rpc_request
                )
            else:
                logger.warning(f"Unexpected request type: {type(json_rpc_request)}")
                raise ValueError(f"Unexpected request type: {type(request)}")

            return self._create_response(result)  # Pass result to response creation

        except json.decoder.JSONDecodeError as e:
            logger.error(f"JSON Parse Error for Request body: <<<{raw_body.decode('utf-8', errors='replace')}>>>\nError: {e}")
            return self._handle_exception(e, request_id_for_log)  # Pass ID if known
        except ValidationError as e:
             logger.error(f"Request Validation Error (ID: {request_id_for_log}): {e.json()}")
             return self._handle_exception(e, request_id_for_log)
        except Exception as e:
             logger.error(f"Unhandled Exception processing request (ID: {request_id_for_log}): {e}", exc_info=True)
             return self._handle_exception(e, request_id_for_log)  # Pass ID if known

    def _handle_exception(self, e: Exception, req_id=None) -> JSONResponse:  # Accept req_id
        if isinstance(e, json.decoder.JSONDecodeError):
            json_rpc_error = JSONParseError()
        elif isinstance(e, ValidationError):
            json_rpc_error = InvalidRequestError(data=json.loads(e.json()))
        else:
            # Log the full exception details
            logger.error(f"Internal Server Error (ReqID: {req_id}): {e}", exc_info=True)
            json_rpc_error = InternalError(message=f"Internal Server Error: {type(e).__name__}")

        response = JSONRPCResponse(id=req_id, error=json_rpc_error)
        response_dump = response.model_dump(exclude_none=True)
        logger.info(f"-> Sending Error Response (ReqID: {req_id}):\n{json.dumps(response_dump, indent=2)}")
        # A2A errors are still sent with HTTP 200
        return JSONResponse(response_dump, status_code=200)

    def _create_response(self, result: Any) -> JSONResponse | EventSourceResponse:
        if isinstance(result, AsyncIterable):
            # Streaming response
            async def event_generator(result_stream) -> AsyncIterable[dict[str, str]]:
                stream_request_id = None  # Capture ID from the first event if possible
                try:
                    async for item in result_stream:
                        # Log each streamed item
                        response_json = item.model_dump_json(exclude_none=True)
                        stream_request_id = item.id  # Update ID
                        logger.info(f"-> Sending SSE Event (ID: {stream_request_id}):\n{json.dumps(json.loads(response_json), indent=2)}")
                        yield {"data": response_json}
                    logger.info(f"SSE Stream ended for request ID: {stream_request_id}")
                except Exception as e:
                    logger.error(f"Error during SSE generation (ReqID: {stream_request_id}): {e}", exc_info=True)
                    # Optionally yield an error event if the protocol allows/requires it
                    # error_payload = JSONRPCResponse(id=stream_request_id, error=InternalError(message=f"SSE Error: {e}"))
                    # yield {"data": error_payload.model_dump_json(exclude_none=True)}

            logger.info("Starting SSE stream...")  # Log stream start
            return EventSourceResponse(event_generator(result))
        elif isinstance(result, JSONRPCResponse):
            # Standard JSON response
            response_dump = result.model_dump(exclude_none=True)
            log_id = result.id if result.id is not None else "N/A (Notification?)"
            log_prefix = "->"
            log_type = "Response"
            if result.error:
                 log_prefix = "-> Sending Error"
                 log_type = "Error Response"

            logger.info(f"{log_prefix} {log_type} (ID: {log_id}):\n{json.dumps(response_dump, indent=2)}")
            return JSONResponse(response_dump)
        else:
            # This should ideally not happen if task manager returns correctly
            logger.error(f"Task manager returned unexpected type: {type(result)}")
            err_resp = JSONRPCResponse(id=None, error=InternalError(message="Invalid internal response type"))
            return JSONResponse(err_resp.model_dump(exclude_none=True), status_code=500)
</file>

<file path="cookbook/pocketflow-a2a/common/server/task_manager.py">
from abc import ABC, abstractmethod
from typing import Union, AsyncIterable, List
from common.types import Task
from common.types import (
    JSONRPCResponse,
    TaskIdParams,
    TaskQueryParams,
    GetTaskRequest,
    TaskNotFoundError,
    SendTaskRequest,
    CancelTaskRequest,
    TaskNotCancelableError,
    SetTaskPushNotificationRequest,
    GetTaskPushNotificationRequest,
    GetTaskResponse,
    CancelTaskResponse,
    SendTaskResponse,
    SetTaskPushNotificationResponse,
    GetTaskPushNotificationResponse,
    PushNotificationNotSupportedError,
    TaskSendParams,
    TaskStatus,
    TaskState,
    TaskResubscriptionRequest,
    SendTaskStreamingRequest,
    SendTaskStreamingResponse,
    Artifact,
    PushNotificationConfig,
    TaskStatusUpdateEvent,
    JSONRPCError,
    TaskPushNotificationConfig,
    InternalError,
)
from common.server.utils import new_not_implemented_error
import asyncio
import logging

logger = logging.getLogger(__name__)

class TaskManager(ABC):
    @abstractmethod
    async def on_get_task(self, request: GetTaskRequest) -> GetTaskResponse:
        pass

    @abstractmethod
    async def on_cancel_task(self, request: CancelTaskRequest) -> CancelTaskResponse:
        pass

    @abstractmethod
    async def on_send_task(self, request: SendTaskRequest) -> SendTaskResponse:
        pass

    @abstractmethod
    async def on_send_task_subscribe(
        self, request: SendTaskStreamingRequest
    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:
        pass

    @abstractmethod
    async def on_set_task_push_notification(
        self, request: SetTaskPushNotificationRequest
    ) -> SetTaskPushNotificationResponse:
        pass

    @abstractmethod
    async def on_get_task_push_notification(
        self, request: GetTaskPushNotificationRequest
    ) -> GetTaskPushNotificationResponse:
        pass

    @abstractmethod
    async def on_resubscribe_to_task(
        self, request: TaskResubscriptionRequest
    ) -> Union[AsyncIterable[SendTaskResponse], JSONRPCResponse]:
        pass


class InMemoryTaskManager(TaskManager):
    def __init__(self):
        self.tasks: dict[str, Task] = {}
        self.push_notification_infos: dict[str, PushNotificationConfig] = {}
        self.lock = asyncio.Lock()
        self.task_sse_subscribers: dict[str, List[asyncio.Queue]] = {}
        self.subscriber_lock = asyncio.Lock()

    async def on_get_task(self, request: GetTaskRequest) -> GetTaskResponse:
        logger.info(f"Getting task {request.params.id}")
        task_query_params: TaskQueryParams = request.params

        async with self.lock:
            task = self.tasks.get(task_query_params.id)
            if task is None:
                return GetTaskResponse(id=request.id, error=TaskNotFoundError())

            task_result = self.append_task_history(
                task, task_query_params.historyLength
            )

        return GetTaskResponse(id=request.id, result=task_result)

    async def on_cancel_task(self, request: CancelTaskRequest) -> CancelTaskResponse:
        logger.info(f"Cancelling task {request.params.id}")
        task_id_params: TaskIdParams = request.params

        async with self.lock:
            task = self.tasks.get(task_id_params.id)
            if task is None:
                return CancelTaskResponse(id=request.id, error=TaskNotFoundError())

        return CancelTaskResponse(id=request.id, error=TaskNotCancelableError())

    @abstractmethod
    async def on_send_task(self, request: SendTaskRequest) -> SendTaskResponse:
        pass

    @abstractmethod
    async def on_send_task_subscribe(
        self, request: SendTaskStreamingRequest
    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:
        pass

    async def set_push_notification_info(self, task_id: str, notification_config: PushNotificationConfig):
        async with self.lock:
            task = self.tasks.get(task_id)
            if task is None:
                raise ValueError(f"Task not found for {task_id}")

            self.push_notification_infos[task_id] = notification_config

        return
    
    async def get_push_notification_info(self, task_id: str) -> PushNotificationConfig:
        async with self.lock:
            task = self.tasks.get(task_id)
            if task is None:
                raise ValueError(f"Task not found for {task_id}")

            return self.push_notification_infos[task_id]
            
        return
    
    async def has_push_notification_info(self, task_id: str) -> bool:
        async with self.lock:
            return task_id in self.push_notification_infos
            

    async def on_set_task_push_notification(
        self, request: SetTaskPushNotificationRequest
    ) -> SetTaskPushNotificationResponse:
        logger.info(f"Setting task push notification {request.params.id}")
        task_notification_params: TaskPushNotificationConfig = request.params

        try:
            await self.set_push_notification_info(task_notification_params.id, task_notification_params.pushNotificationConfig)
        except Exception as e:
            logger.error(f"Error while setting push notification info: {e}")
            return JSONRPCResponse(
                id=request.id,
                error=InternalError(
                    message="An error occurred while setting push notification info"
                ),
            )
            
        return SetTaskPushNotificationResponse(id=request.id, result=task_notification_params)

    async def on_get_task_push_notification(
        self, request: GetTaskPushNotificationRequest
    ) -> GetTaskPushNotificationResponse:
        logger.info(f"Getting task push notification {request.params.id}")
        task_params: TaskIdParams = request.params

        try:
            notification_info = await self.get_push_notification_info(task_params.id)
        except Exception as e:
            logger.error(f"Error while getting push notification info: {e}")
            return GetTaskPushNotificationResponse(
                id=request.id,
                error=InternalError(
                    message="An error occurred while getting push notification info"
                ),
            )
        
        return GetTaskPushNotificationResponse(id=request.id, result=TaskPushNotificationConfig(id=task_params.id, pushNotificationConfig=notification_info))

    async def upsert_task(self, task_send_params: TaskSendParams) -> Task:
        logger.info(f"Upserting task {task_send_params.id}")
        async with self.lock:
            task = self.tasks.get(task_send_params.id)
            if task is None:
                task = Task(
                    id=task_send_params.id,
                    sessionId = task_send_params.sessionId,
                    messages=[task_send_params.message],
                    status=TaskStatus(state=TaskState.SUBMITTED),
                    history=[task_send_params.message],
                )
                self.tasks[task_send_params.id] = task
            else:
                task.history.append(task_send_params.message)

            return task

    async def on_resubscribe_to_task(
        self, request: TaskResubscriptionRequest
    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:
        return new_not_implemented_error(request.id)

    async def update_store(
        self, task_id: str, status: TaskStatus, artifacts: list[Artifact]
    ) -> Task:
        async with self.lock:
            try:
                task = self.tasks[task_id]
            except KeyError:
                logger.error(f"Task {task_id} not found for updating the task")
                raise ValueError(f"Task {task_id} not found")

            task.status = status

            if status.message is not None:
                task.history.append(status.message)

            if artifacts is not None:
                if task.artifacts is None:
                    task.artifacts = []
                task.artifacts.extend(artifacts)

            return task

    def append_task_history(self, task: Task, historyLength: int | None):
        new_task = task.model_copy()
        if historyLength is not None and historyLength > 0:
            new_task.history = new_task.history[-historyLength:]
        else:
            new_task.history = []

        return new_task        

    async def setup_sse_consumer(self, task_id: str, is_resubscribe: bool = False):
        async with self.subscriber_lock:
            if task_id not in self.task_sse_subscribers:
                if is_resubscribe:
                    raise ValueError("Task not found for resubscription")
                else:
                    self.task_sse_subscribers[task_id] = []

            sse_event_queue = asyncio.Queue(maxsize=0) # <=0 is unlimited
            self.task_sse_subscribers[task_id].append(sse_event_queue)
            return sse_event_queue

    async def enqueue_events_for_sse(self, task_id, task_update_event):
        async with self.subscriber_lock:
            if task_id not in self.task_sse_subscribers:
                return

            current_subscribers = self.task_sse_subscribers[task_id]
            for subscriber in current_subscribers:
                await subscriber.put(task_update_event)

    async def dequeue_events_for_sse(
        self, request_id, task_id, sse_event_queue: asyncio.Queue
    ) -> AsyncIterable[SendTaskStreamingResponse] | JSONRPCResponse:
        try:
            while True:                
                event = await sse_event_queue.get()
                if isinstance(event, JSONRPCError):
                    yield SendTaskStreamingResponse(id=request_id, error=event)
                    break
                                                
                yield SendTaskStreamingResponse(id=request_id, result=event)
                if isinstance(event, TaskStatusUpdateEvent) and event.final:
                    break
        finally:
            async with self.subscriber_lock:
                if task_id in self.task_sse_subscribers:
                    self.task_sse_subscribers[task_id].remove(sse_event_queue)
</file>

<file path="cookbook/pocketflow-a2a/common/server/utils.py">
from common.types import (
    JSONRPCResponse,
    ContentTypeNotSupportedError,
    UnsupportedOperationError,
)
from typing import List


def are_modalities_compatible(
    server_output_modes: List[str], client_output_modes: List[str]
):
    """Modalities are compatible if they are both non-empty
    and there is at least one common element."""
    if client_output_modes is None or len(client_output_modes) == 0:
        return True

    if server_output_modes is None or len(server_output_modes) == 0:
        return True

    return any(x in server_output_modes for x in client_output_modes)


def new_incompatible_types_error(request_id):
    return JSONRPCResponse(id=request_id, error=ContentTypeNotSupportedError())


def new_not_implemented_error(request_id):
    return JSONRPCResponse(id=request_id, error=UnsupportedOperationError())
</file>

<file path="cookbook/pocketflow-a2a/common/utils/in_memory_cache.py">
"""In Memory Cache utility."""

import threading
import time
from typing import Any, Dict, Optional


class InMemoryCache:
    """A thread-safe Singleton class to manage cache data.

    Ensures only one instance of the cache exists across the application.
    """

    _instance: Optional["InMemoryCache"] = None
    _lock: threading.Lock = threading.Lock()
    _initialized: bool = False

    def __new__(cls):
        """Override __new__ to control instance creation (Singleton pattern).

        Uses a lock to ensure thread safety during the first instantiation.

        Returns:
            The singleton instance of InMemoryCache.
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """Initialize the cache storage.

        Uses a flag (_initialized) to ensure this logic runs only on the very first
        creation of the singleton instance.
        """
        if not self._initialized:
            with self._lock:
                if not self._initialized:
                    # print("Initializing SessionCache storage")
                    self._cache_data: Dict[str, Dict[str, Any]] = {}
                    self._ttl: Dict[str, float] = {}
                    self._data_lock: threading.Lock = threading.Lock()
                    self._initialized = True

    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """Set a key-value pair.

        Args:
            key: The key for the data.
            value: The data to store.
            ttl: Time to live in seconds. If None, data will not expire.
        """
        with self._data_lock:
            self._cache_data[key] = value

            if ttl is not None:
                self._ttl[key] = time.time() + ttl
            else:
                if key in self._ttl:
                    del self._ttl[key]

    def get(self, key: str, default: Any = None) -> Any:
        """Get the value associated with a key.

        Args:
            key: The key for the data within the session.
            default: The value to return if the session or key is not found.

        Returns:
            The cached value, or the default value if not found.
        """
        with self._data_lock:
            if key in self._ttl and time.time() > self._ttl[key]:
                del self._cache_data[key]
                del self._ttl[key]
                return default
            return self._cache_data.get(key, default)

    def delete(self, key: str) -> None:
        """Delete a specific key-value pair from a cache.

        Args:
            key: The key to delete.

        Returns:
            True if the key was found and deleted, False otherwise.
        """

        with self._data_lock:
            if key in self._cache_data:
                del self._cache_data[key]
                if key in self._ttl:
                    del self._ttl[key]
                return True
            return False

    def clear(self) -> bool:
        """Remove all data.

        Returns:
            True if the data was cleared, False otherwise.
        """
        with self._data_lock:
            self._cache_data.clear()
            self._ttl.clear()
            return True
        return False
</file>

<file path="cookbook/pocketflow-a2a/common/utils/push_notification_auth.py">
from jwcrypto import jwk
import uuid
from starlette.responses import JSONResponse
from starlette.requests import Request
from typing import Any

import jwt
import time
import json
import hashlib
import httpx
import logging

from jwt import PyJWK, PyJWKClient

logger = logging.getLogger(__name__)
AUTH_HEADER_PREFIX = 'Bearer '

class PushNotificationAuth:
    def _calculate_request_body_sha256(self, data: dict[str, Any]):
        """Calculates the SHA256 hash of a request body.

        This logic needs to be same for both the agent who signs the payload and the client verifier.
        """
        body_str = json.dumps(
            data,
            ensure_ascii=False,
            allow_nan=False,
            indent=None,
            separators=(",", ":"),
        )
        return hashlib.sha256(body_str.encode()).hexdigest()

class PushNotificationSenderAuth(PushNotificationAuth):
    def __init__(self):
        self.public_keys = []
        self.private_key_jwk: PyJWK = None

    @staticmethod
    async def verify_push_notification_url(url: str) -> bool:
        async with httpx.AsyncClient(timeout=10) as client:
            try:
                validation_token = str(uuid.uuid4())
                response = await client.get(
                    url,
                    params={"validationToken": validation_token}
                )
                response.raise_for_status()
                is_verified = response.text == validation_token

                logger.info(f"Verified push-notification URL: {url} => {is_verified}")            
                return is_verified                
            except Exception as e:
                logger.warning(f"Error during sending push-notification for URL {url}: {e}")

        return False

    def generate_jwk(self):
        key = jwk.JWK.generate(kty='RSA', size=2048, kid=str(uuid.uuid4()), use="sig")
        self.public_keys.append(key.export_public(as_dict=True))
        self.private_key_jwk = PyJWK.from_json(key.export_private())
    
    def handle_jwks_endpoint(self, _request: Request):
        """Allow clients to fetch public keys.
        """
        return JSONResponse({
            "keys": self.public_keys
        })
    
    def _generate_jwt(self, data: dict[str, Any]):
        """JWT is generated by signing both the request payload SHA digest and time of token generation.

        Payload is signed with private key and it ensures the integrity of payload for client.
        Including iat prevents from replay attack.
        """
        
        iat = int(time.time())

        return jwt.encode(
            {"iat": iat, "request_body_sha256": self._calculate_request_body_sha256(data)},
            key=self.private_key_jwk,
            headers={"kid": self.private_key_jwk.key_id},
            algorithm="RS256"
        )

    async def send_push_notification(self, url: str, data: dict[str, Any]):
        jwt_token = self._generate_jwt(data)
        headers = {'Authorization': f"Bearer {jwt_token}"}
        async with httpx.AsyncClient(timeout=10) as client: 
            try:
                response = await client.post(
                    url,
                    json=data,
                    headers=headers
                )
                response.raise_for_status()
                logger.info(f"Push-notification sent for URL: {url}")                            
            except Exception as e:
                logger.warning(f"Error during sending push-notification for URL {url}: {e}")

class PushNotificationReceiverAuth(PushNotificationAuth):
    def __init__(self):
        self.public_keys_jwks = []
        self.jwks_client = None

    async def load_jwks(self, jwks_url: str):
        self.jwks_client = PyJWKClient(jwks_url)
    
    async def verify_push_notification(self, request: Request) -> bool:
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith(AUTH_HEADER_PREFIX):
            print("Invalid authorization header")
            return False
        
        token = auth_header[len(AUTH_HEADER_PREFIX):]
        signing_key = self.jwks_client.get_signing_key_from_jwt(token)

        decode_token = jwt.decode(
            token,
            signing_key,
            options={"require": ["iat", "request_body_sha256"]},
            algorithms=["RS256"],
        )

        actual_body_sha256 = self._calculate_request_body_sha256(await request.json())
        if actual_body_sha256 != decode_token["request_body_sha256"]:
            # Payload signature does not match the digest in signed token.
            raise ValueError("Invalid request body")
        
        if time.time() - decode_token["iat"] > 60 * 5:
            # Do not allow push-notifications older than 5 minutes.
            # This is to prevent replay attack.
            raise ValueError("Token is expired")
        
        return True
</file>

<file path="cookbook/pocketflow-a2a/common/types.py">
from typing import Union, Any
from pydantic import BaseModel, Field, TypeAdapter
from typing import Literal, List, Annotated, Optional
from datetime import datetime
from pydantic import model_validator, ConfigDict, field_serializer
from uuid import uuid4
from enum import Enum
from typing_extensions import Self


class TaskState(str, Enum):
    SUBMITTED = "submitted"
    WORKING = "working"
    INPUT_REQUIRED = "input-required"
    COMPLETED = "completed"
    CANCELED = "canceled"
    FAILED = "failed"
    UNKNOWN = "unknown"


class TextPart(BaseModel):
    type: Literal["text"] = "text"
    text: str
    metadata: dict[str, Any] | None = None


class FileContent(BaseModel):
    name: str | None = None
    mimeType: str | None = None
    bytes: str | None = None
    uri: str | None = None

    @model_validator(mode="after")
    def check_content(self) -> Self:
        if not (self.bytes or self.uri):
            raise ValueError("Either 'bytes' or 'uri' must be present in the file data")
        if self.bytes and self.uri:
            raise ValueError(
                "Only one of 'bytes' or 'uri' can be present in the file data"
            )
        return self


class FilePart(BaseModel):
    type: Literal["file"] = "file"
    file: FileContent
    metadata: dict[str, Any] | None = None


class DataPart(BaseModel):
    type: Literal["data"] = "data"
    data: dict[str, Any]
    metadata: dict[str, Any] | None = None


Part = Annotated[Union[TextPart, FilePart, DataPart], Field(discriminator="type")]


class Message(BaseModel):
    role: Literal["user", "agent"]
    parts: List[Part]
    metadata: dict[str, Any] | None = None


class TaskStatus(BaseModel):
    state: TaskState
    message: Message | None = None
    timestamp: datetime = Field(default_factory=datetime.now)

    @field_serializer("timestamp")
    def serialize_dt(self, dt: datetime, _info):
        return dt.isoformat()


class Artifact(BaseModel):
    name: str | None = None
    description: str | None = None
    parts: List[Part]
    metadata: dict[str, Any] | None = None
    index: int = 0
    append: bool | None = None
    lastChunk: bool | None = None


class Task(BaseModel):
    id: str
    sessionId: str | None = None
    status: TaskStatus
    artifacts: List[Artifact] | None = None
    history: List[Message] | None = None
    metadata: dict[str, Any] | None = None


class TaskStatusUpdateEvent(BaseModel):
    id: str
    status: TaskStatus
    final: bool = False
    metadata: dict[str, Any] | None = None


class TaskArtifactUpdateEvent(BaseModel):
    id: str
    artifact: Artifact    
    metadata: dict[str, Any] | None = None


class AuthenticationInfo(BaseModel):
    model_config = ConfigDict(extra="allow")

    schemes: List[str]
    credentials: str | None = None


class PushNotificationConfig(BaseModel):
    url: str
    token: str | None = None
    authentication: AuthenticationInfo | None = None


class TaskIdParams(BaseModel):
    id: str
    metadata: dict[str, Any] | None = None


class TaskQueryParams(TaskIdParams):
    historyLength: int | None = None


class TaskSendParams(BaseModel):
    id: str
    sessionId: str = Field(default_factory=lambda: uuid4().hex)
    message: Message
    acceptedOutputModes: Optional[List[str]] = None
    pushNotification: PushNotificationConfig | None = None
    historyLength: int | None = None
    metadata: dict[str, Any] | None = None


class TaskPushNotificationConfig(BaseModel):
    id: str
    pushNotificationConfig: PushNotificationConfig


## RPC Messages


class JSONRPCMessage(BaseModel):
    jsonrpc: Literal["2.0"] = "2.0"
    id: int | str | None = Field(default_factory=lambda: uuid4().hex)


class JSONRPCRequest(JSONRPCMessage):
    method: str
    params: dict[str, Any] | None = None


class JSONRPCError(BaseModel):
    code: int
    message: str
    data: Any | None = None


class JSONRPCResponse(JSONRPCMessage):
    result: Any | None = None
    error: JSONRPCError | None = None


class SendTaskRequest(JSONRPCRequest):
    method: Literal["tasks/send"] = "tasks/send"
    params: TaskSendParams


class SendTaskResponse(JSONRPCResponse):
    result: Task | None = None


class SendTaskStreamingRequest(JSONRPCRequest):
    method: Literal["tasks/sendSubscribe"] = "tasks/sendSubscribe"
    params: TaskSendParams


class SendTaskStreamingResponse(JSONRPCResponse):
    result: TaskStatusUpdateEvent | TaskArtifactUpdateEvent | None = None


class GetTaskRequest(JSONRPCRequest):
    method: Literal["tasks/get"] = "tasks/get"
    params: TaskQueryParams


class GetTaskResponse(JSONRPCResponse):
    result: Task | None = None


class CancelTaskRequest(JSONRPCRequest):
    method: Literal["tasks/cancel",] = "tasks/cancel"
    params: TaskIdParams


class CancelTaskResponse(JSONRPCResponse):
    result: Task | None = None


class SetTaskPushNotificationRequest(JSONRPCRequest):
    method: Literal["tasks/pushNotification/set",] = "tasks/pushNotification/set"
    params: TaskPushNotificationConfig


class SetTaskPushNotificationResponse(JSONRPCResponse):
    result: TaskPushNotificationConfig | None = None


class GetTaskPushNotificationRequest(JSONRPCRequest):
    method: Literal["tasks/pushNotification/get",] = "tasks/pushNotification/get"
    params: TaskIdParams


class GetTaskPushNotificationResponse(JSONRPCResponse):
    result: TaskPushNotificationConfig | None = None


class TaskResubscriptionRequest(JSONRPCRequest):
    method: Literal["tasks/resubscribe",] = "tasks/resubscribe"
    params: TaskIdParams


A2ARequest = TypeAdapter(
    Annotated[
        Union[
            SendTaskRequest,
            GetTaskRequest,
            CancelTaskRequest,
            SetTaskPushNotificationRequest,
            GetTaskPushNotificationRequest,
            TaskResubscriptionRequest,
            SendTaskStreamingRequest,
        ],
        Field(discriminator="method"),
    ]
)

## Error types


class JSONParseError(JSONRPCError):
    code: int = -32700
    message: str = "Invalid JSON payload"
    data: Any | None = None


class InvalidRequestError(JSONRPCError):
    code: int = -32600
    message: str = "Request payload validation error"
    data: Any | None = None


class MethodNotFoundError(JSONRPCError):
    code: int = -32601
    message: str = "Method not found"
    data: None = None


class InvalidParamsError(JSONRPCError):
    code: int = -32602
    message: str = "Invalid parameters"
    data: Any | None = None


class InternalError(JSONRPCError):
    code: int = -32603
    message: str = "Internal error"
    data: Any | None = None


class TaskNotFoundError(JSONRPCError):
    code: int = -32001
    message: str = "Task not found"
    data: None = None


class TaskNotCancelableError(JSONRPCError):
    code: int = -32002
    message: str = "Task cannot be canceled"
    data: None = None


class PushNotificationNotSupportedError(JSONRPCError):
    code: int = -32003
    message: str = "Push Notification is not supported"
    data: None = None


class UnsupportedOperationError(JSONRPCError):
    code: int = -32004
    message: str = "This operation is not supported"
    data: None = None


class ContentTypeNotSupportedError(JSONRPCError):
    code: int = -32005
    message: str = "Incompatible content types"
    data: None = None


class AgentProvider(BaseModel):
    organization: str
    url: str | None = None


class AgentCapabilities(BaseModel):
    streaming: bool = False
    pushNotifications: bool = False
    stateTransitionHistory: bool = False


class AgentAuthentication(BaseModel):
    schemes: List[str]
    credentials: str | None = None


class AgentSkill(BaseModel):
    id: str
    name: str
    description: str | None = None
    tags: List[str] | None = None
    examples: List[str] | None = None
    inputModes: List[str] | None = None
    outputModes: List[str] | None = None


class AgentCard(BaseModel):
    name: str
    description: str | None = None
    url: str
    provider: AgentProvider | None = None
    version: str
    documentationUrl: str | None = None
    capabilities: AgentCapabilities
    authentication: AgentAuthentication | None = None
    defaultInputModes: List[str] = ["text"]
    defaultOutputModes: List[str] = ["text"]
    skills: List[AgentSkill]


class A2AClientError(Exception):
    pass


class A2AClientHTTPError(A2AClientError):
    def __init__(self, status_code: int, message: str):
        self.status_code = status_code
        self.message = message
        super().__init__(f"HTTP Error {status_code}: {message}")


class A2AClientJSONError(A2AClientError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"JSON Error: {message}")


class MissingAPIKeyError(Exception):
    """Exception for missing API key."""

    pass
</file>

<file path="cookbook/pocketflow-a2a/a2a_client.py">
import asyncio
import asyncclick as click # Using asyncclick for async main
from uuid import uuid4
import json # For potentially inspecting raw errors
import anyio
import functools
import logging

# Import from the common directory placed alongside this script
from common.client import A2AClient
from common.types import (
    TaskState,
    A2AClientError,
    TextPart, # Used to construct the message
    JSONRPCResponse # Potentially useful for error checking
)

# --- Configure logging ---
# Set level to INFO to see client requests and responses
# Set level to DEBUG to see raw response bodies and SSE data lines
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
# Optionally silence overly verbose libraries
# logging.getLogger("httpx").setLevel(logging.WARNING)
# logging.getLogger("httpcore").setLevel(logging.WARNING)

# --- ANSI Colors (Optional but helpful) ---
C_RED = "\x1b[31m"
C_GREEN = "\x1b[32m"
C_YELLOW = "\x1b[33m"
C_BLUE = "\x1b[34m"
C_MAGENTA = "\x1b[35m"
C_CYAN = "\x1b[36m"
C_WHITE = "\x1b[37m"
C_GRAY = "\x1b[90m"
C_BRIGHT_MAGENTA = "\x1b[95m"
C_RESET = "\x1b[0m"
C_BOLD = "\x1b[1m"

def colorize(color, text):
    return f"{color}{text}{C_RESET}"

@click.command()
@click.option(
    "--agent-url",
    default="http://localhost:10003", # Default to the port used in server __main__
    help="URL of the PocketFlow A2A agent server.",
)
async def cli(agent_url: str):
    """Minimal CLI client to interact with an A2A agent."""

    print(colorize(C_BRIGHT_MAGENTA, f"Connecting to agent at: {agent_url}"))

    # Instantiate the client - only URL is needed if not fetching card first
    # Note: The PocketFlow wrapper doesn't expose much via the AgentCard,
    # so we skip fetching it for this minimal client.
    client = A2AClient(url=agent_url)

    sessionId = uuid4().hex # Generate a new session ID for this run
    print(colorize(C_GRAY, f"Using Session ID: {sessionId}"))

    while True:
        taskId = uuid4().hex # Generate a new task ID for each interaction
        try:
            # Use functools.partial to prepare the prompt function call
            prompt_func = functools.partial(
                click.prompt,
                colorize(C_CYAN, "\nEnter your question (:q or quit to exit)"),
                prompt_suffix=" > ",
                type=str
            )
            # Run the synchronous prompt function in a worker thread
            prompt = await anyio.to_thread.run_sync(prompt_func)
        except (EOFError, RuntimeError, KeyboardInterrupt):
            # Catch potential errors during input or if stdin closes
            print(colorize(C_RED, "\nInput closed or interrupted. Exiting."))
            break

        if prompt.lower() in [":q", "quit"]:
            print(colorize(C_YELLOW, "Exiting client."))
            break

        # --- Construct A2A Request Payload ---
        payload = {
            "id": taskId,
            "sessionId": sessionId,
            "message": {
                "role": "user",
                "parts": [
                    {
                        "type": "text", # Explicitly match TextPart structure
                        "text": prompt,
                    }
                ],
            },
            "acceptedOutputModes": ["text", "text/plain"], # What the client wants back
            # historyLength could be added if needed
        }

        print(colorize(C_GRAY, f"Sending task {taskId}..."))

        try:
            # --- Send Task (Non-Streaming) ---
            response = await client.send_task(payload)

            # --- Process Response ---
            if response.error:
                print(colorize(C_RED, f"Error from agent (Code: {response.error.code}): {response.error.message}"))
                if response.error.data:
                    print(colorize(C_GRAY, f"Error Data: {response.error.data}"))
            elif response.result:
                task_result = response.result
                print(colorize(C_GREEN, f"Task {task_result.id} finished with state: {task_result.status.state}"))

                final_answer = "Agent did not provide a final artifact."
                # Extract answer from artifacts (as implemented in PocketFlowTaskManager)
                if task_result.artifacts:
                    try:
                        # Find the first text part in the first artifact
                        first_artifact = task_result.artifacts[0]
                        first_text_part = next(
                            (p for p in first_artifact.parts if isinstance(p, TextPart)),
                            None
                        )
                        if first_text_part:
                            final_answer = first_text_part.text
                        else:
                             final_answer = f"(Non-text artifact received: {first_artifact.parts})"
                    except (IndexError, AttributeError, TypeError) as e:
                        final_answer = f"(Error parsing artifact: {e})"
                elif task_result.status.message and task_result.status.message.parts:
                     # Fallback to status message if no artifact
                     try:
                        first_text_part = next(
                             (p for p in task_result.status.message.parts if isinstance(p, TextPart)),
                             None
                         )
                        if first_text_part:
                            final_answer = f"(Final status message: {first_text_part.text})"

                     except (AttributeError, TypeError) as e:
                         final_answer = f"(Error parsing status message: {e})"


                print(colorize(C_BOLD + C_WHITE, f"\nAgent Response:\n{final_answer}"))

            else:
                # Should not happen if error is None
                print(colorize(C_YELLOW, "Received response with no result and no error."))

        except A2AClientError as e:
            print(colorize(C_RED, f"\nClient Error: {e}"))
        except Exception as e:
            print(colorize(C_RED, f"\nAn unexpected error occurred: {e}"))

if __name__ == "__main__":
    asyncio.run(cli())
</file>

<file path="cookbook/pocketflow-a2a/a2a_server.py">
import click
import logging
import os

# Import from the common code you copied
from common.server import A2AServer
from common.types import AgentCard, AgentCapabilities, AgentSkill, MissingAPIKeyError

# Import your custom TaskManager (which now imports from your original files)
from task_manager import PocketFlowTaskManager

# --- Configure logging ---
# Set level to INFO to see server start, requests, responses
# Set level to DEBUG to see raw response bodies from client
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
# Optionally silence overly verbose libraries
# logging.getLogger("httpx").setLevel(logging.WARNING)
# logging.getLogger("httpcore").setLevel(logging.WARNING)
# logging.getLogger("uvicorn.access").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

@click.command()
@click.option("--host", "host", default="localhost")
@click.option("--port", "port", default=10003) # Use a different port from other agents
def main(host, port):
    """Starts the PocketFlow A2A Agent server."""
    try:
        # Check for necessary API keys (add others if needed)
        if not os.getenv("OPENAI_API_KEY"):
            raise MissingAPIKeyError("OPENAI_API_KEY environment variable not set.")

        # --- Define the Agent Card ---
        capabilities = AgentCapabilities(
            streaming=False, # This simple implementation is synchronous
            pushNotifications=False,
            stateTransitionHistory=False # PocketFlow state isn't exposed via A2A history
        )
        skill = AgentSkill(
            id="web_research_qa",
            name="Web Research and Answering",
            description="Answers questions using web search results when necessary.",
            tags=["research", "qa", "web search"],
            examples=[
                "Who won the Nobel Prize in Physics 2024?",
                "What is quantum computing?",
                "Summarize the latest news about AI.",
            ],
            # Input/Output modes defined in the TaskManager
            inputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,
            outputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,
        )
        agent_card = AgentCard(
            name="PocketFlow Research Agent (A2A Wrapped)",
            description="A simple research agent based on PocketFlow, made accessible via A2A.",
            url=f"http://{host}:{port}/", # The endpoint A2A clients will use
            version="0.1.0-a2a",
            capabilities=capabilities,
            skills=[skill],
            # Assuming no specific provider or auth for this example
            provider=None,
            authentication=None,
            defaultInputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,
            defaultOutputModes=PocketFlowTaskManager.SUPPORTED_CONTENT_TYPES,
        )

        # --- Initialize and Start Server ---
        task_manager = PocketFlowTaskManager() # Instantiate your custom manager
        server = A2AServer(
            agent_card=agent_card,
            task_manager=task_manager,
            host=host,
            port=port,
        )

        logger.info(f"Starting PocketFlow A2A server on http://{host}:{port}")
        server.start()

    except MissingAPIKeyError as e:
        logger.error(f"Configuration Error: {e}")
        exit(1)
    except Exception as e:
        logger.error(f"An error occurred during server startup: {e}", exc_info=True)
        exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-a2a/flow.py">
from pocketflow import Flow
from nodes import DecideAction, SearchWeb, AnswerQuestion

def create_agent_flow():
    """
    Create and connect the nodes to form a complete agent flow.
    
    The flow works like this:
    1. DecideAction node decides whether to search or answer
    2. If search, go to SearchWeb node
    3. If answer, go to AnswerQuestion node
    4. After SearchWeb completes, go back to DecideAction
    
    Returns:
        Flow: A complete research agent flow
    """
    # Create instances of each node
    decide = DecideAction()
    search = SearchWeb()
    answer = AnswerQuestion()
    
    # Connect the nodes
    # If DecideAction returns "search", go to SearchWeb
    decide - "search" >> search
    
    # If DecideAction returns "answer", go to AnswerQuestion
    decide - "answer" >> answer
    
    # After SearchWeb completes and returns "decide", go back to DecideAction
    search - "decide" >> decide
    
    # Create and return the flow, starting with the DecideAction node
    return Flow(start=decide)
</file>

<file path="cookbook/pocketflow-a2a/main.py">
import sys
from flow import create_agent_flow

def main():
    """Simple function to process a question."""
    # Default question
    default_question = "Who won the Nobel Prize in Physics 2024?"
    
    # Get question from command line if provided with --
    question = default_question
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            question = arg[2:]
            break
    
    # Create the agent flow
    agent_flow = create_agent_flow()
    
    # Process the question
    shared = {"question": question}
    print(f"ðŸ¤” Processing question: {question}")
    agent_flow.run(shared)
    print("\nðŸŽ¯ Final Answer:")
    print(shared.get("answer", "No answer found"))

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-a2a/nodes.py">
from pocketflow import Node
from utils import call_llm, search_web
import yaml

class DecideAction(Node):
    def prep(self, shared):
        """Prepare the context and question for the decision-making process."""
        # Get the current context (default to "No previous search" if none exists)
        context = shared.get("context", "No previous search")
        # Get the question from the shared store
        question = shared["question"]
        # Return both for the exec step
        return question, context
        
    def exec(self, inputs):
        """Call the LLM to decide whether to search or answer."""
        question, context = inputs
        
        print(f"ðŸ¤” Agent deciding what to do next...")
        
        # Create a prompt to help the LLM decide what to do next with proper yaml formatting
        prompt = f"""
### CONTEXT
You are a research assistant that can search the web.
Question: {question}
Previous Research: {context}

### ACTION SPACE
[1] search
  Description: Look up more information on the web
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Answer the question with current knowledge
  Parameters:
    - answer (str): Final answer to the question

## NEXT ACTION
Decide the next action based on the context and available actions.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: search OR answer
reason: <why you chose this action>
answer: <if action is answer>
search_query: <specific search query if action is search>
```
IMPORTANT: Make sure to:
1. Use proper indentation (4 spaces) for all multi-line fields
2. Use the | character for multi-line text fields
3. Keep single-line fields without the | character
"""
        
        # Call the LLM to make a decision
        response = call_llm(prompt)
        
        # Parse the response to get the decision
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        decision = yaml.safe_load(yaml_str)
        
        return decision
    
    def post(self, shared, prep_res, exec_res):
        """Save the decision and determine the next step in the flow."""
        # If LLM decided to search, save the search query
        if exec_res["action"] == "search":
            shared["search_query"] = exec_res["search_query"]
            print(f"ðŸ” Agent decided to search for: {exec_res['search_query']}")
        else:
            shared["context"] = exec_res["answer"] #save the context if LLM gives the answer without searching.
            print(f"ðŸ’¡ Agent decided to answer the question")
        
        # Return the action to determine the next node in the flow
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        """Get the search query from the shared store."""
        return shared["search_query"]
        
    def exec(self, search_query):
        """Search the web for the given query."""
        # Call the search utility function
        print(f"ðŸŒ Searching the web for: {search_query}")
        results = search_web(search_query)
        return results
    
    def post(self, shared, prep_res, exec_res):
        """Save the search results and go back to the decision node."""
        # Add the search results to the context in the shared store
        previous = shared.get("context", "")
        shared["context"] = previous + "\n\nSEARCH: " + shared["search_query"] + "\nRESULTS: " + exec_res
        
        print(f"ðŸ“š Found information, analyzing results...")
        
        # Always go back to the decision node after searching
        return "decide"

class AnswerQuestion(Node):
    def prep(self, shared):
        """Get the question and context for answering."""
        return shared["question"], shared.get("context", "")
        
    def exec(self, inputs):
        """Call the LLM to generate a final answer."""
        question, context = inputs
        
        print(f"âœï¸ Crafting final answer...")
        
        # Create a prompt for the LLM to answer the question
        prompt = f"""
### CONTEXT
Based on the following information, answer the question.
Question: {question}
Research: {context}

## YOUR ANSWER:
Provide a comprehensive answer using the research results.
"""
        # Call the LLM to generate an answer
        answer = call_llm(prompt)
        return answer
    
    def post(self, shared, prep_res, exec_res):
        """Save the final answer and complete the flow."""
        # Save the answer in the shared store
        shared["answer"] = exec_res
        
        print(f"âœ… Answer generated successfully")
        
        # We're done - no need to continue the flow
        return "done"
</file>

<file path="cookbook/pocketflow-a2a/README.md">
# PocketFlow Agent with A2A Protocol

This project demonstrates how to take an existing agent built with the PocketFlow library and make it accessible to other agents using the **Agent-to-Agent (A2A) communication protocol**.

This implementation is based  on the tutorial: [A2A Protocol Simply Explained: Here are 3 key differences to MCP!](https://zacharyhuang.substack.com/p/a2a-protocol-simply-explained-here)

## How it Works: A2A Integration

This project combines two main parts:

1.  **PocketFlow Agent Logic:** The original agent code ([`nodes.py`](nodes.py), [`utils.py`](utils.py), [`flow.py`](flow.py)) defines the internal workflow (Decide -> Search -> Answer). This code is taken directly from the [PocketFlow Agent Tutorial](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent).
2.  **A2A Server Wrapper:** Code from the [google/A2A samples repository](https://github.com/google/A2A/tree/main/samples/python) (`common/` directory) provides the necessary infrastructure to host the agent as an A2A-compliant server. *Note: Minor modifications were made to the common server/client code to add detailed logging for educational purposes.*
3.  **The Bridge ([`task_manager.py`](task_manager.py)):** A custom `PocketFlowTaskManager` class acts as the bridge. It receives A2A requests (like `tasks/send`), extracts the user query, runs the PocketFlow `agent_flow`, takes the final result from the flow's shared state, and packages it back into an A2A `Task` object with the answer as an `Artifact`.

This demonstrates how a non-A2A agent framework can be exposed over the A2A protocol by implementing a specific `TaskManager`.

## Simplified Interaction Sequence

```mermaid
sequenceDiagram
    participant Client as "Client ([minimal_a2a_client.py](a2a_client.py))"
    participant Server as "Server (localhost:10003)"
    
    Note over Client: User enters question
    Client->>+Server: POST / (JSON-RPC Request: tasks/send)
    Note over Server: Processes request internally (runs PocketFlow)
    Server-->>-Client: HTTP 200 OK (JSON-RPC Response: result=Task)
    Note over Client: Displays final answer
```

## Getting Started

### Prerequisites

*   Python 3.10+ (due to type hinting used in the A2A `common` code)
*   An OpenAI API Key

### Installation


1.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

2. Set your OpenAI API key as an environment variable:

    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```

    Let's do a quick check to make sure your API key is working properly:

    ```bash
    python utils.py
    ```
3. Run the server from this directory:

    ```bash
    python a2a_server.py --port 10003
    ```

    You should see logs indicating the server has started on `http://localhost:10003`.


4.  Run the Client in a *separate terminal* 

    ```bash
    python a2a_client.py --agent-url http://localhost:10003
    ```

5.  Follow the instructions in the client terminal to ask questions. Type `:q` or `quit` to exit the client.

## Example Interaction Logs

**(Server Log - showing internal PocketFlow steps)**

```
2025-04-12 17:20:40,893 - __main__ - INFO - Starting PocketFlow A2A server on http://localhost:10003
INFO:     Started server process [677223]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:10003 (Press CTRL+C to quit)
2025-04-12 17:20:57,647 - A2AServer - INFO - <- Received Request (ID: d3f3fb93350d47d9a94ca12bb62b656b):
{
  "jsonrpc": "2.0",
  "id": "d3f3fb93350d47d9a94ca12bb62b656b",
  "method": "tasks/send",
  "params": {
    "id": "46c3ce7b941a4fff9b8e3b644d6db5f4",
    "sessionId": "f3e12b8424c44241be881cd4bb8a269f",
    "message": {
      "role": "user",
      "parts": [
        {
          "type": "text",
          "text": "Who won the Nobel Prize in Physics 2024?"
        }
      ]
    },
    "acceptedOutputModes": [
      "text",
      "text/plain"
    ]
  }
}
2025-04-12 17:20:57,647 - task_manager - INFO - Received task send request: 46c3ce7b941a4fff9b8e3b644d6db5f4
2025-04-12 17:20:57,647 - common.server.task_manager - INFO - Upserting task 46c3ce7b941a4fff9b8e3b644d6db5f4
2025-04-12 17:20:57,647 - task_manager - INFO - Running PocketFlow for task 46c3ce7b941a4fff9b8e3b644d6db5f4...
ðŸ¤” Agent deciding what to do next...
2025-04-12 17:20:59,213 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
ðŸ” Agent decided to search for: 2024 Nobel Prize in Physics winner
ðŸŒ Searching the web for: 2024 Nobel Prize in Physics winner
2025-04-12 17:20:59,974 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200
ðŸ“š Found information, analyzing results...
ðŸ¤” Agent deciding what to do next...
2025-04-12 17:21:01,619 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
ðŸ’¡ Agent decided to answer the question
âœï¸ Crafting final answer...
2025-04-12 17:21:03,833 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
âœ… Answer generated successfully
2025-04-12 17:21:03,834 - task_manager - INFO - PocketFlow completed for task 46c3ce7b941a4fff9b8e3b644d6db5f4
2025-04-12 17:21:03,834 - A2AServer - INFO - -> Response (ID: d3f3fb93350d47d9a94ca12bb62b656b):
{
  "jsonrpc": "2.0",
  "id": "d3f3fb93350d47d9a94ca12bb62b656b",
  "result": {
    "id": "46c3ce7b941a4fff9b8e3b644d6db5f4",
    "sessionId": "f3e12b8424c44241be881cd4bb8a269f",
    "status": {
      "state": "completed",
      "timestamp": "2025-04-12T17:21:03.834542"
    },
    "artifacts": [
      {
        "parts": [
          {
            "type": "text",
            "text": "The 2024 Nobel Prize in Physics was awarded to John J. Hopfield and Geoffrey Hinton for their foundational discoveries and inventions that have significantly advanced the field of machine learning through the use of artificial neural networks. Their pioneering work has been crucial in the development and implementation of algorithms that enable machines to learn and process information in a manner that mimics human cognitive functions. This advancement in artificial intelligence technology has had a profound impact on numerous industries, facilitating innovations across various applications, from image and speech recognition to self-driving cars."
          }
        ],
        "index": 0
      }
    ],
    "history": []
  }
}
```

**(Client Log - showing request/response)**

```
Connecting to agent at: http://localhost:10003
Using Session ID: f3e12b8424c44241be881cd4bb8a269f

Enter your question (:q or quit to exit) > Who won the Nobel Prize in Physics 2024?
Sending task 46c3ce7b941a4fff9b8e3b644d6db5f4...
2025-04-12 17:20:57,643 - A2AClient - INFO - -> Sending Request (ID: d3f3fb93350d47d9a94ca12bb62b656b, Method: tasks/send):
{
  "jsonrpc": "2.0",
  "id": "d3f3fb93350d47d9a94ca12bb62b656b",
  "method": "tasks/send",
  "params": {
    "id": "46c3ce7b941a4fff9b8e3b644d6db5f4",
    "sessionId": "f3e12b8424c44241be881cd4bb8a269f",
    "message": {
      "role": "user",
      "parts": [
        {
          "type": "text",
          "text": "Who won the Nobel Prize in Physics 2024?"
        }
      ]
    },
    "acceptedOutputModes": [
      "text",
      "text/plain"
    ]
  }
}
2025-04-12 17:21:03,835 - httpx - INFO - HTTP Request: POST http://localhost:10003 "HTTP/1.1 200 OK"
2025-04-12 17:21:03,836 - A2AClient - INFO - <- Received HTTP Status 200 for Request (ID: d3f3fb93350d47d9a94ca12bb62b656b)
2025-04-12 17:21:03,836 - A2AClient - INFO - <- Received Success Response (ID: d3f3fb93350d47d9a94ca12bb62b656b):
{
  "jsonrpc": "2.0",
  "id": "d3f3fb93350d47d9a94ca12bb62b656b",
  "result": {
    "id": "46c3ce7b941a4fff9b8e3b644d6db5f4",
    "sessionId": "f3e12b8424c44241be881cd4bb8a269f",
    "status": {
      "state": "completed",
      "timestamp": "2025-04-12T17:21:03.834542"
    },
    "artifacts": [
      {
        "parts": [
          {
            "type": "text",
            "text": "The 2024 Nobel Prize in Physics was awarded to John J. Hopfield and Geoffrey Hinton for their foundational discoveries and inventions that have significantly advanced the field of machine learning through the use of artificial neural networks. Their pioneering work has been crucial in the development and implementation of algorithms that enable machines to learn and process information in a manner that mimics human cognitive functions. This advancement in artificial intelligence technology has had a profound impact on numerous industries, facilitating innovations across various applications, from image and speech recognition to self-driving cars."
          }
        ],
        "index": 0
      }
    ],
    "history": []
  }
}
Task 46c3ce7b941a4fff9b8e3b644d6db5f4 finished with state: completed

Agent Response:
The 2024 Nobel Prize in Physics was awarded to John J. Hopfield and Geoffrey Hinton for their foundational discoveries and inventions that have significantly advanced the field of machine learning through the use of artificial neural networks. Their pioneering work has been crucial in the development and implementation of algorithms that enable machines to learn and process information in a manner that mimics human cognitive functions. This advancement in artificial intelligence technology has had a profound impact on numerous industries, facilitating innovations across various applications, from image and speech recognition to self-driving cars.
```

## Key A2A Integration Points

To make the PocketFlow agent A2A-compatible, the following were essential:

1.  **A2A Server ([`common/server/server.py`](common/server/server.py)):** An ASGI application (using Starlette/Uvicorn) that listens for HTTP POST requests, parses JSON-RPC, and routes requests based on the `method` field.
2.  **A2A Data Types ([`common/types.py`](common/types.py)):** Pydantic models defining the structure of A2A messages, tasks, artifacts, errors, and the agent card, ensuring compliance with the `a2a.json` specification.
3.  **Task Manager ([`task_manager.py`](task_manager.py)):** A custom class (`PocketFlowTaskManager`) inheriting from the common `InMemoryTaskManager`. Its primary role is implementing the `on_send_task` method (and potentially others like `on_send_task_subscribe` if streaming were supported). This method:
    *   Receives the validated A2A `SendTaskRequest`.
    *   Extracts the user's query (`TextPart`) from the request's `message`.
    *   Initializes the PocketFlow `shared_data` dictionary.
    *   Creates and runs the PocketFlow `agent_flow`.
    *   Retrieves the final answer from the `shared_data` dictionary *after* the flow completes.
    *   Updates the task's state (e.g., to `COMPLETED` or `FAILED`) in the `InMemoryTaskManager`'s store.
    *   Packages the final answer into an A2A `Artifact` containing a `TextPart`.
    *   Constructs the final A2A `Task` object for the response.
4.  **Agent Card ([`a2a_server.py`](a2a_server.py)):** A Pydantic model (`AgentCard`) defining the agent's metadata (name, description, URL, capabilities, skills) served at `/.well-known/agent.json`.
5.  **Server Entry Point ([`a2a_server.py`](a2a_server.py)):** A script that initializes the `AgentCard`, the `PocketFlowTaskManager`, and the `A2AServer`, then starts the Uvicorn server process.
</file>

<file path="cookbook/pocketflow-a2a/requirements.txt">
# For PocketFlow Agent Logic
pocketflow>=0.0.1
openai>=1.0.0
duckduckgo-search>=7.5.2
pyyaml>=5.1

# For A2A Server Infrastructure (from common)
starlette>=0.37.2,<0.38.0
uvicorn[standard]>=0.29.0,<0.30.0
sse-starlette>=1.8.2,<2.0.0
pydantic>=2.0.0,<3.0.0
httpx>=0.27.0,<0.28.0
anyio>=3.0.0,<5.0.0 # Dependency of starlette/httpx

# For running __main__.py
click>=8.0.0,<9.0.0

# For A2A Client
httpx>=0.27.0,<0.28.0
httpx-sse>=0.4.0
asyncclick>=8.1.8 # Or just 'click' if you prefer asyncio.run
pydantic>=2.0.0,<3.0.0 # For common.types
</file>

<file path="cookbook/pocketflow-a2a/task_manager.py">
# FILE: pocketflow_a2a_agent/task_manager.py
import logging
from typing import AsyncIterable, Union
import asyncio

# Import from the common code you copied
from common.server.task_manager import InMemoryTaskManager
from common.types import (
    JSONRPCResponse, SendTaskRequest, SendTaskResponse,
    SendTaskStreamingRequest, SendTaskStreamingResponse, Task, TaskSendParams,
    TaskState, TaskStatus, TextPart, Artifact, UnsupportedOperationError,
    InternalError, InvalidParamsError, 
    Message
)
import common.server.utils as server_utils

# Import directly from your original PocketFlow files
from flow import create_agent_flow

logger = logging.getLogger(__name__)

class PocketFlowTaskManager(InMemoryTaskManager):
    """ TaskManager implementation that runs the PocketFlow agent. """

    SUPPORTED_CONTENT_TYPES = ["text", "text/plain"] # Define what the agent accepts/outputs

    async def on_send_task(self, request: SendTaskRequest) -> SendTaskResponse:
        """Handles non-streaming task requests."""
        logger.info(f"Received task send request: {request.params.id}")

        # Validate output modes
        if not server_utils.are_modalities_compatible(
            request.params.acceptedOutputModes, self.SUPPORTED_CONTENT_TYPES
        ):
            logger.warning(
                "Unsupported output mode. Received %s, Support %s",
                request.params.acceptedOutputModes, self.SUPPORTED_CONTENT_TYPES
            )
            return SendTaskResponse(id=request.id, error=server_utils.new_incompatible_types_error(request.id).error)

        # Upsert the task in the store (initial state: submitted)
        # We create the task first so its state can be tracked, even if the sync execution fails
        await self.upsert_task(request.params)
        # Update state to working before running
        await self.update_store(request.params.id, TaskStatus(state=TaskState.WORKING), [])


        # --- Run the PocketFlow logic ---
        task_params: TaskSendParams = request.params
        query = self._get_user_query(task_params)
        if query is None:
            fail_status = TaskStatus(state=TaskState.FAILED, message=Message(role="agent", parts=[TextPart(text="No text query found")]))
            await self.update_store(task_params.id, fail_status, [])
            return SendTaskResponse(id=request.id, error=InvalidParamsError(message="No text query found in message parts"))

        shared_data = {"question": query}
        agent_flow = create_agent_flow() # Create the flow instance

        try:
            # Run the synchronous PocketFlow
            # In a real async server, you might run this in a separate thread/process
            # executor to avoid blocking the event loop. For simplicity here, we run it directly.
            # Consider adding a timeout if flows can hang.
            logger.info(f"Running PocketFlow for task {task_params.id}...")
            agent_flow.run(shared_data) # Run the flow, modifying shared_data in place
            logger.info(f"PocketFlow completed for task {task_params.id}")
            # Access the original shared_data dictionary, which was modified by the flow
            answer_text = shared_data.get("answer", "Agent did not produce a final answer text.")

            # --- Package result into A2A Task ---
            final_task_status = TaskStatus(state=TaskState.COMPLETED)
            # Package the answer as an artifact
            final_artifact = Artifact(parts=[TextPart(text=answer_text)])

            # Update the task in the store with final status and artifact
            final_task = await self.update_store(
                task_params.id, final_task_status, [final_artifact]
            )

            # Prepare and return the A2A response
            task_result = self.append_task_history(final_task, task_params.historyLength)
            return SendTaskResponse(id=request.id, result=task_result)

        except Exception as e:
            logger.error(f"Error executing PocketFlow for task {task_params.id}: {e}", exc_info=True)
            # Update task state to FAILED
            fail_status = TaskStatus(
                state=TaskState.FAILED,
                message=Message(role="agent", parts=[TextPart(text=f"Agent execution failed: {e}")])
            )
            await self.update_store(task_params.id, fail_status, [])
            return SendTaskResponse(id=request.id, error=InternalError(message=f"Agent error: {e}"))

    async def on_send_task_subscribe(
        self, request: SendTaskStreamingRequest
    ) -> Union[AsyncIterable[SendTaskStreamingResponse], JSONRPCResponse]:
        """Handles streaming requests - Not implemented for this synchronous agent."""
        logger.warning(f"Streaming requested for task {request.params.id}, but not supported by this PocketFlow agent implementation.")
        # Return an error indicating streaming is not supported
        return JSONRPCResponse(id=request.id, error=UnsupportedOperationError(message="Streaming not supported by this agent"))

    def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        """Extracts the first text part from the user message."""
        if not task_send_params.message or not task_send_params.message.parts:
            logger.warning(f"No message parts found for task {task_send_params.id}")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get("type") == "text" and "text" in part_dict:
                 return part_dict["text"]
        logger.warning(f"No text part found in message for task {task_send_params.id}")
        return None # No text part found
</file>

<file path="cookbook/pocketflow-a2a/utils.py">
from openai import OpenAI
import os
from duckduckgo_search import DDGS

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

def search_web(query):
    results = DDGS().text(query, max_results=5)
    # Convert results to a string
    results_str = "\n\n".join([f"Title: {r['title']}\nURL: {r['href']}\nSnippet: {r['body']}" for r in results])
    return results_str
    
if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")

    print("## Testing search_web")
    query = "Who won the Nobel Prize in Physics 2024?"
    print(f"## Query: {query}")
    results = search_web(query)
    print(f"## Results: {results}")
</file>

<file path="cookbook/pocketflow-agent/demo.ipynb">
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8MeqVASIxKBH"
      },
      "outputs": [],
      "source": [
        "! pip install pocketflow>=0.0.1\n",
        "! pip install aiohttp>=3.8.0\n",
        "! pip install openai>=1.0.0\n",
        "! pip install duckduckgo-search>=7.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUp_sNU1xKBI",
        "outputId": "a647f919-b253-48c8-c132-5eef582e29c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Testing call_llm\n",
            "## Prompt: In a few words, what is the meaning of life?\n",
            "## Response: The meaning of life is a deeply personal and philosophical question. For many, it involves seeking happiness, forming relationships, pursuing knowledge, or finding purpose and fulfillment. It's a journey that varies for each individual.\n",
            "## Testing search_web\n",
            "## Query: Who won the Nobel Prize in Physics 2024?\n",
            "## Results: Title: Press release: The Nobel Prize in Physics 2024 - NobelPrize.org\n",
            "URL: https://www.nobelprize.org/prizes/physics/2024/press-release/\n",
            "Snippet: The Nobel Prize in Physics 2024 was awarded jointly to John J. Hopfield and Geoffrey Hinton \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\"\n",
            "\n",
            "Title: Pioneers in artificial intelligence win the Nobel Prize in physics\n",
            "URL: https://apnews.com/article/nobel-prize-physics-fc0567de3f2ca45f81a7359a017cd542\n",
            "Snippet: Two pioneers of artificial intelligence have won the Nobel Prize in physics. John Hopfield and Geoffrey Hinton were awarded the prize Tuesday for discoveries and inventions that formed the building blocks of machine learning.\n",
            "\n",
            "Title: Nobel Prize 2024: All the Winners | TIME\n",
            "URL: https://time.com/7065011/nobel-prize-2024-winners/\n",
            "Snippet: The 2024 Nobel Prize announcements began on Oct. 7, recognizing groundbreaking contributions to humanity. The first prize, in the category of physiology or medicine, went to a pair of American ...\n",
            "\n",
            "Title: Nobel physics prize 2024 won by AI pioneers John Hopfield and Geoffrey ...\n",
            "URL: https://www.reuters.com/science/hopfield-hinton-win-2024-nobel-prize-physics-2024-10-08/\n",
            "Snippet: John Hopfield and Geoffrey Hinton won for discoveries that paved the way for the AI boom.\n",
            "\n",
            "Title: Nobel Prize in physics 2024 awarded for work on artificial intelligence ...\n",
            "URL: https://www.cnn.com/2024/10/08/science/nobel-prize-physics-hopfield-hinton-machine-learning-intl/index.html\n",
            "Snippet: The 2024 Nobel Prize in physics has been awarded to John Hopfield and Geoffrey Hinton for their fundamental discoveries in machine learning, which paved the way for how artificial intelligence is ...\n"
          ]
        }
      ],
      "source": [
        "# utils.py\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def call_llm(prompt):\n",
        "    client = OpenAI(api_key=\"your-api-key\")\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return r.choices[0].message.content\n",
        "\n",
        "def search_web(query):\n",
        "    results = DDGS().text(query, max_results=5)\n",
        "    # Convert results to a string\n",
        "    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n",
        "    return results_str\n",
        "\n",
        "print(\"## Testing call_llm\")\n",
        "prompt = \"In a few words, what is the meaning of life?\"\n",
        "print(f\"## Prompt: {prompt}\")\n",
        "response = call_llm(prompt)\n",
        "print(f\"## Response: {response}\")\n",
        "\n",
        "print(\"## Testing search_web\")\n",
        "query = \"Who won the Nobel Prize in Physics 2024?\"\n",
        "print(f\"## Query: {query}\")\n",
        "results = search_web(query)\n",
        "print(f\"## Results: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "T0ETd4C2xKBI"
      },
      "outputs": [],
      "source": [
        "# nodes.py\n",
        "from pocketflow import Node\n",
        "import yaml\n",
        "\n",
        "class DecideAction(Node):\n",
        "    def prep(self, shared):\n",
        "        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n",
        "        # Get the current context (default to \"No previous search\" if none exists)\n",
        "        context = shared.get(\"context\", \"No previous search\")\n",
        "        # Get the question from the shared store\n",
        "        question = shared[\"question\"]\n",
        "        # Return both for the exec step\n",
        "        return question, context\n",
        "\n",
        "    def exec(self, inputs):\n",
        "        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n",
        "        question, context = inputs\n",
        "\n",
        "        print(f\"ðŸ¤” Agent deciding what to do next...\")\n",
        "\n",
        "        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n",
        "        prompt = f\"\"\"\n",
        "### CONTEXT\n",
        "You are a research assistant that can search the web.\n",
        "Question: {question}\n",
        "Previous Research: {context}\n",
        "\n",
        "### ACTION SPACE\n",
        "[1] search\n",
        "  Description: Look up more information on the web\n",
        "  Parameters:\n",
        "    - query (str): What to search for\n",
        "\n",
        "[2] answer\n",
        "  Description: Answer the question with current knowledge\n",
        "  Parameters:\n",
        "    - answer (str): Final answer to the question\n",
        "\n",
        "## NEXT ACTION\n",
        "Decide the next action based on the context and available actions.\n",
        "Return your response in this format:\n",
        "\n",
        "```yaml\n",
        "thinking: |\n",
        "    <your step-by-step reasoning process>\n",
        "action: search OR answer\n",
        "reason: <why you chose this action>\n",
        "answer: <if action is answer>\n",
        "search_query: <specific search query if action is search>\n",
        "```\n",
        "IMPORTANT: Make sure to:\n",
        "1. Use proper indentation (4 spaces) for all multi-line fields\n",
        "2. Use the | character for multi-line text fields\n",
        "3. Keep single-line fields without the | character\n",
        "\"\"\"\n",
        "\n",
        "        # Call the LLM to make a decision\n",
        "        response = call_llm(prompt)\n",
        "\n",
        "        # Parse the response to get the decision\n",
        "        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n",
        "        decision = yaml.safe_load(yaml_str)\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def post(self, shared, prep_res, exec_res):\n",
        "        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n",
        "        # If LLM decided to search, save the search query\n",
        "        if exec_res[\"action\"] == \"search\":\n",
        "            shared[\"search_query\"] = exec_res[\"search_query\"]\n",
        "            print(f\"ðŸ” Agent decided to search for: {exec_res['search_query']}\")\n",
        "        else:\n",
        "            shared[\"context\"] = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n",
        "            print(f\"ðŸ’¡ Agent decided to answer the question\")\n",
        "\n",
        "        # Return the action to determine the next node in the flow\n",
        "        return exec_res[\"action\"]\n",
        "\n",
        "class SearchWeb(Node):\n",
        "    def prep(self, shared):\n",
        "        \"\"\"Get the search query from the shared store.\"\"\"\n",
        "        return shared[\"search_query\"]\n",
        "\n",
        "    def exec(self, search_query):\n",
        "        \"\"\"Search the web for the given query.\"\"\"\n",
        "        # Call the search utility function\n",
        "        print(f\"ðŸŒ Searching the web for: {search_query}\")\n",
        "        results = search_web(search_query)\n",
        "        return results\n",
        "\n",
        "    def post(self, shared, prep_res, exec_res):\n",
        "        \"\"\"Save the search results and go back to the decision node.\"\"\"\n",
        "        # Add the search results to the context in the shared store\n",
        "        previous = shared.get(\"context\", \"\")\n",
        "        shared[\"context\"] = previous + \"\\n\\nSEARCH: \" + shared[\"search_query\"] + \"\\nRESULTS: \" + exec_res\n",
        "\n",
        "        print(f\"ðŸ“š Found information, analyzing results...\")\n",
        "\n",
        "        # Always go back to the decision node after searching\n",
        "        return \"decide\"\n",
        "\n",
        "class AnswerQuestion(Node):\n",
        "    def prep(self, shared):\n",
        "        \"\"\"Get the question and context for answering.\"\"\"\n",
        "        return shared[\"question\"], shared.get(\"context\", \"\")\n",
        "\n",
        "    def exec(self, inputs):\n",
        "        \"\"\"Call the LLM to generate a final answer.\"\"\"\n",
        "        question, context = inputs\n",
        "\n",
        "        print(f\"âœï¸ Crafting final answer...\")\n",
        "\n",
        "        # Create a prompt for the LLM to answer the question\n",
        "        prompt = f\"\"\"\n",
        "### CONTEXT\n",
        "Based on the following information, answer the question.\n",
        "Question: {question}\n",
        "Research: {context}\n",
        "\n",
        "## YOUR ANSWER:\n",
        "Provide a comprehensive answer using the research results.\n",
        "\"\"\"\n",
        "        # Call the LLM to generate an answer\n",
        "        answer = call_llm(prompt)\n",
        "        return answer\n",
        "\n",
        "    def post(self, shared, prep_res, exec_res):\n",
        "        \"\"\"Save the final answer and complete the flow.\"\"\"\n",
        "        # Save the answer in the shared store\n",
        "        shared[\"answer\"] = exec_res\n",
        "\n",
        "        print(f\"âœ… Answer generated successfully\")\n",
        "\n",
        "        # We're done - no need to continue the flow\n",
        "        return \"done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "0B4jCAmXxKBI"
      },
      "outputs": [],
      "source": [
        "# flow.py\n",
        "from pocketflow import Flow\n",
        "\n",
        "def create_agent_flow():\n",
        "    \"\"\"\n",
        "    Create and connect the nodes to form a complete agent flow.\n",
        "\n",
        "    The flow works like this:\n",
        "    1. DecideAction node decides whether to search or answer\n",
        "    2. If search, go to SearchWeb node\n",
        "    3. If answer, go to AnswerQuestion node\n",
        "    4. After SearchWeb completes, go back to DecideAction\n",
        "\n",
        "    Returns:\n",
        "        Flow: A complete research agent flow\n",
        "    \"\"\"\n",
        "    # Create instances of each node\n",
        "    decide = DecideAction()\n",
        "    search = SearchWeb()\n",
        "    answer = AnswerQuestion()\n",
        "\n",
        "    # Connect the nodes\n",
        "    # If DecideAction returns \"search\", go to SearchWeb\n",
        "    decide - \"search\" >> search\n",
        "\n",
        "    # If DecideAction returns \"answer\", go to AnswerQuestion\n",
        "    decide - \"answer\" >> answer\n",
        "\n",
        "    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n",
        "    search - \"decide\" >> decide\n",
        "\n",
        "    # Create and return the flow, starting with the DecideAction node\n",
        "    return Flow(start=decide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIwsNEDCxKBI",
        "outputId": "e6c02020-6fae-4377-8f0a-01d2580dd659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤” Processing question: Who won the Nobel Prize in Physics 2024?\n",
            "ðŸ¤” Agent deciding what to do next...\n",
            "ðŸ” Agent decided to search for: 2024 Nobel Prize in Physics winner\n",
            "ðŸŒ Searching the web for: 2024 Nobel Prize in Physics winner\n",
            "ðŸ“š Found information, analyzing results...\n",
            "ðŸ¤” Agent deciding what to do next...\n",
            "ðŸ’¡ Agent decided to answer the question\n",
            "âœï¸ Crafting final answer...\n",
            "âœ… Answer generated successfully\n",
            "\n",
            "ðŸŽ¯ Final Answer:\n",
            "John J. Hopfield and Geoffrey Hinton won the 2024 Nobel Prize in Physics. They were awarded this prestigious recognition for their foundational discoveries and inventions that have significantly advanced the field of machine learning by enabling the use of artificial neural networks. These contributions have had a profound impact on the development and application of machine learning technologies.\n"
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "import sys\n",
        "\n",
        "def main():\n",
        "    \"\"\"Simple function to process a question.\"\"\"\n",
        "    # Default question\n",
        "    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n",
        "\n",
        "    # Get question from command line if provided with --\n",
        "    question = default_question\n",
        "    for arg in sys.argv[1:]:\n",
        "        if arg.startswith(\"--\"):\n",
        "            question = arg[2:]\n",
        "            break\n",
        "\n",
        "    # Create the agent flow\n",
        "    agent_flow = create_agent_flow()\n",
        "\n",
        "    # Process the question\n",
        "    shared = {\"question\": question}\n",
        "    print(f\"ðŸ¤” Processing question: {question}\")\n",
        "    agent_flow.run(shared)\n",
        "    print(\"\\nðŸŽ¯ Final Answer:\")\n",
        "    print(shared.get(\"answer\", \"No answer found\"))\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
</file>

<file path="cookbook/pocketflow-agent/flow.py">
from pocketflow import Flow
from nodes import DecideAction, SearchWeb, AnswerQuestion

def create_agent_flow():
    """
    Create and connect the nodes to form a complete agent flow.
    
    The flow works like this:
    1. DecideAction node decides whether to search or answer
    2. If search, go to SearchWeb node
    3. If answer, go to AnswerQuestion node
    4. After SearchWeb completes, go back to DecideAction
    
    Returns:
        Flow: A complete research agent flow
    """
    # Create instances of each node
    decide = DecideAction()
    search = SearchWeb()
    answer = AnswerQuestion()
    
    # Connect the nodes
    # If DecideAction returns "search", go to SearchWeb
    decide - "search" >> search
    
    # If DecideAction returns "answer", go to AnswerQuestion
    decide - "answer" >> answer
    
    # After SearchWeb completes and returns "decide", go back to DecideAction
    search - "decide" >> decide
    
    # Create and return the flow, starting with the DecideAction node
    return Flow(start=decide)
</file>

<file path="cookbook/pocketflow-agent/main.py">
import sys
from flow import create_agent_flow

def main():
    """Simple function to process a question."""
    # Default question
    default_question = "Who won the Nobel Prize in Physics 2024?"
    
    # Get question from command line if provided with --
    question = default_question
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            question = arg[2:]
            break
    
    # Create the agent flow
    agent_flow = create_agent_flow()
    
    # Process the question
    shared = {"question": question}
    print(f"ðŸ¤” Processing question: {question}")
    agent_flow.run(shared)
    print("\nðŸŽ¯ Final Answer:")
    print(shared.get("answer", "No answer found"))

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-agent/nodes.py">
from pocketflow import Node
from utils import call_llm, search_web_duckduckgo
import yaml

class DecideAction(Node):
    def prep(self, shared):
        """Prepare the context and question for the decision-making process."""
        # Get the current context (default to "No previous search" if none exists)
        context = shared.get("context", "No previous search")
        # Get the question from the shared store
        question = shared["question"]
        # Return both for the exec step
        return question, context
        
    def exec(self, inputs):
        """Call the LLM to decide whether to search or answer."""
        question, context = inputs
        
        print(f"ðŸ¤” Agent deciding what to do next...")
        
        # Create a prompt to help the LLM decide what to do next with proper yaml formatting
        prompt = f"""
### CONTEXT
You are a research assistant that can search the web.
Question: {question}
Previous Research: {context}

### ACTION SPACE
[1] search
  Description: Look up more information on the web
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Answer the question with current knowledge
  Parameters:
    - answer (str): Final answer to the question

## NEXT ACTION
Decide the next action based on the context and available actions.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: search OR answer
reason: <why you chose this action>
answer: <if action is answer>
search_query: <specific search query if action is search>
```
IMPORTANT: Make sure to:
1. Use proper indentation (4 spaces) for all multi-line fields
2. Use the | character for multi-line text fields
3. Keep single-line fields without the | character
"""
        
        # Call the LLM to make a decision
        response = call_llm(prompt)
        
        # Parse the response to get the decision
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        decision = yaml.safe_load(yaml_str)
        
        return decision
    
    def post(self, shared, prep_res, exec_res):
        """Save the decision and determine the next step in the flow."""
        # If LLM decided to search, save the search query
        if exec_res["action"] == "search":
            shared["search_query"] = exec_res["search_query"]
            print(f"ðŸ” Agent decided to search for: {exec_res['search_query']}")
        else:
            shared["context"] = exec_res["answer"] #save the context if LLM gives the answer without searching.
            print(f"ðŸ’¡ Agent decided to answer the question")
        
        # Return the action to determine the next node in the flow
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        """Get the search query from the shared store."""
        return shared["search_query"]
        
    def exec(self, search_query):
        """Search the web for the given query."""
        # Call the search utility function
        print(f"ðŸŒ Searching the web for: {search_query}")
        results = search_web_duckduckgo(search_query)
        return results
    
    def post(self, shared, prep_res, exec_res):
        """Save the search results and go back to the decision node."""
        # Add the search results to the context in the shared store
        previous = shared.get("context", "")
        shared["context"] = previous + "\n\nSEARCH: " + shared["search_query"] + "\nRESULTS: " + exec_res
        
        print(f"ðŸ“š Found information, analyzing results...")
        
        # Always go back to the decision node after searching
        return "decide"

class AnswerQuestion(Node):
    def prep(self, shared):
        """Get the question and context for answering."""
        return shared["question"], shared.get("context", "")
        
    def exec(self, inputs):
        """Call the LLM to generate a final answer."""
        question, context = inputs
        
        print(f"âœï¸ Crafting final answer...")
        
        # Create a prompt for the LLM to answer the question
        prompt = f"""
### CONTEXT
Based on the following information, answer the question.
Question: {question}
Research: {context}

## YOUR ANSWER:
Provide a comprehensive answer using the research results.
"""
        # Call the LLM to generate an answer
        answer = call_llm(prompt)
        return answer
    
    def post(self, shared, prep_res, exec_res):
        """Save the final answer and complete the flow."""
        # Save the answer in the shared store
        shared["answer"] = exec_res
        
        print(f"âœ… Answer generated successfully")
        
        # We're done - no need to continue the flow
        return "done"
</file>

<file path="cookbook/pocketflow-agent/README.md">
# Research Agent

This project demonstrates a simple yet powerful LLM-powered research agent. This implementation is based directly on the tutorial: [LLM Agents are simply Graph â€” Tutorial For Dummies](https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial).

ðŸ‘‰ Run the tutorial in your browser: [Try Google Colab Notebook](
https://colab.research.google.com/github/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-agent/demo.ipynb)

## Features

- Performs web searches to gather information
- Makes decisions about when to search vs. when to answer
- Generates comprehensive answers based on research findings

## Getting Started

1. Install the packages you need with this simple command:
```bash
pip install -r requirements.txt
```

2. Let's get your OpenAI API key ready:

```bash
export OPENAI_API_KEY="your-api-key-here"
```

3. Let's do a quick check to make sure your API key is working properly:

```bash
python utils.py
```

This will test both the LLM call and web search features. If you see responses, you're good to go!

4. Try out the agent with the default question (about Nobel Prize winners):

```bash
python main.py
```

5. Got a burning question? Ask anything you want by using the `--` prefix:

```bash
python main.py --"What is quantum computing?"
```

## How It Works?

The magic happens through a simple but powerful graph structure with three main parts:

```mermaid
graph TD
    A[DecideAction] -->|"search"| B[SearchWeb]
    A -->|"answer"| C[AnswerQuestion]
    B -->|"decide"| A
```

Here's what each part does:
1. **DecideAction**: The brain that figures out whether to search or answer
2. **SearchWeb**: The researcher that goes out and finds information
3. **AnswerQuestion**: The writer that crafts the final answer

Here's what's in each file:
- [`main.py`](./main.py): The starting point - runs the whole show!
- [`flow.py`](./flow.py): Connects everything together into a smart agent
- [`nodes.py`](./nodes.py): The building blocks that make decisions and take actions
- [`utils.py`](./utils.py): Helper functions for talking to the LLM and searching the web
</file>

<file path="cookbook/pocketflow-agent/requirements.txt">
pocketflow>=0.0.1
duckduckgo-search>=7.5.2     # For web search
aiohttp>=3.8.0               # For HTTP requests
openai>=1.0.0                # For LLM calls 
requests>=2.25.1             # For HTTP requests
PyYAML>=6.0.2                # For YAML parsing
</file>

<file path="cookbook/pocketflow-agent/utils.py">
from openai import OpenAI
import os
from duckduckgo_search import DDGS
import requests

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

def search_web_duckduckgo(query):
    results = DDGS().text(query, max_results=5)
    # Convert results to a string
    results_str = "\n\n".join([f"Title: {r['title']}\nURL: {r['href']}\nSnippet: {r['body']}" for r in results])
    return results_str

def search_web_brave(query):

    url = f"https://api.search.brave.com/res/v1/web/search?q={query}"
    api_key = "your brave search api key"

    headers = {
        "accept": "application/json",
        "Accept-Encoding": "gzip",
        "x-subscription-token": api_key
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        results = data['web']['results']
        results_str = "\n\n".join([f"Title: {r['title']}\nURL: {r['url']}\nDescription: {r['description']}" for r in results])     
    else:
        print(f"Request failed with status code: {response.status_code}")
    return results_str
    
if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")

    print("## Testing search_web")
    query = "Who won the Nobel Prize in Physics 2024?"
    print(f"## Query: {query}")
    results = search_web_duckduckgo(query)
    print(f"## Results: {results}")
</file>

<file path="cookbook/pocketflow-async-basic/flow.py">
"""AsyncFlow implementation for recipe finder."""

from pocketflow import AsyncFlow, Node
from nodes import FetchRecipes, SuggestRecipe, GetApproval

class NoOp(Node):
    """Node that does nothing, used to properly end the flow."""
    pass

def create_flow():
    """Create and connect nodes into a flow."""
    
    # Create nodes
    fetch = FetchRecipes()
    suggest = SuggestRecipe()
    approve = GetApproval()
    end = NoOp()
    
    # Connect nodes
    fetch - "suggest" >> suggest
    suggest - "approve" >> approve
    approve - "retry" >> suggest  # Loop back for another suggestion
    approve - "accept" >> end     # Properly end the flow
    
    # Create flow starting with fetch
    flow = AsyncFlow(start=fetch)
    return flow
</file>

<file path="cookbook/pocketflow-async-basic/main.py">
import asyncio
from flow import create_flow

async def main():
    """Run the recipe finder flow."""
    # Create flow
    flow = create_flow()
    
    # Create shared store
    shared = {}
    
    # Run flow
    print("\nWelcome to Recipe Finder!")
    print("------------------------")
    await flow.run_async(shared)
    print("\nThanks for using Recipe Finder!")

if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-async-basic/nodes.py">
from pocketflow import AsyncNode
from utils import fetch_recipes, call_llm_async, get_user_input

class FetchRecipes(AsyncNode):
    """AsyncNode that fetches recipes."""
    
    async def prep_async(self, shared):
        """Get ingredient from user."""
        ingredient = await get_user_input("Enter ingredient: ")
        return ingredient
    
    async def exec_async(self, ingredient):
        """Fetch recipes asynchronously."""
        recipes = await fetch_recipes(ingredient)
        return recipes
    
    async def post_async(self, shared, prep_res, recipes):
        """Store recipes and continue."""
        shared["recipes"] = recipes
        shared["ingredient"] = prep_res
        return "suggest"

class SuggestRecipe(AsyncNode):
    """AsyncNode that suggests a recipe using LLM."""
    
    async def prep_async(self, shared):
        """Get recipes from shared store."""
        return shared["recipes"]
    
    async def exec_async(self, recipes):
        """Get suggestion from LLM."""
        suggestion = await call_llm_async(
            f"Choose best recipe from: {', '.join(recipes)}"
        )
        return suggestion
    
    async def post_async(self, shared, prep_res, suggestion):
        """Store suggestion and continue."""
        shared["suggestion"] = suggestion
        return "approve"

class GetApproval(AsyncNode):
    """AsyncNode that gets user approval."""
    
    async def prep_async(self, shared):
        """Get current suggestion."""
        return shared["suggestion"]
    
    async def exec_async(self, suggestion):
        """Ask for user approval."""
        answer = await get_user_input(f"\nAccept this recipe? (y/n): ")
        return answer
    
    async def post_async(self, shared, prep_res, answer):
        """Handle user's decision."""
        if answer == "y":
            print("\nGreat choice! Here's your recipe...")
            print(f"Recipe: {shared['suggestion']}")
            print(f"Ingredient: {shared['ingredient']}")
            return "accept"
        else:
            print("\nLet's try another recipe...")
            return "retry"
</file>

<file path="cookbook/pocketflow-async-basic/README.md">
# PocketFlow Async Basic Example

This example demonstrates async operations using a simple Recipe Finder that:
1. Fetches recipes from an API (async HTTP)
2. Processes them with an LLM (async LLM)
3. Waits for user confirmation (async input)

## What this Example Does

When you run the example:
1. You enter an ingredient (e.g., "chicken")
2. It searches for recipes (async API call)
3. It suggests a recipe (async LLM call)
4. You approve or reject the suggestion
5. If rejected, it tries again with a different recipe

## How it Works

1. **FetchRecipes (AsyncNode)**
   ```python
   async def prep_async(self, shared):
       ingredient = input("Enter ingredient: ")
       return ingredient

   async def exec_async(self, ingredient):
       # Async API call
       recipes = await fetch_recipes(ingredient)
       return recipes
   ```

2. **SuggestRecipe (AsyncNode)**
   ```python
   async def exec_async(self, recipes):
       # Async LLM call
       suggestion = await call_llm_async(
           f"Choose best recipe from: {recipes}"
       )
       return suggestion
   ```

3. **GetApproval (AsyncNode)**
   ```python
   async def post_async(self, shared, prep_res, suggestion):
       # Async user input
       answer = await get_user_input(
           f"Accept {suggestion}? (y/n): "
       )
       return "accept" if answer == "y" else "retry"
   ```

## Running the Example

```bash
pip install -r requirements.txt
python main.py
```

## Sample Interaction

```
Enter ingredient: chicken
Fetching recipes...
Found 3 recipes.

Suggesting best recipe...
How about: Grilled Chicken with Herbs

Accept this recipe? (y/n): n
Suggesting another recipe...
How about: Chicken Stir Fry

Accept this recipe? (y/n): y
Great choice! Here's your recipe...
```

## Key Concepts

1. **Async Operations**: Using `async/await` for:
   - API calls (non-blocking I/O)
   - LLM calls (potentially slow)
   - User input (waiting for response)

2. **AsyncNode Methods**:
   - `prep_async`: Setup and data gathering
   - `exec_async`: Main async processing
   - `post_async`: Post-processing and decisions

3. **Flow Control**:
   - Actions ("accept"/"retry") control flow
   - Retry loop for rejected suggestions
</file>

<file path="cookbook/pocketflow-async-basic/requirements.txt">
pocketflow
aiohttp>=3.8.0  # For async HTTP requests
openai>=1.0.0   # For async LLM calls
</file>

<file path="cookbook/pocketflow-async-basic/utils.py">
import asyncio
import aiohttp
from openai import AsyncOpenAI

async def fetch_recipes(ingredient):
    """Fetch recipes from an API asynchronously."""
    print(f"Fetching recipes for {ingredient}...")
    
    # Simulate API call with delay
    await asyncio.sleep(1)
    
    # Mock recipes (in real app, would fetch from API)
    recipes = [
        f"{ingredient} Stir Fry",
        f"Grilled {ingredient} with Herbs",
        f"Baked {ingredient} with Vegetables"
    ]
    
    print(f"Found {len(recipes)} recipes.")
    
    return recipes

async def call_llm_async(prompt):
    """Make async LLM call."""
    print("\nSuggesting best recipe...")
    
    # Simulate LLM call with delay
    await asyncio.sleep(1)
    
    # Mock LLM response (in real app, would call OpenAI)
    recipes = prompt.split(": ")[1].split(", ")
    suggestion = recipes[1]  # Always suggest second recipe
    
    print(f"How about: {suggestion}")
    return suggestion

async def get_user_input(prompt):
    """Get user input asynchronously."""
    # Create event loop to handle async input
    loop = asyncio.get_event_loop()
    
    # Get input in a non-blocking way
    answer = await loop.run_in_executor(None, input, prompt)

    return answer.lower()
</file>

<file path="cookbook/pocketflow-batch/translations/README_CHINESE.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | ä¸­æ–‡ | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow æ˜¯ä¸€ä¸ª[100è¡Œä»£ç ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)çš„æžç®€ä¸»ä¹‰LLMæ¡†æž¶

- **è½»é‡çº§**ï¼šä»…100è¡Œä»£ç ã€‚é›¶è‡ƒè‚¿ï¼Œé›¶ä¾èµ–ï¼Œé›¶ä¾›åº”å•†é”å®šã€‚
  
- **è¡¨è¾¾åŠ›å¼º**ï¼šåŒ…å«ä½ å–œçˆ±çš„ä¸€åˆ‡â€”([å¤š-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[æ™ºèƒ½ä½“](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ï¼Œ[å·¥ä½œæµ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ï¼Œ[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ç­‰ç­‰ã€‚

- **[æ™ºèƒ½ä½“ç¼–ç ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**ï¼šè®©AIæ™ºèƒ½ä½“ï¼ˆä¾‹å¦‚Cursor AIï¼‰æž„å»ºæ™ºèƒ½ä½“â€”ç”Ÿäº§åŠ›æå‡10å€ï¼

Pocket Flowå…¥é—¨ï¼š
- å®‰è£…æ–¹å¼ï¼Œ```pip install pocketflow```æˆ–è€…ç›´æŽ¥å¤åˆ¶[æºä»£ç ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ï¼ˆä»…100è¡Œï¼‰ã€‚
- äº†è§£æ›´å¤šï¼ŒæŸ¥çœ‹[æ–‡æ¡£](https://the-pocket.github.io/PocketFlow/)ã€‚äº†è§£åŠ¨æœºï¼Œé˜…è¯»[æ•…äº‹](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)ã€‚
- æœ‰é—®é¢˜ï¼ŸæŸ¥çœ‹è¿™ä¸ª[AIåŠ©æ‰‹](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant)ï¼Œæˆ–[åˆ›å»ºissueï¼](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ åŠ å…¥æˆ‘ä»¬çš„[Discord](https://discord.gg/hUHHE9Sa6T)ï¼Œä¸Žå…¶ä»–ä½¿ç”¨Pocket Flowæž„å»ºåº”ç”¨çš„å¼€å‘è€…äº¤æµï¼
- ðŸŽ‰ Pocket Flowæœ€åˆæ˜¯Pythonç‰ˆæœ¬ï¼Œä½†æˆ‘ä»¬çŽ°åœ¨æœ‰[Typescript](https://github.com/The-Pocket/PocketFlow-Typescript)ï¼Œ[Java](https://github.com/The-Pocket/PocketFlow-Java)ï¼Œ[C++](https://github.com/The-Pocket/PocketFlow-CPP)å’Œ[Go](https://github.com/The-Pocket/PocketFlow-Go)ç‰ˆæœ¬ï¼

## ä¸ºä»€ä¹ˆé€‰æ‹©Pocket Flowï¼Ÿ

å½“å‰çš„LLMæ¡†æž¶è¿‡äºŽè‡ƒè‚¿... ä½ åªéœ€è¦100è¡Œä»£ç å°±èƒ½æž„å»ºLLMæ¡†æž¶ï¼

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **æŠ½è±¡**          | **åº”ç”¨ç‰¹å®šåŒ…è£…å™¨**                                      | **ä¾›åº”å•†ç‰¹å®šåŒ…è£…å™¨**                                    | **ä»£ç è¡Œæ•°**       | **å¤§å°**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒQA, æ‘˜è¦)</sub></sup>              | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒOpenAI, Pineconeç­‰)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒFileReadTool, SerperDevTool)</sub></sup>         | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒOpenAI, Anthropic, Pineconeç­‰)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒCodeAgent, VisitWebTool)</sub></sup>         | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒDuckDuckGo, Hugging Faceç­‰)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼Œè¯­ä¹‰æœç´¢)</sub></sup>                     | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒPostgresStore, SqliteSaverç­‰) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒTool Agent, Chat Agent)</sub></sup>              | å¾ˆå¤š <sup><sub>[å¯é€‰]<br> (ä¾‹å¦‚ï¼ŒOpenAI, Pineconeç­‰)</sub></sup>        | 7K <br><sup><sub>(ä»…æ ¸å¿ƒ)</sub></sup>    | +26MB <br><sup><sub>(ä»…æ ¸å¿ƒ)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **æ— **                                                 | **æ— **                                                  | **100**       | **+56KB**                  |

</div>

## Pocket Flowå¦‚ä½•å·¥ä½œï¼Ÿ

è¿™[100è¡Œä»£ç ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)æ•æ‰äº†LLMæ¡†æž¶çš„æ ¸å¿ƒæŠ½è±¡ï¼šå›¾ï¼ˆGraphï¼‰ï¼
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ä»Žè¿™é‡Œå¼€å§‹ï¼Œå¾ˆå®¹æ˜“å®žçŽ°æµè¡Œçš„è®¾è®¡æ¨¡å¼ï¼Œå¦‚([å¤š-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[æ™ºèƒ½ä½“](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ï¼Œ[å·¥ä½œæµ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ï¼Œ[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ç­‰ã€‚
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ä»¥ä¸‹æ˜¯åŸºç¡€æ•™ç¨‹ï¼š

<div align="center">
  
|  åç§°  | éš¾åº¦    |  æè¿°  |  
| :-------------:  | :-------------: | :--------------------- |  
| [èŠå¤©](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *å…¥é—¨*   | å¸¦æœ‰å¯¹è¯åŽ†å²çš„åŸºç¡€èŠå¤©æœºå™¨äºº |
| [ç»“æž„åŒ–è¾“å‡º](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *å…¥é—¨* | é€šè¿‡æç¤ºä»Žç®€åŽ†ä¸­æå–ç»“æž„åŒ–æ•°æ® |
| [å·¥ä½œæµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *å…¥é—¨*   | ä¸€ä¸ªå†™ä½œå·¥ä½œæµï¼ŒåŒ…æ‹¬å¤§çº²ç¼–å†™ã€å†…å®¹åˆ›ä½œå’Œæ ·å¼åº”ç”¨ |
| [æ™ºèƒ½ä½“](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *å…¥é—¨*   | ä¸€ä¸ªå¯ä»¥æœç´¢ç½‘ç»œå¹¶å›žç­”é—®é¢˜çš„ç ”ç©¶æ™ºèƒ½ä½“ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *å…¥é—¨*   | ä¸€ä¸ªç®€å•çš„æ£€ç´¢å¢žå¼ºç”Ÿæˆè¿‡ç¨‹ |
| [æ‰¹å¤„ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *å…¥é—¨* | ä¸€ä¸ªå°†markdownå†…å®¹ç¿»è¯‘æˆå¤šç§è¯­è¨€çš„æ‰¹å¤„ç†å™¨ |
| [æµå¼å¤„ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *å…¥é—¨*   | å…·æœ‰ç”¨æˆ·ä¸­æ–­åŠŸèƒ½çš„å®žæ—¶LLMæµå¼æ¼”ç¤º |
| [èŠå¤©æŠ¤æ ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *å…¥é—¨*  | ä¸€ä¸ªä»…å¤„ç†æ—…è¡Œç›¸å…³æŸ¥è¯¢çš„æ—…è¡Œé¡¾é—®èŠå¤©æœºå™¨äºº |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *åˆçº§* | ä½¿ç”¨map-reduceæ¨¡å¼è¿›è¡Œæ‰¹é‡è¯„ä¼°çš„ç®€åŽ†èµ„æ ¼å¤„ç†å™¨ |
| [å¤šæ™ºèƒ½ä½“](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *åˆçº§* | ä¸¤ä¸ªæ™ºèƒ½ä½“ä¹‹é—´å¼‚æ­¥é€šä¿¡çš„ç¦å¿Œè¯æ¸¸æˆ |
| [ç›‘ç£è€…](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *åˆçº§* | ç ”ç©¶æ™ºèƒ½ä½“å˜å¾—ä¸å¯é ...è®©æˆ‘ä»¬æž„å»ºä¸€ä¸ªç›‘ç£æµç¨‹|
| [å¹¶è¡Œ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *åˆçº§*   | å±•ç¤º3å€åŠ é€Ÿçš„å¹¶è¡Œæ‰§è¡Œæ¼”ç¤º |
| [å¹¶è¡Œæµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *åˆçº§*   | å±•ç¤ºä½¿ç”¨å¤šä¸ªè¿‡æ»¤å™¨å®žçŽ°8å€åŠ é€Ÿçš„å¹¶è¡Œå›¾åƒå¤„ç†æ¼”ç¤º |
| [å¤šæ•°æŠ•ç¥¨](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *åˆçº§* | é€šè¿‡èšåˆå¤šæ¬¡è§£å†³æ–¹æ¡ˆå°è¯•æ¥æé«˜æŽ¨ç†å‡†ç¡®æ€§ |
| [æ€è€ƒ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *åˆçº§*   | é€šè¿‡æ€ç»´é“¾è§£å†³å¤æ‚æŽ¨ç†é—®é¢˜ |
| [è®°å¿†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *åˆçº§* | å…·æœ‰çŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„èŠå¤©æœºå™¨äºº |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *åˆçº§* | ä½¿ç”¨è‡ªåŠ¨è°ƒè¯•å¾ªçŽ¯å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢ |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *åˆçº§* |  ä½¿ç”¨æ¨¡åž‹ä¸Šä¸‹æ–‡åè®®è¿›è¡Œæ•°å€¼è¿ç®—çš„æ™ºèƒ½ä½“ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *åˆçº§* | ä½¿ç”¨æ™ºèƒ½ä½“åˆ°æ™ºèƒ½ä½“åè®®åŒ…è£…çš„æ™ºèƒ½ä½“ï¼Œç”¨äºŽæ™ºèƒ½ä½“é—´é€šä¿¡ |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *åˆçº§* | å…·æœ‰SSEæ›´æ–°çš„äººå·¥å®¡æ ¸å¾ªçŽ¯çš„æœ€å°WebæœåŠ¡ |

</div>

ðŸ‘€ æƒ³çœ‹å…¶ä»–å…¥é—¨æ•™ç¨‹ï¼Ÿ[åˆ›å»ºä¸€ä¸ªissueï¼](https://github.com/The-Pocket/PocketFlow/issues/new)

## å¦‚ä½•ä½¿ç”¨Pocket Flowï¼Ÿ

ðŸš€ é€šè¿‡**æ™ºèƒ½ä½“ç¼–ç **â€”æœ€å¿«çš„LLMåº”ç”¨å¼€å‘èŒƒå¼â€”*äººç±»è®¾è®¡*ï¼Œ*æ™ºèƒ½ä½“ç¼–ç *ï¼

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ä»¥ä¸‹æ˜¯æ›´å¤æ‚LLMåº”ç”¨çš„ç¤ºä¾‹ï¼š

<div align="center">
  
|  åº”ç”¨åç§°     |  éš¾åº¦    | ä¸»é¢˜  | äººç±»è®¾è®¡ | æ™ºèƒ½ä½“ä»£ç  |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [ç”¨Cursoræž„å»ºCursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>æˆ‘ä»¬å¾ˆå¿«å°†è¾¾åˆ°å¥‡ç‚¹...</sup></sub> | â˜…â˜…â˜… <br> *é«˜çº§*   | [æ™ºèƒ½ä½“](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ä»£ç åº“çŸ¥è¯†æž„å»ºå™¨](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>ç”Ÿå‘½å¤ªçŸ­æš‚ï¼Œä¸åº”è¯¥å›°æƒ‘åœ°ç›¯ç€ä»–äººçš„ä»£ç </sup></sub> |  â˜…â˜…â˜† <br> *ä¸­çº§* | [å·¥ä½œæµ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [è¯¢é—®AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>è¯¢é—®AI Paul Grahamï¼Œä»¥é˜²ä½ æ²¡è¢«å½•å–</sup></sub> | â˜…â˜…â˜† <br> *ä¸­çº§*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtubeæ‘˜è¦å™¨](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> åƒä½ 5å²ä¸€æ ·å‘ä½ è§£é‡ŠYouTubeè§†é¢‘ </sup></sub> | â˜…â˜†â˜† <br> *åˆçº§*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [å†·å¯åŠ¨ç”Ÿæˆå™¨](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> å°†å†·é—¨çº¿ç´¢è½¬å˜ä¸ºçƒ­é—¨çš„å³æ—¶ç ´å†°å·¥å…· </sup></sub> | â˜…â˜†â˜† <br> *åˆçº§*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Webæœç´¢](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- æƒ³å­¦ä¹ **æ™ºèƒ½ä½“ç¼–ç **ï¼Ÿ

  - æŸ¥çœ‹[æˆ‘çš„YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)èŽ·å–å…³äºŽå¦‚ä½•åˆ¶ä½œä¸Šè¿°åº”ç”¨çš„è§†é¢‘æ•™ç¨‹ï¼

  - æƒ³æž„å»ºè‡ªå·±çš„LLMåº”ç”¨ï¼Ÿé˜…è¯»è¿™ç¯‡[æ–‡ç« ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)ï¼ä»Ž[è¿™ä¸ªæ¨¡æ¿](https://github.com/The-Pocket/PocketFlow-Template-Python)å¼€å§‹ï¼
</file>

<file path="cookbook/pocketflow-batch/translations/README_FRENCH.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ framework LLM minimaliste en 100 lignes" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | FranÃ§ais | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow est un framework LLM minimaliste en [100 lignes](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **LÃ©ger** : Seulement 100 lignes. ZÃ©ro superflu, zÃ©ro dÃ©pendance, zÃ©ro verrouillage fournisseur.
  
- **Expressif** : Tout ce que vous aimez â€” ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), et plus encore.

- **[Programmation Agentique](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)** : Laissez les Agents IA (par exemple, Cursor AI) crÃ©er des Agents â€” augmentez votre productivitÃ© par 10 !

Commencer avec Pocket Flow :
- Pour installer, ```pip install pocketflow``` ou copiez simplement le [code source](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (seulement 100 lignes).
- Pour en savoir plus, consultez la [documentation](https://the-pocket.github.io/PocketFlow/). Pour comprendre la motivation, lisez l'[histoire](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Des questions ? Consultez cet [Assistant IA](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), ou [crÃ©ez une issue !](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Rejoignez notre [Discord](https://discord.gg/hUHHE9Sa6T) pour vous connecter avec d'autres dÃ©veloppeurs utilisant Pocket Flow !
- ðŸŽ‰ Pocket Flow est initialement en Python, mais nous avons maintenant des versions en [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) et [Go](https://github.com/The-Pocket/PocketFlow-Go) !

## Pourquoi Pocket Flow ?

Les frameworks LLM actuels sont surchargÃ©s... Vous n'avez besoin que de 100 lignes pour un framework LLM !

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **Abstraction**          | **Wrappers spÃ©cifiques aux applications**                                      | **Wrappers spÃ©cifiques aux fournisseurs**                                    | **Lignes**       | **Taille**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | Nombreux <br><sup><sub>(ex., QA, RÃ©sumÃ©)</sub></sup>              | Nombreux <br><sup><sub>(ex., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | Nombreux <br><sup><sub>(ex., FileReadTool, SerperDevTool)</sub></sup>         | Nombreux <br><sup><sub>(ex., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | Quelques <br><sup><sub>(ex., CodeAgent, VisitWebTool)</sub></sup>         | Quelques <br><sup><sub>(ex., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | Quelques <br><sup><sub>(ex., Recherche SÃ©mantique)</sub></sup>                     | Quelques <br><sup><sub>(ex., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | Quelques <br><sup><sub>(ex., Tool Agent, Chat Agent)</sub></sup>              | Nombreux <sup><sub>[Optionnel]<br> (ex., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(core-only)</sub></sup>    | +26MB <br><sup><sub>(core-only)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **Aucun**                                                 | **Aucun**                                                  | **100**       | **+56KB**                  |

</div>

## Comment fonctionne Pocket Flow ?

Les [100 lignes](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturent l'abstraction fondamentale des frameworks LLM : le Graph !
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

De lÃ , il est facile d'implÃ©menter des modÃ¨les de conception populaires comme ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Voici des tutoriels de base :

<div align="center">
  
|  Nom  | DifficultÃ©    |  Description  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un chatbot basique avec historique de conversation |
| [Sortie StructurÃ©e](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *DÃ©butant* | Extraction de donnÃ©es structurÃ©es Ã  partir de CV par prompt |
| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un workflow d'Ã©criture qui planifie, rÃ©dige du contenu et applique un style |
| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un agent de recherche qui peut chercher sur le web et rÃ©pondre aux questions |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un processus simple de gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration |
| [Batch](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *DÃ©butant* | Un processeur par lots qui traduit du contenu markdown en plusieurs langues |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *DÃ©butant*   | Une dÃ©mo de streaming LLM en temps rÃ©el avec capacitÃ© d'interruption utilisateur |
| [Garde-fou de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *DÃ©butant*  | Un chatbot conseiller de voyage qui ne traite que les requÃªtes liÃ©es au voyage |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un processeur de qualification de CV utilisant le modÃ¨le map-reduce pour l'Ã©valuation par lots |
| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un jeu de Tabou pour la communication asynchrone entre deux agents |
| [Superviseur](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | L'agent de recherche devient peu fiable... Construisons un processus de supervision |
| [ParallÃ¨le](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | Une dÃ©mo d'exÃ©cution parallÃ¨le montrant une accÃ©lÃ©ration de 3x |
| [Flux ParallÃ¨le](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | Une dÃ©mo de traitement d'image parallÃ¨le montrant une accÃ©lÃ©ration de 8x avec plusieurs filtres |
| [Vote Majoritaire](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | AmÃ©liorer la prÃ©cision du raisonnement en agrÃ©geant plusieurs tentatives de solution |
| [RÃ©flexion](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | RÃ©soudre des problÃ¨mes de raisonnement complexes grÃ¢ce Ã  la ChaÃ®ne de PensÃ©e |
| [MÃ©moire](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un chatbot avec mÃ©moire Ã  court et long terme |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Convertir le langage naturel en requÃªtes SQL avec une boucle d'auto-dÃ©bogage |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *IntermÃ©diaire* |  Agent utilisant le Protocole de Contexte de ModÃ¨le pour les opÃ©rations numÃ©riques |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Agent encapsulÃ© avec le protocole Agent-to-Agent pour la communication inter-agent |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un service web minimal pour une boucle de rÃ©vision humaine avec mises Ã  jour SSE |

</div>

ðŸ‘€ Vous voulez voir d'autres tutoriels pour dÃ©butants ? [CrÃ©ez une issue !](https://github.com/The-Pocket/PocketFlow/issues/new)

## Comment utiliser Pocket Flow ?

ðŸš€ Par la **Programmation Agentique** â€” le paradigme de dÃ©veloppement d'applications LLM le plus rapide â€” oÃ¹ *les humains conÃ§oivent* et *les agents programment* !

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Voici des exemples d'applications LLM plus complexes :

<div align="center">
  
|  Nom de l'application     |  DifficultÃ©    | Sujets  | Conception Humaine | Code Agent |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Construire Cursor avec Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Nous atteindrons bientÃ´t la singularitÃ© ...</sup></sub> | â˜…â˜…â˜… <br> *AvancÃ©*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Constructeur de Connaissances de Base de Code](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>La vie est trop courte pour rester perplexe devant le code des autres</sup></sub> |  â˜…â˜…â˜† <br> *Moyen* | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Interroger l'IA Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Interrogez l'IA Paul Graham, au cas oÃ¹ vous ne seriez pas acceptÃ©</sup></sub> | â˜…â˜…â˜† <br> *Moyen*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [RÃ©sumeur Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Vous explique les vidÃ©os YouTube comme si vous aviez 5 ans </sup></sub> | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Document de conception](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [GÃ©nÃ©rateur d'Accroche pour Email](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Des brise-glaces instantanÃ©s qui transforment les prospects froids en prospects chauds </sup></sub> | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Recherche Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Document de conception](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Vous voulez apprendre la **Programmation Agentique** ?

  - Consultez [ma chaÃ®ne YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) pour des tutoriels vidÃ©o sur la faÃ§on dont certaines applications ci-dessus sont crÃ©Ã©es !

  - Vous voulez crÃ©er votre propre application LLM ? Lisez cet [article](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to) ! Commencez avec [ce modÃ¨le](https://github.com/The-Pocket/PocketFlow-Template-Python) !
</file>

<file path="cookbook/pocketflow-batch/translations/README_GERMAN.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-Zeilen minimalistisches LLM-Framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | Deutsch | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![Lizenz: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow ist ein [100-zeiliges](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalistisches LLM-Framework

- **Leichtgewichtig**: Nur 100 Zeilen. Kein Ballast, keine AbhÃ¤ngigkeiten, keine Anbieterbindung.
  
- **Ausdrucksstark**: Alles, was Sie liebenâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agenten](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), und mehr.

- **[Agenten-basiertes Programmieren](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Lassen Sie KI-Agenten (z.B. Cursor AI) Agenten bauenâ€”10-fache ProduktivitÃ¤tssteigerung!

Erste Schritte mit Pocket Flow:
- Zur Installation, ```pip install pocketflow```oder kopieren Sie einfach den [Quellcode](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (nur 100 Zeilen).
- Um mehr zu erfahren, schauen Sie in die [Dokumentation](https://the-pocket.github.io/PocketFlow/). Um die Motivation zu verstehen, lesen Sie die [Geschichte](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Haben Sie Fragen? Schauen Sie sich diesen [KI-Assistenten](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant) an, oder [erstellen Sie ein Issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Treten Sie unserem [Discord](https://discord.gg/hUHHE9Sa6T) bei, um sich mit anderen Entwicklern zu vernetzen, die mit Pocket Flow arbeiten!
- ðŸŽ‰ Pocket Flow ist ursprÃ¼nglich in Python geschrieben, aber wir haben jetzt auch Versionen fÃ¼r [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) und [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## Warum Pocket Flow?

Aktuelle LLM-Frameworks sind aufgeblÃ¤ht... Sie brauchen nur 100 Zeilen fÃ¼r ein LLM-Framework!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **Abstraktion**          | **App-spezifische Wrapper**                                      | **Anbieter-spezifische Wrapper**                                    | **Zeilen**       | **GrÃ¶ÃŸe**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | Viele <br><sup><sub>(z.B. QA, Zusammenfassung)</sub></sup>              | Viele <br><sup><sub>(z.B. OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | Viele <br><sup><sub>(z.B. FileReadTool, SerperDevTool)</sub></sup>         | Viele <br><sup><sub>(z.B. OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | Einige <br><sup><sub>(z.B. CodeAgent, VisitWebTool)</sub></sup>         | Einige <br><sup><sub>(z.B. DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | Einige <br><sup><sub>(z.B. Semantic Search)</sub></sup>                     | Einige <br><sup><sub>(z.B. PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | Einige <br><sup><sub>(z.B. Tool Agent, Chat Agent)</sub></sup>              | Viele <sup><sub>[Optional]<br> (z.B. OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(nur Kern)</sub></sup>    | +26MB <br><sup><sub>(nur Kern)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **Keine**                                                 | **Keine**                                                  | **100**       | **+56KB**                  |

</div>

## Wie funktioniert Pocket Flow?

Die [100 Zeilen](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) erfassen die Kernabstraktion von LLM-Frameworks: Graph!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

Von dort aus ist es einfach, beliebte Designmuster wie ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agenten](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc. zu implementieren.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Hier sind grundlegende Tutorials:

<div align="center">
  
|  Name  | Schwierigkeit    |  Beschreibung  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein einfacher Chatbot mit GesprÃ¤chsverlauf |
| [Strukturierte Ausgabe](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *AnfÃ¤nger* | Extraktion strukturierter Daten aus LebenslÃ¤ufen durch Prompting |
| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein Schreib-Workflow, der gliedert, Inhalte schreibt und Formatierungen anwendet |
| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein Recherche-Agent, der im Web suchen und Fragen beantworten kann |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein einfacher Abrufsaugmentierter Generierungsprozess |
| [Batch](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *AnfÃ¤nger* | Ein Batch-Prozessor, der Markdown-Inhalte in mehrere Sprachen Ã¼bersetzt |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Eine Echtzeit-LLM-Streaming-Demo mit Benutzer-Unterbrechungsfunktion |
| [Chat-Leitplanke](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *AnfÃ¤nger*  | Ein Reiseberater-Chatbot, der nur reisebezogene Anfragen verarbeitet |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *Einsteiger* | Ein Lebenslauf-Qualifikationsprozessor, der das Map-Reduce-Muster fÃ¼r Batch-Auswertungen verwendet |
| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Einsteiger* | Ein Tabu-Wortspiel fÃ¼r asynchrone Kommunikation zwischen zwei Agenten |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Einsteiger* | Forschungsagent wird unzuverlÃ¤ssig... Bauen wir einen Ãœberwachungsprozess auf|
| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Einsteiger*   | Eine parallele AusfÃ¼hrungsdemo, die 3-fache Beschleunigung zeigt |
| [Paralleler Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Einsteiger*   | Eine parallele Bildverarbeitungsdemo, die 8-fache Beschleunigung mit mehreren Filtern zeigt |
| [Mehrheitswahl](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *Einsteiger* | Verbesserte Schlussfolgerungsgenauigkeit durch Aggregation mehrerer LÃ¶sungsversuche |
| [Denken](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Einsteiger*   | LÃ¶sen komplexer Schlussfolgerungsprobleme durch Chain-of-Thought |
| [GedÃ¤chtnis](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Einsteiger* | Ein Chatbot mit Kurz- und LangzeitgedÃ¤chtnis |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *Einsteiger* | Konvertierung natÃ¼rlicher Sprache in SQL-Abfragen mit Auto-Debug-Schleife |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Einsteiger* |  Agent mit Model Context Protocol fÃ¼r numerische Operationen |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *Einsteiger* | Agent mit Agent-to-Agent-Protokoll fÃ¼r Inter-Agenten-Kommunikation |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *Einsteiger* | Ein minimaler Webdienst fÃ¼r eine menschliche ÃœberprÃ¼fungsschleife mit SSE-Updates |

</div>

ðŸ‘€ MÃ¶chten Sie andere Tutorials fÃ¼r AnfÃ¤nger sehen? [Erstellen Sie ein Issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Wie verwendet man Pocket Flow?

ðŸš€ Durch **Agenten-basiertes Programmieren**â€”das schnellste LLM-App-Entwicklungsparadigma, bei dem *Menschen entwerfen* und *Agenten programmieren*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Hier sind Beispiele fÃ¼r komplexere LLM-Apps:

<div align="center">
  
|  App-Name     |  Schwierigkeit    | Themen  | Menschlicher Entwurf | Agent-Code |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Cursor mit Cursor bauen](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Wir werden bald die SingularitÃ¤t erreichen ...</sup></sub> | â˜…â˜…â˜… <br> *Fortgeschritten*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Codebase-Wissensgenerator](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>Das Leben ist zu kurz, um ratlos fremden Code anzustarren</sup></sub> |  â˜…â˜…â˜† <br> *Mittel* | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Frage KI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Frage KI Paul Graham, falls du nicht reinkommst</sup></sub> | â˜…â˜…â˜† <br> *Mittel*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtube-Zusammenfasser](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> ErklÃ¤rt YouTube-Videos so, als wÃ¤rst du 5 </sup></sub> | â˜…â˜†â˜† <br> *Einsteiger*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Design-Dokument](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Cold-Opener-Generator](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Sofortige Eisbrecher, die kalte Leads heiÃŸ machen </sup></sub> | â˜…â˜†â˜† <br> *Einsteiger*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Web-Suche](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Design-Dokument](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- MÃ¶chten Sie **Agenten-basiertes Programmieren** lernen?

  - Schauen Sie sich [meinen YouTube-Kanal](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) fÃ¼r Video-Tutorials an, wie einige der oben genannten Apps erstellt wurden!

  - MÃ¶chten Sie Ihre eigene LLM-App erstellen? Lesen Sie diesen [Beitrag](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Beginnen Sie mit [dieser Vorlage](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-batch/translations/README_JAPANESE.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100è¡Œã®ãƒŸãƒ‹ãƒžãƒªã‚¹ãƒˆLLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

English | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flowã¯[ãŸã£ãŸ100è¡Œ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ã®ãƒŸãƒ‹ãƒžãƒªã‚¹ãƒˆLLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™

- **è»½é‡**: ã‚ãšã‹100è¡Œã€‚ä½™åˆ†ãªã‚‚ã®ãªã—ã€ä¾å­˜é–¢ä¿‚ãªã—ã€ãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ãªã—ã€‚
  
- **è¡¨ç¾åŠ›è±Šã‹**: ã‚ãªãŸãŒæ„›ã™ã‚‹ã™ã¹ã¦ã®ã‚‚ã®â€”([ãƒžãƒ«ãƒ](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ã€[ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ã€[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ãªã©ã€‚

- **[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆä¾‹ï¼šCursor AIï¼‰ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã•ã›ã‚‹â€”ç”Ÿç”£æ€§ãŒ10å€å‘ä¸Šï¼

Pocket Flowã‚’å§‹ã‚ã‚‹ã«ã¯ï¼š
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€```pip install pocketflow```ã¾ãŸã¯[ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ï¼ˆã‚ãšã‹100è¡Œï¼‰ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã ã‘ã§ã™ã€‚
- è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/)ã‚’ã”è¦§ãã ã•ã„ã€‚é–‹ç™ºã®å‹•æ©Ÿã«ã¤ã„ã¦ã¯ã€[ã‚¹ãƒˆãƒ¼ãƒªãƒ¼](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚
- è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã“ã®[AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant)ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‹ã€[å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ [Discord](https://discord.gg/hUHHE9Sa6T)ã«å‚åŠ ã—ã¦ã€Pocket Flowã§é–‹ç™ºã—ã¦ã„ã‚‹ä»–ã®é–‹ç™ºè€…ã¨ã¤ãªãŒã‚Šã¾ã—ã‚‡ã†ï¼
- ðŸŽ‰ Pocket Flowã¯æœ€åˆã¯Pythonã§ã™ãŒã€ç¾åœ¨ã¯[Typescript](https://github.com/The-Pocket/PocketFlow-Typescript)ã€[Java](https://github.com/The-Pocket/PocketFlow-Java)ã€[C++](https://github.com/The-Pocket/PocketFlow-CPP)ã€[Go](https://github.com/The-Pocket/PocketFlow-Go)ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚ã‚ã‚Šã¾ã™ï¼

## ãªãœPocket Flowï¼Ÿ

ç¾åœ¨ã®LLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯è†¨å¤§ã™ãŽã¾ã™... LLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã¯100è¡Œã ã‘ã§ååˆ†ã§ã™ï¼

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **æŠ½è±¡åŒ–**          | **ã‚¢ãƒ—ãƒªå›ºæœ‰ã®ãƒ©ãƒƒãƒ‘ãƒ¼**                                      | **ãƒ™ãƒ³ãƒ€ãƒ¼å›ºæœ‰ã®ãƒ©ãƒƒãƒ‘ãƒ¼**                                    | **è¡Œæ•°**       | **ã‚µã‚¤ã‚º**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒã‚§ãƒ¼ãƒ³               | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šQAã€è¦ç´„)</sub></sup>              | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šOpenAIã€Pineconeãªã©)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒã‚§ãƒ¼ãƒ³            | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šFileReadToolã€SerperDevTool)</sub></sup>         | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šOpenAIã€Anthropicã€Pineconeãªã©)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ                      | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šCodeAgentã€VisitWebTool)</sub></sup>         | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šDuckDuckGoã€Hugging Faceãªã©)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã‚°ãƒ©ãƒ•           | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢)</sub></sup>                     | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šPostgresStoreã€SqliteSaverãªã©) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ                | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šãƒ„ãƒ¼ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)</sub></sup>              | å¤šæ•° <sup><sub>[ã‚ªãƒ—ã‚·ãƒ§ãƒ³]<br> (ä¾‹ï¼šOpenAIã€Pineconeãªã©)</sub></sup>        | 7K <br><sup><sub>(ã‚³ã‚¢ã®ã¿)</sub></sup>    | +26MB <br><sup><sub>(ã‚³ã‚¢ã®ã¿)</sub></sup>          |
| **PocketFlow** | **ã‚°ãƒ©ãƒ•**                    | **ãªã—**                                                 | **ãªã—**                                                  | **100**       | **+56KB**                  |

</div>

## Pocket Flowã¯ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ

[100è¡Œ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ãŒLLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸­æ ¸çš„æŠ½è±¡åŒ–ã‚’æ‰ãˆã¦ã„ã¾ã™ï¼šã‚°ãƒ©ãƒ•ï¼
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ãã“ã‹ã‚‰ã€([ãƒžãƒ«ãƒ](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ã€[ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ã€[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ãªã©ã®äººæ°—ã®ã‚ã‚‹ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç°¡å˜ã«å®Ÿè£…ã§ãã¾ã™ã€‚
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ä»¥ä¸‹ã¯åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã™ï¼š

<div align="center">
  
|  åå‰  | é›£æ˜“åº¦    |  èª¬æ˜Ž  |  
| :-------------:  | :-------------: | :--------------------- |  
| [ãƒãƒ£ãƒƒãƒˆ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ä¼šè©±å±¥æ­´ã‚’æŒã¤åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ |
| [æ§‹é€ åŒ–å‡ºåŠ›](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜* | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã£ã¦å±¥æ­´æ›¸ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ |
| [ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ä½œæˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã€ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨ã‚’è¡Œã†ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ |
| [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ã‚¦ã‚§ãƒ–ã‚’æ¤œç´¢ã—ã¦è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã‚‹èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢æ‹¡å¼µç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ |
| [ãƒãƒƒãƒå‡¦ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜* | ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ã®è¨€èªžã«ç¿»è¨³ã™ã‚‹ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µ |
| [ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰²ã‚Šè¾¼ã¿æ©Ÿèƒ½ã‚’å‚™ãˆãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¢ |
| [ãƒãƒ£ãƒƒãƒˆã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*  | æ—…è¡Œé–¢é€£ã®ã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡¦ç†ã™ã‚‹æ—…è¡Œã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ |
| [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *åˆç´š* | ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ãŸãƒãƒƒãƒè©•ä¾¡ã®å±¥æ­´æ›¸è³‡æ ¼å‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ  |
| [ãƒžãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *åˆç´š* | 2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®éžåŒæœŸé€šä¿¡ã®ãŸã‚ã®ã‚¿ãƒ–ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ  |
| [ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¤ã‚¶ãƒ¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *åˆç´š* | èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä¿¡é ¼æ€§ã‚’å¤±ã£ã¦ã„ã¾ã™... ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã† |
| [ä¸¦åˆ—å‡¦ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *åˆç´š*   | 3å€ã®é«˜é€ŸåŒ–ã‚’ç¤ºã™ä¸¦åˆ—å®Ÿè¡Œãƒ‡ãƒ¢ |
| [ä¸¦åˆ—ãƒ•ãƒ­ãƒ¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *åˆç´š*   | è¤‡æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚‹8å€ã®é«˜é€ŸåŒ–ã‚’ç¤ºã™ä¸¦åˆ—ç”»åƒå‡¦ç†ãƒ‡ãƒ¢ |
| [å¤šæ•°æ±º](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *åˆç´š* | è¤‡æ•°ã®è§£æ±ºç­–ã‚’é›†ç´„ã—ã¦æŽ¨è«–ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ |
| [æ€è€ƒ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *åˆç´š*   | æ€è€ƒã®é€£éŽ–ã‚’é€šã˜ã¦è¤‡é›‘ãªæŽ¨è«–å•é¡Œã‚’è§£æ±ºã™ã‚‹ |
| [ãƒ¡ãƒ¢ãƒª](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *åˆç´š* | çŸ­æœŸè¨˜æ†¶ã¨é•·æœŸè¨˜æ†¶ã‚’æŒã¤ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *åˆç´š* | è‡ªå‹•ãƒ‡ãƒãƒƒã‚°ãƒ«ãƒ¼ãƒ—ã‚’å‚™ãˆãŸè‡ªç„¶è¨€èªžã‹ã‚‰SQLã‚¯ã‚¨ãƒªã¸ã®å¤‰æ› |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *åˆç´š* | æ•°å€¤æ¼”ç®—ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *åˆç´š* | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ã®ãŸã‚ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *åˆç´š* | SSEæ›´æ–°ã‚’å‚™ãˆãŸäººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã®ãƒŸãƒ‹ãƒžãƒ«ãªã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ |

</div>

ðŸ‘€ ä»–ã®è¶…åˆå¿ƒè€…å‘ã‘ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’è¦‹ãŸã„ã§ã™ã‹ï¼Ÿ[å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼](https://github.com/The-Pocket/PocketFlow/issues/new)

## Pocket Flowã®ä½¿ã„æ–¹

ðŸš€ **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**ã‚’é€šã˜ã¦â€”*äººé–“ãŒè¨­è¨ˆã—*ã€*ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹*æœ€é€Ÿã®LLMã‚¢ãƒ—ãƒªé–‹ç™ºãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ï¼

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="ã‚¤ãƒ¡ãƒ¼ã‚¸ä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆ" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ä»¥ä¸‹ã¯ã‚ˆã‚Šè¤‡é›‘ãªLLMã‚¢ãƒ—ãƒªã®ä¾‹ã§ã™ï¼š

<div align="center">
  
|  ã‚¢ãƒ—ãƒªå     |  é›£æ˜“åº¦    | ãƒˆãƒ”ãƒƒã‚¯  | äººé–“ã®è¨­è¨ˆ | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‰ |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Cursorã§Cursorã‚’æ§‹ç¯‰ã™ã‚‹](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>ã‚‚ã†ã™ãã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£ã«é”ã—ã¾ã™...</sup></sub> | â˜…â˜…â˜… <br> *ä¸Šç´š*   | [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹çŸ¥è­˜ãƒ“ãƒ«ãƒ€ãƒ¼](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>ä»–äººã®ã‚³ãƒ¼ãƒ‰ã‚’æ··ä¹±ã—ã¦è¦‹ã¤ã‚ã‚‹ã»ã©äººç”Ÿã¯çŸ­ããªã„</sup></sub> |  â˜…â˜…â˜† <br> *ä¸­ç´š* | [ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [AI Paul Grahamã«è³ªå•ã™ã‚‹](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>æŽ¡ç”¨ã•ã‚Œãªã„å ´åˆã«å‚™ãˆã¦ã€AI Paul Grahamã«è³ªå•ã—ã¾ã—ã‚‡ã†</sup></sub> | â˜…â˜…â˜† <br> *ä¸­ç´š*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtubeã‚µãƒžãƒ©ã‚¤ã‚¶ãƒ¼](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> 5æ­³å…ã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«YouTubeå‹•ç”»ã‚’èª¬æ˜Ž </sup></sub> | â˜…â˜†â˜† <br> *åˆç´š*   | [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [ã‚³ãƒ¼ãƒ«ãƒ‰ã‚ªãƒ¼ãƒ—ãƒŠãƒ¼ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> å†·ãŸã„ãƒªãƒ¼ãƒ‰ã‚’ç†±ãã™ã‚‹å³å¸­ã‚¢ã‚¤ã‚¹ãƒ–ãƒ¬ã‚¤ã‚«ãƒ¼ </sup></sub> | â˜…â˜†â˜† <br> *åˆç´š*   | [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [ã‚¦ã‚§ãƒ–æ¤œç´¢](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**ã‚’å­¦ã³ãŸã„ã§ã™ã‹ï¼Ÿ

  - ä¸Šè¨˜ã®ã‚¢ãƒ—ãƒªã®ä½œã‚Šæ–¹ã«é–¢ã™ã‚‹ãƒ“ãƒ‡ã‚ªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã¤ã„ã¦ã¯ã€[ç§ã®YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼

  - è‡ªåˆ†ã®LLMã‚¢ãƒ—ãƒªã‚’æ§‹ç¯‰ã—ãŸã„ã§ã™ã‹ï¼Ÿã“ã®[æŠ•ç¨¿](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)ã‚’èª­ã‚“ã§ãã ã•ã„ï¼[ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ](https://github.com/The-Pocket/PocketFlow-Template-Python)ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼
</file>

<file path="cookbook/pocketflow-batch/translations/README_KOREAN.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | í•œêµ­ì–´

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket FlowëŠ” [100ì¤„](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ì˜ ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸ LLM í”„ë ˆìž„ì›Œí¬ìž…ë‹ˆë‹¤

- **ê²½ëŸ‰í™”**: ë‹¨ 100ì¤„. ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì—†ìŒ, ì˜ì¡´ì„± ì—†ìŒ, ë²¤ë” ì¢…ì†ì„± ì—†ìŒ.
  
- **í‘œí˜„ë ¥**: ì—¬ëŸ¬ë¶„ì´ ì¢‹ì•„í•˜ëŠ” ëª¨ë“  ê²ƒâ€”([ë©€í‹°-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ì—ì´ì „íŠ¸](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [ì›Œí¬í”Œë¡œìš°](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) ë“±.

- **[ì—ì´ì „íŠ¸ ì½”ë”©](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: AI ì—ì´ì „íŠ¸(ì˜ˆ: Cursor AI)ê°€ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ë„ë¡ í•˜ì„¸ìš”â€”ìƒì‚°ì„± 10ë°° í–¥ìƒ!

Pocket Flow ì‹œìž‘í•˜ê¸°:
- ì„¤ì¹˜í•˜ë ¤ë©´ ```pip install pocketflow```ë‚˜ [ì†ŒìŠ¤ ì½”ë“œ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)(ë‹¨ 100ì¤„)ë¥¼ ë³µì‚¬í•˜ì„¸ìš”.
- ë” ì•Œì•„ë³´ë ¤ë©´ [ë¬¸ì„œ](https://the-pocket.github.io/PocketFlow/)ë¥¼ í™•ì¸í•˜ì„¸ìš”. ê°œë°œ ë™ê¸°ì— ëŒ€í•´ ì•Œê³  ì‹¶ë‹¤ë©´ [ì´ì•¼ê¸°](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)ë¥¼ ì½ì–´ë³´ì„¸ìš”.
- ì§ˆë¬¸ì´ ìžˆìœ¼ì‹ ê°€ìš”? [AI ì–´ì‹œìŠ¤í„´íŠ¸](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant)ë¥¼ í™•ì¸í•˜ê±°ë‚˜, [ì´ìŠˆë¥¼ ìƒì„±í•˜ì„¸ìš”!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Pocket Flowë¡œ ê°œë°œí•˜ëŠ” ë‹¤ë¥¸ ê°œë°œìžë“¤ê³¼ ì†Œí†µí•˜ë ¤ë©´ [Discord](https://discord.gg/hUHHE9Sa6T)ì— ê°€ìž…í•˜ì„¸ìš”!
- ðŸŽ‰ Pocket FlowëŠ” ì²˜ìŒì— Pythonìœ¼ë¡œ ê°œë°œë˜ì—ˆì§€ë§Œ, ì´ì œ [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) ë° [Go](https://github.com/The-Pocket/PocketFlow-Go) ë²„ì „ë„ ìžˆìŠµë‹ˆë‹¤!

## ì™œ Pocket Flowì¸ê°€?

í˜„ìž¬ LLM í”„ë ˆìž„ì›Œí¬ë“¤ì€ ë„ˆë¬´ ë¹„ëŒ€í•©ë‹ˆë‹¤... LLM í”„ë ˆìž„ì›Œí¬ëŠ” ë‹¨ 100ì¤„ì´ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **ì¶”ìƒí™”**          | **ì•± íŠ¹í™” ëž˜í¼**                                      | **ë²¤ë” íŠ¹í™” ëž˜í¼**                                    | **ì½”ë“œ ì¤„**       | **í¬ê¸°**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: QA, ìš”ì•½)</sub></sup>              | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: OpenAI, Pinecone ë“±)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: FileReadTool, SerperDevTool)</sub></sup>         | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: OpenAI, Anthropic, Pinecone ë“±)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: CodeAgent, VisitWebTool)</sub></sup>         | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: DuckDuckGo, Hugging Face ë“±)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: Semantic Search)</sub></sup>                     | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: PostgresStore, SqliteSaver ë“±) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: Tool Agent, Chat Agent)</sub></sup>              | ë§ŽìŒ <sup><sub>[ì„ íƒì ]<br> (ì˜ˆ: OpenAI, Pinecone ë“±)</sub></sup>        | 7K <br><sup><sub>(í•µì‹¬ë§Œ)</sub></sup>    | +26MB <br><sup><sub>(í•µì‹¬ë§Œ)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **ì—†ìŒ**                                                 | **ì—†ìŒ**                                                  | **100**       | **+56KB**                  |

</div>

## Pocket FlowëŠ” ì–´ë–»ê²Œ ìž‘ë™í•˜ë‚˜ìš”?

[100ì¤„](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ì˜ ì½”ë“œëŠ” LLM í”„ë ˆìž„ì›Œí¬ì˜ í•µì‹¬ ì¶”ìƒí™”ì¸ ê·¸ëž˜í”„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ([ë©€í‹°-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ì—ì´ì „íŠ¸](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [ì›Œí¬í”Œë¡œìš°](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) ë“±ì˜ ì¸ê¸° ìžˆëŠ” ë””ìžì¸ íŒ¨í„´ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ì•„ëž˜ëŠ” ê¸°ë³¸ íŠœí† ë¦¬ì–¼ìž…ë‹ˆë‹¤:

<div align="center">
  
|  ì´ë¦„  | ë‚œì´ë„    |  ì„¤ëª…  |  
| :-------------:  | :-------------: | :--------------------- |  
| [ì±„íŒ…](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ëŒ€í™” ê¸°ë¡ì„ ê°€ì§„ ê¸°ë³¸ ì±„íŒ…ë´‡ |
| [êµ¬ì¡°í™”ëœ ì¶œë ¥](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *ì´ˆë³´* | í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì´ë ¥ì„œì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ |
| [ì›Œí¬í”Œë¡œìš°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ê°œìš” ìž‘ì„±, ë‚´ìš© ìž‘ì„±, ìŠ¤íƒ€ì¼ ì ìš©ì´ í¬í•¨ëœ ìž‘ì„± ì›Œí¬í”Œë¡œìš° |
| [ì—ì´ì „íŠ¸](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ì›¹ì„ ê²€ìƒ‰í•˜ê³  ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìžˆëŠ” ì—°êµ¬ ì—ì´ì „íŠ¸ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ê°„ë‹¨í•œ ê²€ìƒ‰ ì¦ê°• ìƒì„± í”„ë¡œì„¸ìŠ¤ |
| [ë°°ì¹˜](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *ì´ˆë³´* | ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ì—¬ëŸ¬ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë°°ì¹˜ í”„ë¡œì„¸ì„œ |
| [ìŠ¤íŠ¸ë¦¬ë°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ì‚¬ìš©ìž ì¤‘ë‹¨ ê¸°ëŠ¥ì´ ìžˆëŠ” ì‹¤ì‹œê°„ LLM ìŠ¤íŠ¸ë¦¬ë° ë°ëª¨ |
| [ì±„íŒ… ê°€ë“œë ˆì¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *ì´ˆë³´*  | ì—¬í–‰ ê´€ë ¨ ì¿¼ë¦¬ë§Œ ì²˜ë¦¬í•˜ëŠ” ì—¬í–‰ ìƒë‹´ ì±„íŒ…ë´‡ |
| [ë§µ-ë¦¬ë“€ìŠ¤](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ë°°ì¹˜ í‰ê°€ë¥¼ ìœ„í•œ ë§µ-ë¦¬ë“€ìŠ¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” ì´ë ¥ì„œ ìžê²© ì²˜ë¦¬ê¸° |
| [ë©€í‹°-ì—ì´ì „íŠ¸](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ë‘ ì—ì´ì „íŠ¸ ê°„ì˜ ë¹„ë™ê¸° í†µì‹ ì„ ìœ„í•œ ê¸ˆì§€ì–´ ê²Œìž„ |
| [ê°ë…ìž](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ì—°êµ¬ ì—ì´ì „íŠ¸ê°€ ë¶ˆì•ˆì •í•  ë•Œ... ê°ë… í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì¶•í•´ ë´…ì‹œë‹¤ |
| [ë³‘ë ¬](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | 3ë°° ì†ë„ í–¥ìƒì„ ë³´ì—¬ì£¼ëŠ” ë³‘ë ¬ ì‹¤í–‰ ë°ëª¨ |
| [ë³‘ë ¬ í”Œë¡œìš°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | ì—¬ëŸ¬ í•„í„°ë¥¼ ì‚¬ìš©í•œ 8ë°° ì†ë„ í–¥ìƒì„ ë³´ì—¬ì£¼ëŠ” ë³‘ë ¬ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°ëª¨ |
| [ë‹¤ìˆ˜ê²° íˆ¬í‘œ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ì—¬ëŸ¬ ì†”ë£¨ì…˜ ì‹œë„ë¥¼ ì§‘ê³„í•˜ì—¬ ì¶”ë¡  ì •í™•ë„ í–¥ìƒ |
| [ì‚¬ê³ ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | Chain-of-Thoughtë¥¼ í†µí•œ ë³µìž¡í•œ ì¶”ë¡  ë¬¸ì œ í•´ê²° |
| [ë©”ëª¨ë¦¬](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ë‹¨ê¸° ë° ìž¥ê¸° ë©”ëª¨ë¦¬ê°€ ìžˆëŠ” ì±„íŒ…ë´‡ |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ìžë™ ë””ë²„ê·¸ ë£¨í”„ê°€ ìžˆëŠ” ìžì—°ì–´ì—ì„œ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜ |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ìˆ˜ì¹˜ ì—°ì‚°ì„ ìœ„í•œ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡œí† ì½œì„ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ì—ì´ì „íŠ¸ ê°„ í†µì‹ ì„ ìœ„í•œ Agent-to-Agent í”„ë¡œí† ì½œë¡œ ëž˜í•‘ëœ ì—ì´ì „íŠ¸ |
| [ì›¹ HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | SSE ì—…ë°ì´íŠ¸ê°€ ìžˆëŠ” ì¸ê°„ ê²€í†  ë£¨í”„ë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ì›¹ ì„œë¹„ìŠ¤ |

</div>

ðŸ‘€ ë” ë§Žì€ ì´ˆë³´ìžìš© íŠœí† ë¦¬ì–¼ì„ ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? [ì´ìŠˆë¥¼ ìƒì„±í•˜ì„¸ìš”!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Pocket Flowë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?

ðŸš€ **ì—ì´ì „íŠ¸ ì½”ë”©**ì„ í†µí•´â€”ê°€ìž¥ ë¹ ë¥¸ LLM ì•± ê°œë°œ íŒ¨ëŸ¬ë‹¤ìž„ìœ¼ë¡œ, *ì¸ê°„ì´ ì„¤ê³„*í•˜ê³  *ì—ì´ì „íŠ¸ê°€ ì½”ë”©*í•©ë‹ˆë‹¤!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ì•„ëž˜ëŠ” ë” ë³µìž¡í•œ LLM ì•±ì˜ ì˜ˆì‹œìž…ë‹ˆë‹¤:

<div align="center">
  
|  ì•± ì´ë¦„     |  ë‚œì´ë„    | ì£¼ì œ  | ì¸ê°„ ì„¤ê³„ | ì—ì´ì „íŠ¸ ì½”ë“œ |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Cursorë¡œ Cursor ë§Œë“¤ê¸°](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>ê³§ ê¸°ìˆ ì  íŠ¹ì´ì ì— ë„ë‹¬í•  ê²ƒìž…ë‹ˆë‹¤...</sup></sub> | â˜…â˜…â˜… <br> *ê³ ê¸‰*   | [ì—ì´ì „íŠ¸](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ì½”ë“œë² ì´ìŠ¤ ì§€ì‹ ë¹Œë”](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>ì¸ìƒì€ ë‹¤ë¥¸ ì‚¬ëžŒì˜ ì½”ë“œë¥¼ í˜¼ëž€ìŠ¤ëŸ½ê²Œ ë°”ë¼ë³¼ ë§Œí¼ ê¸¸ì§€ ì•ŠìŠµë‹ˆë‹¤</sup></sub> |  â˜…â˜…â˜† <br> *ì¤‘ê¸‰* | [ì›Œí¬í”Œë¡œìš°](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [AI Paul Grahamì—ê²Œ ë¬¼ì–´ë³´ê¸°](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>í•©ê²©í•˜ì§€ ëª»í•œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ AI Paul Grahamì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”</sup></sub> | â˜…â˜…â˜† <br> *ì¤‘ê¸‰*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [ë§µ ë¦¬ë“€ìŠ¤](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [ìœ íŠœë¸Œ ìš”ì•½ê¸°](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> 5ì‚´ ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ YouTube ë™ì˜ìƒ ì„¤ëª… </sup></sub> | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | [ë§µ ë¦¬ë“€ìŠ¤](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [ì½œë“œ ì˜¤í”„ë„ˆ ìƒì„±ê¸°](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> ì°¨ê°€ìš´ ìž ìž¬ ê³ ê°ì„ ëœ¨ê²ê²Œ ë§Œë“œëŠ” ì¦‰ê°ì ì¸ ì•„ì´ìŠ¤ë¸Œë ˆì´ì»¤ </sup></sub> | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | [ë§µ ë¦¬ë“€ìŠ¤](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [ì›¹ ê²€ìƒ‰](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- **ì—ì´ì „íŠ¸ ì½”ë”©**ì„ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?

  - ìœ„ì— ì†Œê°œëœ ì•±ë“¤ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼ì„ ë³´ë ¤ë©´ [ì œ YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)ë¥¼ í™•ì¸í•˜ì„¸ìš”!

  - ìžì‹ ë§Œì˜ LLM ì•±ì„ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ì´ [í¬ìŠ¤íŠ¸](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)ë¥¼ ì½ì–´ë³´ì„¸ìš”! [ì´ í…œí”Œë¦¿](https://github.com/The-Pocket/PocketFlow-Template-Python)ìœ¼ë¡œ ì‹œìž‘í•˜ì„¸ìš”!
</file>

<file path="cookbook/pocketflow-batch/translations/README_PORTUGUESE.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | PortuguÃªs | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow Ã© um framework minimalista para LLM com [apenas 100 linhas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **Leve**: Apenas 100 linhas. Zero inchaÃ§o, zero dependÃªncias, zero aprisionamento a fornecedores.
  
- **Expressivo**: Tudo o que vocÃª adoraâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), e mais.

- **[CodificaÃ§Ã£o AgÃªntica](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Deixe que Agentes de IA (ex: Cursor AI) construam Agentesâ€”aumento de produtividade de 10x!

Comece com o Pocket Flow:
- Para instalar, ```pip install pocketflow``` ou apenas copie o [cÃ³digo-fonte](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (apenas 100 linhas).
- Para saber mais, consulte a [documentaÃ§Ã£o](https://the-pocket.github.io/PocketFlow/). Para entender a motivaÃ§Ã£o, leia a [histÃ³ria](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Tem perguntas? Consulte este [Assistente de IA](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), ou [crie uma issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Junte-se ao nosso [Discord](https://discord.gg/hUHHE9Sa6T) para se conectar com outros desenvolvedores construindo com o Pocket Flow!
- ðŸŽ‰ O Pocket Flow Ã© inicialmente em Python, mas agora temos versÃµes em [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) e [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## Por que Pocket Flow?

Os frameworks LLM atuais sÃ£o pesados... VocÃª sÃ³ precisa de 100 linhas para um Framework LLM!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **AbstraÃ§Ã£o**          | **Wrappers EspecÃ­ficos para AplicaÃ§Ãµes**                                      | **Wrappers EspecÃ­ficos para Fornecedores**                                    | **Linhas**       | **Tamanho**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agente, Cadeia               | Muitos <br><sup><sub>(ex: QA, Summarization)</sub></sup>              | Muitos <br><sup><sub>(ex: OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agente, Cadeia            | Muitos <br><sup><sub>(ex: FileReadTool, SerperDevTool)</sub></sup>         | Muitos <br><sup><sub>(ex: OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agente                      | Alguns <br><sup><sub>(ex: CodeAgent, VisitWebTool)</sub></sup>         | Alguns <br><sup><sub>(ex: DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agente, Grafo           | Alguns <br><sup><sub>(ex: Semantic Search)</sub></sup>                     | Alguns <br><sup><sub>(ex: PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agente                | Alguns <br><sup><sub>(ex: Tool Agent, Chat Agent)</sub></sup>              | Muitos <sup><sub>[Opcional]<br> (ex: OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(somente core)</sub></sup>    | +26MB <br><sup><sub>(somente core)</sub></sup>          |
| **PocketFlow** | **Grafo**                    | **Nenhum**                                                 | **Nenhum**                                                  | **100**       | **+56KB**                  |

</div>

## Como funciona o Pocket Flow?

As [100 linhas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturam a abstraÃ§Ã£o central dos frameworks LLM: o Grafo!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

A partir daÃ­, Ã© fÃ¡cil implementar padrÃµes de design populares como ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Abaixo estÃ£o tutoriais bÃ¡sicos:

<div align="center">
  
|  Nome  | Dificuldade    |  DescriÃ§Ã£o  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um chatbot bÃ¡sico com histÃ³rico de conversaÃ§Ã£o |
| [SaÃ­da Estruturada](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *BÃ¡sico* | Extraindo dados estruturados de currÃ­culos por prompt |
| [Fluxo de Trabalho](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um fluxo de escrita que esboÃ§a, escreve conteÃºdo e aplica estilo |
| [Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um agente de pesquisa que pode buscar na web e responder perguntas |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um processo simples de GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o |
| [Processamento em Lote](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *BÃ¡sico* | Um processador em lote que traduz conteÃºdo markdown para vÃ¡rios idiomas |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Uma demonstraÃ§Ã£o de streaming LLM em tempo real com capacidade de interrupÃ§Ã£o pelo usuÃ¡rio |
| [Guardrail de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *BÃ¡sico*  | Um chatbot de consultoria de viagens que processa apenas consultas relacionadas a viagens |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *Iniciante* | Um processador de qualificaÃ§Ã£o de currÃ­culos usando o padrÃ£o map-reduce para avaliaÃ§Ã£o em lote |
| [Multi-Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Iniciante* | Um jogo de Tabu para comunicaÃ§Ã£o assÃ­ncrona entre dois agentes |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Iniciante* | O agente de pesquisa estÃ¡ ficando pouco confiÃ¡vel... Vamos criar um processo de supervisÃ£o |
| [Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Iniciante*   | Uma demonstraÃ§Ã£o de execuÃ§Ã£o paralela que mostra um aumento de velocidade de 3x |
| [Fluxo Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Iniciante*   | Uma demonstraÃ§Ã£o de processamento de imagem paralelo mostrando um aumento de velocidade de 8x com mÃºltiplos filtros |
| [Voto MajoritÃ¡rio](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *Iniciante* | Melhore a precisÃ£o de raciocÃ­nio agregando mÃºltiplas tentativas de soluÃ§Ã£o |
| [Pensamento](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Iniciante*   | Resolva problemas complexos de raciocÃ­nio atravÃ©s de Cadeia-de-Pensamento |
| [MemÃ³ria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Iniciante* | Um chatbot com memÃ³ria de curto e longo prazo |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *Iniciante* | Converta linguagem natural para consultas SQL com um loop de autodepuraÃ§Ã£o |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Iniciante* | Agente usando o Protocolo de Contexto de Modelo para operaÃ§Ãµes numÃ©ricas |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *Iniciante* | Agente envolvido com o protocolo Agente-para-Agente para comunicaÃ§Ã£o entre agentes |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *Iniciante* | Um serviÃ§o web mÃ­nimo para um loop de revisÃ£o humana com atualizaÃ§Ãµes SSE |

</div>

ðŸ‘€ Quer ver outros tutoriais para iniciantes? [Crie uma issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Como usar o Pocket Flow?

ðŸš€ AtravÃ©s da **CodificaÃ§Ã£o AgÃªntica**â€”o paradigma mais rÃ¡pido de desenvolvimento de aplicativos LLMâ€”onde *humanos projetam* e *agentes codificam*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Abaixo estÃ£o exemplos de aplicativos LLM mais complexos:

<div align="center">
  
|  Nome do Aplicativo     |  Dificuldade    | TÃ³picos  | Design Humano | CÃ³digo do Agente |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Construir Cursor com Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Logo chegaremos Ã  singularidade ...</sup></sub> | â˜…â˜…â˜… <br> *AvanÃ§ado*   | [Agente](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Construtor de Conhecimento de Base de CÃ³digo](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>A vida Ã© curta demais para ficar olhando o cÃ³digo dos outros em confusÃ£o</sup></sub> |  â˜…â˜…â˜† <br> *MÃ©dio* | [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Pergunte Ã  IA Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Pergunte Ã  IA Paul Graham, caso vocÃª nÃ£o consiga entrar</sup></sub> | â˜…â˜…â˜† <br> *MÃ©dio*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Resumidor de Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explica vÃ­deos do YouTube para vocÃª como se vocÃª tivesse 5 anos </sup></sub> | â˜…â˜†â˜† <br> *Iniciante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Doc de Design](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Gerador de Abertura a Frio](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Quebra-gelos instantÃ¢neos que transformam leads frios em quentes </sup></sub> | â˜…â˜†â˜† <br> *Iniciante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Busca Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Doc de Design](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Quer aprender **CodificaÃ§Ã£o AgÃªntica**?

  - Confira [meu YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) para tutoriais em vÃ­deo sobre como alguns dos aplicativos acima sÃ£o feitos!

  - Quer construir seu prÃ³prio aplicativo LLM? Leia este [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Comece com [este modelo](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-batch/translations/README_RUSSIAN.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | Ð ÑƒÑÑÐºÐ¸Ð¹| [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow â€” ÑÑ‚Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ LLM Ð²ÑÐµÐ³Ð¾ Ð² [100 ÑÑ‚Ñ€Ð¾Ðº](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **Ð›ÐµÐ³ÐºÐ¸Ð¹**: Ð’ÑÐµÐ³Ð¾ 100 ÑÑ‚Ñ€Ð¾Ðº. ÐÐ¸ÐºÐ°ÐºÐ¾Ð³Ð¾ Ð»Ð¸ÑˆÐ½ÐµÐ³Ð¾ Ð²ÐµÑÐ°, Ð½Ð¸ÐºÐ°ÐºÐ¸Ñ… Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹, Ð½Ð¸ÐºÐ°ÐºÐ¾Ð¹ Ð¿Ñ€Ð¸Ð²ÑÐ·ÐºÐ¸ Ðº Ð²ÐµÐ½Ð´Ð¾Ñ€Ð°Ð¼.
  
- **Ð’Ñ‹Ñ€Ð°Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹**: Ð’ÑÑ‘, Ñ‡Ñ‚Ð¾ Ð²Ñ‹ Ð»ÑŽÐ±Ð¸Ñ‚Ðµ â€” ([ÐœÑƒÐ»ÑŒÑ‚Ð¸-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ÐÐ³ÐµÐ½Ñ‚Ñ‹](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) Ð¸ Ð¼Ð½Ð¾Ð³Ð¾Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ðµ.

- **[ÐÐ³ÐµÐ½Ñ‚ÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: ÐŸÐ¾Ð·Ð²Ð¾Ð»ÑŒÑ‚Ðµ Ð˜Ð˜-Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Cursor AI) ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² â€” Ð¿Ð¾Ð²Ñ‹ÑÑŒÑ‚Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð² 10 Ñ€Ð°Ð·!

ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Pocket Flow:
- Ð”Ð»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸, ```pip install pocketflow``` Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ [Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ ÐºÐ¾Ð´](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (Ð²ÑÐµÐ³Ð¾ 100 ÑÑ‚Ñ€Ð¾Ðº).
- Ð§Ñ‚Ð¾Ð±Ñ‹ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ, Ð¾Ð·Ð½Ð°ÐºÐ¾Ð¼ÑŒÑ‚ÐµÑÑŒ Ñ [Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹](https://the-pocket.github.io/PocketFlow/). Ð§Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð½ÑÑ‚ÑŒ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸ÑŽ, Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ [Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Ð•ÑÑ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹? Ð¡Ð¿Ñ€Ð¾ÑÐ¸Ñ‚Ðµ ÑÑ‚Ð¾Ð³Ð¾ [Ð˜Ð˜-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant) Ð¸Ð»Ð¸ [ÑÐ¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ ÐŸÑ€Ð¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÐ¹Ñ‚ÐµÑÑŒ Ðº Ð½Ð°ÑˆÐµÐ¼Ñƒ [Discord](https://discord.gg/hUHHE9Sa6T), Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±Ñ‰Ð°Ñ‚ÑŒÑÑ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ°Ð¼Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Pocket Flow!
- ðŸŽ‰ Pocket Flow Ð¸Ð·Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½ Ð½Ð° Python, Ð½Ð¾ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð½Ð° [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) Ð¸ [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Pocket Flow?

Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¸ Ð´Ð»Ñ LLM ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð³Ñ€Ð¾Ð¼Ð¾Ð·Ð´ÐºÐ¸Ðµ... Ð”Ð»Ñ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ° LLM Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð²ÑÐµÐ³Ð¾ 100 ÑÑ‚Ñ€Ð¾Ðº!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **ÐÐ±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸Ñ**          | **ÐžÐ±ÐµÑ€Ñ‚ÐºÐ¸ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹**                                      | **ÐžÐ±ÐµÑ€Ñ‚ÐºÐ¸ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð²ÐµÐ½Ð´Ð¾Ñ€Ð¾Ð²**                                    | **Ð¡Ñ‚Ñ€Ð¾Ðº**       | **Ð Ð°Ð·Ð¼ÐµÑ€**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., QA, Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ)</sub></sup>              | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., OpenAI, Pinecone Ð¸ Ñ‚.Ð´.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., FileReadTool, SerperDevTool)</sub></sup>         | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., OpenAI, Anthropic, Pinecone Ð¸ Ñ‚.Ð´.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., CodeAgent, VisitWebTool)</sub></sup>         | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., DuckDuckGo, Hugging Face Ð¸ Ñ‚.Ð´.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., Semantic Search)</sub></sup>                     | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., PostgresStore, SqliteSaver Ð¸ Ñ‚.Ð´.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., Tool Agent, Chat Agent)</sub></sup>              | ÐœÐ½Ð¾Ð³Ð¾ <sup><sub>[ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾]<br> (Ð½Ð°Ð¿Ñ€., OpenAI, Pinecone Ð¸ Ñ‚.Ð´.)</sub></sup>        | 7K <br><sup><sub>(Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ´Ñ€Ð¾)</sub></sup>    | +26MB <br><sup><sub>(Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ´Ñ€Ð¾)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **ÐÐµÑ‚**                                                 | **ÐÐµÑ‚**                                                  | **100**       | **+56KB**                  |

</div>

## ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Pocket Flow?

[100 ÑÑ‚Ñ€Ð¾Ðº](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) Ð¾Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸ÑŽ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð² LLM: Ð“Ñ€Ð°Ñ„!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ÐžÑ‚ÑÑŽÐ´Ð° Ð»ÐµÐ³ÐºÐ¾ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº ([ÐœÑƒÐ»ÑŒÑ‚Ð¸-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ÐÐ³ÐµÐ½Ñ‚Ñ‹](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ÐÐ¸Ð¶Ðµ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð°:

<div align="center">
  
|  ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ  | Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ    |  ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Ð§Ð°Ñ‚](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÐµÐ¹ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° |
| [Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹* | Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð² |
| [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ Ð¿Ð»Ð°Ð½, Ð¿Ð¸ÑˆÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ ÑÑ‚Ð¸Ð»Ð¸ |
| [ÐÐ³ÐµÐ½Ñ‚](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¸ÑÐºÐ°Ñ‚ÑŒ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ |
| [ÐŸÐ°ÐºÐµÑ‚Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹* | ÐŸÐ°ÐºÐµÑ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ markdown-ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ·Ñ‹ÐºÐ¾Ð² |
| [ÐŸÐ¾Ñ‚Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ LLM Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€ÐµÑ€Ñ‹Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ |
| [ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*  | Ð§Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ‚ÑƒÑ€Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚Ð°, Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‰Ð¸Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð¿ÑƒÑ‚ÐµÑˆÐµÑÑ‚Ð²Ð¸ÑÐ¼Ð¸ |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐŸÑ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ñ€ÐµÐ·ÑŽÐ¼Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½ map-reduce Ð´Ð»Ñ Ð¿Ð°ÐºÐµÑ‚Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ |
| [ÐœÑƒÐ»ÑŒÑ‚Ð¸-Ð°Ð³ÐµÐ½Ñ‚](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | Ð˜Ð³Ñ€Ð° Ð¢Ð°Ð±Ñƒ Ð´Ð»Ñ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ð´Ð²ÑƒÐ¼Ñ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼Ð¸ |
| [Ð¡ÑƒÐ¿ÐµÑ€Ð²Ð¸Ð·Ð¾Ñ€](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ð½ÐµÐ½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¼... Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ ÑÐ¾Ð·Ð´Ð°Ð´Ð¸Ð¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð½Ð°Ð´Ð·Ð¾Ñ€Ð°|
| [ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‰Ð°Ñ 3-ÐºÑ€Ð°Ñ‚Ð½Ð¾Ðµ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ |
| [ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ð¾Ðº](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‰Ð°Ñ 8-ÐºÑ€Ð°Ñ‚Ð½Ð¾Ðµ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ð¼Ð¸ |
| [Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ð¾Ð¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¿ÑƒÑ‚ÐµÐ¼ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº Ñ€ÐµÑˆÐµÐ½Ð¸Ñ |
| [ÐœÑ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | Ð ÐµÑˆÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ð¹ |
| [ÐŸÐ°Ð¼ÑÑ‚ÑŒ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | Ð§Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¸ Ð´Ð¾Ð»Ð³Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð² SQL-Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐÐ³ÐµÐ½Ñ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐÐ³ÐµÐ½Ñ‚, Ð¾Ð±ÐµÑ€Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð¾Ð¼ Ð°Ð³ÐµÐ½Ñ‚-Ðº-Ð°Ð³ÐµÐ½Ñ‚Ñƒ Ð´Ð»Ñ Ð¼ÐµÐ¶Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð²ÐµÐ±-ÑÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ñ†Ð¸ÐºÐ»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¼ Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÑÐ¼Ð¸ SSE |

</div>

ðŸ‘€ Ð¥Ð¾Ñ‚Ð¸Ñ‚Ðµ ÑƒÐ²Ð¸Ð´ÐµÑ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð° Ð´Ð»Ñ Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‰Ð¸Ñ…? [Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## ÐšÐ°Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Pocket Flow?

ðŸš€ Ð§ÐµÑ€ÐµÐ· **ÐÐ³ÐµÐ½Ñ‚ÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ** â€” ÑÐ°Ð¼ÑƒÑŽ Ð±Ñ‹ÑÑ‚Ñ€ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ñƒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ LLM-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, Ð³Ð´Ðµ *Ð»ÑŽÐ´Ð¸ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€ÑƒÑŽÑ‚*, Ð° *Ð°Ð³ÐµÐ½Ñ‚Ñ‹ ÐºÐ¾Ð´Ð¸Ñ€ÑƒÑŽÑ‚*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ÐÐ¸Ð¶Ðµ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… LLM-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹:

<div align="center">
  
|  ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ     |  Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ    | Ð¢ÐµÐ¼Ñ‹  | Ð”Ð¸Ð·Ð°Ð¹Ð½ Ð¾Ñ‚ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° | ÐšÐ¾Ð´ Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Cursor Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Ð¡ÐºÐ¾Ñ€Ð¾ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð½ÐµÐ¼ ÑÐ¸Ð½Ð³ÑƒÐ»ÑÑ€Ð½Ð¾ÑÑ‚Ð¸ ...</sup></sub> | â˜…â˜…â˜… <br> *ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ð¹*   | [ÐÐ³ÐµÐ½Ñ‚](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ÐšÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¾Ñ€ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¾ ÐºÐ¾Ð´Ð¾Ð²Ð¾Ð¹ Ð±Ð°Ð·Ðµ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>Ð–Ð¸Ð·Ð½ÑŒ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð² Ñ€Ð°ÑÑ‚ÐµÑ€ÑÐ½Ð½Ð¾ÑÑ‚Ð¸ ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð° Ñ‡ÑƒÐ¶Ð¾Ð¹ ÐºÐ¾Ð´</sup></sub> |  â˜…â˜…â˜† <br> *Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹* | [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Ð¡Ð¿Ñ€Ð¾ÑÐ¸ Ð˜Ð˜ ÐŸÐ¾Ð»Ð° Ð“Ñ€ÑÐ¼Ð°](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Ð¡Ð¿Ñ€Ð¾ÑÐ¸ Ð˜Ð˜ ÐŸÐ¾Ð»Ð° Ð“Ñ€ÑÐ¼Ð°, ÐµÑÐ»Ð¸ Ñ‚ÐµÐ±Ñ Ð½Ðµ Ð¿Ñ€Ð¸Ð½ÑÐ»Ð¸</sup></sub> | â˜…â˜…â˜† <br> *Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ YouTube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> ÐžÐ±ÑŠÑÑÐ½ÑÐµÑ‚ YouTube-Ð²Ð¸Ð´ÐµÐ¾ ÐºÐ°Ðº Ð´Ð»Ñ 5-Ð»ÐµÑ‚Ð½ÐµÐ³Ð¾ </sup></sub> | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Ð“ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ñ…Ð¾Ð»Ð¾Ð´Ð½Ñ‹Ñ… Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ð¹](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> ÐœÐ³Ð½Ð¾Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð»ÐµÐ´Ð¾ÐºÐ¾Ð»Ñ‹, Ð¿Ñ€ÐµÐ²Ñ€Ð°Ñ‰Ð°ÑŽÑ‰Ð¸Ðµ Ñ…Ð¾Ð»Ð¾Ð´Ð½Ñ‹Ñ… Ð»Ð¸Ð´Ð¾Ð² Ð² Ð³Ð¾Ñ€ÑÑ‡Ð¸Ñ… </sup></sub> | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Ð’ÐµÐ±-Ð¿Ð¾Ð¸ÑÐº](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Ð¥Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¸Ð·ÑƒÑ‡Ð¸Ñ‚ÑŒ **ÐÐ³ÐµÐ½Ñ‚ÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ**?

  - ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ñ‚Ðµ [Ð¼Ð¾Ð¹ YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) Ð´Ð»Ñ Ð²Ð¸Ð´ÐµÐ¾ÑƒÑ€Ð¾ÐºÐ¾Ð² Ð¾ Ñ‚Ð¾Ð¼, ÐºÐ°Ðº ÑÐ¾Ð·Ð´Ð°ÑŽÑ‚ÑÑ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¸Ð· Ð²Ñ‹ÑˆÐµÐ¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹!

  - Ð¥Ð¾Ñ‚Ð¸Ñ‚Ðµ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐ²Ð¾Ðµ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ LLM-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ? ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ ÑÑ‚Ñƒ [ÑÑ‚Ð°Ñ‚ÑŒÑŽ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! ÐÐ°Ñ‡Ð½Ð¸Ñ‚Ðµ Ñ [ÑÑ‚Ð¾Ð³Ð¾ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-batch/translations/README_SPANISH.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | EspaÃ±ol | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow es un framework minimalista de LLM de [100 lÃ­neas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **Ligero**: Solo 100 lÃ­neas. Cero hinchazÃ³n, cero dependencias, cero vinculaciÃ³n a proveedores.
  
- **Expresivo**: Todo lo que amasâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Flujo de Trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), y mÃ¡s.

- **[ProgramaciÃ³n mediante Agentes](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Permite que los Agentes de IA (por ejemplo, Cursor AI) construyan Agentesâ€”Â¡multiplicando la productividad por 10!

Comienza con Pocket Flow:
- Para instalar, ```pip install pocketflow``` o simplemente copia el [cÃ³digo fuente](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (solo 100 lÃ­neas).
- Para aprender mÃ¡s, consulta la [documentaciÃ³n](https://the-pocket.github.io/PocketFlow/). Para conocer la motivaciÃ³n, lee la [historia](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Â¿Tienes preguntas? Consulta este [Asistente de IA](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), o [Â¡crea un issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Â¡Ãšnete a nuestro [Discord](https://discord.gg/hUHHE9Sa6T) para conectar con otros desarrolladores construyendo con Pocket Flow!
- ðŸŽ‰ Pocket Flow inicialmente estÃ¡ en Python, Â¡pero ahora tenemos versiones en [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) y [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## Â¿Por quÃ© Pocket Flow?

Los frameworks actuales de LLM estÃ¡n sobrecargados... Â¡Solo necesitas 100 lÃ­neas para un framework de LLM!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **AbstracciÃ³n**          | **Envolturas EspecÃ­ficas de AplicaciÃ³n**                                      | **Envolturas EspecÃ­ficas de Proveedor**                                    | **LÃ­neas**       | **TamaÃ±o**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agente, Cadena               | Muchas <br><sup><sub>(p.ej., QA, Resumen)</sub></sup>              | Muchas <br><sup><sub>(p.ej., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agente, Cadena            | Muchas <br><sup><sub>(p.ej., FileReadTool, SerperDevTool)</sub></sup>         | Muchas <br><sup><sub>(p.ej., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agente                      | Algunas <br><sup><sub>(p.ej., CodeAgent, VisitWebTool)</sub></sup>         | Algunas <br><sup><sub>(p.ej., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agente, Grafo           | Algunas <br><sup><sub>(p.ej., BÃºsqueda SemÃ¡ntica)</sub></sup>                     | Algunas <br><sup><sub>(p.ej., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agente                | Algunas <br><sup><sub>(p.ej., Tool Agent, Chat Agent)</sub></sup>              | Muchas <sup><sub>[Opcional]<br> (p.ej., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(solo-nÃºcleo)</sub></sup>    | +26MB <br><sup><sub>(solo-nÃºcleo)</sub></sup>          |
| **PocketFlow** | **Grafo**                    | **Ninguna**                                                 | **Ninguna**                                                  | **100**       | **+56KB**                  |

</div>

## Â¿CÃ³mo funciona Pocket Flow?

Las [100 lÃ­neas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturan la abstracciÃ³n principal de los frameworks de LLM: Â¡el Grafo!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

A partir de ahÃ­, es fÃ¡cil implementar patrones de diseÃ±o populares como ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Flujo de Trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ A continuaciÃ³n se presentan tutoriales bÃ¡sicos:

<div align="center">
  
|  Nombre  | Dificultad    |  DescripciÃ³n  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *Principiante*   | Un chatbot bÃ¡sico con historial de conversaciÃ³n |
| [Salida Estructurada](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *Principiante* | ExtracciÃ³n de datos estructurados de currÃ­culums mediante prompts |
| [Flujo de Trabajo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *Principiante*   | Un flujo de escritura que esquematiza, escribe contenido y aplica estilo |
| [Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *Principiante*   | Un agente de investigaciÃ³n que puede buscar en la web y responder preguntas |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *Principiante*   | Un simple proceso de GeneraciÃ³n aumentada por RecuperaciÃ³n |
| [Procesamiento por Lotes](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *Principiante* | Un procesador por lotes que traduce contenido markdown a mÃºltiples idiomas |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *Principiante*   | Una demostraciÃ³n de streaming LLM en tiempo real con capacidad de interrupciÃ³n del usuario |
| [ProtecciÃ³n de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *Principiante*  | Un chatbot asesor de viajes que solo procesa consultas relacionadas con viajes |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *Inicial* | Un procesador de calificaciÃ³n de currÃ­culums que utiliza el patrÃ³n map-reduce para evaluaciÃ³n por lotes |
| [Multi-Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Inicial* | Un juego de palabras TabÃº para comunicaciÃ³n asÃ­ncrona entre dos agentes |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Inicial* | El agente de investigaciÃ³n se vuelve poco fiable... Construyamos un proceso de supervisiÃ³n|
| [Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Inicial*   | Una demostraciÃ³n de ejecuciÃ³n paralela que muestra una aceleraciÃ³n de 3x |
| [Flujo Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Inicial*   | Una demostraciÃ³n de procesamiento de imÃ¡genes en paralelo que muestra una aceleraciÃ³n de 8x con mÃºltiples filtros |
| [Voto por MayorÃ­a](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *Inicial* | Mejora de la precisiÃ³n del razonamiento mediante la agregaciÃ³n de mÃºltiples intentos de soluciÃ³n |
| [Pensamiento](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Inicial*   | Resolver problemas de razonamiento complejos a travÃ©s de Cadena de Pensamiento |
| [Memoria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Inicial* | Un chatbot con memoria a corto y largo plazo |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *Inicial* | Convertir lenguaje natural a consultas SQL con un bucle de auto-depuraciÃ³n |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Inicial* | Agente que utiliza el Protocolo de Contexto de Modelo para operaciones numÃ©ricas |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *Inicial* | Agente envuelto con protocolo Agente-a-Agente para comunicaciÃ³n entre agentes |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *Inicial* | Un servicio web mÃ­nimo para un bucle de revisiÃ³n humana con actualizaciones SSE |

</div>

ðŸ‘€ Â¿Quieres ver otros tutoriales para principiantes? [Â¡Crea un issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Â¿CÃ³mo usar Pocket Flow?

ðŸš€ A travÃ©s de la **ProgramaciÃ³n mediante Agentes**â€”el paradigma de desarrollo de aplicaciones LLM mÃ¡s rÃ¡pido- donde *los humanos diseÃ±an* y *los agentes programan*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ A continuaciÃ³n hay ejemplos de aplicaciones LLM mÃ¡s complejas:

<div align="center">
  
|  Nombre de la App     |  Dificultad    | Temas  | DiseÃ±o Humano | CÃ³digo del Agente |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Construir Cursor con Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Pronto alcanzaremos la singularidad ...</sup></sub> | â˜…â˜…â˜… <br> *Avanzado*   | [Agente](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Constructor de Conocimiento de CÃ³digo Base](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>La vida es demasiado corta para mirar el cÃ³digo de otros con confusiÃ³n</sup></sub> |  â˜…â˜…â˜† <br> *Medio* | [Flujo de Trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Pregunta a AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Pregunta a AI Paul Graham, en caso de que no entres</sup></sub> | â˜…â˜…â˜† <br> *Medio*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Resumidor de Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explica videos de YouTube como si tuvieras 5 aÃ±os </sup></sub> | â˜…â˜†â˜† <br> *Principiante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Generador de IntroducciÃ³n para Email FrÃ­o](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Rompehielos instantÃ¡neos que convierten leads frÃ­os en calientes </sup></sub> | â˜…â˜†â˜† <br> *Principiante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [BÃºsqueda Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Â¿Quieres aprender **ProgramaciÃ³n mediante Agentes**?

  - Â¡Consulta [mi YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) para ver tutoriales en video sobre cÃ³mo se crearon algunas de las aplicaciones anteriores!

  - Â¿Quieres construir tu propia aplicaciÃ³n LLM? Â¡Lee este [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Â¡Comienza con [esta plantilla](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-batch/main.py">
import os
import time
from pocketflow import BatchNode, Flow
from utils import call_llm

class TranslateTextNode(BatchNode):
    def prep(self, shared):
        text = shared.get("text", "(No text provided)")
        languages = shared.get("languages", ["Chinese", "Spanish", "Japanese", "German", 
                              "Russian", "Portuguese", "French", "Korean"])
        
        # Create batches for each language translation
        return [(text, lang) for lang in languages]

    def exec(self, data_tuple):
        text, language = data_tuple
        
        prompt = f"""
Please translate the following markdown file into {language}. 
But keep the original markdown format, links and code blocks.
Directly return the translated text, without any other text or comments.

Original: 
{text}

Translated:"""
        
        result = call_llm(prompt)
        print(f"Translated {language} text")
        return {"language": language, "translation": result}

    def post(self, shared, prep_res, exec_res_list):
        # Create output directory if it doesn't exist
        output_dir = shared.get("output_dir", "translations")
        os.makedirs(output_dir, exist_ok=True)
        
        # Write each translation to a file
        for result in exec_res_list:
            language, translation = result["language"], result["translation"]
            
            # Write to file
            filename = os.path.join(output_dir, f"README_{language.upper()}.md")
            with open(filename, "w", encoding="utf-8") as f:
                f.write(translation)
            
            print(f"Saved translation to {filename}")

if __name__ == "__main__":
    # read the text from ../../README.md
    with open("../../README.md", "r") as f:
        text = f.read()
    
    # Default settings
    shared = {
        "text": text,
        "languages": ["Chinese", "Spanish", "Japanese", "German", "Russian", "Portuguese", "French", "Korean"],
        "output_dir": "translations"
    }

    # --- Time Measurement Start ---
    print(f"Starting sequential translation into {len(shared['languages'])} languages...")
    start_time = time.perf_counter()

    # Run the translation flow
    translate_node = TranslateTextNode(max_retries=3)
    flow = Flow(start=translate_node)
    flow.run(shared)

    # --- Time Measurement End ---
    end_time = time.perf_counter()
    duration = end_time - start_time

    print(f"\nTotal sequential translation time: {duration:.4f} seconds") # Print duration
    print("\n=== Translation Complete ===")
    print(f"Translations saved to: {shared['output_dir']}")
    print("============================")
</file>

<file path="cookbook/pocketflow-batch/README.md">
# Batch Translation Process

This project demonstrates a batch processing implementation that enables LLMs to translate documents into multiple languages simultaneously. It's designed to efficiently handle the translation of markdown files while preserving formatting.

## Features

- Translates markdown content into multiple languages in parallel
- Saves translated files to specified output directory

## Getting Started

1. Install the required packages:
```bash
pip install -r requirements.txt
```

2. Set up your API key:
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```

3. Run the translation process:
```bash
python main.py
```

## How It Works

The implementation uses a `TranslateTextNode` that processes batches of translation requests:

```mermaid
flowchart LR
    batch[TranslateTextNode]
```

The `TranslateTextNode`:
1. Prepares batches for multiple language translations
2. Executes translations in parallel using the model
3. Saves the translated content to individual files
4. Maintains the original markdown structure

This approach demonstrates how PocketFlow can efficiently process multiple related tasks in parallel.

## Example Output

When you run the translation process, you'll see output similar to this:

```
Translated Chinese text
Translated Spanish text
Translated Japanese text
Translated German text
Translated Russian text
Translated Portuguese text
Translated French text
Translated Korean text
Saved translation to translations/README_CHINESE.md
Saved translation to translations/README_SPANISH.md
Saved translation to translations/README_JAPANESE.md
Saved translation to translations/README_GERMAN.md
Saved translation to translations/README_RUSSIAN.md
Saved translation to translations/README_PORTUGUESE.md
Saved translation to translations/README_FRENCH.md
Saved translation to translations/README_KOREAN.md

=== Translation Complete ===
Translations saved to: translations
============================
```

## Files

- [`main.py`](./main.py): Implementation of the batch translation node
- [`utils.py`](./utils.py): Simple wrapper for calling the Anthropic model
- [`requirements.txt`](./requirements.txt): Project dependencies

The translations are saved to the `translations` directory, with each file named according to the target language.
</file>

<file path="cookbook/pocketflow-batch/requirements.txt">
pocketflow>=0.0.1
anthropic>=0.15.0
pyyaml>=6.0
</file>

<file path="cookbook/pocketflow-batch/utils.py">
from anthropic import Anthropic
import os

def call_llm(prompt):
    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
    response = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=20000,
        thinking={
            "type": "enabled",
            "budget_tokens": 16000
        },
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[1].text

if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")
</file>

<file path="cookbook/pocketflow-batch-flow/flow.py">
from pocketflow import Flow, BatchFlow
from nodes import LoadImage, ApplyFilter, SaveImage

def create_base_flow():
    """Create the base Flow for processing a single image."""
    # Create nodes
    load = LoadImage()
    filter_node = ApplyFilter()
    save = SaveImage()
    
    # Connect nodes
    load - "apply_filter" >> filter_node
    filter_node - "save" >> save
    
    # Create and return flow
    return Flow(start=load)

class ImageBatchFlow(BatchFlow):
    """BatchFlow for processing multiple images with different filters."""
    
    def prep(self, shared):
        """Generate parameters for each image-filter combination."""
        # List of images to process
        images = ["cat.jpg", "dog.jpg", "bird.jpg"]
        
        # List of filters to apply
        filters = ["grayscale", "blur", "sepia"]
        
        # Generate all combinations
        params = []
        for img in images:
            for f in filters:
                params.append({
                    "input": img,
                    "filter": f
                })
        
        return params

def create_flow():
    """Create the complete batch processing flow."""
    # Create base flow for single image processing
    base_flow = create_base_flow()
    
    # Wrap in BatchFlow for multiple images
    batch_flow = ImageBatchFlow(start=base_flow)
    
    return batch_flow
</file>

<file path="cookbook/pocketflow-batch-flow/main.py">
import os
from PIL import Image
import numpy as np
from flow import create_flow

def main():
    # Create and run flow
    print("Processing images with filters...")
    
    flow = create_flow()
    flow.run({}) 
    
    print("\nAll images processed successfully!")
    print("Check the 'output' directory for results.")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-batch-flow/nodes.py">
"""Node implementations for image processing."""

import os
from PIL import Image, ImageEnhance, ImageFilter
from pocketflow import Node

class LoadImage(Node):
    """Node that loads an image file."""
    
    def prep(self, shared):
        """Get image path from parameters."""
        return os.path.join("images", self.params["input"])
    
    def exec(self, image_path):
        """Load the image using PIL."""
        return Image.open(image_path)
    
    def post(self, shared, prep_res, exec_res):
        """Store the image in shared store."""
        shared["image"] = exec_res
        return "apply_filter"

class ApplyFilter(Node):
    """Node that applies a filter to an image."""
    
    def prep(self, shared):
        """Get image and filter type."""
        return shared["image"], self.params["filter"]
    
    def exec(self, inputs):
        """Apply the specified filter."""
        image, filter_type = inputs
        
        if filter_type == "grayscale":
            return image.convert("L")
        elif filter_type == "blur":
            return image.filter(ImageFilter.BLUR)
        elif filter_type == "sepia":
            # Sepia implementation
            enhancer = ImageEnhance.Color(image)
            grayscale = enhancer.enhance(0.3)
            colorize = ImageEnhance.Brightness(grayscale)
            return colorize.enhance(1.2)
        else:
            raise ValueError(f"Unknown filter: {filter_type}")
    
    def post(self, shared, prep_res, exec_res):
        """Store the filtered image."""
        shared["filtered_image"] = exec_res
        return "save"

class SaveImage(Node):
    """Node that saves the processed image."""
    
    def prep(self, shared):
        """Get filtered image and prepare output path."""
        # Create output directory if it doesn't exist
        os.makedirs("output", exist_ok=True)
        
        # Generate output filename
        input_name = os.path.splitext(self.params["input"])[0]
        filter_name = self.params["filter"]
        output_path = os.path.join("output", f"{input_name}_{filter_name}.jpg")
        
        return shared["filtered_image"], output_path
    
    def exec(self, inputs):
        """Save the image to file."""
        image, output_path = inputs
        image.save(output_path, "JPEG")
        return output_path
    
    def post(self, shared, prep_res, exec_res):
        """Print success message."""
        print(f"Saved filtered image to: {exec_res}")
        return "default"
</file>

<file path="cookbook/pocketflow-batch-flow/README.md">
# PocketFlow BatchFlow Example

This example demonstrates the BatchFlow concept in PocketFlow by implementing an image processor that applies different filters to multiple images.

## What this Example Demonstrates

- How to use BatchFlow to run a Flow multiple times with different parameters
- Key concepts of BatchFlow:
  1. Creating a base Flow for single-item processing
  2. Using BatchFlow to process multiple items with different parameters
  3. Managing parameters across multiple Flow executions

## Project Structure
```
pocketflow-batch-flow/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ cat.jpg        # Sample image 1
â”‚   â”œâ”€â”€ dog.jpg        # Sample image 2
â”‚   â””â”€â”€ bird.jpg       # Sample image 3
â”œâ”€â”€ main.py            # Entry point
â”œâ”€â”€ flow.py            # Flow and BatchFlow definitions
â””â”€â”€ nodes.py           # Node implementations for image processing
```

## How it Works

The example processes multiple images with different filters:

1. **Base Flow**: Processes a single image
   - Load image
   - Apply filter (grayscale, blur, or sepia)
   - Save processed image

2. **BatchFlow**: Processes multiple image-filter combinations
   - Takes a list of parameters (image + filter combinations)
   - Runs the base Flow for each parameter set
   - Organizes output in a structured way

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```

## Sample Output

```
Processing images with filters...

Processing cat.jpg with grayscale filter...
Processing cat.jpg with blur filter...
Processing dog.jpg with sepia filter...
...

All images processed successfully!
Check the 'output' directory for results.
```

## Key Concepts Illustrated

1. **Parameter Management**: Shows how BatchFlow manages different parameter sets
2. **Flow Reuse**: Demonstrates running the same Flow multiple times
3. **Batch Processing**: Shows how to process multiple items efficiently
4. **Real-world Application**: Provides a practical example of batch processing
</file>

<file path="cookbook/pocketflow-batch-flow/requirements.txt">
pocketflow
Pillow>=10.0.0
</file>

<file path="cookbook/pocketflow-batch-node/flow.py">
from pocketflow import Flow, Node
from nodes import CSVProcessor

class ShowStats(Node):
    """Node to display the final statistics."""
    
    def prep(self, shared):
        """Get statistics from shared store."""
        return shared["statistics"]
    
    def post(self, shared, prep_res, exec_res):
        """Display the statistics."""
        stats = prep_res
        print("\nFinal Statistics:")
        print(f"- Total Sales: ${stats['total_sales']:,.2f}")
        print(f"- Average Sale: ${stats['average_sale']:,.2f}")
        print(f"- Total Transactions: {stats['total_transactions']:,}\n")
        return "end"

def create_flow():
    """Create and return the processing flow."""
    # Create nodes
    processor = CSVProcessor(chunk_size=1000)
    show_stats = ShowStats()
    
    # Connect nodes
    processor - "show_stats" >> show_stats
    
    # Create and return flow
    return Flow(start=processor)
</file>

<file path="cookbook/pocketflow-batch-node/main.py">
import os
from flow import create_flow

def main():
    """Run the batch processing example."""
    # Create data directory if it doesn't exist
    os.makedirs("data", exist_ok=True)
    
    # Create sample CSV if it doesn't exist
    if not os.path.exists("data/sales.csv"):
        print("Creating sample sales.csv...")
        import pandas as pd
        import numpy as np
        
        # Generate sample data
        np.random.seed(42)
        n_rows = 10000
        df = pd.DataFrame({
            "date": pd.date_range("2024-01-01", periods=n_rows),
            "amount": np.random.normal(100, 30, n_rows).round(2),
            "product": np.random.choice(["A", "B", "C"], n_rows)
        })
        df.to_csv("data/sales.csv", index=False)
    
    # Initialize shared store
    shared = {
        "input_file": "data/sales.csv"
    }
    
    # Create and run flow
    print(f"Processing sales.csv in chunks...")
    flow = create_flow()
    flow.run(shared)

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-batch-node/nodes.py">
import pandas as pd
from pocketflow import BatchNode

class CSVProcessor(BatchNode):
    """BatchNode that processes a large CSV file in chunks."""
    
    def __init__(self, chunk_size=1000):
        """Initialize with chunk size."""
        super().__init__()
        self.chunk_size = chunk_size
    
    def prep(self, shared):
        """Split CSV file into chunks.
        
        Returns an iterator of DataFrames, each containing chunk_size rows.
        """
        # Read CSV in chunks
        chunks = pd.read_csv(
            shared["input_file"],
            chunksize=self.chunk_size
        )
        return chunks
    
    def exec(self, chunk):
        """Process a single chunk of the CSV.
        
        Args:
            chunk: pandas DataFrame containing chunk_size rows
            
        Returns:
            dict: Statistics for this chunk
        """
        return {
            "total_sales": chunk["amount"].sum(),
            "num_transactions": len(chunk),
            "total_amount": chunk["amount"].sum()
        }
    
    def post(self, shared, prep_res, exec_res_list):
        """Combine results from all chunks.
        
        Args:
            prep_res: Original chunks iterator
            exec_res_list: List of results from each chunk
            
        Returns:
            str: Action to take next
        """
        # Combine statistics from all chunks
        total_sales = sum(res["total_sales"] for res in exec_res_list)
        total_transactions = sum(res["num_transactions"] for res in exec_res_list)
        total_amount = sum(res["total_amount"] for res in exec_res_list)
        
        # Calculate final statistics
        shared["statistics"] = {
            "total_sales": total_sales,
            "average_sale": total_amount / total_transactions,
            "total_transactions": total_transactions
        }
        
        return "show_stats"
</file>

<file path="cookbook/pocketflow-batch-node/README.md">
# PocketFlow BatchNode Example

This example demonstrates the BatchNode concept in PocketFlow by implementing a CSV processor that handles large files by processing them in chunks.

## What this Example Demonstrates

- How to use BatchNode to process large inputs in chunks
- The three key methods of BatchNode:
  1. `prep`: Splits input into chunks
  2. `exec`: Processes each chunk independently
  3. `post`: Combines results from all chunks

## Project Structure
```
pocketflow-batch-node/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales.csv      # Sample large CSV file
â”œâ”€â”€ main.py            # Entry point
â”œâ”€â”€ flow.py            # Flow definition
â””â”€â”€ nodes.py           # BatchNode implementation
```

## How it Works

The example processes a large CSV file containing sales data:

1. **Chunking (prep)**: The CSV file is read and split into chunks of N rows
2. **Processing (exec)**: Each chunk is processed to calculate:
   - Total sales
   - Average sale value
   - Number of transactions
3. **Combining (post)**: Results from all chunks are aggregated into final statistics

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```

## Sample Output

```
Processing sales.csv in chunks...

Final Statistics:
- Total Sales: $1,234,567.89
- Average Sale: $123.45
- Total Transactions: 10,000
```

## Key Concepts Illustrated

1. **Chunk-based Processing**: Shows how BatchNode handles large inputs by breaking them into manageable pieces
2. **Independent Processing**: Demonstrates how each chunk is processed separately
3. **Result Aggregation**: Shows how individual results are combined into a final output
</file>

<file path="cookbook/pocketflow-batch-node/requirements.txt">
pocketflow
pandas>=2.0.0
</file>

<file path="cookbook/pocketflow-chat/main.py">
from pocketflow import Node, Flow
from utils import call_llm

class ChatNode(Node):
    def prep(self, shared):
        # Initialize messages if this is the first run
        if "messages" not in shared:
            shared["messages"] = []
            print("Welcome to the chat! Type 'exit' to end the conversation.")
        
        # Get user input
        user_input = input("\nYou: ")
        
        # Check if user wants to exit
        if user_input.lower() == 'exit':
            return None
        
        # Add user message to history
        shared["messages"].append({"role": "user", "content": user_input})
        
        # Return all messages for the LLM
        return shared["messages"]

    def exec(self, messages):
        if messages is None:
            return None
        
        # Call LLM with the entire conversation history
        response = call_llm(messages)
        return response

    def post(self, shared, prep_res, exec_res):
        if prep_res is None or exec_res is None:
            print("\nGoodbye!")
            return None  # End the conversation
        
        # Print the assistant's response
        print(f"\nAssistant: {exec_res}")
        
        # Add assistant message to history
        shared["messages"].append({"role": "assistant", "content": exec_res})
        
        # Loop back to continue the conversation
        return "continue"

# Create the flow with self-loop
chat_node = ChatNode()
chat_node - "continue" >> chat_node  # Loop back to continue conversation

flow = Flow(start=chat_node)

# Start the chat
if __name__ == "__main__":
    shared = {}
    flow.run(shared)
</file>

<file path="cookbook/pocketflow-chat/README.md">
#  Simple PocketFlow Chat

A basic chat application using PocketFlow with OpenAI's GPT-4o model.

## Features

- Conversational chat interface in the terminal
- Maintains full conversation history for context
- Simple implementation demonstrating PocketFlow's node and flow concepts

## Run It

1. Make sure your OpenAI API key is set:
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```
    Alternatively, you can edit the `utils.py` file to include your API key directly.

2. Install requirements and run the application:
    ```bash
    pip install -r requirements.txt
    python main.py
    ```

## How It Works

```mermaid
flowchart LR
    chat[ChatNode] -->|continue| chat
```

The chat application uses:
- A single `ChatNode` with a self-loop that:
  - Takes user input in the `prep` method
  - Sends the complete conversation history to GPT-4o
  - Adds responses to the conversation history
  - Loops back to continue the chat until the user types 'exit'


## Files

- [`main.py`](./main.py): Implementation of the ChatNode and chat flow
- [`utils.py`](./utils.py): Simple wrapper for calling the OpenAI API
</file>

<file path="cookbook/pocketflow-chat/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
</file>

<file path="cookbook/pocketflow-chat/utils.py">
from openai import OpenAI
import os

def call_llm(messages):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content

if __name__ == "__main__":
    # Test the LLM call
    messages = [{"role": "user", "content": "In a few words, what's the meaning of life?"}]
    response = call_llm(messages)
    print(f"Prompt: {messages[0]['content']}")
    print(f"Response: {response}")
</file>

<file path="cookbook/pocketflow-chat-guardrail/main.py">
from pocketflow import Node, Flow
from utils import call_llm

class UserInputNode(Node):
    def prep(self, shared):
        # Initialize messages if this is the first run
        if "messages" not in shared:
            shared["messages"] = []
            print("Welcome to the Travel Advisor Chat! Type 'exit' to end the conversation.")
        
        return None

    def exec(self, _):
        # Get user input
        user_input = input("\nYou: ")
        return user_input

    def post(self, shared, prep_res, exec_res):
        user_input = exec_res
        
        # Check if user wants to exit
        if user_input and user_input.lower() == 'exit':
            print("\nGoodbye! Safe travels!")
            return None  # End the conversation
        
        # Store user input in shared
        shared["user_input"] = user_input
        
        # Move to guardrail validation
        return "validate"

class GuardrailNode(Node):
    def prep(self, shared):
        # Get the user input from shared data
        user_input = shared.get("user_input", "")
        return user_input
    
    def exec(self, user_input):
        # Basic validation checks
        if not user_input or user_input.strip() == "":
            return False, "Your query is empty. Please provide a travel-related question."
        
        if len(user_input.strip()) < 3:
            return False, "Your query is too short. Please provide more details about your travel question."
        
        # LLM-based validation for travel topics
        prompt = f"""
Evaluate if the following user query is related to travel advice, destinations, planning, or other travel topics.
The chat should ONLY answer travel-related questions and reject any off-topic, harmful, or inappropriate queries.
User query: {user_input}
Return your evaluation in YAML format:
```yaml
valid: true/false
reason: [Explain why the query is valid or invalid]
```"""
        
        # Call LLM with the validation prompt
        messages = [{"role": "user", "content": prompt}]
        response = call_llm(messages)
        
        # Extract YAML content
        yaml_content = response.split("```yaml")[1].split("```")[0].strip() if "```yaml" in response else response
        
        import yaml
        result = yaml.safe_load(yaml_content)
        assert result is not None, "Error: Invalid YAML format"
        assert "valid" in result and "reason" in result, "Error: Invalid YAML format"
        is_valid = result.get("valid", False)
        reason = result.get("reason", "Missing reason in YAML response")
        
        return is_valid, reason
    
    def post(self, shared, prep_res, exec_res):
        is_valid, message = exec_res
        
        if not is_valid:
            # Display error message to user
            print(f"\nTravel Advisor: {message}")
            # Skip LLM call and go back to user input
            return "retry"
        
        # Valid input, add to message history
        shared["messages"].append({"role": "user", "content": shared["user_input"]})
        # Proceed to LLM processing
        return "process"

class LLMNode(Node):
    def prep(self, shared):
        # Add system message if not present
        if not any(msg.get("role") == "system" for msg in shared["messages"]):
            shared["messages"].insert(0, {
                "role": "system", 
                "content": "You are a helpful travel advisor that provides information about destinations, travel planning, accommodations, transportation, activities, and other travel-related topics. Only respond to travel-related queries and keep responses informative and friendly. Your response are concise in 100 words."
            })
        
        # Return all messages for the LLM
        return shared["messages"]

    def exec(self, messages):
        # Call LLM with the entire conversation history
        response = call_llm(messages)
        return response

    def post(self, shared, prep_res, exec_res):
        # Print the assistant's response
        print(f"\nTravel Advisor: {exec_res}")
        
        # Add assistant message to history
        shared["messages"].append({"role": "assistant", "content": exec_res})
        
        # Loop back to continue the conversation
        return "continue"

# Create the flow with nodes and connections
user_input_node = UserInputNode()
guardrail_node = GuardrailNode()
llm_node = LLMNode()

# Create flow connections
user_input_node - "validate" >> guardrail_node
guardrail_node - "retry" >> user_input_node  # Loop back if input is invalid
guardrail_node - "process" >> llm_node
llm_node - "continue" >> user_input_node     # Continue conversation

flow = Flow(start=user_input_node)

# Start the chat
if __name__ == "__main__":
    shared = {}
    flow.run(shared)
</file>

<file path="cookbook/pocketflow-chat-guardrail/README.md">
#  Travel Advisor Chat with Guardrails

A travel-focused chat application using PocketFlow with OpenAI's GPT-4o model, enhanced with input validation to ensure only travel-related queries are processed.

## Features

- Travel advisor chatbot that answers questions about destinations, planning, accommodations, etc.
- **Topic-specific guardrails** to ensure only travel-related queries are accepted

## Run It

1. Make sure your OpenAI API key is set:
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```
    Alternatively, you can edit the `utils.py` file to include your API key directly.

2. Install requirements and run the application:
    ```bash
    pip install -r requirements.txt
    python main.py
    ```

## How It Works

```mermaid
flowchart LR
    user[UserInputNode] -->|validate| guardrail[GuardrailNode]
    guardrail -->|retry| user
    guardrail -->|process| llm[LLMNode]
    llm -->|continue| user
```

The chat application uses:
- A `UserInputNode` that collects user input in its `exec` method
- A `GuardrailNode` that validates if the query is travel-related using:
  - Basic validation checks (empty input, too short)
  - LLM-based validation to determine if the query relates to travel
- An `LLMNode` that processes valid travel queries using GPT-4o with a travel advisor system prompt
- Flow connections that route inputs through validation before processing and handle retries for non-travel related queries

## Files

- [`main.py`](./main.py): Implementation of the nodes and chat flow
- [`utils.py`](./utils.py): Utilities for calling the OpenAI API

## Example Outputs

``` 
Welcome to the Travel Advisor Chat! Type 'exit' to end the conversation.

You: Plan my trip to Thailand

Travel Advisor: Thailand offers a blend of vibrant cities, serene beaches, and rich culture. Begin in Bangkok to explore the Grand Palace and local markets. Head north to Chiang Mai for temples and elephant sanctuaries. Fly south to the islands; Phuket or Koh Samui for beaches, diving, and nightlife. Consider visiting during the cool season (November-February) for pleasant weather. Accommodation ranges from budget hostels to luxury resorts. Internal flights, trains, and buses connect major destinations. Donâ€™t miss local cuisine, from street food to fine dining. Ensure your passport is valid for six months and consider travel insurance for peace of mind. Enjoy!

You: exit

Goodbye! Safe travels!
```

```
Welcome to the Travel Advisor Chat! Type 'exit' to end the conversation.

You: How to study large language models?

Travel Advisor: The query is not related to travel advice, destinations, planning, or other travel topics. It is about studying large language models, which is a topic related to artificial intelligence and machine learning.

You: exit

Goodbye! Safe travels!
```
</file>

<file path="cookbook/pocketflow-chat-guardrail/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
</file>

<file path="cookbook/pocketflow-chat-guardrail/utils.py">
from openai import OpenAI
import os

def call_llm(messages):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content

if __name__ == "__main__":
    # Test the LLM call
    messages = [{"role": "user", "content": "In a few words, what's the meaning of life?"}]
    response = call_llm(messages)
    print(f"Prompt: {messages[0]['content']}")
    print(f"Response: {response}")
</file>

<file path="cookbook/pocketflow-chat-memory/utils/__init__.py">

</file>

<file path="cookbook/pocketflow-chat-memory/utils/call_llm.py">
import os
from openai import OpenAI

def call_llm(messages):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content

if __name__ == "__main__":
    # Test the LLM call
    messages = [{"role": "user", "content": "In a few words, what's the meaning of life?"}]
    response = call_llm(messages)
    print(f"Prompt: {messages[0]['content']}")
    print(f"Response: {response}")
</file>

<file path="cookbook/pocketflow-chat-memory/utils/get_embedding.py">
import os
import numpy as np
from openai import OpenAI

def get_embedding(text):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "YOUR_API_KEY"))
    
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    
    # Extract the embedding vector from the response
    embedding = response.data[0].embedding
    
    # Convert to numpy array for consistency with other embedding functions
    return np.array(embedding, dtype=np.float32)


if __name__ == "__main__":
    # Test the embedding function
    text1 = "The quick brown fox jumps over the lazy dog."
    text2 = "Python is a popular programming language for data science."
    
    emb1 = get_embedding(text1)
    emb2 = get_embedding(text2)
    
    print(f"Embedding 1 shape: {emb1.shape}")
    print(f"Embedding 2 shape: {emb2.shape}")
    
    # Calculate similarity (dot product)
    similarity = np.dot(emb1, emb2)
    print(f"Similarity between texts: {similarity:.4f}")
</file>

<file path="cookbook/pocketflow-chat-memory/utils/vector_index.py">
import numpy as np
import faiss

def create_index(dimension=1536):
    return faiss.IndexFlatL2(dimension)

def add_vector(index, vector):
    # Make sure the vector is a numpy array with the right shape for FAISS
    vector = np.array(vector).reshape(1, -1).astype(np.float32)
    
    # Add the vector to the index
    index.add(vector)
    
    # Return the position (index.ntotal is the total number of vectors in the index)
    return index.ntotal - 1

def search_vectors(index, query_vector, k=1):
    """Search for the k most similar vectors to the query vector
    
    Args:
        index: The FAISS index
        query_vector: The query vector (numpy array or list)
        k: Number of results to return (default: 1)
        
    Returns:
        tuple: (indices, distances) where:
            - indices is a list of positions in the index
            - distances is a list of the corresponding distances
    """
    # Make sure we don't try to retrieve more vectors than exist in the index
    k = min(k, index.ntotal)
    if k == 0:
        return [], []
        
    # Make sure the query is a numpy array with the right shape for FAISS
    query_vector = np.array(query_vector).reshape(1, -1).astype(np.float32)
    
    # Search the index
    distances, indices = index.search(query_vector, k)
    
    return indices[0].tolist(), distances[0].tolist()

# Example usage
if __name__ == "__main__":
    # Create a new index
    index = create_index(dimension=3)
    
    # Add some random vectors and track them separately
    items = []
    for i in range(5):
        vector = np.random.random(3)
        position = add_vector(index, vector)
        items.append(f"Item {i}")
        print(f"Added vector at position {position}")
        
    print(f"Index contains {index.ntotal} vectors")
    
    # Search for a similar vector
    query = np.random.random(3)
    indices, distances = search_vectors(index, query, k=2)
    
    print("Query:", query)
    print("Found indices:", indices)
    print("Distances:", distances)
    print("Retrieved items:", [items[idx] for idx in indices])
</file>

<file path="cookbook/pocketflow-chat-memory/flow.py">
from pocketflow import Flow
from nodes import GetUserQuestionNode, RetrieveNode, AnswerNode, EmbedNode

def create_chat_flow():
    # Create the nodes
    question_node = GetUserQuestionNode()
    retrieve_node = RetrieveNode()
    answer_node = AnswerNode()
    embed_node = EmbedNode()
    
    # Connect the flow:
    # 1. Start with getting a question
    # 2. Retrieve relevant conversations
    # 3. Generate an answer
    # 4. Optionally embed old conversations
    # 5. Loop back to get the next question

    # Main flow path
    question_node - "retrieve" >> retrieve_node
    retrieve_node - "answer" >> answer_node
    
    # When we need to embed old conversations
    answer_node - "embed" >> embed_node
    
    # Loop back for next question
    answer_node - "question" >> question_node
    embed_node - "question" >> question_node
    
    # Create the flow starting with question node
    return Flow(start=question_node)

# Initialize the flow
chat_flow = create_chat_flow()
</file>

<file path="cookbook/pocketflow-chat-memory/main.py">
from flow import chat_flow

def run_chat_memory_demo():
    """
    Run an interactive chat interface with memory retrieval.
    
    Features:
    1. Maintains a window of the 3 most recent conversation pairs
    2. Archives older conversations with embeddings
    3. Retrieves 1 relevant past conversation when needed
    4. Total context to LLM: 3 recent pairs + 1 retrieved pair
    """
    
    print("=" * 50)
    print("PocketFlow Chat with Memory")
    print("=" * 50)
    print("This chat keeps your 3 most recent conversations")
    print("and brings back relevant past conversations when helpful")
    print("Type 'exit' to end the conversation")
    print("=" * 50)
    
    # Run the chat flow
    chat_flow.run({})

if __name__ == "__main__":
    run_chat_memory_demo()
</file>

<file path="cookbook/pocketflow-chat-memory/nodes.py">
from pocketflow import Node
from utils.vector_index import create_index, add_vector, search_vectors
from utils.call_llm import call_llm
from utils.get_embedding import get_embedding

class GetUserQuestionNode(Node):
    def prep(self, shared):
        """Initialize messages if first run"""
        if "messages" not in shared:
            shared["messages"] = []
            print("Welcome to the interactive chat! Type 'exit' to end the conversation.")
        
        return None
    
    def exec(self, _):
        """Get user input interactively"""
        # Get interactive input from user
        user_input = input("\nYou: ")
            
        # Check if user wants to exit
        if user_input.lower() == 'exit':
            return None
            
        return user_input
    
    def post(self, shared, prep_res, exec_res):
        # If exec_res is None, the user wants to exit
        if exec_res is None:
            print("\nGoodbye!")
            return None  # End the conversation
            
        # Add user message to current messages
        shared["messages"].append({"role": "user", "content": exec_res})
        
        return "retrieve"

class AnswerNode(Node):
    def prep(self, shared):
        """Prepare context for the LLM"""
        if not shared.get("messages"):
            return None
            
        # 1. Get the last 3 conversation pairs (or fewer if not available)
        recent_messages = shared["messages"][-6:] if len(shared["messages"]) > 6 else shared["messages"]
        
        # 2. Add the retrieved relevant conversation if available
        context = []
        if shared.get("retrieved_conversation"):
            # Add a system message to indicate this is a relevant past conversation
            context.append({
                "role": "system", 
                "content": "The following is a relevant past conversation that may help with the current query:"
            })
            context.extend(shared["retrieved_conversation"])
            context.append({
                "role": "system", 
                "content": "Now continue the current conversation:"
            })
        
        # 3. Add the recent messages
        context.extend(recent_messages)
        
        return context
    
    def exec(self, messages):
        """Generate a response using the LLM"""
        if messages is None:
            return None
        
        # Call LLM with the context
        response = call_llm(messages)
        return response
    
    def post(self, shared, prep_res, exec_res):
        """Process the LLM response"""
        if prep_res is None or exec_res is None:
            return None  # End the conversation
        
        # Print the assistant's response
        print(f"\nAssistant: {exec_res}")
        
        # Add assistant message to history
        shared["messages"].append({"role": "assistant", "content": exec_res})
        
        # If we have more than 6 messages (3 conversation pairs), archive the oldest pair
        if len(shared["messages"]) > 6:
            return "embed"
        
        # We only end if the user explicitly typed 'exit'
        # Even if last_question is set, we continue in interactive mode
        return "question"

class EmbedNode(Node):
    def prep(self, shared):
        """Extract the oldest conversation pair for embedding"""
        if len(shared["messages"]) <= 6:
            return None
            
        # Extract the oldest user-assistant pair
        oldest_pair = shared["messages"][:2]
        # Remove them from current messages
        shared["messages"] = shared["messages"][2:]
        
        return oldest_pair
    
    def exec(self, conversation):
        """Embed a conversation"""
        if not conversation:
            return None
            
        # Combine user and assistant messages into a single text for embedding
        user_msg = next((msg for msg in conversation if msg["role"] == "user"), {"content": ""})
        assistant_msg = next((msg for msg in conversation if msg["role"] == "assistant"), {"content": ""})
        combined = f"User: {user_msg['content']} Assistant: {assistant_msg['content']}"
        
        # Generate embedding
        embedding = get_embedding(combined)
        
        return {
            "conversation": conversation,
            "embedding": embedding
        }
    
    def post(self, shared, prep_res, exec_res):
        """Store the embedding and add to index"""
        if not exec_res:
            # If there's nothing to embed, just continue with the next question
            return "question"
            
        # Initialize vector index if not exist
        if "vector_index" not in shared:
            shared["vector_index"] = create_index()
            shared["vector_items"] = []  # Track items separately
            
        # Add the embedding to the index and store the conversation
        position = add_vector(shared["vector_index"], exec_res["embedding"])
        shared["vector_items"].append(exec_res["conversation"])
        
        print(f"âœ… Added conversation to index at position {position}")
        print(f"âœ… Index now contains {len(shared['vector_items'])} conversations")
        
        # Continue with the next question
        return "question"

class RetrieveNode(Node):
    def prep(self, shared):
        """Get the current query for retrieval"""
        if not shared.get("messages"):
            return None
            
        # Get the latest user message for searching
        latest_user_msg = next((msg for msg in reversed(shared["messages"]) 
                                if msg["role"] == "user"), {"content": ""})
        
        # Check if we have a vector index with items
        if ("vector_index" not in shared or 
            "vector_items" not in shared or 
            len(shared["vector_items"]) == 0):
            return None
            
        return {
            "query": latest_user_msg["content"],
            "vector_index": shared["vector_index"],
            "vector_items": shared["vector_items"]
        }
    
    def exec(self, inputs):
        """Find the most relevant past conversation"""
        if not inputs:
            return None
            
        query = inputs["query"]
        vector_index = inputs["vector_index"]
        vector_items = inputs["vector_items"]
        
        print(f"ðŸ” Finding relevant conversation for: {query[:30]}...")
        
        # Create embedding for the query
        query_embedding = get_embedding(query)
        
        # Search for the most similar conversation
        indices, distances = search_vectors(vector_index, query_embedding, k=1)
        
        if not indices:
            return None
            
        # Get the corresponding conversation
        conversation = vector_items[indices[0]]
        
        return {
            "conversation": conversation,
            "distance": distances[0]
        }
    
    def post(self, shared, prep_res, exec_res):
        """Store the retrieved conversation"""
        if exec_res is not None:
            shared["retrieved_conversation"] = exec_res["conversation"]
            print(f"ðŸ“„ Retrieved conversation (distance: {exec_res['distance']:.4f})")
        else:
            shared["retrieved_conversation"] = None
        
        return "answer"
</file>

<file path="cookbook/pocketflow-chat-memory/README.md">
# PocketFlow Chat with Memory

A chat application with memory retrieval using PocketFlow. This example maintains a sliding window of recent conversations while retrieving relevant past conversations based on context. 

This implementation is based directly on the tutorial: [Build AI Agent Memory From Scratch â€” Tutorial For Dummies](https://zacharyhuang.substack.com/p/build-ai-agent-memory-from-scratch).

## Features

- Maintains a window of 3 most recent conversation pairs
- Archives older conversations with embeddings
- Uses vector similarity to retrieve the most relevant past conversation
- Combines recent context (3 pairs) with retrieved context (1 pair) for better responses

## Run It

1. Make sure your OpenAI API key is set:
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```

2. Install requirements and run the application:
    ```bash
    pip install -r requirements.txt
    python main.py
    ```
    
## How It Works

```mermaid
flowchart LR
    Question[GetUserQuestionNode] -->|retrieve| Retrieve[RetrieveNode]
    Retrieve -->|answer| Answer[AnswerNode]
    Answer -->|question| Question
    Answer -->|embed| Embed[EmbedNode]
    Embed -->|question| Question
```

The chat application uses:
- Four specialized nodes:
  - `GetUserQuestionNode`: Handles interactive user input
  - `RetrieveNode`: Finds relevant past conversations using vector similarity
  - `AnswerNode`: Generates responses using both recent and retrieved context
  - `EmbedNode`: Archives older conversations with embeddings
- A sliding window approach that maintains only the 3 most recent conversation pairs in active context

## Files

- [`nodes.py`](./nodes.py): Four node implementations with clear separation of concerns
- [`flow.py`](./flow.py): Chat flow structure definition
- [`main.py`](./main.py): Entry point for running the demo
- [`utils/`](./utils/): Utility functions for embeddings, LLM calls, and vector operations


## Example Output

```
==================================================
PocketFlow Chat with Memory
==================================================
This chat keeps your 3 most recent conversations
and brings back relevant past conversations when helpful
Type 'exit' to end the conversation
==================================================
Welcome to the interactive chat! Type 'exit' to end the conversation.

You: remember, my cat name is Whiskers and she only drinks from running water taps

Assistant: Got it! Whiskers only drinks from running water taps. If you have any questions or need advice related to her, feel free to ask!

You: remember, I have a peanut allergy but I can eat almonds and cashews without problems

Assistant: Understood! You have a peanut allergy but can safely eat almonds and cashews. If you need any assistance or information related to your dietary preferences or restrictions, just let me know!

You: remember, my anniversary is June 17th and I always forget to buy a gift until the last minute

Assistant: Got it! Your anniversary is on June 17th, and you often remember to buy a gift at the last minute. If you'd like, I can help you brainstorm gift ideas or set reminders as the date approaches!

You: remember, I lived in Portugal for 3 years when I was a child and still speak some Portuguese

Assistant: That's great to know! You lived in Portugal for three years during your childhood and still speak some Portuguese. If you ever want to practice or need help with anything related to Portuguese language or culture, feel free to ask!
âœ… Added conversation to index at position 0
âœ… Index now contains 1 conversations

You: remember, my favorite dessert is tiramisu but only if it's made with real mascarpone cheese
ðŸ” Finding relevant conversation for: remember, my favorite dessert ...
ðŸ“„ Retrieved conversation (distance: 0.5008)

Assistant: Got it! Your favorite dessert is tiramisu, but only when it's made with real mascarpone cheese. If you ever need recommendations or recipes, just let me know!
âœ… Added conversation to index at position 1
âœ… Index now contains 2 conversations

You: remember, I collect vintage mechanical watches and my most valuable one is a 1965 Omega Seamaster
ðŸ” Finding relevant conversation for: remember, I collect vintage me...
ðŸ“„ Retrieved conversation (distance: 0.5374)

Assistant: Got it! You collect vintage mechanical watches, and your most valuable piece is a 1965 Omega Seamaster. If you have questions about watches or need assistance with your collection, feel free to reach out!
âœ… Added conversation to index at position 2
âœ… Index now contains 3 conversations

You: what's my cat name?
ðŸ” Finding relevant conversation for: what's my cat name?...
ðŸ“„ Retrieved conversation (distance: 0.3643)

Assistant: Your cat's name is Whiskers.
âœ… Added conversation to index at position 3
âœ… Index now contains 4 conversations
```
</file>

<file path="cookbook/pocketflow-chat-memory/requirements.txt">
pocketflow>=0.0.2
numpy>=1.20.0
faiss-cpu>=1.7.0
openai>=1.0.0
</file>

<file path="cookbook/pocketflow-cli-hitl/docs/design.md">
# Design Doc: Command-Line Joke Generator

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

The system will be a command-line application that:
1. Asks the user for a topic for a joke.
2. Generates a joke based on the provided topic.
3. Asks the user if they approve of the joke.
4. If the user approves, the application can end or offer to generate another joke (for simplicity, we'll end for now).
5. If the user does not approve, the application should:
    a. Take note that the user disliked the previous joke.
    b. Generate a new joke about the same topic, attempting to make it different from the disliked one.
    c. Repeat step 3.

## Flow Design

> Notes for AI:
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

**Agent**: The system acts as an agent that interacts with the user. It takes user input (topic, feedback), performs an action (generates a joke), and then decides the next step based on user feedback (either end or try generating another joke). This iterative process of generation and feedback fits the agent pattern.

### Flow high-level Design:

1.  **GetTopicNode**: Prompts the user to enter the topic for the joke.
2.  **GenerateJokeNode**: Generates a joke based on the topic and any previous feedback.
3.  **GetFeedbackNode**: Presents the joke to the user and asks for approval. Based on the feedback, it either transitions to end the flow or back to `GenerateJokeNode`.

```mermaid
flowchart TD
    GetTopic[GetTopicNode] --> GenerateJoke[GenerateJokeNode]
    GenerateJoke --> GetFeedback[GetFeedbackNode]
    GetFeedback -- "Approve" --> Z((End))
    GetFeedback -- "Disapprove" --> GenerateJoke
```
## Utility Functions

> Notes for AI:
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1.  **Call LLM** (`utils/call_llm.py`)
    *   *Input*: `prompt` (str), potentially including context like previously disliked jokes.
    *   *Output*: `response` (str) - the generated joke.
    *   *Necessity*: Used by `GenerateJokeNode` to generate jokes.

## Node Design

### Shared Store

> Notes for AI: Try to minimize data redundancy

The shared store structure is organized as follows:

```python
shared = {
    "topic": None,             # Stores the user-provided joke topic
    "current_joke": None,      # Stores the most recently generated joke
    "disliked_jokes": [],    # A list to store jokes the user didn't like, for context
    "user_feedback": None      # Stores the user's latest feedback (e.g., "approve", "disapprove")
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1.  **GetTopicNode**
    *   *Purpose*: To get the desired joke topic from the user.
    *   *Type*: Regular
    *   *Steps*:
        *   `prep`: (None needed for the first run, or could check if a topic already exists if we were to loop for a new topic)
        *   `exec`: Prompt the user via `input()` for a joke topic.
        *   `post`: Store the user's input topic into `shared["topic"]`. Return `"default"` action to proceed to `GenerateJokeNode`.

2.  **GenerateJokeNode**
    *   *Purpose*: To generate a joke using an LLM, based on the topic and any previously disliked jokes.
    *   *Type*: Regular
    *   *Steps*:
        *   `prep`: Read `shared["topic"]` and `shared["disliked_jokes"]`. Construct a prompt for the LLM, including the topic and a message like "The user did not like the following jokes: [list of disliked jokes]. Please generate a new, different joke about [topic]."
        *   `exec`: Call the `call_llm` utility function with the prepared prompt.
        *   `post`: Store the generated joke in `shared["current_joke"]`. Print the joke to the console. Return `"default"` action to proceed to `GetFeedbackNode`.

3.  **GetFeedbackNode**
    *   *Purpose*: To get feedback from the user about the generated joke and decide the next step.
    *   *Type*: Regular
    *   *Steps*:
        *   `prep`: Read `shared["current_joke"]`.
        *   `exec`: Prompt the user (e.g., "Did you like this joke? (yes/no) or (approve/disapprove): "). Get user's input.
        *   `post`:
            *   If user input indicates approval (e.g., "yes", "approve"):
                *   Store "approve" in `shared["user_feedback"]`.
                *   Return `"Approve"` action (leading to flow termination or a thank you message).
            *   If user input indicates disapproval (e.g., "no", "disapprove"):
                *   Store "disapprove" in `shared["user_feedback"]`.
                *   Add `shared["current_joke"]` to the `shared["disliked_jokes"]` list.
                *   Return `"Disapprove"` action (leading back to `GenerateJokeNode`).
</file>

<file path="cookbook/pocketflow-cli-hitl/utils/call_llm.py">
from anthropic import Anthropic
import os

def call_llm(prompt: str) -> str:
    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-anthropic-api-key")) # Default if key not found
    response = client.messages.create(
        model="claude-3-haiku-20240307", # Using a smaller model for jokes
        max_tokens=150, # Jokes don't need to be very long
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

if __name__ == "__main__":
    print("Testing Anthropic LLM call for jokes:")
    joke_prompt = "Tell me a one-liner joke about a cat."
    print(f"Prompt: {joke_prompt}")
    try:
        response = call_llm(joke_prompt)
        print(f"Response: {response}")
    except Exception as e:
        print(f"Error calling LLM: {e}")
        print("Please ensure your ANTHROPIC_API_KEY environment variable is set correctly.")
</file>

<file path="cookbook/pocketflow-cli-hitl/flow.py">
from pocketflow import Flow
from nodes import GetTopicNode, GenerateJokeNode, GetFeedbackNode

def create_joke_flow() -> Flow:
    """Creates and returns the joke generation flow."""
    get_topic_node = GetTopicNode()
    generate_joke_node = GenerateJokeNode()
    get_feedback_node = GetFeedbackNode()

    get_topic_node >> generate_joke_node
    generate_joke_node >> get_feedback_node
    get_feedback_node - "Disapprove" >> generate_joke_node

    joke_flow = Flow(start=get_topic_node)
    return joke_flow
</file>

<file path="cookbook/pocketflow-cli-hitl/main.py">
from flow import create_joke_flow

def main():
    """Main function to run the joke generator application."""
    print("Welcome to the Command-Line Joke Generator!")

    shared = {
        "topic": None,
        "current_joke": None,
        "disliked_jokes": [],
        "user_feedback": None
    }

    joke_flow = create_joke_flow()
    joke_flow.run(shared)

    print("\nThanks for using the Joke Generator!")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-cli-hitl/nodes.py">
from pocketflow import Node
from utils.call_llm import call_llm

class GetTopicNode(Node):
    """Prompts the user to enter the topic for the joke."""
    def exec(self, _shared):
        return input("What topic would you like a joke about? ")

    def post(self, shared, _prep_res, exec_res):
        shared["topic"] = exec_res

class GenerateJokeNode(Node):
    """Generates a joke based on the topic and any previous feedback."""
    def prep(self, shared):
        topic = shared.get("topic", "anything")
        disliked_jokes = shared.get("disliked_jokes", [])
        
        prompt = f"Please generate an one-liner joke about: {topic}. Make it short and funny."
        if disliked_jokes:
            disliked_str = "; ".join(disliked_jokes)
            prompt = f"The user did not like the following jokes: [{disliked_str}]. Please generate a new, different joke about {topic}."
        return prompt

    def exec(self, prep_res):
        return call_llm(prep_res)

    def post(self, shared, _prep_res, exec_res):
        shared["current_joke"] = exec_res
        print(f"\nJoke: {exec_res}")

class GetFeedbackNode(Node):
    """Presents the joke to the user and asks for approval."""
    def exec(self, _prep_res):
        while True:
            feedback = input("Did you like this joke? (yes/no): ").strip().lower()
            if feedback in ["yes", "y", "no", "n"]:
                return feedback
            print("Invalid input. Please type 'yes' or 'no'.")

    def post(self, shared, _prep_res, exec_res):
        if exec_res in ["yes", "y"]:
            shared["user_feedback"] = "approve"
            print("Great! Glad you liked it.")
            return "Approve"
        else:
            shared["user_feedback"] = "disapprove"
            current_joke = shared.get("current_joke")
            if current_joke:
                if "disliked_jokes" not in shared:
                    shared["disliked_jokes"] = []
                shared["disliked_jokes"].append(current_joke)
            print("Okay, let me try another one.")
            return "Disapprove"
</file>

<file path="cookbook/pocketflow-cli-hitl/README.md">
# PocketFlow Command-Line Joke Generator (Human-in-the-Loop Example)

A simple, interactive command-line application that generates jokes based on user-provided topics and direct human feedback. This serves as a clear example of a Human-in-the-Loop (HITL) workflow orchestrated by PocketFlow.

## Features

- **Interactive Joke Generation**: Ask for jokes on any topic.
- **Human-in-the-Loop Feedback**: Dislike a joke? Your feedback directly influences the next generation attempt.
- **Minimalist Design**: A straightforward example of using PocketFlow for HITL tasks.
- **Powered by LLMs**: (Uses Anthropic Claude via an API call for joke generation).

## Getting Started

This project is part of the PocketFlow cookbook examples. It's assumed you have already cloned the [PocketFlow repository](https://github.com/the-pocket/PocketFlow) and are in the `cookbook/pocketflow-cli-hitl` directory.

1.  **Install required dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

2.  **Set up your Anthropic API key**:
    The application uses Anthropic Claude to generate jokes. You need to set your API key as an environment variable.
    ```bash
    export ANTHROPIC_API_KEY="your-anthropic-api-key-here"
    ```
    You can test if your `call_llm.py` utility is working by running it directly:
    ```bash
    python utils/call_llm.py
    ```

3.  **Run the Joke Generator**:
    ```bash
    python main.py
    ```

## How It Works

The system uses a simple PocketFlow workflow:

```mermaid
flowchart TD
    GetTopic[GetTopicNode] --> GenerateJoke[GenerateJokeNode]
    GenerateJoke --> GetFeedback[GetFeedbackNode]
    GetFeedback -- "Approve" --> Z((End))
    GetFeedback -- "Disapprove" --> GenerateJoke
```

1.  **GetTopicNode**: Prompts the user to enter a topic for the joke.
2.  **GenerateJokeNode**: Sends the topic (and any previously disliked jokes as context) to an LLM to generate a new joke.
3.  **GetFeedbackNode**: Shows the joke to the user and asks if they liked it.
    *   If **yes** (approved), the application ends.
    *   If **no** (disapproved), the disliked joke is recorded, and the flow loops back to `GenerateJokeNode` to try again.

## Sample Output

Here's an example of an interaction with the Joke Generator:

```
Welcome to the Command-Line Joke Generator!
What topic would you like a joke about? Pocket Flow: 100-line LLM framework

Joke: Pocket Flow: Finally, an LLM framework that fits in your pocket! Too bad your model still needs a data center.
Did you like this joke? (yes/no): no
Okay, let me try another one.

Joke: Pocket Flow: A 100-line LLM framework where 99 lines are imports and the last line is `print("TODO: implement intelligence")`.
Did you like this joke? (yes/no): yes
Great! Glad you liked it.

Thanks for using the Joke Generator!
```

## Files

-   [`main.py`](./main.py): Entry point for the application.
-   [`flow.py`](./flow.py): Defines the PocketFlow graph and node connections.
-   [`nodes.py`](./nodes.py): Contains the definitions for `GetTopicNode`, `GenerateJokeNode`, and `GetFeedbackNode`.
-   [`utils/call_llm.py`](./utils/call_llm.py): Utility function to interact with the LLM (Anthropic Claude).
-   [`requirements.txt`](./requirements.txt): Lists project dependencies.
-   [`docs/design.md`](./docs/design.md): The design document for this application.
</file>

<file path="cookbook/pocketflow-cli-hitl/requirements.txt">
pocketflow>=0.0.1
anthropic>=0.20.0 # Or a recent version
</file>

<file path="cookbook/pocketflow-code-generator/doc/design.md">
# Design Doc: PocketFlow Code Generator

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

**User Story**: As a developer, I want an AI system that can take a LeetCode-style coding problem and automatically:
1. Generate comprehensive test cases including edge cases
2. Implement a solution function
3. Test the implementation against the test cases
4. When tests fail, intelligently decide whether to revise the test cases or the function
5. Iterate until all tests pass

**Sample Problem**: Two Sum - Given an array of integers and a target, return indices of two numbers that add up to the target.

This is well-suited for AI because:
- âœ… Routine task: Test case generation follows patterns
- âœ… Creative task: Code generation from clear problem descriptions  
- âœ… Clear decision criteria: Whether to revise tests vs implementation

## Flow Design

> Notes for AI:
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

1. **Workflow Pattern**: Sequential steps of test generation â†’ coding â†’ testing
2. **Agent Pattern**: Decision-making when tests fail with structured output
   - *Context*: Test results, current test cases, and function code
   - *Actions*: Structured output to revise test cases and/or function

### Flow high-level Design:

1. **Generate Test Cases**: Create comprehensive input/output test pairs from problem description
2. **Implement Function**: Write `def run_code` function based on problem and current test cases  
3. **Run Tests**: Execute function against all test cases using batch processing
4. **Revise**: Analyze failures and output structured revisions for test cases and/or function
5. **Loop back to Run Tests** until all pass

```mermaid
flowchart TD
    start[Problem Input] --> generateTests[Generate Test Cases]
    generateTests --> implement[Implement Function]
    implement --> runTests[Run Tests - Batch]
    runTests --> decision{All Tests Pass?}
    decision -->|Yes| success[Success!]
    decision -->|No| revise[Revise]
    revise --> runTests
```

## Utility Functions

> Notes for AI:
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1. **Call LLM** (`utils/call_llm.py`)
   - *Input*: prompt (str)
   - *Output*: response (str)
   - Used by all LLM-powered nodes for generating tests, code, and analysis

2. **Execute Python Code** (`utils/code_executor.py`)
   - *Input*: function_code (str), input (dict/list/any)
   - *Output*: output (any), error (str)
   - Used by RunTests batch node to safely execute generated code against individual input

## Node Design

### Shared Memory

> Notes for AI: Try to minimize data redundancy

The shared memory structure is organized as follows:

```python
shared = {
    "problem": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.",
    "test_cases": [
        {"name": "Basic case", "input": {"nums": [2,7,11,15], "target": 9}, "expected": [0,1]},
        {"name": "Different order", "input": {"nums": [3,2,4], "target": 6}, "expected": [1,2]},
        # ... more test cases
    ],
    "function_code": "def run_code(nums, target): ...",
    "test_results": [
        {"test_case": {...}, "passed": True/False, "error": "..."},
        # ... results for each test case
    ],
    "iteration_count": 0,
    "max_iterations": 5
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1. **GenerateTestCases Node**
  - *Purpose*: Create comprehensive test cases including edge cases from problem description
  - *Type*: Regular Node
  - *Steps*:
    - *prep*: Read problem description from shared store
    - *exec*: Call LLM to generate diverse test cases in structured format
    - *post*: Store test cases directly in shared["test_cases"]

2. **ImplementFunction Node**
  - *Purpose*: Generate `def run_code` function based on problem and current test cases
  - *Type*: Regular Node  
  - *Steps*:
    - *prep*: Read problem description and test cases from shared store
    - *exec*: Call LLM to implement `def run_code` function with clean code output
    - *post*: Store function code directly in shared["function_code"]

3. **RunTests Node**
  - *Purpose*: Execute function against all test cases using batch processing
  - *Type*: Batch Node
  - *Steps*:
    - *prep*: Read function code from shared store, return list of test cases
    - *exec*: Use code executor utility to run function against each individual test case
    - *post*: Store all results in shared["test_results"], return "success" if all pass else "failure"

4. **Revise Node** (Agent with Structured Output)
  - *Purpose*: Analyze test failures and output structured revisions for test cases and/or function
  - *Type*: Regular Node (Agent decision-making)
  - *Steps*:
    - *prep*: Read test results, test cases, function code, iteration count from shared store
    - *exec*: Call LLM to analyze failures and output structured YAML with revised test cases and/or function code
    - *post*: Update shared["test_cases"] and/or shared["function_code"] based on structured output
</file>

<file path="cookbook/pocketflow-code-generator/utils/call_llm.py">
from anthropic import Anthropic
import os

def call_llm(prompt):
    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=6000,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")
</file>

<file path="cookbook/pocketflow-code-generator/utils/code_executor.py">
import sys
import io
import traceback
from contextlib import redirect_stdout, redirect_stderr

def execute_python(function_code, input):
    try:
        namespace = {"__builtins__": __builtins__}
        stdout_capture = io.StringIO()
        stderr_capture = io.StringIO()
        
        with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
            exec(function_code, namespace)
            
            if "run_code" not in namespace:
                return None, "Function 'run_code' not found"
            
            run_code = namespace["run_code"]
            
            if isinstance(input, dict):
                result = run_code(**input)
            elif isinstance(input, (list, tuple)):
                result = run_code(*input)
            else:
                result = run_code(input)
            
            return result, None
                
    except Exception as e:
        return None, f"{type(e).__name__}: {str(e)}"

if __name__ == "__main__":
    # Test 1: Working function
    function_code = """
def run_code(nums, target):
    for i in range(len(nums)):
        for j in range(i + 1, len(nums)):
            if nums[i] + nums[j] == target:
                return [i, j]
    return []
"""
    
    input = {"nums": [2, 7, 11, 15], "target": 9}
    output, error = execute_python(function_code, input)
    print(f"Output: {output}")
    print(f"Error: {error}")
    
    # Test 2: Function with error
    broken_function_code = """
def run_code(nums, target):
    return nums[100]  # Index error
"""
    
    output2, error2 = execute_python(broken_function_code, input)
    print(f"Output: {output2}")
    print(f"Error: {error2}")
</file>

<file path="cookbook/pocketflow-code-generator/flow.py">
from pocketflow import Flow
from nodes import GenerateTestCases, ImplementFunction, RunTests, Revise

def create_code_generator_flow():
    """Creates and returns the code generator flow."""
    # Create nodes
    generate_tests = GenerateTestCases()
    implement_function = ImplementFunction()
    run_tests = RunTests()
    revise = Revise()

    # Define transitions
    generate_tests >> implement_function
    implement_function >> run_tests
    run_tests - "failure" >> revise
    revise >> run_tests

    # Create flow starting with test generation
    flow = Flow(start=generate_tests)
    return flow
</file>

<file path="cookbook/pocketflow-code-generator/main.py">
import sys
from flow import create_code_generator_flow

def main():
    """Runs the PocketFlow Code Generator application."""
    print("Starting PocketFlow Code Generator...")
    
    # Check if problem is provided as argument
    if len(sys.argv) > 1:
        problem = " ".join(sys.argv[1:])
    else:
        # Default Two Sum problem
        problem = """Two Sum

Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.

You may assume that each input would have exactly one solution, and you may not use the same element twice.

Example 1:
Input: nums = [2,7,11,15], target = 9
Output: [0,1]

Example 2:
Input: nums = [3,2,4], target = 6
Output: [1,2]

Example 3:
Input: nums = [3,3], target = 6
Output: [0,1]"""

    shared = {
        "problem": problem,
        "test_cases": [],  # Will be populated with [{name, input, expected}, ...]
        "function_code": "",
        "test_results": [],
        "iteration_count": 0,
        "max_iterations": 5
    }

    # Create and run the flow
    flow = create_code_generator_flow()
    flow.run(shared)
    
    print("\n=== Final Results ===")
    print(f"Problem: {shared['problem'][:50]}...")
    print(f"Iterations: {shared['iteration_count']}")
    print(f"Function:\n{shared['function_code']}")
    print(f"Test Results: {len([r for r in shared['test_results'] if r['passed']])}/{len(shared['test_results'])} passed")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-code-generator/nodes.py">
import yaml
from pocketflow import Node, BatchNode
from utils.call_llm import call_llm
from utils.code_executor import execute_python

class GenerateTestCases(Node):
    def prep(self, shared):
        return shared["problem"]

    def exec(self, problem):
        prompt = f"""Generate 5-7 test cases for this coding problem:

{problem}

Output in this YAML format with reasoning:
```yaml
reasoning: |
    The input parameters should be: param1 as a string, and param2 as a number.
    To test the function, I will consider basic cases, edge cases, and corner cases.
    For this problem, I need to test...
test_cases:
  - name: "Basic case"
    input: {{param1: value1, param2: value2}}
    expected: result1
  - name: "Edge case - empty"
    input: {{param1: value3, param2: value4}}
    expected: result2
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        # Validation asserts
        assert "test_cases" in result, "Result must have 'test_cases' field"
        assert isinstance(result["test_cases"], list), "test_cases must be a list"
        
        for i, test_case in enumerate(result["test_cases"]):
            assert "name" in test_case, f"Test case {i} missing 'name' field"
            assert isinstance(test_case["name"], str), f"Test case {i} 'name' must be string"
            assert "input" in test_case, f"Test case {i} missing 'input' field"
            assert isinstance(test_case["input"], dict), f"Test case {i} 'input' must be dict"
            assert "expected" in test_case, f"Test case {i} missing 'expected' field"
        
        return result

    def post(self, shared, prep_res, exec_res):
        shared["test_cases"] = exec_res["test_cases"]
        
        # Print all generated test cases
        print(f"\n=== Generated {len(exec_res['test_cases'])} Test Cases ===")
        for i, test_case in enumerate(exec_res["test_cases"], 1):
            print(f"{i}. {test_case['name']}")
            print(f"   input: {test_case['input']}")
            print(f"   expected: {test_case['expected']}")

class ImplementFunction(Node):
    def prep(self, shared):
        return shared["problem"], shared["test_cases"]

    def exec(self, inputs):
        problem, test_cases = inputs
        
        # Format test cases nicely for the prompt
        formatted_tests = ""
        for i, test in enumerate(test_cases, 1):
            formatted_tests += f"{i}. {test['name']}\n"
            formatted_tests += f"   input: {test['input']}\n"
            formatted_tests += f"   expected: {test['expected']}\n\n"
        
        prompt = f"""Implement a solution for this problem:

{problem}

Test cases to consider:
{formatted_tests}

IMPORTANT: The function name must be exactly "run_code"

Output in this YAML format:
```yaml
reasoning: |
    To implement this function, I will...
    My approach is...
function_code: |
    def run_code(...):
        # your implementation
        return result
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        # Validation asserts
        assert "function_code" in result, "Result must have 'function_code' field"
        assert isinstance(result["function_code"], str), "function_code must be string"
        assert "def run_code" in result["function_code"], "Function must be named 'run_code'"
        
        return result["function_code"]

    def post(self, shared, prep_res, exec_res):
        shared["function_code"] = exec_res
        
        # Print the implemented function
        print(f"\n=== Implemented Function ===")
        print(exec_res)

class RunTests(BatchNode):
    def prep(self, shared):
        function_code = shared["function_code"]
        test_cases = shared["test_cases"]
        # Return list of tuples (function_code, test_case)
        return [(function_code, test_case) for test_case in test_cases]

    def exec(self, test_data):
        function_code, test_case = test_data
        output, error = execute_python(function_code, test_case["input"])
        
        if error:
            return {
                "test_case": test_case,
                "passed": False,
                "actual": None,
                "expected": test_case["expected"],
                "error": error
            }
        
        passed = output == test_case["expected"]
        return {
            "test_case": test_case,
            "passed": passed,
            "actual": output,
            "expected": test_case["expected"],
            "error": None if passed else f"Expected {test_case['expected']}, got {output}"
        }

    def post(self, shared, prep_res, exec_res_list):
        shared["test_results"] = exec_res_list
        all_passed = all(result["passed"] for result in exec_res_list)
        shared["iteration_count"] = shared.get("iteration_count", 0) + 1
        
        # Print test results
        passed_count = len([r for r in exec_res_list if r["passed"]])
        total_count = len(exec_res_list)
        print(f"\n=== Test Results: {passed_count}/{total_count} Passed ===")
        
        failed_tests = [r for r in exec_res_list if not r["passed"]]
        if failed_tests:
            print("Failed tests:")
            for i, result in enumerate(failed_tests, 1):
                test_case = result['test_case']
                print(f"{i}. {test_case['name']}:")
                if result['error']:
                    print(f"   error: {result['error']}")
                else:
                    print(f"   output: {result['actual']}")
                print(f"   expected: {result['expected']}")
        
        if all_passed:
            return "success"
        elif shared["iteration_count"] >= shared.get("max_iterations", 5):
            return "max_iterations"
        else:
            return "failure"

class Revise(Node):
    def prep(self, shared):
        failed_tests = [r for r in shared["test_results"] if not r["passed"]]
        return {
            "problem": shared["problem"],
            "test_cases": shared["test_cases"],
            "function_code": shared["function_code"],
            "failed_tests": failed_tests
        }

    def exec(self, inputs):
        # Format current test cases nicely
        formatted_tests = ""
        for i, test in enumerate(inputs['test_cases'], 1):
            formatted_tests += f"{i}. {test['name']}\n"
            formatted_tests += f"   input: {test['input']}\n"
            formatted_tests += f"   expected: {test['expected']}\n\n"
        
        # Format failed tests nicely
        formatted_failures = ""
        for i, result in enumerate(inputs['failed_tests'], 1):
            test_case = result['test_case']
            formatted_failures += f"{i}. {test_case['name']}:\n"
            if result['error']:
                formatted_failures += f"   error: {result['error']}\n"
            else:
                formatted_failures += f"   output: {result['actual']}\n"
            formatted_failures += f"   expected: {result['expected']}\n\n"

        prompt = f"""Problem: {inputs['problem']}

Current test cases:
{formatted_tests}

Current function:
```python
{inputs['function_code']}
```

Failed tests:
{formatted_failures}

Analyze the failures and output revisions in YAML. You can revise test cases, function code, or both:

```yaml
reasoning: |
    Looking at the failures, I see that...
    The issue appears to be...
    I will revise...
test_cases:  # Dictionary mapping test case index (1-based) to revised test case
  1:
    name: "Revised test name"
    input: {{...}}
    expected: ...
function_code: |  # Include this if revising function
  def run_code(...):
    return ...
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        # Validation asserts
        if "test_cases" in result:
            assert isinstance(result["test_cases"], dict), "test_cases must be a dictionary"
            for index_str, test_case in result["test_cases"].items():
                assert isinstance(index_str, (str, int)), "test_cases keys must be strings or ints"
                assert "name" in test_case, f"Revised test case {index_str} missing 'name' field"
                assert "input" in test_case, f"Revised test case {index_str} missing 'input' field"
                assert "expected" in test_case, f"Revised test case {index_str} missing 'expected' field"
        
        if "function_code" in result:
            assert isinstance(result["function_code"], str), "function_code must be string"
            assert "def run_code" in result["function_code"], "Function must be named 'run_code'"
        
        return result

    def post(self, shared, prep_res, exec_res):
        # Print what is being revised
        print(f"\n=== Revisions (Iteration {shared['iteration_count']}) ===")
        
        # Handle test case revisions - map indices to actual test cases
        if "test_cases" in exec_res:
            current_tests = shared["test_cases"].copy()
            print("Revising test cases:")
            for index_str, revised_test in exec_res["test_cases"].items():
                index = int(index_str) - 1  # Convert to 0-based
                if 0 <= index < len(current_tests):
                    old_test = current_tests[index]
                    print(f"  Test {index_str}: '{old_test['name']}' -> '{revised_test['name']}'")
                    print(f"    old input: {old_test['input']}")
                    print(f"    new input: {revised_test['input']}")
                    print(f"    old expected: {old_test['expected']}")
                    print(f"    new expected: {revised_test['expected']}")
                    current_tests[index] = revised_test
            shared["test_cases"] = current_tests
            
        if "function_code" in exec_res:
            print("Revising function code:")
            print("New function:")
            print(exec_res["function_code"])
            shared["function_code"] = exec_res["function_code"]
</file>

<file path="cookbook/pocketflow-code-generator/README.md">
# PocketFlow Code Generator

An intelligent AI system that takes LeetCode-style coding problems and automatically generates comprehensive test cases, implements solutions, and iteratively improves them until all tests pass.

- Check out the [Substack Post Tutorial](https://pocketflow.substack.com/p/build-your-own-ai-code-generator) for more!

## Features

- **Automatic Test Case Generation**: Creates diverse test cases including edge cases
- **Intelligent Code Implementation**: Generates `run_code` functions with proper algorithms
- **Iterative Improvement**: Analyzes failures and decides whether to revise tests or code
- **Rich Debugging Output**: Detailed progress tracking and validation

## Getting Started

1. Install required dependencies:
```bash
pip install -r requirements.txt
```

2. Set up your Anthropic API key:
    ```bash
    export ANTHROPIC_API_KEY="your-api-key-here"
    ```
    Test your API key is working:
    ```bash
    python utils/call_llm.py
    ```

3. Run the code generator with the default Two Sum problem:
```bash
python main.py
```

4. Or provide your own problem:
```bash
python main.py "Reverse a linked list. Given the head of a singly linked list, reverse the list and return the reversed list."
```

## How It Works

The system follows an intelligent workflow combining **Agent** and **Workflow** design patterns:

```mermaid
flowchart TD
    start[Problem Input] --> generateTests[Generate Test Cases]
    generateTests --> implement[Implement Function]
    implement --> runTests[Run Tests - Batch]
    runTests --> decision{All Tests Pass?}
    decision -->|Yes| success[Success!]
    decision -->|No| revise[Revise - Agent Decision]
    revise --> runTests
    decision -->|Max Iterations| maxIter[Max Iterations Reached]
```

### The Process

1. **GenerateTestCases**: Creates 5-7 comprehensive test cases from problem description
2. **ImplementFunction**: Writes a `run_code` function based on problem and test cases  
3. **RunTests**: Executes function against all test cases using batch processing
4. **Revise**: Analyzes failures and makes intelligent decisions to revise test cases and/or function code
5. **Loop**: Continues until all tests pass or max iterations reached

## Sample Output

Here's what you'll see when running the Two Sum example:

```
Starting PocketFlow Code Generator...

=== Generated 7 Test Cases ===
1. Basic case - solution at beginning
   input: {'nums': [2, 7, 11, 15], 'target': 9}
   expected: [0, 1]
2. Basic case - solution in middle
   input: {'nums': [3, 2, 4], 'target': 6}
   expected: [1, 2]
3. Edge case - minimum array size with duplicates
   input: {'nums': [3, 3], 'target': 6}
   expected: [0, 1]
4. Case with negative numbers
   input: {'nums': [-1, -2, -3, -4, -5], 'target': -8}
   expected: [2, 4]
5. Case with zero and negative target
   input: {'nums': [0, 4, 3, 0], 'target': 0}
   expected: [0, 3]
6. Case with solution at the end
   input: {'nums': [1, 2, 3, 4, 5, 6], 'target': 11}
   expected: [4, 5]
7. Larger array case
   input: {'nums': [5, 75, 25, 45, 42, 2, 11, 9, 55, 12], 'target': 14}
   expected: [2, 6]

=== Implemented Function ===
def run_code(nums, target):
    # Dictionary to store number -> index mapping
    num_to_index = {}
    
    # Iterate through the array
    for i, num in enumerate(nums):
        # Calculate what number we need to reach the target
        complement = target - num
        
        # Check if the complement exists in our map
        if complement in num_to_index:
            # Found the pair! Return indices
            return [num_to_index[complement], i]
        
        # Store current number and its index
        num_to_index[num] = i
    
    # Should never reach here given problem constraints
    return []

=== Test Results: 6/7 Passed ===
Failed tests:
1. Larger array case:
   error: Expected [2, 6], got [0, 7]
   expected: [2, 6]

=== Revisions (Iteration 1) ===
Revising test cases:
  Test 7: 'Larger array case' -> 'Larger array case'
    old input: {'nums': [5, 75, 25, 45, 42, 2, 11, 9, 55, 12], 'target': 14}
    new input: {'nums': [5, 75, 25, 45, 42, 2, 11, 9, 55, 12], 'target': 14}
    old expected: [2, 6]
    new expected: [0, 7]

=== Test Results: 7/7 Passed ===
```

## Key Features

### Intelligent Decision Making
The **Revise** node acts as an agent that analyzes test failures and decides whether to:
- Fix test cases (if they have incorrect expected outputs)  
- Fix the function implementation (if the logic is wrong)
- Or both

### Structured Output with Validation
All LLM interactions use YAML format with:
- **Reasoning fields**: Transparent decision-making process
- **Validation asserts**: Ensures outputs match expected structure
- **Rich debugging**: Comprehensive logging of all steps

### Batch Processing
The **RunTests** node uses PocketFlow's BatchNode to efficiently test the function against all test cases in parallel.

## Files

- [`main.py`](./main.py): Entry point with sample Two Sum problem
- [`flow.py`](./flow.py): Connects all nodes into the complete workflow  
- [`nodes.py`](./nodes.py): Core logic nodes with validation and debugging
- [`utils/call_llm.py`](./utils/call_llm.py): Anthropic Claude API wrapper
- [`utils/code_executor.py`](./utils/code_executor.py): Safe Python code execution utility
- [`doc/design.md`](./doc/design.md): Detailed system design documentation

## Design Patterns Used

- **[Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)**: Sequential steps of test generation â†’ coding â†’ testing
- **[Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)**: Intelligent decision-making when tests fail
- **[Batch](https://the-pocket.github.io/PocketFlow/core_abstraction/batch.html)**: Efficient parallel test execution
- **[Structured Output](https://the-pocket.github.io/PocketFlow/design_pattern/structure.html)**: YAML validation for reliable LLM outputs
</file>

<file path="cookbook/pocketflow-code-generator/requirements.txt">
anthropic
pocketflow
pyyaml
</file>

<file path="cookbook/pocketflow-communication/flow.py">
"""Flow configuration for the communication example."""

from pocketflow import Flow
from nodes import TextInput, WordCounter, ShowStats, EndNode

def create_flow():
    """Create and configure the flow with all nodes."""
    # Create nodes
    text_input = TextInput()
    word_counter = WordCounter()
    show_stats = ShowStats()
    end_node = EndNode()
    
    # Configure transitions
    text_input - "count" >> word_counter
    word_counter - "show" >> show_stats
    show_stats - "continue" >> text_input
    text_input - "exit" >> end_node
    
    # Create and return flow
    return Flow(start=text_input)
</file>

<file path="cookbook/pocketflow-communication/main.py">
from flow import create_flow

def main():
    """Run the communication example."""
    flow = create_flow()
    shared = {}
    flow.run(shared)

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-communication/nodes.py">
"""Node implementations for the communication example."""

from pocketflow import Node

class EndNode(Node):
    """Node that handles flow termination."""
    pass

class TextInput(Node):
    """Node that reads text input and initializes the shared store."""
    
    def prep(self, shared):
        """Get user input and ensure shared store is initialized."""
        return input("Enter text (or 'q' to quit): ")
    
    def post(self, shared, prep_res, exec_res):
        """Store text and initialize/update statistics."""
        if prep_res == 'q':
            return "exit"
        
        # Store the text
        shared["text"] = prep_res
        
        # Initialize statistics if they don't exist
        if "stats" not in shared:
            shared["stats"] = {
                "total_texts": 0,
                "total_words": 0
            }
        shared["stats"]["total_texts"] += 1
        
        return "count"

class WordCounter(Node):
    """Node that counts words in the text."""
    
    def prep(self, shared):
        """Get text from shared store."""
        return shared["text"]
    
    def exec(self, text):
        """Count words in the text."""
        return len(text.split())
    
    def post(self, shared, prep_res, exec_res):
        """Update word count statistics."""
        shared["stats"]["total_words"] += exec_res
        return "show"

class ShowStats(Node):
    """Node that displays statistics from the shared store."""
    
    def prep(self, shared):
        """Get statistics from shared store."""
        return shared["stats"]
    
    def post(self, shared, prep_res, exec_res):
        """Display statistics and continue the flow."""
        stats = prep_res
        print(f"\nStatistics:")
        print(f"- Texts processed: {stats['total_texts']}")
        print(f"- Total words: {stats['total_words']}")
        print(f"- Average words per text: {stats['total_words'] / stats['total_texts']:.1f}\n")
        return "continue"
</file>

<file path="cookbook/pocketflow-communication/README.md">
# PocketFlow Communication Example

This example demonstrates the [Communication](https://the-pocket.github.io/PocketFlow/communication.html) concept in PocketFlow, specifically focusing on the Shared Store pattern.

## Overview

The example implements a simple word counter that shows how nodes can communicate using a shared store. It demonstrates:

- How to initialize and structure a shared store
- How nodes can read from and write to the shared store
- How to maintain state across multiple node executions
- Best practices for shared store usage

## Project Structure

```
pocketflow-communication/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â”œâ”€â”€ flow.py
â””â”€â”€ nodes.py
```

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```

Enter text when prompted. The program will:
1. Count words in the text
2. Store statistics in the shared store
3. Display running statistics (total texts, total words, average)

Enter 'q' to quit.

## How it Works

The example uses three nodes:

1. `TextInput`: Reads user input and initializes the shared store
2. `WordCounter`: Counts words and updates statistics in the shared store
3. `ShowStats`: Displays statistics from the shared store

This demonstrates how nodes can share and maintain state using the shared store pattern.
</file>

<file path="cookbook/pocketflow-communication/requirements.txt">
pocketflow==0.1.0
</file>

<file path="cookbook/pocketflow-fastapi-background/docs/design.md">
# Design Doc: PocketFlow FastAPI Background Job with SSE Progress

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

**User Story**: As a user, I want to submit an article topic via a web API and receive real-time progress updates while the article is being generated in the background, so I can see the workflow progress without blocking the UI.

**Core Requirements**:
1. Submit article topic via REST API endpoint
2. Start background job for article generation workflow
3. Receive real-time progress updates via Server-Sent Events (SSE)
4. Get final article result when workflow completes
5. Handle multiple concurrent requests

**Technical Requirements**:
- FastAPI web server with REST endpoints
- Background task processing using asyncio
- Server-Sent Events for progress streaming
- Simple web interface to test the functionality

## Flow Design

> Notes for AI:
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

**Workflow Pattern**: Sequential processing of article generation steps with progress reporting at each stage.

### Flow High-level Design:

1. **Generate Outline Node**: Creates a structured outline for the article topic
2. **Write Content Node**: Writes content for each section in the outline  
3. **Apply Style Node**: Applies conversational styling to the final article

Each node puts progress updates into an asyncio.Queue for SSE streaming.

```mermaid
flowchart LR
    outline[Generate Outline] --> content[Write Content]
    content --> styling[Apply Style]
```

## Utility Functions

> Notes for AI:
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1. **Call LLM** (`utils/call_llm.py`)
   - *Input*: prompt (str)
   - *Output*: response (str)
   - Used by all workflow nodes for LLM tasks

## Node Design

### Shared Store

> Notes for AI: Try to minimize data redundancy

The shared store structure is organized as follows:

```python
shared = {
    "topic": "user-provided-topic",
    "sse_queue": asyncio.Queue(),  # For sending SSE updates
    "sections": ["section1", "section2", "section3"],
    "draft": "combined-section-content",
    "final_article": "styled-final-article"
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1. **Generate Outline Node**
   - *Purpose*: Create a structured outline with 3 main sections using YAML output
   - *Type*: Regular Node (synchronous LLM call)
   - *Steps*:
     - *prep*: Read "topic" from shared store
     - *exec*: Call LLM to generate YAML outline, parse and validate structure
     - *post*: Write "sections" to shared store, put progress update in sse_queue

2. **Write Content Node**
   - *Purpose*: Generate concise content for each outline section
   - *Type*: BatchNode (processes each section independently)
   - *Steps*:
     - *prep*: Read "sections" from shared store (returns list of sections)
     - *exec*: For one section, call LLM to write 100-word content
     - *post*: Combine all section content into "draft", put progress update in sse_queue

3. **Apply Style Node**
   - *Purpose*: Apply conversational, engaging style to the combined content
   - *Type*: Regular Node (single LLM call for styling)
   - *Steps*:
     - *prep*: Read "draft" from shared store
     - *exec*: Call LLM to rewrite in conversational style
     - *post*: Write "final_article" to shared store, put completion update in sse_queue
</file>

<file path="cookbook/pocketflow-fastapi-background/static/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Article Generator</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .container {
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            max-width: 500px;
            width: 100%;
            text-align: center;
        }

        .logo {
            font-size: 2.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #6b7280;
            font-size: 1.1rem;
            margin-bottom: 40px;
            font-weight: 400;
        }

        .form-group {
            margin-bottom: 30px;
            text-align: left;
        }

        label {
            display: block;
            font-weight: 600;
            color: #374151;
            margin-bottom: 8px;
            font-size: 0.95rem;
        }

        input[type="text"] {
            width: 100%;
            padding: 16px 20px;
            border: 2px solid #e5e7eb;
            border-radius: 12px;
            font-size: 1rem;
            transition: all 0.3s ease;
            background: #f9fafb;
        }

        input[type="text"]:focus {
            outline: none;
            border-color: #667eea;
            background: white;
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
        }

        .submit-btn {
            width: 100%;
            padding: 16px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            border-radius: 12px;
            font-size: 1.1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            margin-top: 10px;
        }

        .submit-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
        }

        .submit-btn:active {
            transform: translateY(0);
        }

        .example-topics {
            margin-top: 30px;
            padding-top: 30px;
            border-top: 1px solid #e5e7eb;
        }

        .example-topics h3 {
            color: #6b7280;
            font-size: 0.9rem;
            font-weight: 600;
            margin-bottom: 15px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .topic-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            justify-content: center;
        }

        .topic-tag {
            background: #f3f4f6;
            color: #6b7280;
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.2s ease;
            border: 1px solid transparent;
        }

        .topic-tag:hover {
            background: #e5e7eb;
            color: #374151;
        }

        @media (max-width: 480px) {
            .container {
                padding: 30px 20px;
            }
            
            .logo {
                font-size: 2rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="logo">âœ¨ Article AI</div>
        <p class="subtitle">Generate engaging articles with AI assistance</p>
        
        <form id="articleForm" action="/start-job" method="post">
            <div class="form-group">
                <label for="topic">What would you like to write about?</label>
                <input type="text" id="topic" name="topic" placeholder="Enter your topic here..." required>
            </div>
            
            <button type="submit" class="submit-btn">Generate Article</button>
        </form>

        <div class="example-topics">
            <h3>Popular Topics</h3>
            <div class="topic-tags">
                <span class="topic-tag" onclick="setTopic('AI Safety')">AI Safety</span>
                <span class="topic-tag" onclick="setTopic('Climate Change')">Climate Change</span>
                <span class="topic-tag" onclick="setTopic('Space Exploration')">Space Exploration</span>
                <span class="topic-tag" onclick="setTopic('Renewable Energy')">Renewable Energy</span>
                <span class="topic-tag" onclick="setTopic('Mental Health')">Mental Health</span>
                <span class="topic-tag" onclick="setTopic('Future of Work')">Future of Work</span>
            </div>
        </div>
    </div>

    <script>
        function setTopic(topic) {
            document.getElementById('topic').value = topic;
        }

        document.getElementById('articleForm').addEventListener('submit', async function(e) {
            e.preventDefault();
            
            const submitBtn = document.querySelector('.submit-btn');
            const originalText = submitBtn.textContent;
            
            // Show loading state
            submitBtn.textContent = 'Starting...';
            submitBtn.disabled = true;
            
            try {
                const formData = new FormData(this);
                const response = await fetch('/start-job', {
                    method: 'POST',
                    body: formData
                });
                
                const result = await response.json();
                
                if (response.ok) {
                    // Redirect to progress page
                    window.location.href = `/progress.html?job_id=${result.job_id}&topic=${encodeURIComponent(result.topic)}`;
                } else {
                    throw new Error('Failed to start job');
                }
            } catch (error) {
                alert('Error starting job: ' + error.message);
                submitBtn.textContent = originalText;
                submitBtn.disabled = false;
            }
        });
    </script>
</body>
</html>
</file>

<file path="cookbook/pocketflow-fastapi-background/static/progress.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generating Article...</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .container {
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            max-width: 600px;
            width: 100%;
            text-align: center;
        }

        .logo {
            font-size: 2rem;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 10px;
        }

        .topic-title {
            color: #374151;
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 30px;
        }

        .progress-container {
            margin: 30px 0;
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background: #f3f4f6;
            border-radius: 10px;
            overflow: hidden;
            margin-bottom: 20px;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea, #764ba2);
            border-radius: 10px;
            width: 0%;
            transition: width 0.5s ease;
            position: relative;
        }

        .progress-fill::after {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3), transparent);
            animation: shimmer 2s infinite;
        }

        @keyframes shimmer {
            0% { transform: translateX(-100%); }
            100% { transform: translateX(100%); }
        }

        .progress-text {
            color: #6b7280;
            font-size: 1rem;
            font-weight: 500;
            margin-bottom: 10px;
        }

        .progress-percentage {
            color: #374151;
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 20px;
        }

        .status-card {
            background: #f9fafb;
            border: 1px solid #e5e7eb;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            text-align: left;
        }

        .status-title {
            color: #374151;
            font-weight: 600;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .status-content {
            color: #6b7280;
            line-height: 1.5;
        }

        .spinner {
            display: inline-block;
            width: 16px;
            height: 16px;
            border: 2px solid #e5e7eb;
            border-top: 2px solid #667eea;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .step-indicator {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            position: relative;
        }

        .step-indicator::before {
            content: '';
            position: absolute;
            top: 15px;
            left: 15px;
            right: 15px;
            height: 2px;
            background: #e5e7eb;
            z-index: 1;
        }

        .step {
            display: flex;
            flex-direction: column;
            align-items: center;
            position: relative;
            z-index: 2;
        }

        .step-circle {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #e5e7eb;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            font-size: 0.8rem;
            margin-bottom: 8px;
            transition: all 0.3s ease;
        }

        .step-circle.active {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
        }

        .step-circle.completed {
            background: #10b981;
            color: white;
        }

        .step-label {
            font-size: 0.8rem;
            color: #6b7280;
            text-align: center;
            max-width: 80px;
        }

        .result-section {
            display: none;
            text-align: left;
            margin-top: 30px;
        }

        .result-section.show {
            display: block;
        }

        .article-content {
            background: #f9fafb;
            border: 1px solid #e5e7eb;
            border-radius: 12px;
            padding: 25px;
            line-height: 1.6;
            color: #374151;
            white-space: pre-wrap;
            max-height: 400px;
            overflow-y: auto;
        }

        .action-buttons {
            display: flex;
            gap: 15px;
            margin-top: 20px;
            justify-content: center;
        }

        .btn {
            padding: 12px 24px;
            border-radius: 10px;
            font-weight: 600;
            text-decoration: none;
            transition: all 0.3s ease;
            cursor: pointer;
            border: none;
            font-size: 0.95rem;
        }

        .btn-primary {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.3);
        }

        .btn-secondary {
            background: #f3f4f6;
            color: #374151;
            border: 1px solid #e5e7eb;
        }

        .btn-secondary:hover {
            background: #e5e7eb;
        }

        .error-message {
            background: #fef2f2;
            border: 1px solid #fecaca;
            color: #dc2626;
            padding: 15px;
            border-radius: 10px;
            margin: 20px 0;
        }

        @media (max-width: 480px) {
            .container {
                padding: 30px 20px;
            }
            
            .step-indicator {
                margin: 20px 0;
            }
            
            .step-label {
                font-size: 0.7rem;
                max-width: 60px;
            }
            
            .action-buttons {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="logo">âœ¨ Article AI</div>
        <div class="topic-title" id="topicTitle">Generating your article...</div>
        
        <div class="progress-container">
            <div class="progress-percentage" id="progressPercentage">0%</div>
            <div class="progress-bar">
                <div class="progress-fill" id="progressFill"></div>
            </div>
            <div class="progress-text" id="progressText">Initializing...</div>
        </div>

        <div class="step-indicator">
            <div class="step">
                <div class="step-circle" id="step1">1</div>
                <div class="step-label">Outline</div>
            </div>
            <div class="step">
                <div class="step-circle" id="step2">2</div>
                <div class="step-label">Content</div>
            </div>
            <div class="step">
                <div class="step-circle" id="step3">3</div>
                <div class="step-label">Style</div>
            </div>
        </div>

        <div class="status-card" id="statusCard">
            <div class="status-title" id="statusTitle">
                <span class="spinner"></span>
                Getting started...
            </div>
            <div class="status-content" id="statusContent">
                Preparing to generate your article. This may take a few moments.
            </div>
        </div>

        <div class="result-section" id="resultSection">
            <h3 style="margin-bottom: 15px; color: #374151;">Your Article is Ready! ðŸŽ‰</h3>
            <div class="article-content" id="articleContent"></div>
            <div class="action-buttons">
                <button class="btn btn-primary" onclick="copyToClipboard()">Copy Article</button>
                <a href="/" class="btn btn-secondary">Generate Another</a>
            </div>
        </div>

        <div class="error-message" id="errorMessage" style="display: none;"></div>
    </div>

    <script>
        const urlParams = new URLSearchParams(window.location.search);
        const jobId = urlParams.get('job_id');
        const topic = urlParams.get('topic');

        if (topic) {
            document.getElementById('topicTitle').textContent = `"${topic}"`;
        }

        if (!jobId) {
            showError('No job ID provided');
        } else {
            connectToProgress();
        }

        function connectToProgress() {
            const eventSource = new EventSource(`/progress/${jobId}`);
            
            eventSource.onmessage = function(event) {
                try {
                    const data = JSON.parse(event.data);
                    handleProgressUpdate(data);
                } catch (error) {
                    console.error('Error parsing SSE data:', error);
                }
            };
            
            eventSource.onerror = function(error) {
                console.error('SSE connection error:', error);
                showError('Connection lost. Please refresh the page.');
                eventSource.close();
            };
        }

        function handleProgressUpdate(data) {
            if (data.error) {
                showError(data.error);
                return;
            }

            if (data.heartbeat) {
                return; // Ignore heartbeat messages
            }

            const progress = data.progress || 0;
            updateProgress(progress);

            switch (data.step) {
                case 'connected':
                    updateStatus('ðŸ”— Connected', 'Successfully connected to the article generation process.');
                    break;
                    
                case 'outline':
                    updateStepIndicator(1);
                    if (data.data && data.data.sections) {
                        updateStatus('ðŸ“ Creating Outline', `Generated outline with ${data.data.sections.length} sections`);
                    } else {
                        updateStatus('ðŸ“ Creating Outline', 'Generating article structure and main points...');
                    }
                    break;
                    
                case 'content':
                    updateStepIndicator(2);
                    if (data.data && data.data.section) {
                        updateStatus('âœï¸ Writing Content', 
                            `Writing section: "${data.data.section}" (${data.data.completed_sections}/${data.data.total_sections})`);
                    } else {
                        updateStatus('âœï¸ Writing Content', 'Creating detailed content for each section...');
                    }
                    break;
                    
                case 'style':
                    updateStepIndicator(3);
                    updateStatus('ðŸŽ¨ Applying Style', 'Polishing the article with engaging, conversational tone...');
                    break;
                    
                case 'complete':
                    updateStepIndicator(3, true);
                    updateProgress(100);
                    updateStatus('âœ… Complete!', 'Your article has been generated successfully.');
                    if (data.data && data.data.final_article) {
                        showResult(data.data.final_article);
                    }
                    break;
            }
        }

        function updateProgress(percentage) {
            document.getElementById('progressPercentage').textContent = `${percentage}%`;
            document.getElementById('progressFill').style.width = `${percentage}%`;
        }

        function updateStatus(title, content) {
            document.getElementById('statusTitle').innerHTML = `<span class="spinner"></span> ${title}`;
            document.getElementById('statusContent').textContent = content;
        }

        function updateStepIndicator(step, completed = false) {
            // Reset all steps
            for (let i = 1; i <= 3; i++) {
                const stepElement = document.getElementById(`step${i}`);
                stepElement.className = 'step-circle';
                if (i < step) {
                    stepElement.classList.add('completed');
                    stepElement.innerHTML = 'âœ“';
                } else if (i === step) {
                    stepElement.classList.add(completed ? 'completed' : 'active');
                    stepElement.innerHTML = completed ? 'âœ“' : i;
                } else {
                    stepElement.innerHTML = i;
                }
            }
        }

        function showResult(article) {
            document.getElementById('statusCard').style.display = 'none';
            document.getElementById('articleContent').textContent = article;
            document.getElementById('resultSection').classList.add('show');
        }

        function showError(message) {
            document.getElementById('errorMessage').textContent = message;
            document.getElementById('errorMessage').style.display = 'block';
            document.getElementById('statusCard').style.display = 'none';
        }

        function copyToClipboard() {
            const article = document.getElementById('articleContent').textContent;
            navigator.clipboard.writeText(article).then(() => {
                const btn = event.target;
                const originalText = btn.textContent;
                btn.textContent = 'Copied!';
                btn.style.background = '#10b981';
                setTimeout(() => {
                    btn.textContent = originalText;
                    btn.style.background = '';
                }, 2000);
            }).catch(err => {
                console.error('Failed to copy: ', err);
                alert('Failed to copy to clipboard');
            });
        }
    </script>
</body>
</html>
</file>

<file path="cookbook/pocketflow-fastapi-background/utils/call_llm.py">
import os
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

if __name__ == "__main__":
    print(call_llm("Tell me a short joke"))
</file>

<file path="cookbook/pocketflow-fastapi-background/flow.py">
from pocketflow import Flow
from nodes import GenerateOutline, WriteContent, ApplyStyle

def create_article_flow():
    """
    Create and configure the article writing workflow
    """
    # Create node instances
    outline_node = GenerateOutline()
    content_node = WriteContent()
    style_node = ApplyStyle()
    
    # Connect nodes in sequence
    outline_node >> content_node >> style_node
    
    # Create flow starting with outline node
    article_flow = Flow(start=outline_node)
    
    return article_flow
</file>

<file path="cookbook/pocketflow-fastapi-background/main.py">
import asyncio
import json
import uuid
from fastapi import FastAPI, BackgroundTasks, Form
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from flow import create_article_flow

app = FastAPI()

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Store active jobs and their SSE queues
active_jobs = {}

def run_article_workflow(job_id: str, topic: str):
    """Run the article workflow in background"""
    try:
        # Get the pre-created queue from active_jobs
        sse_queue = active_jobs[job_id]
        shared = {
            "topic": topic,
            "sse_queue": sse_queue,
            "sections": [],
            "draft": "",
            "final_article": ""
        }
        
        # Run the workflow
        flow = create_article_flow()
        flow.run(shared)
        
    except Exception as e:
        # Send error message
        error_msg = {"step": "error", "progress": 0, "data": {"error": str(e)}}
        if job_id in active_jobs:
            active_jobs[job_id].put_nowait(error_msg)

@app.post("/start-job")
async def start_job(background_tasks: BackgroundTasks, topic: str = Form(...)):
    """Start a new article generation job"""
    job_id = str(uuid.uuid4())
    
    # Create SSE queue and register job immediately
    sse_queue = asyncio.Queue()
    active_jobs[job_id] = sse_queue
    
    # Start background task
    background_tasks.add_task(run_article_workflow, job_id, topic)
    
    return {"job_id": job_id, "topic": topic, "status": "started"}

@app.get("/progress/{job_id}")
async def get_progress(job_id: str):
    """Stream progress updates via SSE"""
    
    async def event_stream():
        if job_id not in active_jobs:
            yield f"data: {json.dumps({'error': 'Job not found'})}\n\n"
            return
            
        sse_queue = active_jobs[job_id]
        
        # Send initial connection confirmation
        yield f"data: {json.dumps({'step': 'connected', 'progress': 0, 'data': {'message': 'Connected to job progress'}})}\n\n"
        
        try:
            while True:
                # Wait for next progress update
                try:
                    # Use asyncio.wait_for to avoid blocking forever
                    progress_msg = await asyncio.wait_for(sse_queue.get(), timeout=1.0)
                    yield f"data: {json.dumps(progress_msg)}\n\n"
                    
                    # If job is complete, clean up and exit
                    if progress_msg.get("step") == "complete":
                        del active_jobs[job_id]
                        break
                        
                except asyncio.TimeoutError:
                    # Send heartbeat to keep connection alive
                    yield f"data: {json.dumps({'heartbeat': True})}\n\n"
                    
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream"
        }
    )

@app.get("/")
async def get_index():
    """Serve the main page"""
    return FileResponse("static/index.html")

@app.get("/progress.html")
async def get_progress_page():
    """Serve the progress page"""
    return FileResponse("static/progress.html")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="cookbook/pocketflow-fastapi-background/nodes.py">
import yaml
from pocketflow import Node, BatchNode
from utils.call_llm import call_llm

class GenerateOutline(Node):
    def prep(self, shared):
        return shared["topic"]
    
    def exec(self, topic):
        prompt = f"""
Create a simple outline for an article about {topic}.
Include at most 3 main sections (no subsections).

Output the sections in YAML format as shown below:

```yaml
sections:
    - First section title
    - Second section title  
    - Third section title
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        structured_result = yaml.safe_load(yaml_str)
        return structured_result
    
    def post(self, shared, prep_res, exec_res):
        sections = exec_res["sections"]
        shared["sections"] = sections
        
        # Send progress update via SSE queue
        progress_msg = {"step": "outline", "progress": 33, "data": {"sections": sections}}
        shared["sse_queue"].put_nowait(progress_msg)
        
        return "default"

class WriteContent(BatchNode):
    def prep(self, shared):
        # Store sections and sse_queue for use in exec
        self.sections = shared.get("sections", [])
        self.sse_queue = shared["sse_queue"]
        return self.sections
    
    def exec(self, section):
        prompt = f"""
Write a short paragraph (MAXIMUM 100 WORDS) about this section:

{section}

Requirements:
- Explain the idea in simple, easy-to-understand terms
- Use everyday language, avoiding jargon
- Keep it very concise (no more than 100 words)
- Include one brief example or analogy
"""
        content = call_llm(prompt)
        
        # Send progress update for this section
        current_section_index = self.sections.index(section) if section in self.sections else 0
        total_sections = len(self.sections)
        
        # Progress from 33% (after outline) to 66% (before styling)
        # Each section contributes (66-33)/total_sections = 33/total_sections percent
        section_progress = 33 + ((current_section_index + 1) * 33 // total_sections)
        
        progress_msg = {
            "step": "content", 
            "progress": section_progress, 
            "data": {
                "section": section,
                "completed_sections": current_section_index + 1,
                "total_sections": total_sections
            }
        }
        self.sse_queue.put_nowait(progress_msg)
        
        return f"## {section}\n\n{content}\n"
    
    def post(self, shared, prep_res, exec_res_list):
        draft = "\n".join(exec_res_list)
        shared["draft"] = draft
        return "default"

class ApplyStyle(Node):
    def prep(self, shared):
        return shared["draft"]
    
    def exec(self, draft):
        prompt = f"""
Rewrite the following draft in a conversational, engaging style:

{draft}

Make it:
- Conversational and warm in tone
- Include rhetorical questions that engage the reader
- Add analogies and metaphors where appropriate
- Include a strong opening and conclusion
"""
        return call_llm(prompt)
    
    def post(self, shared, prep_res, exec_res):
        shared["final_article"] = exec_res
        
        # Send completion update via SSE queue
        progress_msg = {"step": "complete", "progress": 100, "data": {"final_article": exec_res}}
        shared["sse_queue"].put_nowait(progress_msg)
        
        return "default"
</file>

<file path="cookbook/pocketflow-fastapi-background/README.md">
# PocketFlow FastAPI Background Jobs with Real-time Progress

A web application demonstrating PocketFlow workflows running as FastAPI background jobs with real-time progress updates via Server-Sent Events (SSE).

<p align="center">
  <img 
    src="./assets/banner.png" width="800"
  />
</p>

## Features

- **Modern Web UI**: Clean interface with real-time progress visualization
- **Background Processing**: Non-blocking article generation using FastAPI BackgroundTasks
- **Server-Sent Events**: Real-time progress streaming without polling
- **Granular Progress**: Section-by-section updates during content generation
- **PocketFlow Integration**: Three-node workflow (Outline â†’ Content â†’ Style)

## How to Run

1. Install Dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Set your OpenAI API key:
   ```bash
   export OPENAI_API_KEY=your_api_key_here
   ```

3. Run the FastAPI Server:
   ```bash
   python main.py
   ```

4. Access the Web UI:
   Open your browser and navigate to `http://localhost:8000`.

5. Use the Application:
   - Enter an article topic or click suggested topics
   - Click "Generate Article" to start background processing
   - Watch real-time progress updates with step indicators
   - Copy the final article when complete

## How It Works

The application uses PocketFlow to define a three-step article generation workflow. FastAPI handles web requests and manages real-time SSE communication for progress updates.

**PocketFlow Workflow:**

```mermaid
flowchart LR
    A[Generate Outline] --> B[Write Content]
    B --> C[Apply Style]
```

1. **`GenerateOutline`**: Creates structured outline with up to 3 sections
2. **`WriteContent` (BatchNode)**: Writes content for each section individually, sending progress updates
3. **`ApplyStyle`**: Polishes the article with conversational tone

**FastAPI & SSE Integration:**

- The `/start-job` endpoint creates a unique job, initializes an SSE queue, and schedules the workflow using `BackgroundTasks`
- Nodes send progress updates to the job-specific `sse_queue` during execution
- The `/progress/{job_id}` endpoint streams real-time updates to the client via Server-Sent Events
- The web UI displays progress with animated bars, step indicators, and detailed status messages

**Progress Updates:**
- 33%: Outline generation complete
- 33-66%: Content writing (individual section updates)
- 66-100%: Style application
- 100%: Article ready

## Files

- [`main.py`](./main.py): FastAPI application with background jobs and SSE endpoints
- [`flow.py`](./flow.py): PocketFlow workflow definition connecting the three nodes
- [`nodes.py`](./nodes.py): Workflow nodes (GenerateOutline, WriteContent BatchNode, ApplyStyle)
- [`utils/call_llm.py`](./utils/call_llm.py): OpenAI LLM utility function
- [`static/index.html`](./static/index.html): Modern job submission form with topic suggestions
- [`static/progress.html`](./static/progress.html): Real-time progress monitoring with animations
</file>

<file path="cookbook/pocketflow-fastapi-background/requirements.txt">
fastapi
uvicorn
openai
pyyaml
python-multipart
</file>

<file path="cookbook/pocketflow-fastapi-hitl/docs/design.md">
#  Human-in-the-Loop Web Service

## 1. Requirements

*   **Goal:** Create a web service for task submission, processing, human review (Approve/Reject loop via UI), and finalization.
*   **Interface:** Simple web UI (HTML/JS) for input, status display, and feedback buttons.
*   **Backend:** FastAPI using PocketFlow for workflow management.
*   **Real-time Updates:** Use Server-Sent Events (SSE) to push status changes (pending, running, waiting_for_review, completed, failed) and intermediate results to the client without page reloads.
*   **State:** Use in-memory storage for task state (Warning: Not suitable for production).

## 2. Flow Design

*   **Core Pattern:** Workflow with a conditional loop based on human feedback. SSE for asynchronous status communication.
*   **Nodes:**
    1.  `ProcessNode` (Regular): Takes input, executes the (simulated) task processing.
    2.  `ReviewNode` (Async): Waits for human feedback signaled via an `asyncio.Event`. Pushes "waiting\_for\_review" status to the SSE queue.
    3.  `ResultNode` (Regular): Marks the task as complete and logs the final result.
*   **Shared Store (`shared` dict per task):**
    *   `task_input`: Initial data from user.
    *   `processed_output`: Result from `ProcessNode`.
    *   `feedback`: 'approved' or 'rejected' set by the `/feedback` endpoint.
    *   `review_event`: `asyncio.Event` used by `ReviewNode` to wait and `/feedback` to signal.
    *   `final_result`: The approved output.
    *   `current_attempt`: Tracks reprocessing count.
    *   `task_id`: Unique identifier for the task.
*   **SSE Communication:** An `asyncio.Queue` (stored alongside the `shared` store in the server's global `tasks` dict, *not directly in PocketFlow's shared store*) is used per task. Nodes (or wrapper code) put status updates onto this queue. The `/stream` endpoint reads from the queue and sends SSE messages.
*   **Mermaid Diagram:**

```mermaid
flowchart TD
    Process[Process Task] -- "default" --> Review{Wait for Feedback}
    Review -- "approved" --> Result[Final Result]
    Review -- "rejected" --> Process
```

## 3. Utilities

For this specific example, the core "utility" is the processing logic itself. Let's simulate it with a simple function. The FastAPI server acts as the external interface.

* `process_task(input_data)`: A placeholder function. In a real scenario, this might call an LLM (`utils/call_llm.py`).

## 4. Node Design (Detailed)

*   **`ProcessNode` (Node):**
    *   `prep`: Reads `task_input`, `current_attempt` from `shared`.
    *   `exec`: Calls `utils.process_task.process_task`.
    *   `post`: Writes `processed_output` to `shared`, increments `current_attempt`. Returns "default".
*   **`ReviewNode` (AsyncNode):**
    *   `prep_async`: (As modified/wrapped by server.py) Reads `review_event`, `processed_output` from `shared`. **Puts "waiting\_for\_review" status onto the task's SSE queue.**
    *   `exec_async`: `await shared["review_event"].wait()`.
    *   `post_async`: Reads `feedback` from `shared`. Clears the event. Returns "approved" or "rejected". If approved, stores `processed_output` into `final_result`.
*   **`ResultNode` (Node):**
    *   `prep`: Reads `final_result` from `shared`.
    *   `exec`: Prints/logs the final result.
    *   `post`: Returns `None` (ends flow).
</file>

<file path="cookbook/pocketflow-fastapi-hitl/static/style.css">
body {
    font-family: sans-serif;
    margin: 0; /* Remove default body margin */
    padding: 20px; /* Add some padding around the content */
    background-color: #f8f9fa; /* Lighter grey background */
    display: flex; /* Enable Flexbox */
    flex-direction: column; /* Stack children vertically */
    align-items: center; /* Center children horizontally */
    min-height: 100vh; /* Ensure body takes at least full viewport height */
    box-sizing: border-box; /* Include padding in height calculation */
}

h1 {
    text-align: center; /* Center the main title */
    color: #343a40;
    margin-bottom: 25px;
}

/* Style the main containers */
.container, .status-container {
    background: #ffffff;
    padding: 20px 25px; /* More padding */
    border: 1px solid #dee2e6; /* Softer border */
    margin-bottom: 20px;
    border-radius: 6px; /* Slightly rounder corners */
    width: 90%; /* Responsive width */
    max-width: 650px; /* Max width for readability */
    box-shadow: 0 2px 5px rgba(0,0,0,0.05); /* Subtle shadow */
    box-sizing: border-box; /* Include padding/border in width */
}

textarea {
    width: 100%; /* Take full width of parent container */
    padding: 10px;
    margin-bottom: 10px;
    border: 1px solid #ced4da;
    border-radius: 4px;
    font-size: 1em;
    min-height: 60px;
    box-sizing: border-box;
}

button {
    padding: 9px 15px; /* Slightly adjusted padding */
    margin-right: 8px;
    cursor: pointer;
    border: none; /* Remove default border */
    border-radius: 4px;
    font-weight: 500;
    transition: background-color 0.2s ease;
}

button:disabled {
    cursor: not-allowed;
    opacity: 0.6;
}

/* Specific button styling */
#submit-button {
    background-color: #0d6efd; /* Bootstrap primary blue */
    color: white;
}
#submit-button:hover:not(:disabled) {
    background-color: #0b5ed7;
}

.approve {
    background-color: #198754; /* Bootstrap success green */
    color: white;
}
.approve:hover:not(:disabled) {
    background-color: #157347;
}

.reject {
    background-color: #dc3545; /* Bootstrap danger red */
    color: white;
}
.reject:hover:not(:disabled) {
    background-color: #bb2d3b;
}


#task-id-display {
    font-size: 0.9em;
    color: #6c757d; /* Bootstrap secondary text color */
    margin-bottom: 8px;
    word-wrap: break-word;
}

#status-display {
    font-weight: bold;
    margin-bottom: 15px;
    padding: 10px;
    background-color: #e9ecef; /* Light grey background */
    border: 1px solid #dee2e6;
    border-radius: 4px;
    color: #495057;
}

.hidden {
    display: none;
}

/* Review/Result Box Styling */
.review-box, .result-box {
    border: 1px solid #dee2e6;
    padding: 15px;
    margin-top: 15px;
    border-radius: 4px;
    background-color: #f8f9fa; /* Very light background */
}

h2, h3 {
    margin-top: 0; /* Remove default top margin */
    margin-bottom: 15px;
    color: #495057;
}

h3 {
     border-bottom: 1px solid #eee;
     padding-bottom: 8px;
}

pre {
    background-color: #e9ecef;
    padding: 12px;
    border: 1px solid #ced4da;
    border-radius: 4px;
    white-space: pre-wrap;
    word-wrap: break-word;
    max-height: 250px; /* Adjusted height */
    overflow-y: auto;
    font-family: monospace;
    font-size: 0.95em;
    color: #212529;
}
</file>

<file path="cookbook/pocketflow-fastapi-hitl/templates/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pocket Flow Web Feedback</title>
    <link rel="stylesheet" href="{{ url_for('static', path='style.css') }}">
</head>
<body>
    <h1>Pocket Flow Web Feedback</h1>

    <div class="container">
        <textarea id="task-input" rows="3" placeholder="Enter text to process..."></textarea>
        <button id="submit-button">Submit</button>
    </div>

    <div class="status-container">
        <h2>Status</h2>
        <div id="task-id-display">Task ID: N/A</div>
        <div id="status-display">Submit a task.</div>

        <div id="review-section" class="hidden review-box">
            <h3>Review Output</h3>
            <pre id="review-output"></pre>
            <button id="approve-button" class="feedback-button approve">Approve</button>
            <button id="reject-button" class="feedback-button reject">Reject</button>
        </div>

        <div id="result-section" class="hidden result-box">
            <h3>Final Result</h3>
            <pre id="final-result"></pre>
        </div>
    </div>

    <script>
        const taskInput = document.getElementById('task-input');
        const submitButton = document.getElementById('submit-button');
        const taskIdDisplay = document.getElementById('task-id-display');
        const statusDisplay = document.getElementById('status-display');
        const reviewSection = document.getElementById('review-section');
        const reviewOutput = document.getElementById('review-output');
        const approveButton = document.getElementById('approve-button');
        const rejectButton = document.getElementById('reject-button');
        const resultSection = document.getElementById('result-section');
        const finalResult = document.getElementById('final-result');

        let currentTaskId = null;
        let eventSource = null;

        submitButton.addEventListener('click', handleSubmit);
        approveButton.addEventListener('click', () => handleFeedback('approved'));
        rejectButton.addEventListener('click', () => handleFeedback('rejected'));

        async function handleSubmit() {
            const data = taskInput.value.trim();
            if (!data) return alert('Input is empty.');

            resetUI();
            statusDisplay.textContent = 'Submitting...';
            submitButton.disabled = true;

            try {
                const response = await fetch('/submit', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ data: data })
                });
                if (!response.ok) throw new Error(`Submit failed: ${response.status}`);
                const result = await response.json();
                currentTaskId = result.task_id;
                taskIdDisplay.textContent = `Task ID: ${currentTaskId}`;
                startSSEListener(currentTaskId);
            } catch (error) {
                console.error('Submit error:', error);
                statusDisplay.textContent = `Submit Error: ${error.message}`;
                resetUI();
            } finally {
                submitButton.disabled = false;
            }
        }

        function startSSEListener(taskId) {
            closeSSEListener(); // Close existing connection
            eventSource = new EventSource(`/stream/${taskId}`);
            eventSource.onmessage = handleSSEMessage;
            eventSource.onerror = handleSSEError;
            eventSource.onopen = () => console.log(`SSE connected for ${taskId}`);
        }

        function handleSSEMessage(event) {
            console.log("SSE data:", event.data);
            try {
                const data = JSON.parse(event.data);
                updateUI(data);
            } catch (e) { console.error("SSE parse error:", e); }
        }

        function handleSSEError(error) {
            console.error("SSE Error:", error);
            statusDisplay.textContent = "Status stream error. Connection closed.";
            closeSSEListener();
        }

        function closeSSEListener() {
            if (eventSource) {
                eventSource.close();
                eventSource = null;
                console.log("SSE connection closed.");
            }
        }

        function updateUI(data) {
             // Always update main status
            statusDisplay.textContent = `Status: ${data.status || 'Unknown'}`;

            // Hide sections, then show relevant one
            reviewSection.classList.add('hidden');
            resultSection.classList.add('hidden');
            approveButton.disabled = false; // Re-enable by default
            rejectButton.disabled = false;

            switch(data.status) {
                case 'waiting_for_review':
                    reviewOutput.textContent = data.output_to_review || '';
                    reviewSection.classList.remove('hidden');
                    break;
                case 'processing_feedback':
                    approveButton.disabled = true; // Disable while processing
                    rejectButton.disabled = true;
                    break;
                case 'completed':
                    finalResult.textContent = data.final_result || '';
                    resultSection.classList.remove('hidden');
                    closeSSEListener();
                    break;
                case 'failed':
                case 'feedback_error':
                     statusDisplay.textContent = `Status: ${data.status} - ${data.error || 'Unknown error'}`;
                     closeSSEListener();
                     break;
                case 'finished_incomplete':
                     statusDisplay.textContent = `Status: Flow finished unexpectedly.`;
                     closeSSEListener();
                     break;
                case 'stream_closed':
                    // Server closed the stream gracefully (usually after completed/failed)
                    if (!['completed', 'failed', 'finished_incomplete'].includes(tasks[currentTaskId]?.status)) {
                         statusDisplay.textContent = "Status: Connection closed by server.";
                    }
                    closeSSEListener();
                    break;
                case 'pending':
                case 'running':
                     // Just update status text, wait for next message
                     break;
            }
        }

       async function handleFeedback(feedbackValue) {
            if (!currentTaskId) return;
            approveButton.disabled = true;
            rejectButton.disabled = true;
            statusDisplay.textContent = `Sending ${feedbackValue}...`; // Optimistic UI update

            try {
                const response = await fetch(`/feedback/${currentTaskId}`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ feedback: feedbackValue })
                });
                if (!response.ok) { // Rely on SSE for status change or error reporting
                     const errorData = await response.json().catch(()=>({error: `Feedback failed: ${response.status}`}));
                     throw new Error(errorData.error);
                }
                console.log(`Feedback ${feedbackValue} POST successful.`);
                // Successful POST - wait for SSE to update status to 'processing', then 'running' etc.
            } catch (error) {
                console.error('Feedback error:', error);
                statusDisplay.textContent = `Feedback Error: ${error.message}`;
                // Re-enable buttons if feedback POST failed
                approveButton.disabled = false;
                rejectButton.disabled = false;
            }
        }

        function resetUI() {
            closeSSEListener();
            currentTaskId = null;
            taskIdDisplay.textContent = 'Task ID: N/A';
            statusDisplay.textContent = 'Submit a task.';
            reviewSection.classList.add('hidden');
            resultSection.classList.add('hidden');
            taskInput.value = '';
            submitButton.disabled = false;
            approveButton.disabled = false;
            rejectButton.disabled = false;
        }
    </script>
</body>
</html>
</file>

<file path="cookbook/pocketflow-fastapi-hitl/utils/process_task.py">
import time

def process_task(input_data):
    """Minimal simulation of processing the input data."""
    print(f"Processing: '{input_data[:50]}...'")
    
    # Simulate work
    time.sleep(2)

    processed_result = f"Processed: {input_data}"
    print(f"Finished processing.")
    return processed_result

# We don't need a separate utils/call_llm.py for this minimal example,
# but you would add it here if ProcessNode used an LLM.
</file>

<file path="cookbook/pocketflow-fastapi-hitl/flow.py">
from pocketflow import AsyncFlow
from nodes import ProcessNode, ReviewNode, ResultNode

def create_feedback_flow():
    """Creates the minimal feedback workflow."""
    process_node = ProcessNode()
    review_node = ReviewNode()
    result_node = ResultNode()

    # Define transitions
    process_node >> review_node
    review_node - "approved" >> result_node
    review_node - "rejected" >> process_node # Loop back

    # Create the AsyncFlow
    flow = AsyncFlow(start=process_node)
    print("Minimal feedback flow created.")
    return flow
</file>

<file path="cookbook/pocketflow-fastapi-hitl/nodes.py">
from pocketflow import Node, AsyncNode
from utils.process_task import process_task

class ProcessNode(Node):
    def prep(self, shared):
        task_input = shared.get("task_input", "No input")
        print("ProcessNode Prep")
        return task_input

    def exec(self, prep_res):
        return process_task(prep_res)

    def post(self, shared, prep_res, exec_res):
        shared["processed_output"] = exec_res
        print("ProcessNode Post: Output stored.")
        return "default" # Go to ReviewNode

class ReviewNode(AsyncNode):
    async def prep_async(self, shared):
        review_event = shared.get("review_event")
        queue = shared.get("sse_queue") # Expect queue in shared
        processed_output = shared.get("processed_output", "N/A")

        if not review_event or not queue:
            print("ERROR: ReviewNode Prep - Missing review_event or sse_queue in shared store!")
            return None # Signal failure

        # Push status update to SSE queue
        status_update = {
            "status": "waiting_for_review",
            "output_to_review": processed_output
        }
        await queue.put(status_update)
        print("ReviewNode Prep: Put 'waiting_for_review' on SSE queue.")

        return review_event # Return event for exec_async

    async def exec_async(self, prep_res):
        review_event = prep_res
        if not review_event:
            print("ReviewNode Exec: Skipping wait (no event from prep).")
            return
        print("ReviewNode Exec: Waiting on review_event...")
        await review_event.wait()
        print("ReviewNode Exec: review_event set.")

    async def post_async(self, shared, prep_res, exec_res):
        feedback = shared.get("feedback")
        print(f"ReviewNode Post: Processing feedback '{feedback}'")

        # Clear the event for potential loops
        review_event = shared.get("review_event")
        if review_event:
            review_event.clear()
        shared["feedback"] = None # Reset feedback

        if feedback == "approved":
            shared["final_result"] = shared.get("processed_output")
            print("ReviewNode Post: Action=approved")
            return "approved"
        else:
            print("ReviewNode Post: Action=rejected")
            return "rejected"

class ResultNode(Node):
     def prep(self, shared):
         print("ResultNode Prep")
         return shared.get("final_result", "No final result.")

     def exec(self, prep_res):
         print(f"--- FINAL RESULT ---")
         print(prep_res)
         print(f"--------------------")
         return prep_res

     def post(self, shared, prep_res, exec_res):
         print("ResultNode Post: Flow finished.")
         return None # End flow
</file>

<file path="cookbook/pocketflow-fastapi-hitl/README.md">
# PocketFlow Web Human-in-the-Loop (HITL) Feedback Service

This project demonstrates a minimal web application for human-in-the-loop workflows using PocketFlow, FastAPI, and Server-Sent Events (SSE). Users can submit text, have it processed (simulated), review the output, and approve or reject it, potentially triggering reprocessing until approved.

<p align="center">
  <img 
    src="./assets/banner.png" width="800"
  />
</p>

## Features

-   **Web UI:** Simple interface for submitting tasks and providing feedback.
-   **PocketFlow Workflow:** Manages the process -> review -> result/reprocess logic.
-   **FastAPI Backend:** Serves the UI and handles API requests asynchronously.
-   **Server-Sent Events (SSE):** Provides real-time status updates to the client without polling.

## How to Run

1.  Install Dependencies:
    ```bash
    pip install -r requirements.txt
    ```

2.  Run the FastAPI Server:
    Use Uvicorn (or another ASGI server):
    ```bash
    uvicorn server:app --reload --port 8000
    ```
    *(The `--reload` flag is useful for development.)*

3.  Access the Web UI:
    Open your web browser and navigate to `http://127.0.0.1:8000`.

4.  Use the Application:
    *   Enter text into the textarea and click "Submit".
    *   Observe the status updates pushed via SSE.
    *   When prompted ("waiting_for_review"), use the "Approve" or "Reject" buttons.
    *   If rejected, the process loops back. If approved, the final result is displayed.

## How It Works

The application uses PocketFlow to define and execute the feedback loop workflow. FastAPI handles web requests and manages the real-time SSE communication.

**PocketFlow Workflow:**

The core logic is orchestrated by an `AsyncFlow` defined in `flow.py`:

```mermaid
flowchart TD
    subgraph FeedbackFlow[MinimalFeedbackFlow]
        Process[ProcessNode] -- default --> Review[ReviewNode]
        Review -- approved --> Result[ResultNode]
        Review -- rejected --> Process
    end
```

1.  **`ProcessNode`**: Receives input text, calls the minimal `process_task` utility, and stores the output.
2.  **`ReviewNode` (Async)**:
    *   Pushes a "waiting_for_review" status with the processed output to the SSE queue.
    *   Waits asynchronously for an external signal (triggered by the `/feedback` API endpoint).
    *   Based on the received feedback ("approved" or "rejected"), determines the next step in the flow. Stores the result if approved.
3.  **`ResultNode`**: Logs the final approved result.

**FastAPI & SSE Integration:**

*   The `/submit` endpoint creates a unique task, initializes the PocketFlow `shared` state (including an `asyncio.Event` for review and an `asyncio.Queue` for SSE), and schedules the flow execution using `BackgroundTasks`.
*   Nodes within the flow (specifically `ReviewNode`'s prep logic) put status updates onto the task-specific `sse_queue`.
*   The `/stream/{task_id}` endpoint uses `StreamingResponse` to read from the task's `sse_queue` and push formatted status updates to the connected client via Server-Sent Events.
*   The `/feedback/{task_id}` endpoint receives the human's decision, updates the `shared` state, and sets the `asyncio.Event` to unblock the waiting `ReviewNode`.

This setup allows for a decoupled workflow logic (PocketFlow) and web interaction layer (FastAPI), with efficient real-time updates pushed to the user.

## Files

-   [`server.py`](./server.py): The main FastAPI application handling HTTP requests, SSE, state management, and background task scheduling.
-   [`nodes.py`](./nodes.py): Defines the PocketFlow `Node` classes (`ProcessNode`, `ReviewNode`, `ResultNode`) for the workflow steps.
-   [`flow.py`](./flow.py): Defines the PocketFlow `AsyncFlow` that connects the nodes into the feedback loop.
-   [`utils/process_task.py`](./utils/process_task.py): Contains the minimal simulation function for task processing.
-   [`templates/index.html`](./templates/index.html): The HTML structure for the frontend user interface.
-   [`static/style.css`](./static/style.css): Basic CSS for styling the frontend.
-   [`requirements.txt`](./requirements.txt): Project dependencies (FastAPI, Uvicorn, Jinja2, PocketFlow).
</file>

<file path="cookbook/pocketflow-fastapi-hitl/requirements.txt">
pocketflow>=0.0.1
fastapi
uvicorn[standard] # ASGI server for FastAPI
jinja2 # For HTML templating
</file>

<file path="cookbook/pocketflow-fastapi-hitl/server.py">
import asyncio
import uuid
import json
import os
from fastapi import FastAPI, Request, HTTPException, status, BackgroundTasks # Import BackgroundTasks
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field # Import Pydantic for request/response models
from typing import Dict, Any, Literal # For type hinting

from flow import create_feedback_flow # PocketFlow imports

# --- Configuration ---
app = FastAPI(title="Minimal Feedback Loop API")

static_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'static'))
if os.path.isdir(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")
else:
    print(f"Warning: Static directory '{static_dir}' not found.")

template_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'templates'))
if os.path.isdir(template_dir):
    templates = Jinja2Templates(directory=template_dir)
else:
    print(f"Warning: Template directory '{template_dir}' not found.")
    templates = None

# --- State Management (In-Memory - NOT FOR PRODUCTION) ---
# Global dictionary to store task state. In production, use Redis, DB, etc.
tasks: Dict[str, Dict[str, Any]] = {}
# Structure: task_id -> {"shared": dict, "status": str, "task_obj": asyncio.Task | None}


# --- Background Flow Runner ---
# This function remains mostly the same, as it defines the work to be done.
# It will be scheduled by FastAPI's BackgroundTasks now.
async def run_flow_background(task_id: str, flow, shared: Dict[str, Any]):
    """Runs the flow in background, uses queue in shared for SSE."""
    # Check if task exists (might have been cancelled/deleted)
    if task_id not in tasks:
        print(f"Background task {task_id}: Task not found, aborting.")
        return
    queue = shared.get("sse_queue")
    if not queue:
        print(f"ERROR: Task {task_id} missing sse_queue in shared store!")
        tasks[task_id]["status"] = "failed"
        # Cannot report failure via SSE if queue is missing
        return

    tasks[task_id]["status"] = "running"
    await queue.put({"status": "running"})
    print(f"Task {task_id}: Background flow starting.")

    final_status = "unknown"
    error_message = None
    try:
        # Execute the potentially long-running PocketFlow
        await flow.run_async(shared)

        # Determine final status based on shared state after flow completion
        if shared.get("final_result") is not None:
            final_status = "completed"
        else:
            # If flow ends without setting final_result
            final_status = "finished_incomplete"
        print(f"Task {task_id}: Flow finished with status: {final_status}")

    except Exception as e:
        final_status = "failed"
        error_message = str(e)
        print(f"Task {task_id}: Flow execution failed: {e}")
        # Consider logging traceback here in production
    finally:
        # Ensure task still exists before updating state
        if task_id in tasks:
            tasks[task_id]["status"] = final_status
            final_update = {"status": final_status}
            if final_status == "completed":
                final_update["final_result"] = shared.get("final_result")
            elif error_message:
                final_update["error"] = error_message
            # Put final status update onto the queue
            await queue.put(final_update)

        # Signal the end of the SSE stream by putting None
        # Must happen regardless of whether task was deleted mid-run
        if queue:
           await queue.put(None)
        print(f"Task {task_id}: Background task ended. Final update sentinel put on queue.")
        # Remove the reference to the completed/failed asyncio Task object
        if task_id in tasks:
            tasks[task_id]["task_obj"] = None

# --- Pydantic Models for Request/Response Validation ---
class SubmitRequest(BaseModel):
    data: str = Field(..., min_length=1, description="Input data for the task")

class SubmitResponse(BaseModel):
    message: str = "Task submitted"
    task_id: str

class FeedbackRequest(BaseModel):
    feedback: Literal["approved", "rejected"] # Use Literal for specific choices

class FeedbackResponse(BaseModel):
    message: str

# --- FastAPI Routes ---
@app.get("/", response_class=HTMLResponse, include_in_schema=False)
async def get_index(request: Request):
    """Serves the main HTML frontend."""
    if templates is None:
        raise HTTPException(status_code=500, detail="Templates directory not configured.")
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/submit", response_model=SubmitResponse, status_code=status.HTTP_202_ACCEPTED)
async def submit_task(
    submit_request: SubmitRequest, # Use Pydantic model for validation
    background_tasks: BackgroundTasks # Inject BackgroundTasks instance
):
    """
    Submits a new task. The actual processing runs in the background.
    Returns immediately with the task ID.
    """
    task_id = str(uuid.uuid4())
    feedback_event = asyncio.Event()
    status_queue = asyncio.Queue()

    shared = {
        "task_input": submit_request.data,
        "processed_output": None,
        "feedback": None,
        "review_event": feedback_event,
        "sse_queue": status_queue,
        "final_result": None,
        "task_id": task_id
    }

    flow = create_feedback_flow()

    # Store task state BEFORE scheduling background task
    tasks[task_id] = {
        "shared": shared,
        "status": "pending",
        "task_obj": None # Placeholder for the asyncio Task created by BackgroundTasks
    }

    await status_queue.put({"status": "pending", "task_id": task_id})

    # Schedule the flow execution using FastAPI's BackgroundTasks
    # This runs AFTER the response has been sent
    background_tasks.add_task(run_flow_background, task_id, flow, shared)
    # Note: We don't get a direct reference to the asyncio Task object this way,
    # which is fine for this minimal example. If cancellation were needed,
    # managing asyncio.create_task manually would be necessary.

    print(f"Task {task_id}: Submitted, scheduled for background execution.")
    return SubmitResponse(task_id=task_id)


@app.post("/feedback/{task_id}", response_model=FeedbackResponse)
async def provide_feedback(task_id: str, feedback_request: FeedbackRequest):
    """Provides feedback (approved/rejected) to potentially unblock a waiting task."""
    if task_id not in tasks:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    task_info = tasks[task_id]
    shared = task_info["shared"]
    queue = shared.get("sse_queue")
    review_event = shared.get("review_event")

    async def report_error(message, status_code=status.HTTP_400_BAD_REQUEST):
        # Helper to log, put status on queue, and raise HTTP exception
        print(f"Task {task_id}: Feedback error - {message}")
        if queue: await queue.put({"status": "feedback_error", "error": message})
        raise HTTPException(status_code=status_code, detail=message)

    if not review_event:
        # This indicates an internal setup error if the task exists but has no event
        await report_error("Task not configured for feedback", status.HTTP_500_INTERNAL_SERVER_ERROR)
    if review_event.is_set():
        # Prevent processing feedback multiple times or if the task isn't waiting
        await report_error("Task not awaiting feedback or feedback already sent", status.HTTP_409_CONFLICT)

    feedback = feedback_request.feedback # Already validated by Pydantic
    print(f"Task {task_id}: Received feedback via POST: {feedback}")

    # Update status *before* setting the event, so client sees 'processing' first
    if queue: await queue.put({"status": "processing_feedback", "feedback_value": feedback})
    tasks[task_id]["status"] = "processing_feedback" # Update central status tracker

    # Store feedback and signal the waiting ReviewNode
    shared["feedback"] = feedback
    review_event.set()

    return FeedbackResponse(message=f"Feedback '{feedback}' received")


# --- SSE Endpoint ---
@app.get("/stream/{task_id}")
async def stream_status(task_id: str):
    """Streams status updates for a given task using Server-Sent Events."""
    if task_id not in tasks or "sse_queue" not in tasks[task_id]["shared"]:
         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task or queue not found")

    queue = tasks[task_id]["shared"]["sse_queue"]

    async def event_generator():
        """Yields SSE messages from the task's queue."""
        print(f"SSE Stream: Client connected for {task_id}")
        try:
            while True:
                # Wait for the next status update from the queue
                update = await queue.get()
                if update is None: # Sentinel value indicates end of stream
                    print(f"SSE Stream: Sentinel received for {task_id}, closing stream.")
                    yield f"data: {json.dumps({'status': 'stream_closed'})}\n\n"
                    break

                sse_data = json.dumps(update)
                print(f"SSE Stream: Sending for {task_id}: {sse_data}")
                yield f"data: {sse_data}\n\n" # SSE format: "data: <json>\n\n"
                queue.task_done() # Acknowledge processing the queue item

        except asyncio.CancelledError:
            # This happens if the client disconnects
            print(f"SSE Stream: Client disconnected for {task_id}.")
        except Exception as e:
            # Log unexpected errors during streaming
            print(f"SSE Stream: Error in generator for {task_id}: {e}")
            # Optionally send an error message to the client if possible
            try:
                yield f"data: {json.dumps({'status': 'stream_error', 'error': str(e)})}\n\n"
            except Exception: # Catch errors if yield fails (e.g., connection already closed)
                pass
        finally:
            print(f"SSE Stream: Generator finished for {task_id}.")
            # Consider cleanup here (e.g., removing task if no longer needed)
            # if task_id in tasks: del tasks[task_id]

    # Use FastAPI/Starlette's StreamingResponse for SSE
    headers = {'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}
    return StreamingResponse(event_generator(), media_type="text/event-stream", headers=headers)

# --- Main Execution Guard (for running with uvicorn) ---
if __name__ == "__main__":
    print("Starting FastAPI server using Uvicorn is recommended:")
    print("uvicorn server:app --reload --host 0.0.0.0 --port 8000")
    # Example using uvicorn programmatically (less common than CLI)
    # import uvicorn
    # uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="cookbook/pocketflow-fastapi-websocket/docs/design.md">
# Design Doc: FastAPI WebSocket Chat Interface

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

**User Story**: As a user, I want to interact with an AI chatbot through a web interface where:
1. I can send messages and receive real-time streaming responses
2. The connection stays persistent (WebSocket)
3. I can see the AI response being typed out in real-time as the LLM generates it
4. The interface is minimal and easy to use

**Technical Requirements**:
- FastAPI backend with WebSocket support
- Real-time bidirectional communication
- True LLM streaming integration using PocketFlow AsyncNode
- Simple HTML/JavaScript frontend
- Minimal dependencies

## Flow Design

> Notes for AI:
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

**Single Async Node Pattern**: One PocketFlow AsyncNode handles the entire LLM streaming process with real-time WebSocket streaming

### Flow high-level Design:

**PocketFlow AsyncFlow**: Just one async node
1. **Streaming Chat Node**: Processes message, calls LLM with real streaming, sends chunks immediately to WebSocket

**Integration**: FastAPI WebSocket endpoint calls the PocketFlow AsyncFlow

```mermaid
flowchart TD
    user((User Browser)) --> websocket(FastAPI WebSocket)
    websocket --> flow[Streaming Chat AsyncNode]
    flow --> websocket
    websocket --> user
    
    style user fill:#e1f5fe
    style websocket fill:#f3e5f5
    style flow fill:#e8f5e8,stroke:#4caf50,stroke-width:3px
```

## Utility Functions

> Notes for AI:
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1. **Stream LLM** (`utils/stream_llm.py`)
   - *Input*: messages (list of chat history)
   - *Output*: generator yielding real-time response chunks from OpenAI API
   - Used by streaming chat node to get LLM chunks as they're generated

## Node Design

### Shared Store

> Notes for AI: Try to minimize data redundancy

The shared store structure is organized as follows:

```python
shared = {
    "websocket": None,           # WebSocket connection object
    "user_message": "",          # Current user message
    "conversation_history": []   # List of message history with roles
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1. **Streaming Chat Node**
  - *Purpose*: Process user message, call LLM with real streaming, and send chunks immediately via WebSocket
  - *Type*: AsyncNode (for real-time streaming)
  - *Steps*:
    - *prep*: Read user message, build conversation history with new message
    - *exec_async*: Call streaming LLM utility, stream each chunk immediately to WebSocket as received
    - *post*: Update conversation history with complete assistant response
</file>

<file path="cookbook/pocketflow-fastapi-websocket/static/index.html">
<!DOCTYPE html>
<html>
<head>
    <title>PocketFlow Chat</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body { 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }
        
        .chat-container { 
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            width: 100%;
            max-width: 600px;
            height: 80vh;
            display: flex;
            flex-direction: column;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
            text-align: center;
        }
        
        .header h1 {
            font-size: 24px;
            font-weight: 600;
            color: #333;
            margin-bottom: 5px;
        }
        
        .status {
            font-size: 14px;
            color: #666;
            font-weight: 500;
        }
        
        .messages { 
            flex: 1;
            overflow-y: auto;
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 16px;
        }
        
        .message { 
            max-width: 80%;
            padding: 12px 16px;
            border-radius: 18px;
            font-size: 15px;
            line-height: 1.4;
            word-wrap: break-word;
        }
        
        .user-message { 
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            align-self: flex-end;
            border-bottom-right-radius: 4px;
        }
        
        .ai-message { 
            background: #f1f3f4;
            color: #333;
            align-self: flex-start;
            border-bottom-left-radius: 4px;
        }
        
        .input-container { 
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-top: 1px solid rgba(255, 255, 255, 0.2);
            display: flex;
            gap: 12px;
        }
        
        #messageInput { 
            flex: 1;
            padding: 12px 16px;
            border: none;
            border-radius: 25px;
            background: white;
            font-size: 15px;
            outline: none;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        #messageInput::placeholder {
            color: #999;
        }
        
        #sendButton { 
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 15px;
            font-weight: 600;
            transition: all 0.2s ease;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        #sendButton:hover:not(:disabled) {
            transform: translateY(-1px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        #sendButton:disabled { 
            background: #ccc;
            cursor: not-allowed;
            transform: none;
        }
        
        .messages::-webkit-scrollbar {
            width: 6px;
        }
        
        .messages::-webkit-scrollbar-track {
            background: transparent;
        }
        
        .messages::-webkit-scrollbar-thumb {
            background: rgba(0,0,0,0.2);
            border-radius: 3px;
        }
    </style>
</head>
<body>
    <div class="chat-container">
        <div class="header">
            <h1>PocketFlow Chat</h1>
            <div class="status" id="status">Connecting...</div>
        </div>
        
        <div class="messages" id="messages"></div>
        
        <div class="input-container">
            <input type="text" id="messageInput" placeholder="Type your message..." disabled>
            <button id="sendButton" disabled>Send</button>
        </div>
    </div>

    <script>
        const ws = new WebSocket(`ws://localhost:8000/ws`);
        const messagesDiv = document.getElementById('messages');
        const messageInput = document.getElementById('messageInput');
        const sendButton = document.getElementById('sendButton');
        const statusDiv = document.getElementById('status');
        
        let isStreaming = false;
        let currentAiMessage = null;

        ws.onopen = function() {
            statusDiv.textContent = 'Connected';
            messageInput.disabled = false;
            sendButton.disabled = false;
            messageInput.focus();
        };

        ws.onmessage = function(event) {
            const data = JSON.parse(event.data);
            
            if (data.type === 'start') {
                isStreaming = true;
                currentAiMessage = document.createElement('div');
                currentAiMessage.className = 'message ai-message';
                messagesDiv.appendChild(currentAiMessage);
                messagesDiv.scrollTop = messagesDiv.scrollHeight;
                sendButton.disabled = true;
                statusDiv.textContent = 'AI is typing...';
                
            } else if (data.type === 'chunk') {
                if (currentAiMessage) {
                    currentAiMessage.textContent += data.content;
                    messagesDiv.scrollTop = messagesDiv.scrollHeight;
                }
                
            } else if (data.type === 'end') {
                isStreaming = false;
                currentAiMessage = null;
                sendButton.disabled = false;
                statusDiv.textContent = 'Connected';
                messageInput.focus();
            }
        };

        ws.onclose = function() {
            statusDiv.textContent = 'Disconnected';
            messageInput.disabled = true;
            sendButton.disabled = true;
        };

        function sendMessage() {
            const message = messageInput.value.trim();
            if (message && !isStreaming) {
                const userMessage = document.createElement('div');
                userMessage.className = 'message user-message';
                userMessage.textContent = message;
                messagesDiv.appendChild(userMessage);
                messagesDiv.scrollTop = messagesDiv.scrollHeight;

                ws.send(JSON.stringify({
                    type: 'message',
                    content: message
                }));

                messageInput.value = '';
                statusDiv.textContent = 'Sending...';
            }
        }

        sendButton.addEventListener('click', sendMessage);
        messageInput.addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                e.preventDefault();
                sendMessage();
            }
        });
    </script>
</body>
</html>
</file>

<file path="cookbook/pocketflow-fastapi-websocket/utils/__init__.py">
# Utils package for FastAPI WebSocket Chat Interface
</file>

<file path="cookbook/pocketflow-fastapi-websocket/utils/stream_llm.py">
import os
from openai import AsyncOpenAI

async def stream_llm(messages):
    client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    
    stream = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        stream=True,
        temperature=0.7
    )
    
    async for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            yield chunk.choices[0].delta.content

if __name__ == "__main__":
    import asyncio
    
    async def test():
        messages = [{"role": "user", "content": "Hello!"}]
        async for chunk in stream_llm(messages):
            print(chunk, end="", flush=True)
        print()
    
    asyncio.run(test())
</file>

<file path="cookbook/pocketflow-fastapi-websocket/flow.py">
from pocketflow import AsyncFlow
from nodes import StreamingChatNode

def create_streaming_chat_flow():
    chat_node = StreamingChatNode()
    return AsyncFlow(start=chat_node)
</file>

<file path="cookbook/pocketflow-fastapi-websocket/main.py">
import json
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from flow import create_streaming_chat_flow

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
async def get_chat_interface():
    return FileResponse("static/index.html")

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    # Initialize conversation history for this connection
    shared_store = {
        "websocket": websocket,
        "conversation_history": []
    }
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            # Update only the current message, keep conversation history
            shared_store["user_message"] = message.get("content", "")
            
            flow = create_streaming_chat_flow()
            await flow.run_async(shared_store)
            
    except WebSocketDisconnect:
        pass

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="cookbook/pocketflow-fastapi-websocket/nodes.py">
import asyncio
import json
from pocketflow import AsyncNode
from utils.stream_llm import stream_llm

class StreamingChatNode(AsyncNode):
    async def prep_async(self, shared):
        user_message = shared.get("user_message", "")
        websocket = shared.get("websocket")
        
        conversation_history = shared.get("conversation_history", [])
        conversation_history.append({"role": "user", "content": user_message})
        
        return conversation_history, websocket
    
    async def exec_async(self, prep_res):
        messages, websocket = prep_res
        
        await websocket.send_text(json.dumps({"type": "start", "content": ""}))
        
        full_response = ""
        async for chunk_content in stream_llm(messages):
            full_response += chunk_content
            await websocket.send_text(json.dumps({
                "type": "chunk", 
                "content": chunk_content
            }))
        
        await websocket.send_text(json.dumps({"type": "end", "content": ""}))
        
        return full_response, websocket
    
    async def post_async(self, shared, prep_res, exec_res):
        full_response, websocket = exec_res
        
        conversation_history = shared.get("conversation_history", [])
        conversation_history.append({"role": "assistant", "content": full_response})
        shared["conversation_history"] = conversation_history
</file>

<file path="cookbook/pocketflow-fastapi-websocket/README.md">
# PocketFlow FastAPI WebSocket Chat

Real-time chat interface with streaming LLM responses using PocketFlow, FastAPI, and WebSocket.

<p align="center">
  <img 
    src="./assets/banner.png" width="800"
  />
</p>

## Features

- **Real-time Streaming**: See AI responses typed out in real-time as the LLM generates them
- **Conversation Memory**: Maintains chat history across messages
- **Modern UI**: Clean, responsive chat interface with gradient design
- **WebSocket Connection**: Persistent connection for instant communication
- **PocketFlow Integration**: Uses PocketFlow `AsyncNode` and `AsyncFlow` for streaming

## How to Run

1. **Set OpenAI API Key:**
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"
   ```

2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Application:**
   ```bash
   python main.py
   ```

4. **Access the Web UI:**
   Open `http://localhost:8000` in your browser.

## Usage

1. **Type Message**: Enter your message in the input field
2. **Send**: Press Enter or click Send button
3. **Watch Streaming**: See the AI response appear in real-time
4. **Continue Chat**: Conversation history is maintained automatically

## Files

- [`main.py`](./main.py): FastAPI application with WebSocket endpoint
- [`nodes.py`](./nodes.py): PocketFlow `StreamingChatNode` definition
- [`flow.py`](./flow.py): PocketFlow `AsyncFlow` for chat processing
- [`utils/stream_llm.py`](./utils/stream_llm.py): OpenAI streaming utility
- [`static/index.html`](./static/index.html): Modern chat interface
- [`requirements.txt`](./requirements.txt): Project dependencies
- [`docs/design.md`](./docs/design.md): System design documentation
- [`README.md`](./README.md): This file
</file>

<file path="cookbook/pocketflow-fastapi-websocket/requirements.txt">
fastapi==0.104.1
uvicorn[standard]==0.24.0
openai==1.3.8
pocketflow
</file>

<file path="cookbook/pocketflow-flow/flow.py">
from pocketflow import Node, Flow

class TextInput(Node):
    def prep(self, shared):
        """Get text input from user."""
        if "text" not in shared:
            text = input("\nEnter text to convert: ")
            shared["text"] = text
        return shared["text"]

    def post(self, shared, prep_res, exec_res):
        print("\nChoose transformation:")
        print("1. Convert to UPPERCASE")
        print("2. Convert to lowercase")
        print("3. Reverse text")
        print("4. Remove extra spaces")
        print("5. Exit")
        
        choice = input("\nYour choice (1-5): ")
        
        if choice == "5":
            return "exit"
        
        shared["choice"] = choice
        return "transform"

class TextTransform(Node):
    def prep(self, shared):
        return shared["text"], shared["choice"]
    
    def exec(self, inputs):
        text, choice = inputs
        
        if choice == "1":
            return text.upper()
        elif choice == "2":
            return text.lower()
        elif choice == "3":
            return text[::-1]
        elif choice == "4":
            return " ".join(text.split())
        else:
            return "Invalid option!"
    
    def post(self, shared, prep_res, exec_res):
        print("\nResult:", exec_res)
        
        if input("\nConvert another text? (y/n): ").lower() == 'y':
            shared.pop("text", None)  # Remove previous text
            return "input"
        return "exit"

class EndNode(Node):
    pass

# Create nodes
text_input = TextInput()
text_transform = TextTransform()
end_node = EndNode()

# Connect nodes
text_input - "transform" >> text_transform
text_transform - "input" >> text_input
text_transform - "exit" >> end_node

# Create flow
flow = Flow(start=text_input)
</file>

<file path="cookbook/pocketflow-flow/main.py">
from flow import flow

def main():
    print("\nWelcome to Text Converter!")
    print("=========================")
    
    # Initialize shared store
    shared = {}
    
    # Run the flow
    flow.run(shared)
    
    print("\nThank you for using Text Converter!")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-flow/README.md">
# Text Converter Flow

This project demonstrates an interactive text transformation tool built with PocketFlow.

## Features

- Convert text to UPPERCASE
- Convert text to lowercase
- Reverse text
- Remove extra spaces
- Interactive command-line interface
- Continuous flow with option to process multiple texts

## Getting Started

1. Install the required dependencies:

```bash
pip install -r requirements.txt
```

2. Run the application:

```bash
python main.py
```

## How It Works

The workflow features an interactive loop with branching paths:

```mermaid
graph TD
    Input[TextInput Node] -->|transform| Transform[TextTransform Node]
    Transform -->|input| Input
    Transform -->|exit| End[End]
    Input -->|exit| End
```

Here's what each part does:
1. **TextInput Node**: Collects text input and handles menu choices
2. **TextTransform Node**: Applies the selected transformation to the text

## Example Output

```
Welcome to Text Converter!
=========================

Enter text to convert: Pocket Flow is a 100-line LLM framework

Choose transformation:
1. Convert to UPPERCASE
2. Convert to lowercase
3. Reverse text
4. Remove extra spaces
5. Exit

Your choice (1-5): 1

Result: POCKET FLOW IS A 100-LINE LLM FRAMEWORK

Convert another text? (y/n): n

Thank you for using Text Converter!
```

## Files

- [`main.py`](./main.py): Main entry point for running the text converter
- [`flow.py`](./flow.py): Defines the nodes and flow for text transformation
- [`requirements.txt`](./requirements.txt): Lists the required dependencies
</file>

<file path="cookbook/pocketflow-flow/requirements.txt">
pocketflow>=0.1.0
</file>

<file path="cookbook/pocketflow-google-calendar/utils/google_calendar.py">
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
import os.path
import os
import pickle
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

CALENDAR_ID = os.getenv('GOOGLE_CALENDAR_ID')
GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
TIMEZONE = os.getenv('TIMEZONE')

SCOPES = ['https://www.googleapis.com/auth/calendar']

def get_calendar_service():
    """Gets the authenticated Google Calendar service."""
    creds = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                GOOGLE_APPLICATION_CREDENTIALS, SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)

    return build('calendar', 'v3', credentials=creds)

def create_event(summary, description, start_time, end_time, timezone=TIMEZONE):
    """Creates a new event in Google Calendar."""
    service = get_calendar_service()
    
    event = {
        'summary': summary,
        'description': description,
        'start': {
            'dateTime': start_time.isoformat(),
            'timeZone': timezone,
        },
        'end': {
            'dateTime': end_time.isoformat(),
            'timeZone': timezone,
        },
    }

    event = service.events().insert(calendarId=CALENDAR_ID, body=event).execute()
    return event

def list_events(days=7):
    """Lists events for the next X days."""
    service = get_calendar_service()
    
    now = datetime.utcnow()
    time_min = now.isoformat() + 'Z'
    time_max = (now + timedelta(days=days)).isoformat() + 'Z'

    events_result = service.events().list(
        calendarId=CALENDAR_ID,
        timeMin=time_min,
        timeMax=time_max,
        singleEvents=True,
        orderBy='startTime'
    ).execute()
    
    return events_result.get('items', [])

def create_custom_calendar(calendar_name, description=""):
    """Creates a new custom calendar in Google Calendar."""
    service = get_calendar_service()
    
    calendar = {
        'summary': calendar_name,
        'description': description,
        'timeZone': TIMEZONE
    }

    created_calendar = service.calendars().insert(body=calendar).execute()
    return created_calendar

def list_calendar_lists():
    """Lists all available calendars for the user."""
    service = get_calendar_service()
    
    calendar_list = service.calendarList().list().execute()
    return calendar_list.get('items', [])
</file>

<file path="cookbook/pocketflow-google-calendar/.env.exemplo">
# Google Calendar API Configuration
GOOGLE_CALENDAR_ID=your_calendar_id@group.calendar.google.com
GOOGLE_APPLICATION_CREDENTIALS=credentials.json

# Application Configuration
TIMEZONE=America/Sao_Paulo  # or your preferred timezone
</file>

<file path="cookbook/pocketflow-google-calendar/.gitignore">
.env
Pipfile.lock
credentials.json
token.pickle
</file>

<file path="cookbook/pocketflow-google-calendar/main.py">
from pocketflow import Flow
from nodes import CreateCalendarEventNode, ListCalendarEventsNode, ListCalendarsNode
from datetime import datetime, timedelta

def create_calendar_flow():
    """Creates a flow to manage calendar events."""
    # Create nodes
    create_event_node = CreateCalendarEventNode()
    list_events_node = ListCalendarEventsNode()
    
    # Connect nodes
    create_event_node - "success" >> list_events_node
    create_event_node - "error" >> None
    
    # Create flow
    return Flow(start=create_event_node)

def list_calendars_flow():
    """Creates a flow to list all user calendars."""
    list_calendars_node = ListCalendarsNode()
    return Flow(start=list_calendars_node)

def main():
    # Example: List all calendars
    print("=== Listing your calendars ===")
    flow = list_calendars_flow()
    shared = {}
    flow.run(shared)
    
    if 'available_calendars' in shared:
        for cal in shared['available_calendars']:
            print(f"- {cal.get('summary')}")

    # Example: Create a simple event
    print("\n=== Creating an example event ===")
    flow = create_calendar_flow()

    shared = {
        'event_summary': 'Example Meeting',
        'event_description': 'An example meeting created by PocketFlow',
        'event_start_time': datetime.now() + timedelta(days=1),
        'event_end_time': datetime.now() + timedelta(days=1, hours=1),
        'days_to_list': 7
    }

    flow.run(shared)
    
    if 'last_created_event' in shared:
        print("Event created successfully!")
        print(f"Event ID: {shared['last_created_event']['id']}")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-google-calendar/nodes.py">
from pocketflow import Node
from utils.google_calendar import create_event, list_events, list_calendar_lists
from datetime import datetime, timedelta

class CreateCalendarEventNode(Node):
    def prep(self, shared):
        """Prepares the necessary data to create an event."""
        return {
            'summary': shared.get('event_summary'),
            'description': shared.get('event_description'),
            'start_time': shared.get('event_start_time'),
            'end_time': shared.get('event_end_time')
        }
    
    def exec(self, event_data):
        """Creates a new calendar event."""
        try:
            event = create_event(
                summary=event_data['summary'],
                description=event_data['description'],
                start_time=event_data['start_time'],
                end_time=event_data['end_time']
            )
            return {'success': True, 'event': event}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def post(self, shared, prep_res, exec_res):
        """Stores the event creation result."""
        if exec_res['success']:
            shared['last_created_event'] = exec_res['event']
            return 'success'
        else:
            shared['error'] = exec_res['error']
            return 'error'

class ListCalendarEventsNode(Node):
    def prep(self, shared):
        """Prepares parameters to list events."""
        return {
            'days': shared.get('days_to_list', 7)
        }
    
    def exec(self, params):
        """Lists calendar events."""
        try:
            events = list_events(days=params['days'])
            return {'success': True, 'events': events}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def post(self, shared, prep_res, exec_res):
        """Stores the list of events."""
        if exec_res['success']:
            shared['calendar_events'] = exec_res['events']
            return 'success'
        else:
            shared['error'] = exec_res['error']
            return 'error'

class ListCalendarsNode(Node):
    def prep(self, shared):
        """No special preparation needed to list calendars."""
        return {}

    def exec(self, params):
        """Lists all available calendars for the user."""
        try:
            calendars = list_calendar_lists()
            return {'success': True, 'calendars': calendars}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def post(self, shared, prep_res, exec_res):
        """Stores the list of calendars in the shared store."""
        if exec_res['success']:
            shared['available_calendars'] = exec_res['calendars']
            return 'success'
        else:
            shared['error'] = exec_res['error']
            return 'error'
</file>

<file path="cookbook/pocketflow-google-calendar/Pipfile">
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
python-dotenv = ">=0.19.0"
pocketflow = ">=0.0.2"
google-auth-oauthlib = ">=1.0.0"
google-auth-httplib2 = ">=0.1.0"
google-api-python-client = ">=2.0.0"

[dev-packages]

[requires]
python_version = "3.13"
</file>

<file path="cookbook/pocketflow-google-calendar/README.md">
# Pocket Google Calendar

An application based on the Pocket Flow framework for Google Calendar integration.

## ðŸ“‹ Description

This project implements a Google Calendar integration using the Pocket Flow framework, allowing efficient management of events and appointments through a simple and intuitive interface.

## ðŸš€ Features

- Google Calendar API Integration
- Event Management
- Appointment Viewing
- Flow-based Interface using Pocket Flow

## ðŸ› ï¸ Technologies Used

- Python
- Pocket Flow Framework
- Google Calendar API
- Pipenv for dependency management

## ðŸ“¦ Installation

1. Clone the repository:
```bash
git clone [REPOSITORY_URL]
cd pocket-google-calendar
```

2. Install dependencies using Pipenv:
```bash
pipenv install
```

## ðŸ”‘ Credentials Setup

1. Go to the [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select an existing one
3. Enable the Google Calendar API for your project
4. Create credentials:
   - Go to "APIs & Services" > "Credentials"
   - Click "Create Credentials" > "OAuth client ID"
   - Choose "Desktop application" as the application type
   - Download the credentials file
   - Rename it to `credentials.json`
   - Place it in the root directory of the project

## ðŸŒ Environment Variables

Create a `.env` file in the root directory with the following variables:

```env
# Google Calendar API Configuration
GOOGLE_CALENDAR_ID=your_calendar_id@group.calendar.google.com
GOOGLE_APPLICATION_CREDENTIALS=credentials.json

# Application Configuration
TIMEZONE=America/Sao_Paulo  # or your preferred timezone
```

## ðŸ”§ Configuration

1. Activate the virtual environment:
```bash
pipenv shell
```

2. Run the application:
```bash
python main.py
```

## Expected Output

When running the example, you'll see an output similar to this:

```
=== Listing your calendars ===
- Primary Calendar
- Work
- Personal

=== Creating an example event ===
Event created successfully!
Event ID: abc123xyz
```


## ðŸ“ Project Structure

```
pocket-google-calendar/
â”œâ”€â”€ main.py           # Application entry point
â”œâ”€â”€ nodes.py          # Pocket Flow node definitions
â”œâ”€â”€ utils/            # Utilities and helper functions
â”œâ”€â”€ Pipfile           # Pipenv configuration
â”œâ”€â”€ credentials.json  # Google Calendar API credentials
â”œâ”€â”€ .env             # Environment variables
â””â”€â”€ token.pickle      # Google Calendar authentication token
```

## ðŸ¤ Contributing

1. Fork the project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ðŸ“ License

This project is under the MIT License. See the [LICENSE](LICENSE) file for more details.

## âœ¨ Acknowledgments

- [Pocket Flow](https://github.com/the-pocket/PocketFlow) - Framework used
- [Google Calendar API](https://developers.google.com/calendar) - Integration API
</file>

<file path="cookbook/pocketflow-gradio-hitl/utils/call_llm.py">
import os

from openai import OpenAI
from openai.types.chat.chat_completion import ChatCompletion

api_key = os.getenv("OPENAI_API_KEY")
base_url = "https://api.openai.com/v1"
model = "gpt-4o"


def call_llm(message: str):
    print(f"Calling LLM with message: \n{message}")
    client = OpenAI(api_key=api_key, base_url=base_url)
    response: ChatCompletion = client.chat.completions.create(
        model=model, messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content


if __name__ == "__main__":
    print(call_llm("Hello, how are you?"))
</file>

<file path="cookbook/pocketflow-gradio-hitl/utils/call_mock_api.py">
import random
from datetime import date, datetime


def call_check_weather_api(city: str, date: date | None):
    if date is None:
        date = datetime.now().date()

    current_date = datetime.now().date()

    # calculate date difference
    date_diff = (date - current_date).days

    # check if the date is within the allowed range
    if abs(date_diff) > 7:
        return f"Failed to check weather: Date {date} is more than 7 days away from current date."

    return f"The weather in {city} on {date} is {random.choice(['sunny', 'cloudy', 'rainy', 'snowy'])}, and the temperature is {random.randint(10, 30)}Â°C."


def call_book_hotel_api(hotel: str, checkin_date: date, checkout_date: date):
    current_date = datetime.now().date()

    # check if the checkin date is after the current date
    if checkin_date <= current_date:
        return (
            f"Failed to book hotel {hotel}: Check-in date must be after current date."
        )

    # check if the checkin date is before the checkout date
    if checkin_date >= checkout_date:
        return f"Failed to book hotel {hotel}, because the checkin date is after the checkout date."

    # check if the date difference is more than 7 days
    date_diff = (checkout_date - checkin_date).days
    if date_diff > 7:
        return f"Failed to book hotel {hotel}: Stay duration cannot exceed 7 days."

    return f"Booked hotel {hotel} from {checkin_date.strftime('%Y-%m-%d')} to {checkout_date.strftime('%Y-%m-%d')} successfully."
</file>

<file path="cookbook/pocketflow-gradio-hitl/utils/conversation.py">
conversation_cache = {}


def load_conversation(conversation_id: str):
    print(f"Loading conversation {conversation_id}")
    return conversation_cache.get(conversation_id, {})


def save_conversation(conversation_id: str, session: dict):
    print(f"Saving conversation {session}")
    conversation_cache[conversation_id] = session
</file>

<file path="cookbook/pocketflow-gradio-hitl/utils/format_chat_history.py">
def format_chat_history(history):
    """
    Format the chat history for LLM

    Args:
        history (list): The chat history list, each element contains role and content

    Returns:
        str: The formatted chat history string
    """
    if not history:
        return "No history"

    formatted_history = []
    for message in history:
        role = "user" if message["role"] == "user" else "assistant"
        content = message["content"]
        # filter out the thinking content
        if role == "assistant":
            if (
                content.startswith("- ðŸ¤”")
                or content.startswith("- âž¡ï¸")
                or content.startswith("- â¬…ï¸")
            ):
                continue
        formatted_history.append(f"{role}: {content}")

    return "\n".join(formatted_history)
</file>

<file path="cookbook/pocketflow-gradio-hitl/flow.py">
from pocketflow import Flow

from nodes import (
    DecideAction,
    CheckWeather,
    BookHotel,
    FollowUp,
    ResultNotification,
)


def create_flow():
    """
    Create and connect the nodes to form a complete agent flow.
    """
    decide_action = DecideAction()
    check_weather = CheckWeather()
    book_hotel = BookHotel()
    follow_up = FollowUp()
    result_notification = ResultNotification()

    decide_action - "check-weather" >> check_weather
    check_weather >> decide_action
    decide_action - "book-hotel" >> book_hotel
    book_hotel >> decide_action
    decide_action - "follow-up" >> follow_up
    decide_action - "result-notification" >> result_notification

    return Flow(start=decide_action)
</file>

<file path="cookbook/pocketflow-gradio-hitl/main.py">
import time
import uuid
from concurrent.futures import ThreadPoolExecutor
from queue import Queue

import gradio as gr
from gradio import ChatMessage

from flow import create_flow

# create global thread pool
chatflow_thread_pool = ThreadPoolExecutor(
    max_workers=5,
    thread_name_prefix="chatflow_worker",
)


def chat_fn(message, history, uuid):
    """
    Main chat function that handles the conversation flow and message processing.
    
    Args:
        message (str): The current user message
        history (list): Previous conversation history
        uuid (UUID): Unique identifier for the conversation
    
    Yields:
        ChatMessage: Streams of thought process and chat responses
    """
    # Log conversation details
    print(f"Conversation ID: {str(uuid)}\nHistory: {history}\nQuery: {message}\n---")
    
    # Initialize queues for chat messages and flow thoughts
    chat_queue = Queue()
    flow_queue = Queue()
    
    # Create shared context for the flow
    shared = {
        "conversation_id": str(uuid),
        "query": message,
        "history": history,
        "queue": chat_queue,
        "flow_queue": flow_queue,
    }
    
    # Create and run the chat flow in a separate thread
    chat_flow = create_flow()
    chatflow_thread_pool.submit(chat_flow.run, shared)

    # Initialize thought response tracking
    start_time = time.time()
    thought_response = ChatMessage(
        content="", metadata={"title": "Flow Log", "id": 0, "status": "pending"}
    )
    yield thought_response

    # Process and accumulate thoughts from the flow queue
    accumulated_thoughts = ""
    while True:
        thought = flow_queue.get()
        if thought is None:
            break
        accumulated_thoughts += f"- {thought}\n\n"
        thought_response.content = accumulated_thoughts.strip()
        yield thought_response
        flow_queue.task_done()

    # Mark thought processing as complete and record duration
    thought_response.metadata["status"] = "done"
    thought_response.metadata["duration"] = time.time() - start_time
    yield thought_response

    # Process and yield chat messages from the chat queue
    while True:
        msg = chat_queue.get()
        if msg is None:
            break
        chat_response = [thought_response, ChatMessage(content=msg)]
        yield chat_response
        chat_queue.task_done()


def clear_fn():
    print("Clearing conversation")
    return uuid.uuid4()


with gr.Blocks(fill_height=True, theme="ocean") as demo:
    uuid_state = gr.State(uuid.uuid4())
    demo.load(clear_fn, outputs=[uuid_state])

    chatbot = gr.Chatbot(type="messages", scale=1)
    chatbot.clear(clear_fn, outputs=[uuid_state])

    gr.ChatInterface(
        fn=chat_fn,
        type="messages",
        additional_inputs=[uuid_state],
        chatbot=chatbot,
        title="PocketFlow Gradio Demo",
    )


demo.launch()
</file>

<file path="cookbook/pocketflow-gradio-hitl/nodes.py">
from datetime import datetime
from queue import Queue

import yaml
from pocketflow import Node

from utils.call_llm import call_llm
from utils.call_mock_api import call_book_hotel_api, call_check_weather_api
from utils.conversation import load_conversation, save_conversation
from utils.format_chat_history import format_chat_history


class DecideAction(Node):
    def prep(self, shared):
        conversation_id = shared["conversation_id"]
        session = load_conversation(conversation_id)
        return session, shared["history"], shared["query"]

    def exec(self, prep_res):
        session, history, query = prep_res
        prompt = f"""
### INSTRUCTIONS
You are a lifestyle assistant capable of helping users book hotels and check weather conditions.
You need to decide the next action based on your last action, action execution result, chat history, and current user question.

### CHAT HISTORY
{format_chat_history(history)}

### CURRENT USER QUESTION
user: {query}

### CONTEXT
Last Action: {session.get("last_action", None)}
Last Action Result: {session.get("action_result", None)}
Current Date: {datetime.now().date()} 

### ACTION SPACE
[1] check-weather
Description: When the user asks about the weather, use this tool.
Parameters:
    - name: city
        description: The city to check the weather
        required: true
        example: Beijing
    - name: date
        description: The date to check the weather, if not provided, use the current date
        required: false
        example: 2025-05-28

[2] book-hotel
Description: When the user wants to book a hotel, use this tool.
Parameters:
    - name: hotel
        description: The name of the hotel to be booked
        required: true
        example: ShanghaiHilton Hotel
    - name: checkin_date
        description: The check-in date
        required: true
        example: 2025-05-28
    - name: checkout_date
        description: The check-out date
        required: true
        example: 2025-05-29

[3] follow-up
Description: 1. When the user's question is out of the scope of booking hotels and checking weather, use this tool to guide the user; 2. When the current information cannot meet the parameter requirements of the corresponding tool, use this tool to ask the user.
Parameters:
    - name: question
        description: Your guidance or follow-up to the user, maintain an enthusiastic and lively language style, and use the same language as the user's question.
        required: true
        example: Which hotel would you like to book?ðŸ˜Š

[4] result-notification
Description: When the booking of a hotel or checking the weather is completed, use this tool to notify the user of the result and ask if they need any other help. If you find that the user's question is not completed in the history conversation, you can guide the user to complete the intention in the last step.
Parameters:
    - name: result
        description: Notify the user of the result based on the Last Action Result. Maintain an enthusiastic and lively language style, and use the same language as the user's question.
        required: true
        example: The hotel has been successfully booked for you. ðŸ˜‰\n\nThe check-in date is XX, and the check-out date is XX. Thank you for using it. Would you like any other help?ðŸ˜€

## NEXT ACTION
Decide the next action based on the context and available actions.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: check-weather OR book-hotel OR follow-up OR result-notification
reason: <why you chose this action>
question: <if action is follow-up>
city: <if action is check-weather> 
hotel: <if action is book-hotel>
checkin_date: <if action is book-hotel>
checkout_date: <if action is book-hotel>
result: <if action is result-notification>
```

IMPORTANT: Make sure to:
1. Use proper indentation (4 spaces) for all multi-line fields
2. Use the | character for multi-line text fields
3. Keep single-line fields without the | character
"""

        response = call_llm(prompt.strip())
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        print(f"ðŸ¤– Agent response: \n{yaml_str}")
        decision = yaml.safe_load(yaml_str)
        return decision

    def post(self, shared, prep_res, exec_res):
        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        """Save the decision and determine the next step in the flow."""
        # If LLM decided to search, save the search query
        session["last_action"] = exec_res["action"]
        flow_log: Queue = shared["flow_queue"]

        for line in exec_res["thinking"].split("\n"):
            line = line.replace("-", "").strip()
            if line:
                flow_log.put(f"ðŸ¤” {line}")

        if exec_res["action"] == "check-weather":
            session["check_weather_params"] = {
                "city": exec_res["city"],
                "date": exec_res.get("date", None),
            }
            flow_log.put(f"âž¡ï¸ Agent decided to check weather for: {exec_res['city']}")
        elif exec_res["action"] == "book-hotel":
            session["book_hotel_params"] = {
                "hotel": exec_res["hotel"],
                "checkin_date": exec_res["checkin_date"],
                "checkout_date": exec_res["checkout_date"],
            }
            flow_log.put(f"âž¡ï¸ Agent decided to book hotel: {exec_res['hotel']}")
        elif exec_res["action"] == "follow-up":
            session["follow_up_params"] = {"question": exec_res["question"]}
            flow_log.put(f"âž¡ï¸ Agent decided to follow up: {exec_res['question']}")
        elif exec_res["action"] == "result-notification":
            session["result_notification_params"] = {"result": exec_res["result"]}
            flow_log.put(f"âž¡ï¸ Agent decided to notify the result: {exec_res['result']}")
        save_conversation(conversation_id, session)
        # Return the action to determine the next node in the flow
        return exec_res["action"]


class CheckWeather(Node):
    def prep(self, shared):
        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        city = session["check_weather_params"]["city"]
        date = session["check_weather_params"].get("date", None)
        return city, date

    def exec(self, prep_res):
        city, date = prep_res
        return call_check_weather_api(city, date)

    def post(self, shared, prep_res, exec_res):
        flow_log: Queue = shared["flow_queue"]
        flow_log.put(f"â¬…ï¸ Check weather result: {exec_res}")

        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        session["action_result"] = exec_res
        save_conversation(conversation_id, session)
        return "default"


class BookHotel(Node):
    def prep(self, shared):
        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)

        hotel = session["book_hotel_params"]["hotel"]
        checkin_date = session["book_hotel_params"]["checkin_date"]
        checkout_date = session["book_hotel_params"]["checkout_date"]
        return hotel, checkin_date, checkout_date

    def exec(self, prep_res):
        hotel, checkin_date, checkout_date = prep_res
        return call_book_hotel_api(hotel, checkin_date, checkout_date)

    def post(self, shared, prep_res, exec_res):
        flow_log: Queue = shared["flow_queue"]
        flow_log.put(f"â¬…ï¸ Book hotel result: {exec_res}")

        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        session["action_result"] = exec_res
        save_conversation(conversation_id, session)
        return "default"


class FollowUp(Node):
    def prep(self, shared):
        flow_log: Queue = shared["flow_queue"]
        flow_log.put(None)

        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        question = session["follow_up_params"]["question"]
        return question, shared["queue"]

    def exec(self, prep_res):
        question, queue = prep_res
        queue.put(question)
        queue.put(None)
        return question

    def post(self, shared, prep_res, exec_res):
        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        session["action_result"] = exec_res
        return "done"


class ResultNotification(Node):
    def prep(self, shared):
        flow_log: Queue = shared["flow_queue"]
        flow_log.put(None)

        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        result = session["result_notification_params"]["result"]
        return result, shared["queue"]

    def exec(self, prep_res):
        result, queue = prep_res
        queue.put(result)
        queue.put(None)
        return result

    def post(self, shared, prep_res, exec_res):
        conversation_id = shared["conversation_id"]
        session: dict = load_conversation(conversation_id)
        session["action_result"] = None
        session["last_action"] = None
        save_conversation(conversation_id, session)
        return "done"
</file>

<file path="cookbook/pocketflow-gradio-hitl/README.md">
# PocketFlow Gradio HITL Example

A web-based application that demonstrates Human-in-the-Loop (HITL) workflow orchestration using PocketFlow and Gradio. This example provides an interactive interface for users to engage with AI-powered tasks while maintaining human oversight and feedback.

## Features

- **Web-based Interface**: Built with Gradio for an accessible and user-friendly experience
- **Human-in-the-Loop Integration**: Seamless integration of human feedback into the AI workflow
- **Modern UI**: Clean and intuitive interface for better user interaction
- **Powered by LLMs**: Utilizes OpenAI's models for intelligent task processing
- **Flow Visualization**: Real-time visualization of node execution sequence and workflow progress
- **Interactive Debugging**: Monitor and understand the decision-making process through visual feedback

## Getting Started

This project is part of the PocketFlow cookbook examples. It's assumed you have already cloned the [PocketFlow repository](https://github.com/the-pocket/PocketFlow) and are in the `cookbook/pocketflow-gradio-hitl` directory.

1. **Install required dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

2. **Set up your OpenAI API key**:
    The application uses OpenAI models for processing. You need to set your API key as an environment variable:
    ```bash
    export OPENAI_API_KEY="your-openai-api-key-here"
    ```

3. **Run the Application**:
    ```bash
    python main.py
    ```
    This will start the Gradio web interface, typically accessible at `http://localhost:7860`

## How It Works

The system implements a PocketFlow workflow with a web interface:

```mermaid
flowchart TD
    DecideAction[Decide Action Node] --> |"check-weather"| CheckWeather[Check Weather Node]
    CheckWeather --> DecideAction
    DecideAction --> |"book-hotel"| BookHotel[Book Hotel Node]
    BookHotel --> DecideAction
    DecideAction --> |"follow-up"| FollowUp[Follow Up Node]
    DecideAction --> |"result-notification"| ResultNotification[Result Notification Node]
```

The workflow consists of the following nodes:

1. **Decide Action Node**: The central decision-making node that determines the next action based on user input and context
2. **Check Weather Node**: Provides weather information for specified cities and dates
3. **Book Hotel Node**: Handles hotel reservation requests with check-in and check-out dates
4. **Follow Up Node**: Manages user interactions by asking clarifying questions or handling out-of-scope requests
5. **Result Notification Node**: Delivers action results and offers additional assistance

The flow is orchestrated through a series of directed connections:
- The Decide Action node can trigger weather checks, hotel bookings, follow-ups, or result notifications
- Weather checks and hotel bookings can feed back to the Decide Action node for further processing
- Follow-up and result notification nodes provide the final steps in the workflow

### Flow Visualization

The application provides real-time visualization of the workflow execution:
- The sequence of node activations is displayed chronologically
- Users can see which decision paths are being taken
- The visualization helps in understanding the AI's decision-making process

![flow visualization](./assets/flow_visualization.png)

## Sample Output

Here's an example of book hotel:

![book hotel](./assets/book_hotel.png)

Here's an example of changing intention mid-conversation:

![change intention](./assets/change_intention.png)

## Files

- [`main.py`](./main.py): Entry point for the application and Gradio interface setup
- [`flow.py`](./flow.py): Defines the PocketFlow graph and node connections
- [`nodes.py`](./nodes.py): Contains the node definitions for the workflow
- [`utils/`](./utils/): Contains utility functions and helper modules
- [`requirements.txt`](./requirements.txt): Lists project dependencies

## Requirements

- Python 3.8+
- PocketFlow >= 0.0.2
- Gradio >= 5.29.1
- OpenAI >= 1.78.1
</file>

<file path="cookbook/pocketflow-gradio-hitl/requirements.txt">
pocketflow>=0.0.2
gradio>=5.29.1
openai>=1.78.1
</file>

<file path="cookbook/pocketflow-hello-world/docs/design.md">
# Your Project Title

## Project Requirements
A description of the project requirements. 

## Utility Functions

1. **Call LLM** (`utils/call_llm.py`)

## Flow Design

1. **First Node**
2. **Second Node**
3. **Third Node**

### Flow Diagram

```mermaid
flowchart TD
    firstNode[First Node] --> secondNode[Second Node]
    secondNode --> thirdNode[Third Node]
```

## Data Structure

The shared memory structure will be organized as follows:

```python
shared = {
    "key": "value"
}
```

## Node Designs

### 1. First Node
- **Purpose**: What the node does
- **Design**: Regular Node (no Batch/Async)
- **Data Access**: 
  - Read: "key" from shared store
  - Write: "key" to shared store

### 2. Second Node
...

### 3. Third Node
</file>

<file path="cookbook/pocketflow-hello-world/utils/call_llm.py">
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content
    
if __name__ == "__main__":
    prompt = "What is the meaning of life?"
    print(call_llm(prompt))
</file>

<file path="cookbook/pocketflow-hello-world/flow.py">
from pocketflow import Node, Flow
from utils.call_llm import call_llm

# An example node and flow
# Please replace this with your own node and flow
class AnswerNode(Node):
    def prep(self, shared):
        # Read question from shared
        return shared["question"]
    
    def exec(self, question):
        return call_llm(question)
    
    def post(self, shared, prep_res, exec_res):
        # Store the answer in shared
        shared["answer"] = exec_res

answer_node = AnswerNode()
qa_flow = Flow(start=answer_node)
</file>

<file path="cookbook/pocketflow-hello-world/main.py">
from flow import qa_flow

# Example main function
# Please replace this with your own main function
def main():
    shared = {
        "question": "In one sentence, what's the end of universe?",
        "answer": None
    }

    qa_flow.run(shared)
    print("Question:", shared["question"])
    print("Answer:", shared["answer"])

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-hello-world/README.md">
# PocketFlow Hello World

Your first PocketFlow application! This simple example demonstrates how to create a basic PocketFlow app from scratch.

## Project Structure

```
.
â”œâ”€â”€ docs/          # Documentation files
â”œâ”€â”€ utils/         # Utility functions
â”œâ”€â”€ flow.py        # PocketFlow implementation
â”œâ”€â”€ main.py        # Main application entry point
â””â”€â”€ README.md      # Project documentation
```

## Setup

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Run the example:
```bash
python main.py
```

## What This Example Demonstrates

- How to create your first PocketFlow application
- Basic PocketFlow concepts and usage
- Simple example of PocketFlow's capabilities

## Additional Resources

- [PocketFlow Documentation](https://the-pocket.github.io/PocketFlow/)
</file>

<file path="cookbook/pocketflow-llm-streaming/main.py">
import time
import threading
from pocketflow import Node, Flow
from utils import fake_stream_llm, stream_llm

class StreamNode(Node):
    def prep(self, shared):
        # Create interrupt event
        interrupt_event = threading.Event()

        # Start a thread to listen for user interrupt
        def wait_for_interrupt():
            input("Press ENTER at any time to interrupt streaming...\n")
            interrupt_event.set()
        listener_thread = threading.Thread(target=wait_for_interrupt)
        listener_thread.start()
        
        # Get prompt from shared store
        prompt = shared["prompt"]
        # Get chunks from LLM function
        chunks = stream_llm(prompt)
        return chunks, interrupt_event, listener_thread

    def exec(self, prep_res):
        chunks, interrupt_event, listener_thread = prep_res
        for chunk in chunks:
            if interrupt_event.is_set():
                print("User interrupted streaming.")
                break
            
            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
                chunk_content = chunk.choices[0].delta.content
                print(chunk_content, end="", flush=True)
                time.sleep(0.1)  # simulate latency
        return interrupt_event, listener_thread

    def post(self, shared, prep_res, exec_res):
        interrupt_event, listener_thread = exec_res
        # Join the interrupt listener so it doesn't linger
        interrupt_event.set()
        listener_thread.join()
        return "default"

# Usage:
node = StreamNode()
flow = Flow(start=node)

shared = {"prompt": "What's the meaning of life?"}
flow.run(shared)
</file>

<file path="cookbook/pocketflow-llm-streaming/README.md">
#  LLM Streaming and Interruption

Demonstrates real-time LLM response streaming with user interrupt capability.

- Check out the [Substack Post Tutorial](https://zacharyhuang.substack.com/p/streaming-llm-responses-tutorial) for more!

## Features

- Real-time display of LLM responses as they're generated
- User interrupt with ENTER key at any time

## Run It

```bash
pip install -r requirements.txt
python main.py
```

## How It Works

StreamNode:
1. Creates interrupt listener thread
2. Fetches content chunks from LLM
3. Displays chunks in real-time
4. Handles user interruption

## API Key

By default, demo uses fake streaming responses. To use real OpenAI streaming:

1. Edit main.py to replace the fake_stream_llm with stream_llm:
```python
# Change this line:
chunks = fake_stream_llm(prompt)
# To this:
chunks = stream_llm(prompt)
```

2. Make sure your OpenAI API key is set:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## Files

- `main.py`: StreamNode implementation
- `utils.py`: Real and fake LLM streaming functions
</file>

<file path="cookbook/pocketflow-llm-streaming/utils.py">
from openai import OpenAI
import os

def stream_llm(prompt):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))

    # Make a streaming chat completion request
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        stream=True  # Enable streaming
    )
    return response

def fake_stream_llm(prompt, predefined_text="This is a fake response. Today is a sunny day. The sun is shining. The birds are singing. The flowers are blooming. The bees are buzzing. The wind is blowing. The clouds are drifting. The sky is blue. The grass is green. The trees are tall. The water is clear. The fish are swimming. The sun is shining. The birds are singing. The flowers are blooming. The bees are buzzing. The wind is blowing. The clouds are drifting. The sky is blue. The grass is green. The trees are tall. The water is clear. The fish are swimming."):
    """
    Returns a list of simple objects that mimic the structure needed
    for OpenAI streaming responses.
    """
    # Split text into small chunks
    chunk_size = 10
    chunks = []
    
    # Create the chunks using a simple class outside the nested structure
    class SimpleObject:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
    
    # Build the chunks
    for i in range(0, len(predefined_text), chunk_size):
        text_chunk = predefined_text[i:i+chunk_size]
        
        # Create the nested structure using simple objects
        delta = SimpleObject(content=text_chunk)
        choice = SimpleObject(delta=delta)
        chunk = SimpleObject(choices=[choice])
        
        chunks.append(chunk)
    
    return chunks

if __name__ == "__main__":
    print("## Testing streaming LLM")
    prompt = "What's the meaning of life?"
    print(f"## Prompt: {prompt}")
    # response = fake_stream_llm(prompt)
    response = stream_llm(prompt)
    print(f"## Response: ")
    for chunk in response:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            chunk_content = chunk.choices[0].delta.content
            # Print the incoming text without a newline (simulate real-time streaming)
            print(chunk_content, end="", flush=True)
</file>

<file path="cookbook/pocketflow-majority-vote/main.py">
import argparse
from pocketflow import BatchNode, Flow
import collections
from utils import call_llm
import yaml

class MajorityVoteNode(BatchNode):
    def prep(self, shared):
        question = shared.get("question", "(No question provided)")
        attempts_count = shared.get("num_tries", 3)
        return [question for _ in range(attempts_count)]

    def exec(self, single_question: str):
        prompt = f"""
You are a helpful assistant. Please answer the user's question below.
Question: {single_question}

Return strictly using the following YAML structure:
```yaml
thinking: |
    (Your thinking process here)
answer: 0.123 # Final answer as a decimal with 3 decimal places
```"""
        raw_response = call_llm(prompt)
        yaml_part = raw_response.split("```yaml")[1].split("```")[0].strip()
        parsed = yaml.safe_load(yaml_part)

        # Validate we have at least 'answer' field
        if not isinstance(parsed, dict) or 'answer' not in parsed:
            raise RuntimeError(f"Missing 'answer' in YAML: {parsed}")

        # Return only the 'answer' field for the majority vote.
        return str(parsed['answer'])
    
    def exec_fallback(self, prep_res, exc):
        return None

    def post(self, shared, prep_res, exec_res_list):
        # Count frequency for non-None answers
        exec_res_list = [res for res in exec_res_list if res is not None]
        counter = collections.Counter(exec_res_list)
        best_answer, freq = counter.most_common(1)[0]

        # Store final
        shared["majority_answer"] = best_answer

        print("========================")
        print("All structured answers:", exec_res_list)
        print("Majority vote =>", best_answer)
        print("Frequency =>", freq)
        print("========================")

        # End the flow
        return "end"

if __name__ == "__main__":
    # Set up argument parser
    parser = argparse.ArgumentParser(description="Run majority vote reasoning on a problem")
    parser.add_argument("--problem", type=str, help="Your reasoning problem to solve")
    parser.add_argument("--tries", type=int, default=5, help="Number of attempts to make (default: 5)")
    args = parser.parse_args()
    
    # Default problem if none provided
    default_problem = """You work at a shoe factory. In front of you, there are three pairs of shoes (six individual shoes) with the following sizes: two size 4s, two size 5s, and two size 6s. The factory defines an "acceptable pair" as two shoes that differ in size by a maximum of one size (e.g., a size 5 and a size 6 would be an acceptable pair). If you close your eyes and randomly pick three pairs of shoes without replacement, what is the probability that you end up drawing three acceptable pairs?"""
    
    shared = {
        "question": args.problem if args.problem else default_problem,
        "num_tries": args.tries
    }

    majority_node = MajorityVoteNode()
    flow = Flow(start=majority_node)
    flow.run(shared)

    print("\n=== Final Answer ===")
    print(shared["majority_answer"])
    print("====================")
</file>

<file path="cookbook/pocketflow-majority-vote/README.md">
# Majority Vote Reasoning

This project demonstrates a majority vote implementation that enables LLMs to solve complex reasoning problems by aggregating multiple independent attempts. It's designed to improve problem-solving accuracy through consensus-based reasoning.

## Features

- Improves model reliability on complex problems through multiple attempts
- Works with models like Claude 3.7 Sonnet
- Solves problems that single attempts often fail on
- Provides detailed reasoning traces for verification
- Uses a consensus approach to reduce the impact of occasional reasoning errors

## Getting Started

1. Install the required packages:
```bash
pip install -r requirements.txt
```

2. Set up your API key:
```bash
export ANTHROPIC_API_KEY="your-api-key-here"
```

3. Run a test problem to see majority voting in action:
```bash
python main.py
```

4. Try your own reasoning problem:
```bash
python main.py --problem "Your complex reasoning problem here" --tries 5
```

## How It Works

The implementation uses a MajorityVoteNode that processes multiple attempts and finds consensus:

```mermaid
flowchart LR
    mv[MajorityVoteNode] 
```

The MajorityVoteNode:
1. Makes multiple independent attempts to solve the same problem
2. Collects structured answers from each attempt
3. Determines the most frequent answer as the final solution
4. Returns the consensus answer

This approach helps overcome occasional reasoning errors that might occur in individual attempts.

## Example Problem

Example Problem from [Quant Interview](https://www.youtube.com/watch?v=SCP7JptxPU0):

```
You work at a shoe factory. In front of you, there are three pairs of shoes (six individual shoes) with the following sizes: two size 4s, two size 5s, and two size 6s. The factory defines an "acceptable pair" as two shoes that differ in size by a maximum of one size (e.g., a size 5 and a size 6 would be an acceptable pair). If you close your eyes and randomly pick three pairs of shoes without replacement, what is the probability that you end up drawing three acceptable pairs?
```

Below is an example of how the majority vote approach uses Claude 3.7 Sonnet to solve this complex problem:

```
========================
All structured answers: ['0.333', '0.333', '0.333', '0.6', '0.333']
Majority vote => 0.333
Frequency => 4
========================

=== Final Answer ===
0.333
====================
```

This shows that 4 out of 5 attempts yielded the same answer (0.333), which is chosen as the final solution.

## Files

- [`main.py`](./main.py): Implementation of the majority vote node and flow
- [`utils.py`](./utils.py): Simple wrapper for calling the Anthropic model
</file>

<file path="cookbook/pocketflow-majority-vote/requirements.txt">
pocketflow>=0.0.1
anthropic>=0.15.0
pyyaml>=6.0
</file>

<file path="cookbook/pocketflow-majority-vote/utils.py">
from anthropic import Anthropic
import os

def call_llm(prompt):
    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
    response = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=10000,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")
</file>

<file path="cookbook/pocketflow-map-reduce/data/resume1.txt">
John Smith
Software Engineer

Education:
- Master of Computer Science, Stanford University, 2018
- Bachelor of Computer Science, MIT, 2016

Experience:
- Senior Software Engineer, Google, 2019-present
  * Led the development of cloud infrastructure projects
  * Implemented scalable solutions using Kubernetes and Docker
  * Reduced system latency by 40% through optimization

- Software Developer, Microsoft, 2016-2019
  * Worked on Azure cloud services
  * Built RESTful APIs for enterprise solutions

Skills:
- Programming: Python, Java, C++, JavaScript
- Technologies: Docker, Kubernetes, AWS, Azure
- Tools: Git, Jenkins, Jira

Projects:
- Developed a recommendation engine that increased user engagement by 25%
- Created a sentiment analysis tool using NLP techniques
</file>

<file path="cookbook/pocketflow-map-reduce/data/resume2.txt">
Emily Johnson
Data Scientist

Education:
- Ph.D. in Statistics, UC Berkeley, 2020
- Master of Science in Mathematics, UCLA, 2016

Experience:
- Data Scientist, Netflix, 2020-present
  * Developed machine learning models for content recommendation
  * Implemented A/B testing frameworks to optimize user experience
  * Collaborated with product teams to define metrics and KPIs

- Data Analyst, Amazon, 2016-2020
  * Analyzed user behavior patterns to improve conversion rates
  * Created dashboards and visualizations for executive decision-making

Skills:
- Programming: R, Python, SQL
- Machine Learning: TensorFlow, PyTorch, scikit-learn
- Data Visualization: Tableau, PowerBI, matplotlib

Publications:
- "Advances in Recommendation Systems" - Journal of Machine Learning, 2021
- "Statistical Methods for Big Data" - Conference on Data Science, 2019
</file>

<file path="cookbook/pocketflow-map-reduce/data/resume3.txt">
Michael Williams
Marketing Manager

Education:
- MBA, Harvard Business School, 2015
- Bachelor of Arts in Communications, NYU, 2010

Experience:
- Marketing Director, Apple, 2018-present
  * Managed a team of 15 marketing professionals
  * Developed and executed global marketing campaigns
  * Increased brand awareness by 30% through digital initiatives

- Marketing Manager, Coca-Cola, 2015-2018
  * Led product launches across North America
  * Coordinated with external agencies on advertising campaigns

Skills:
- Digital Marketing: SEO, SEM, Social Media Marketing
- Analytics: Google Analytics, Adobe Analytics
- Tools: HubSpot, Salesforce, Marketo

Achievements:
- Marketing Excellence Award, 2020
- Led campaign that won Cannes Lions Award, 2019
</file>

<file path="cookbook/pocketflow-map-reduce/data/resume4.txt">
Lisa Chen
Frontend Developer

Education:
- Bachelor of Fine Arts, Rhode Island School of Design, 2019

Experience:
- UI/UX Designer, Airbnb, 2020-present
  * Designed user interfaces for mobile and web applications
  * Created wireframes and prototypes for new features
  * Conducted user research and usability testing

- Junior Designer, Freelance, 2019-2020
  * Worked with small businesses on branding and website design
  * Developed responsive web designs using HTML, CSS, and JavaScript

Skills:
- Design: Figma, Sketch, Adobe XD
- Development: HTML, CSS, JavaScript, React
- Tools: Git, Zeplin

Portfolio Highlights:
- Redesigned checkout flow resulting in 15% conversion increase
- Created custom icon set for mobile application
- Designed responsive email templates

Certifications:
- UI/UX Design Certificate, Coursera, 2019
</file>

<file path="cookbook/pocketflow-map-reduce/data/resume5.txt">
Robert Taylor
Sales Representative

Education:
- Bachelor of Business Administration, University of Texas, 2017

Experience:
- Account Executive, Salesforce, 2019-present
  * Exceeded sales targets by 25% for three consecutive quarters
  * Managed a portfolio of 50+ enterprise clients
  * Developed and implemented strategic account plans

- Sales Associate, Oracle, 2017-2019
  * Generated new business opportunities through cold calling
  * Assisted senior sales representatives with client presentations

Skills:
- CRM Systems: Salesforce, HubSpot
- Communication: Negotiation, Public Speaking
- Tools: Microsoft Office Suite, Google Workspace

Achievements:
- Top Sales Representative Award, Q2 2020
- President's Club, 2021

Interests:
- Volunteer sales coach for local small businesses
- Member of Toastmasters International
</file>

<file path="cookbook/pocketflow-map-reduce/flow.py">
from pocketflow import Flow
from nodes import ReadResumesNode, EvaluateResumesNode, ReduceResultsNode

def create_resume_processing_flow():
    """Create a map-reduce flow for processing resumes."""
    # Create nodes
    read_resumes_node = ReadResumesNode()
    evaluate_resumes_node = EvaluateResumesNode()
    reduce_results_node = ReduceResultsNode()
    
    # Connect nodes
    read_resumes_node >> evaluate_resumes_node >> reduce_results_node
    
    # Create flow
    return Flow(start=read_resumes_node)
</file>

<file path="cookbook/pocketflow-map-reduce/main.py">
from flow import create_resume_processing_flow

def main():
    # Initialize shared store
    shared = {}
    
    # Create the resume processing flow
    resume_flow = create_resume_processing_flow()
    
    # Run the flow
    print("Starting resume qualification processing...")
    resume_flow.run(shared)
    
    # Display final summary information (additional to what's already printed in ReduceResultsNode)
    if "summary" in shared:
        print("\nDetailed evaluation results:")
        for filename, evaluation in shared.get("evaluations", {}).items():
            qualified = "âœ“" if evaluation.get("qualifies", False) else "âœ—"
            name = evaluation.get("candidate_name", "Unknown")
            print(f"{qualified} {name} ({filename})")
    
    print("\nResume processing complete!")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-map-reduce/nodes.py">
from pocketflow import Node, BatchNode
from utils import call_llm
import yaml
import os

class ReadResumesNode(Node):
    """Map phase: Read all resumes from the data directory into shared storage."""
    
    def exec(self, _):
        resume_files = {}
        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
        
        for filename in os.listdir(data_dir):
            if filename.endswith(".txt"):
                file_path = os.path.join(data_dir, filename)
                with open(file_path, 'r', encoding='utf-8') as file:
                    resume_files[filename] = file.read()
        
        return resume_files
    
    def post(self, shared, prep_res, exec_res):
        shared["resumes"] = exec_res
        return "default"


class EvaluateResumesNode(BatchNode):
    """Batch processing: Evaluate each resume to determine if the candidate qualifies."""
    
    def prep(self, shared):
        return list(shared["resumes"].items())
    
    def exec(self, resume_item):
        """Evaluate a single resume."""
        filename, content = resume_item
        
        prompt = f"""
Evaluate the following resume and determine if the candidate qualifies for an advanced technical role.
Criteria for qualification:
- At least a bachelor's degree in a relevant field
- At least 3 years of relevant work experience
- Strong technical skills relevant to the position

Resume:
{content}

Return your evaluation in YAML format:
```yaml
candidate_name: [Name of the candidate]
qualifies: [true/false]
reasons:
  - [First reason for qualification/disqualification]
  - [Second reason, if applicable]
```
"""
        response = call_llm(prompt)
        
        # Extract YAML content
        yaml_content = response.split("```yaml")[1].split("```")[0].strip() if "```yaml" in response else response
        result = yaml.safe_load(yaml_content)
        
        return (filename, result)

    def post(self, shared, prep_res, exec_res_list):
        shared["evaluations"] = {filename: result for filename, result in exec_res_list}
        return "default"


class ReduceResultsNode(Node):
    """Reduce node: Count and print out how many candidates qualify."""
    
    def prep(self, shared):
        return shared["evaluations"]
    
    def exec(self, evaluations):
        qualified_count = 0
        total_count = len(evaluations)
        qualified_candidates = []
        
        for filename, evaluation in evaluations.items():
            if evaluation.get("qualifies", False):
                qualified_count += 1
                qualified_candidates.append(evaluation.get("candidate_name", "Unknown"))
        
        summary = {
            "total_candidates": total_count,
            "qualified_count": qualified_count,
            "qualified_percentage": round(qualified_count / total_count * 100, 1) if total_count > 0 else 0,
            "qualified_names": qualified_candidates
        }
        
        return summary
    
    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        
        print("\n===== Resume Qualification Summary =====")
        print(f"Total candidates evaluated: {exec_res['total_candidates']}")
        print(f"Qualified candidates: {exec_res['qualified_count']} ({exec_res['qualified_percentage']}%)")
        
        if exec_res['qualified_names']:
            print("\nQualified candidates:")
            for name in exec_res['qualified_names']:
                print(f"- {name}")
        
        return "default"
</file>

<file path="cookbook/pocketflow-map-reduce/README.md">
# Resume Qualification - Map Reduce Example

A PocketFlow example that demonstrates how to implement a Map-Reduce pattern for processing and evaluating resumes.

## Features

- Read and process multiple resume files using a Map-Reduce pattern
- Evaluate each resume individually using an LLM with structured YAML output
- Determine if candidates qualify for technical roles based on specific criteria
- Aggregate results to generate qualification statistics and summaries

## Getting Started

1. Install the required dependencies:

```bash
pip install -r requirements.txt
```

2. Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=your_api_key_here
```

3. Run the application:

```bash
python main.py
```

## How It Works

The workflow follows a classic Map-Reduce pattern with three sequential nodes:

```mermaid
flowchart LR
    ReadResumes[Map: Read Resumese] --> EvaluateResumes[Batch: Evaluate Resumes]
    EvaluateResumes --> ReduceResults[Reduce: Aggregate Results]
```

Here's what each node does:

1. **ReadResumesNode (Map Phase)**: Reads all resume files from the data directory and stores them in the shared data store
2. **EvaluateResumesNode (Batch Processing)**: Processes each resume individually using an LLM to determine if candidates qualify
3. **ReduceResultsNode (Reduce Phase)**: Aggregates evaluation results and produces a summary of qualified candidates

## Files

- [`main.py`](./main.py): Main entry point for running the resume qualification workflow
- [`flow.py`](./flow.py): Defines the flow that connects the nodes
- [`nodes.py`](./nodes.py): Contains the node classes for each step in the workflow
- [`utils.py`](./utils.py): Utility functions including the LLM wrapper
- [`requirements.txt`](./requirements.txt): Lists the required dependencies
- [`data/`](./data/): Directory containing sample resume files for evaluation

## Example Output

```
Starting resume qualification processing...

===== Resume Qualification Summary =====
Total candidates evaluated: 5
Qualified candidates: 2 (40.0%)

Qualified candidates:
- Emily Johnson
- John Smith

Detailed evaluation results:
âœ— Michael Williams (resume3.txt)
âœ“ Emily Johnson (resume2.txt)
âœ— Lisa Chen (resume4.txt)
âœ— Robert Taylor (resume5.txt)
âœ“ John Smith (resume1.txt)

Resume processing complete!
```
</file>

<file path="cookbook/pocketflow-map-reduce/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
pyyaml>=6.0
</file>

<file path="cookbook/pocketflow-map-reduce/utils.py">
import os
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

# Example usage
if __name__ == "__main__":
    print(call_llm("Tell me a short joke"))
</file>

<file path="cookbook/pocketflow-mcp/main.py">
from pocketflow import Node, Flow
from utils import call_llm, get_tools, call_tool
import yaml
import sys

class GetToolsNode(Node):
    def prep(self, shared):
        """Initialize and get tools"""
        # The question is now passed from main via shared
        print("ðŸ” Getting available tools...")
        return "simple_server.py"

    def exec(self, server_path):
        """Retrieve tools from the MCP server"""
        tools = get_tools(server_path)
        return tools

    def post(self, shared, prep_res, exec_res):
        """Store tools and process to decision node"""
        tools = exec_res
        shared["tools"] = tools
        
        # Format tool information for later use
        tool_info = []
        for i, tool in enumerate(tools, 1):
            properties = tool.inputSchema.get('properties', {})
            required = tool.inputSchema.get('required', [])
            
            params = []
            for param_name, param_info in properties.items():
                param_type = param_info.get('type', 'unknown')
                req_status = "(Required)" if param_name in required else "(Optional)"
                params.append(f"    - {param_name} ({param_type}): {req_status}")
            
            tool_info.append(f"[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n" + "\n".join(params))
        
        shared["tool_info"] = "\n".join(tool_info)
        return "decide"

class DecideToolNode(Node):
    def prep(self, shared):
        """Prepare the prompt for LLM to process the question"""
        tool_info = shared["tool_info"]
        question = shared["question"]
        
        prompt = f"""
### CONTEXT
You are an assistant that can use tools via Model Context Protocol (MCP).

### ACTION SPACE
{tool_info}

### TASK
Answer this question: "{question}"

## NEXT ACTION
Analyze the question, extract any numbers or parameters, and decide which tool to use.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning about what the question is asking and what numbers to extract>
tool: <name of the tool to use>
reason: <why you chose this tool>
parameters:
    <parameter_name>: <parameter_value>
    <parameter_name>: <parameter_value>
```
IMPORTANT: 
1. Extract numbers from the question properly
2. Use proper indentation (4 spaces) for multi-line fields
3. Use the | character for multi-line text fields
"""
        return prompt

    def exec(self, prompt):
        """Call LLM to process the question and decide which tool to use"""
        print("ðŸ¤” Analyzing question and deciding which tool to use...")
        response = call_llm(prompt)
        return response

    def post(self, shared, prep_res, exec_res):
        """Extract decision from YAML and save to shared context"""
        try:
            yaml_str = exec_res.split("```yaml")[1].split("```")[0].strip()
            decision = yaml.safe_load(yaml_str)
            
            shared["tool_name"] = decision["tool"]
            shared["parameters"] = decision["parameters"]
            shared["thinking"] = decision.get("thinking", "")
            
            print(f"ðŸ’¡ Selected tool: {decision['tool']}")
            print(f"ðŸ”¢ Extracted parameters: {decision['parameters']}")
            
            return "execute"
        except Exception as e:
            print(f"âŒ Error parsing LLM response: {e}")
            print("Raw response:", exec_res)
            return None

class ExecuteToolNode(Node):
    def prep(self, shared):
        """Prepare tool execution parameters"""
        return shared["tool_name"], shared["parameters"]

    def exec(self, inputs):
        """Execute the chosen tool"""
        tool_name, parameters = inputs
        print(f"ðŸ”§ Executing tool '{tool_name}' with parameters: {parameters}")
        result = call_tool("simple_server.py", tool_name, parameters)
        return result

    def post(self, shared, prep_res, exec_res):
        print(f"\nâœ… Final Answer: {exec_res}")
        return "done"


if __name__ == "__main__":
    # Default question
    default_question = "What is 982713504867129384651 plus 73916582047365810293746529?"
    
    # Get question from command line if provided with --
    question = default_question
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            question = arg[2:]
            break
    
    print(f"ðŸ¤” Processing question: {question}")
    
    # Create nodes
    get_tools_node = GetToolsNode()
    decide_node = DecideToolNode()
    execute_node = ExecuteToolNode()
    
    # Connect nodes
    get_tools_node - "decide" >> decide_node
    decide_node - "execute" >> execute_node
    
    # Create and run flow
    flow = Flow(start=get_tools_node)
    shared = {"question": question}
    flow.run(shared)
</file>

<file path="cookbook/pocketflow-mcp/README.md">
# PocketFlow MCP Demo

This project shows how to build an agent that performs addition using PocketFlow and Model Context Protocol (MCP). It presents a comparison between using MCP and basic function calling approaches.

This implementation is based  on the tutorial: [MCP Simply Explained: Function Calling Rebranded or Genuine Breakthrough?](https://zacharyhuang.substack.com/p/mcp-simply-explained-function-calling)

## Features

- Mathematical operation tools through a simple terminal interface
- Integration with Model Context Protocol (MCP)
- Comparison between MCP and direct function calling
- **Simple toggle** between MCP and local function calling

## How to Run

1. Set your API key:
   ```bash
   export OPENAI_API_KEY="your-api-key-here"
   ```
   Or update it directly in `utils.py`

2. Install and run:
   ```bash
   pip install -r requirements.txt
   python main.py
   ```

## MCP vs Function Calling

To compare both approaches, this demo provides local function alternatives that don't require MCP:

- **Toggle with a simple flag:** Set `MCP = True` or `MCP = False` at the top of `utils.py` to switch between MCP and local implementations.
- No code changes needed! The application automatically uses either:
  - MCP server tools when `MCP = True`
  - Local function implementations when `MCP = False`

This allows you to see the difference between the two approaches while keeping the same workflow.

### Function Calling
- Functions are directly embedded in application code
- Each new tool requires modifying the application
- Tools are defined within the application itself

### MCP Approach
- Tools live in separate MCP servers
- Standard protocol for all tool interactions
- New tools can be added without changing the agent
- AI can interact with tools through a consistent interface

## How It Works

```mermaid
flowchart LR
    tools[GetToolsNode] -->|decide| decide[DecideToolNode]
    decide -->|execute| execute[ExecuteToolNode]
```

The agent uses PocketFlow to create a workflow where:
1. It takes user input about numbers
2. Connects to the MCP server for mathematical operations (or uses local functions based on the `MCP` flag)
3. Returns the result

## Files

- [`main.py`](./main.py): Implementation of the addition agent using PocketFlow
- [`utils.py`](./utils.py): Helper functions for API calls and MCP integration
- [`simple_server.py`](./simple_server.py): MCP server that provides the addition tool
</file>

<file path="cookbook/pocketflow-mcp/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
fastmcp
pyyaml
</file>

<file path="cookbook/pocketflow-mcp/simple_server.py">
from fastmcp import FastMCP

# Create a named server
mcp = FastMCP("Math Operations Server")

# Define mathematical operation tools
@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers together"""
    return a + b

@mcp.tool()
def subtract(a: int, b: int) -> int:
    """Subtract b from a"""
    return a - b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers together"""
    return a * b

@mcp.tool()
def divide(a: int, b: int) -> float:
    """Divide a by b"""
    if b == 0:
        raise ValueError("Division by zero is not allowed")
    return a / b

# Start the server
if __name__ == "__main__":
    mcp.run()
</file>

<file path="cookbook/pocketflow-mcp/utils.py">
from openai import OpenAI
import os
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Global flag to control whether to use MCP or local implementation
MCP = False

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

def get_tools(server_script_path=None):
    """Get available tools, either from MCP server or locally based on MCP global setting."""
    if MCP:
        return mcp_get_tools(server_script_path)
    else:
        return local_get_tools(server_script_path)
    
def mcp_get_tools(server_script_path):
    """Get available tools from an MCP server.
    """
    async def _get_tools():
        server_params = StdioServerParameters(
            command="python",
            args=[server_script_path]
        )
        
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                tools_response = await session.list_tools()
                return tools_response.tools
    
    return asyncio.run(_get_tools())

def local_get_tools(server_script_path=None):
    """A simple dummy implementation of get_tools without MCP."""
    tools = [
        {
            "name": "add",
            "description": "Add two numbers together",
            "inputSchema": {
                "properties": {
                    "a": {"type": "integer"},
                    "b": {"type": "integer"}
                },
                "required": ["a", "b"]
            }
        },
        {
            "name": "subtract",
            "description": "Subtract b from a",
            "inputSchema": {
                "properties": {
                    "a": {"type": "integer"},
                    "b": {"type": "integer"}
                },
                "required": ["a", "b"]
            }
        },
        {
            "name": "multiply",
            "description": "Multiply two numbers together",
            "inputSchema": {
                "properties": {
                    "a": {"type": "integer"},
                    "b": {"type": "integer"}
                },
                "required": ["a", "b"]
            }
        },
        {
            "name": "divide",
            "description": "Divide a by b",
            "inputSchema": {
                "properties": {
                    "a": {"type": "integer"},
                    "b": {"type": "integer"}
                },
                "required": ["a", "b"]
            }
        }
    ]

    class DictObject(dict):
        """A simple class that behaves both as a dictionary and as an object with attributes."""
        def __init__(self, data):
            super().__init__(data)
            for key, value in data.items():
                if isinstance(value, dict):
                    self[key] = DictObject(value)
                elif isinstance(value, list) and value and isinstance(value[0], dict):
                    self[key] = [DictObject(item) for item in value]
        
        def __getattr__(self, key):
            try:
                return self[key]
            except KeyError:
                raise AttributeError(f"'DictObject' object has no attribute '{key}'")

    return [DictObject(tool) for tool in tools]

def call_tool(server_script_path=None, tool_name=None, arguments=None):
    """Call a tool, either from MCP server or locally based on MCP global setting."""
    if MCP:
        return mcp_call_tool(server_script_path, tool_name, arguments)
    else:
        return local_call_tool(server_script_path, tool_name, arguments)
    
def mcp_call_tool(server_script_path=None, tool_name=None, arguments=None):
    """Call a tool on an MCP server.
    """
    async def _call_tool():
        server_params = StdioServerParameters(
            command="python",
            args=[server_script_path]
        )
        
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                result = await session.call_tool(tool_name, arguments)
                return result.content[0].text
    
    return asyncio.run(_call_tool())

def local_call_tool(server_script_path=None, tool_name=None, arguments=None):
    """A simple dummy implementation of call_tool without MCP."""
    # Simple implementation of tools
    if tool_name == "add":
        if "a" in arguments and "b" in arguments:
            return arguments["a"] + arguments["b"]
        else:
            return "Error: Missing required arguments 'a' or 'b'"
    elif tool_name == "subtract":
        if "a" in arguments and "b" in arguments:
            return arguments["a"] - arguments["b"]
        else:
            return "Error: Missing required arguments 'a' or 'b'"
    elif tool_name == "multiply":
        if "a" in arguments and "b" in arguments:
            return arguments["a"] * arguments["b"]
        else:
            return "Error: Missing required arguments 'a' or 'b'"
    elif tool_name == "divide":
        if "a" in arguments and "b" in arguments:
            if arguments["b"] == 0:
                return "Error: Division by zero is not allowed"
            return arguments["a"] / arguments["b"]
        else:
            return "Error: Missing required arguments 'a' or 'b'"
    else:
        return f"Error: Unknown tool '{tool_name}'"

if __name__ == "__main__":
    print("=== Testing call_llm ===")
    prompt = "In a few words, what is the meaning of life?"
    print(f"Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"Response: {response}")

        # Find available tools
    print("=== Finding available tools ===")
    tools = get_tools("simple_server.py")
    
    # Print tool information nicely formatted
    for i, tool in enumerate(tools, 1):
        print(f"\nTool {i}: {tool.name}")
        print("=" * (len(tool.name) + 8))
        print(f"Description: {tool.description}")
        
        # Parameters section
        print("Parameters:")
        properties = tool.inputSchema.get('properties', {})
        required = tool.inputSchema.get('required', [])
        
        # No parameters case
        if not properties:
            print("  None")
        
        # Print each parameter with its details
        for param_name, param_info in properties.items():
            param_type = param_info.get('type', 'unknown')
            req_status = "(Required)" if param_name in required else "(Optional)"
            print(f"  â€¢ {param_name}: {param_type} {req_status}")
    
    # Call a tool
    print("\n=== Calling the add tool ===")
    a, b = 5, 3
    result = call_tool("simple_server.py", "add", {"a": a, "b": b})
    print(f"Result of {a} + {b} = {result}")
    
    # You can easily call with different parameters
    a, b = 10, 20
    result = call_tool("simple_server.py", "add", {"a": a, "b": b})
    print(f"Result of {a} + {b} = {result}")
</file>

<file path="cookbook/pocketflow-multi-agent/main.py">
import asyncio
from pocketflow import AsyncNode, AsyncFlow
from utils import call_llm

class AsyncHinter(AsyncNode):
    async def prep_async(self, shared):
        # Wait for message from guesser (or empty string at start)
        guess = await shared["hinter_queue"].get()
        if guess == "GAME_OVER":
            return None
        return shared["target_word"], shared["forbidden_words"], shared.get("past_guesses", [])

    async def exec_async(self, inputs):
        if inputs is None:
            return None
        target, forbidden, past_guesses = inputs
        prompt = f"Generate hint for '{target}'\nForbidden words: {forbidden}"
        if past_guesses:
            prompt += f"\nPrevious wrong guesses: {past_guesses}\nMake hint more specific."
        prompt += "\nUse at most 5 words."
        
        hint = call_llm(prompt)
        print(f"\nHinter: Here's your hint - {hint}")
        return hint

    async def post_async(self, shared, prep_res, exec_res):
        if exec_res is None:
            return "end"
        # Send hint to guesser
        await shared["guesser_queue"].put(exec_res)
        return "continue"

class AsyncGuesser(AsyncNode):
    async def prep_async(self, shared):
        # Wait for hint from hinter
        hint = await shared["guesser_queue"].get()
        return hint, shared.get("past_guesses", [])

    async def exec_async(self, inputs):
        hint, past_guesses = inputs
        prompt = f"Given hint: {hint}, past wrong guesses: {past_guesses}, make a new guess. Directly reply a single word:"
        guess = call_llm(prompt)
        print(f"Guesser: I guess it's - {guess}")
        return guess

    async def post_async(self, shared, prep_res, exec_res):
        # Check if guess is correct
        if exec_res.lower() == shared["target_word"].lower():
            print("Game Over - Correct guess!")
            await shared["hinter_queue"].put("GAME_OVER")
            return "end"
            
        # Store the guess in shared state
        if "past_guesses" not in shared:
            shared["past_guesses"] = []
        shared["past_guesses"].append(exec_res)
        
        # Send guess to hinter
        await shared["hinter_queue"].put(exec_res)
        return "continue"

async def main():
    # Set up game
    shared = {
        "target_word": "nostalgic",
        "forbidden_words": ["memory", "past", "remember", "feeling", "longing"],
        "hinter_queue": asyncio.Queue(),
        "guesser_queue": asyncio.Queue()
    }
    
    print("=========== Taboo Game Starting! ===========")
    print(f"Target word: {shared['target_word']}")
    print(f"Forbidden words: {shared['forbidden_words']}")
    print("============================================")

    # Initialize by sending empty guess to hinter
    await shared["hinter_queue"].put("")

    # Create nodes and flows
    hinter = AsyncHinter()
    guesser = AsyncGuesser()

    # Set up flows
    hinter_flow = AsyncFlow(start=hinter)
    guesser_flow = AsyncFlow(start=guesser)

    # Connect nodes to themselves for looping
    hinter - "continue" >> hinter
    guesser - "continue" >> guesser

    # Run both agents concurrently
    await asyncio.gather(
        hinter_flow.run_async(shared),
        guesser_flow.run_async(shared)
    )
    
    print("=========== Game Complete! ===========")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-multi-agent/README.md">
# Multi-Agent Taboo Game

A PocketFlow example that demonstrates how to implement asynchronous multi-agent communication using the Taboo word guessing game.

## Features

- Implement asynchronous communication between two AI agents (Hinter and Guesser)
- Use AsyncNode for non-blocking agent interactions
- Create dynamic conversation flow through asyncio message queues
- Demonstrate complex turn-based game mechanics with LLMs
- Automatically terminate the game when the correct word is guessed

## Getting Started

1. Install the required dependencies:

```bash
pip install -r requirements.txt
```

2. Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=your_api_key_here
```

3. Run the application:

```bash
python main.py
```

## How It Works

The workflow follows an asynchronous multi-agent communication pattern:

```mermaid
flowchart LR
    AsyncHinter[AsyncHinter Node] <--> MessageQueue{Message Queue}
    MessageQueue <--> AsyncGuesser[AsyncGuesser Node]
```

Here's what each component does:

1. **AsyncHinter Node**: Generates hints about the target word while avoiding forbidden words
2. **AsyncGuesser Node**: Makes guesses based on the hints received from the Hinter
3. **Message Queue**: Facilitates asynchronous communication between the agents

## Files

- [`main.py`](./main.py): Main entry point implementing the AsyncHinter and AsyncGuesser nodes and game flow
- [`utils.py`](./utils.py): Utility functions including LLM wrappers for generating hints and guesses
- [`requirements.txt`](./requirements.txt): Lists the required dependencies

## Example Output

```
=========== Taboo Game Starting! ===========
Target word: nostalgic
Forbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']
============================================

Hinter: Here's your hint - Sentiment for earlier times.
Guesser: I guess it's - Nostalgia

Hinter: Here's your hint - Sentiment for earlier times.
Guesser: I guess it's - Reminiscence

Hinter: Here's your hint - Yearning for days gone by.
Guesser: I guess it's - Sentimentality

Hinter: Here's your hint - Reliving cherished moments or experiences.
Guesser: I guess it's - Memories

Hinter: Here's your hint - Recollection of cherished experiences.
Guesser: I guess it's - Reflection

Hinter: Here's your hint - Yearning for earlier times.
Guesser: I guess it's - Longing

Hinter: Here's your hint - Sentiment for earlier times.
Guesser: I guess it's - Nostalgic
Game Over - Correct guess!
</file>

<file path="cookbook/pocketflow-multi-agent/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
pyyaml>=6.0
</file>

<file path="cookbook/pocketflow-multi-agent/utils.py">
import os
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

# Example usage
if __name__ == "__main__":
    print(call_llm("Tell me a short joke"))
</file>

<file path="cookbook/pocketflow-nested-batch/school/class_a/student1.txt">
7.5
8.0
9.0
</file>

<file path="cookbook/pocketflow-nested-batch/school/class_a/student2.txt">
8.5
7.0
9.5
</file>

<file path="cookbook/pocketflow-nested-batch/school/class_b/student3.txt">
6.5
8.5
7.0
</file>

<file path="cookbook/pocketflow-nested-batch/school/class_b/student4.txt">
9.0
9.5
8.0
</file>

<file path="cookbook/pocketflow-nested-batch/flow.py">
import os
from pocketflow import Flow, BatchFlow
from nodes import LoadGrades, CalculateAverage

def create_base_flow():
    """Create base flow for processing one student's grades."""
    # Create nodes
    load = LoadGrades()
    calc = CalculateAverage()
    
    # Connect nodes
    load - "calculate" >> calc
    
    # Create and return flow
    return Flow(start=load)

class ClassBatchFlow(BatchFlow):
    """BatchFlow for processing all students in a class."""
    
    def prep(self, shared):
        """Generate parameters for each student in the class."""
        # Get class folder from parameters
        class_folder = self.params["class"]
        
        # List all student files
        class_path = os.path.join("school", class_folder)
        students = [f for f in os.listdir(class_path) if f.endswith(".txt")]
        
        # Return parameters for each student
        return [{"student": student} for student in students]
    
    def post(self, shared, prep_res, exec_res):
        """Calculate and print class average."""
        class_name = self.params["class"]
        class_results = shared["results"][class_name]
        class_average = sum(class_results.values()) / len(class_results)
        
        print(f"Class {class_name.split('_')[1].upper()} Average: {class_average:.2f}\n")
        return "default"

class SchoolBatchFlow(BatchFlow):
    """BatchFlow for processing all classes in the school."""
    
    def prep(self, shared):
        """Generate parameters for each class."""
        # List all class folders
        classes = [d for d in os.listdir("school") if os.path.isdir(os.path.join("school", d))]
        
        # Return parameters for each class
        return [{"class": class_name} for class_name in classes]
    
    def post(self, shared, prep_res, exec_res):
        """Calculate and print school average."""
        all_grades = []
        for class_results in shared["results"].values():
            all_grades.extend(class_results.values())
            
        school_average = sum(all_grades) / len(all_grades)
        print(f"School Average: {school_average:.2f}")
        return "default"

def create_flow():
    """Create the complete nested batch processing flow."""
    # Create base flow for single student
    base_flow = create_base_flow()
    
    # Wrap in ClassBatchFlow for processing all students in a class
    class_flow = ClassBatchFlow(start=base_flow)
    
    # Wrap in SchoolBatchFlow for processing all classes
    school_flow = SchoolBatchFlow(start=class_flow)
    
    return school_flow
</file>

<file path="cookbook/pocketflow-nested-batch/main.py">
import os
from flow import create_flow

def create_sample_data():
    """Create sample grade files."""
    # Create directory structure
    os.makedirs("school/class_a", exist_ok=True)
    os.makedirs("school/class_b", exist_ok=True)
    
    # Sample grades
    data = {
        "class_a": {
            "student1.txt": [7.5, 8.0, 9.0],
            "student2.txt": [8.5, 7.0, 9.5]
        },
        "class_b": {
            "student3.txt": [6.5, 8.5, 7.0],
            "student4.txt": [9.0, 9.5, 8.0]
        }
    }
    
    # Create files
    for class_name, students in data.items():
        for student, grades in students.items():
            file_path = os.path.join("school", class_name, student)
            with open(file_path, 'w') as f:
                for grade in grades:
                    f.write(f"{grade}\n")

def main():
    """Run the nested batch example."""
    # Create sample data
    create_sample_data()
    
    print("Processing school grades...\n")
    
    # Create and run flow
    flow = create_flow()
    flow.run({})

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-nested-batch/nodes.py">
import os
from pocketflow import Node

class LoadGrades(Node):
    """Node that loads grades from a student's file."""
    
    def prep(self, shared):
        """Get file path from parameters."""
        class_name = self.params["class"]
        student_file = self.params["student"]
        return os.path.join("school", class_name, student_file)
    
    def exec(self, file_path):
        """Load and parse grades from file."""
        with open(file_path, 'r') as f:
            # Each line is a grade
            grades = [float(line.strip()) for line in f]
        return grades
    
    def post(self, shared, prep_res, grades):
        """Store grades in shared store."""
        shared["grades"] = grades
        return "calculate"

class CalculateAverage(Node):
    """Node that calculates average grade."""
    
    def prep(self, shared):
        """Get grades from shared store."""
        return shared["grades"]
    
    def exec(self, grades):
        """Calculate average."""
        return sum(grades) / len(grades)
    
    def post(self, shared, prep_res, average):
        """Store and print result."""
        # Store in results dictionary
        if "results" not in shared:
            shared["results"] = {}
        
        class_name = self.params["class"]
        student = self.params["student"]
        
        if class_name not in shared["results"]:
            shared["results"][class_name] = {}
            
        shared["results"][class_name][student] = average
        
        # Print individual result
        print(f"- {student}: Average = {average:.1f}")
        return "default"
</file>

<file path="cookbook/pocketflow-nested-batch/README.md">
# PocketFlow Nested BatchFlow Example

This example demonstrates Nested BatchFlow using a simple school grades calculator.

## What this Example Does

Calculates average grades for:
1. Each student in a class
2. Each class in the school

## Structure
```
school/
â”œâ”€â”€ class_a/
â”‚   â”œâ”€â”€ student1.txt  (grades: 7.5, 8.0, 9.0)
â”‚   â””â”€â”€ student2.txt  (grades: 8.5, 7.0, 9.5)
â””â”€â”€ class_b/
    â”œâ”€â”€ student3.txt  (grades: 6.5, 8.5, 7.0)
    â””â”€â”€ student4.txt  (grades: 9.0, 9.5, 8.0)
```

## How it Works

1. **Outer BatchFlow (SchoolBatchFlow)**
   - Processes each class folder
   - Returns parameters like: `{"class": "class_a"}`

2. **Inner BatchFlow (ClassBatchFlow)**
   - Processes each student file in a class
   - Returns parameters like: `{"student": "student1.txt"}`

3. **Base Flow**
   - Loads student grades
   - Calculates average
   - Saves result

## Running the Example

```bash
pip install -r requirements.txt
python main.py
```

## Expected Output

```
Processing class_a...
- student1: Average = 8.2
- student2: Average = 8.3
Class A Average: 8.25

Processing class_b...
- student3: Average = 7.3
- student4: Average = 8.8
Class B Average: 8.05

School Average: 8.15
```

## Key Concepts

1. **Nested BatchFlow**: One BatchFlow inside another
2. **Parameter Inheritance**: Inner flow gets parameters from outer flow
3. **Hierarchical Processing**: Process data in a tree-like structure
</file>

<file path="cookbook/pocketflow-nested-batch/requirements.txt">
pocketflow
</file>

<file path="cookbook/pocketflow-node/utils/call_llm.py">
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content
    
if __name__ == "__main__":
    prompt = "What is the meaning of life?"
    print(call_llm(prompt))
</file>

<file path="cookbook/pocketflow-node/flow.py">
from pocketflow import Node, Flow
from utils.call_llm import call_llm

class Summarize(Node):
    def prep(self, shared):
        """Read and preprocess data from shared store."""
        return shared["data"]

    def exec(self, prep_res):
        """Execute the summarization using LLM."""
        if not prep_res:
            return "Empty text"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    def exec_fallback(self, shared, prep_res, exc):
        """Provide a simple fallback instead of crashing."""
        return "There was an error processing your request."

    def post(self, shared, prep_res, exec_res):
        """Store the summary in shared store."""
        shared["summary"] = exec_res
        # Return "default" by not returning

# Create the flow
summarize_node = Summarize(max_retries=3)
flow = Flow(start=summarize_node)
</file>

<file path="cookbook/pocketflow-node/main.py">
from flow import flow

def main():
    # Example text to summarize
    text = """
    PocketFlow is a minimalist LLM framework that models workflows as a Nested Directed Graph.
    Nodes handle simple LLM tasks, connecting through Actions for Agents.
    Flows orchestrate these nodes for Task Decomposition, and can be nested.
    It also supports Batch processing and Async execution.
    """

    # Initialize shared store
    shared = {"data": text}
    
    # Run the flow
    flow.run(shared)
    
    # Print result
    print("\nInput text:", text)
    print("\nSummary:", shared["summary"])

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-node/README.md">
# PocketFlow Summarize

A practical example demonstrating how to use PocketFlow to build a robust text summarization tool with error handling and retries. This example showcases core PocketFlow concepts in a real-world application.

## Features

- Text summarization using LLMs (Large Language Models)
- Automatic retry mechanism (up to 3 attempts) on API failures
- Graceful error handling with fallback responses
- Clean separation of concerns using PocketFlow's Node architecture

## Project Structure

```
.
â”œâ”€â”€ docs/          # Documentation files
â”œâ”€â”€ utils/         # Utility functions (LLM API wrapper)
â”œâ”€â”€ flow.py        # PocketFlow implementation with Summarize Node
â”œâ”€â”€ main.py        # Main application entry point
â””â”€â”€ README.md      # Project documentation
```

## Implementation Details

The example implements a simple but robust text summarization workflow:

1. **Summarize Node** (`flow.py`):
   - `prep()`: Retrieves text from the shared store
   - `exec()`: Calls LLM to summarize text in 10 words
   - `exec_fallback()`: Provides graceful error handling
   - `post()`: Stores the summary back in shared store

2. **Flow Structure**:
   - Single node flow for demonstration
   - Configured with 3 retries for reliability
   - Uses shared store for data passing

## Setup

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure your environment:
   - Set up your LLM API key (check utils/call_llm.py for configuration)

4. Run the example:
```bash
python main.py
```

## Example Usage

The example comes with a sample text about PocketFlow, but you can modify `main.py` to summarize your own text:

```python
shared = {"data": "Your text to summarize here..."}
flow.run(shared)
print("Summary:", shared["summary"])
```

## What You'll Learn

This example demonstrates several key PocketFlow concepts:

- **Node Architecture**: How to structure LLM tasks using prep/exec/post pattern
- **Error Handling**: Implementing retry mechanisms and fallbacks
- **Shared Store**: Using shared storage for data flow between steps
- **Flow Creation**: Setting up a basic PocketFlow workflow

## Additional Resources

- [PocketFlow Documentation](https://the-pocket.github.io/PocketFlow/)
- [Node Concept Guide](https://the-pocket.github.io/PocketFlow/node.html)
- [Flow Design Patterns](https://the-pocket.github.io/PocketFlow/flow.html)
</file>

<file path="cookbook/pocketflow-node/requirements.txt">
pocketflow
openai>=1.0.0
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_CHINESE.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | ä¸­æ–‡ | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow æ˜¯ä¸€ä¸ª[100è¡Œä»£ç ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)çš„æžç®€ä¸»ä¹‰LLMæ¡†æž¶

- **è½»é‡çº§**ï¼šä»…100è¡Œä»£ç ã€‚é›¶è‡ƒè‚¿ï¼Œé›¶ä¾èµ–ï¼Œé›¶ä¾›åº”å•†é”å®šã€‚
  
- **è¡¨è¾¾åŠ›å¼º**ï¼šåŒ…å«ä½ å–œçˆ±çš„ä¸€åˆ‡â€”([å¤š-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[æ™ºèƒ½ä½“](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ï¼Œ[å·¥ä½œæµ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ï¼Œ[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ç­‰ç­‰ã€‚

- **[æ™ºèƒ½ä½“ç¼–ç ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**ï¼šè®©AIæ™ºèƒ½ä½“ï¼ˆä¾‹å¦‚Cursor AIï¼‰æž„å»ºæ™ºèƒ½ä½“â€”ç”Ÿäº§åŠ›æå‡10å€ï¼

Pocket Flowå…¥é—¨ï¼š
- å®‰è£…æ–¹å¼ï¼Œ```pip install pocketflow```æˆ–è€…ç›´æŽ¥å¤åˆ¶[æºä»£ç ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ï¼ˆä»…100è¡Œï¼‰ã€‚
- äº†è§£æ›´å¤šï¼ŒæŸ¥çœ‹[æ–‡æ¡£](https://the-pocket.github.io/PocketFlow/)ã€‚äº†è§£åŠ¨æœºï¼Œé˜…è¯»[æ•…äº‹](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)ã€‚
- æœ‰é—®é¢˜ï¼ŸæŸ¥çœ‹è¿™ä¸ª[AIåŠ©æ‰‹](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant)ï¼Œæˆ–[åˆ›å»ºissueï¼](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ åŠ å…¥æˆ‘ä»¬çš„[Discord](https://discord.gg/hUHHE9Sa6T)ï¼Œä¸Žå…¶ä»–ä½¿ç”¨Pocket Flowæž„å»ºåº”ç”¨çš„å¼€å‘è€…äº¤æµï¼
- ðŸŽ‰ Pocket Flowæœ€åˆæ˜¯Pythonç‰ˆæœ¬ï¼Œä½†æˆ‘ä»¬çŽ°åœ¨æœ‰[Typescript](https://github.com/The-Pocket/PocketFlow-Typescript)ï¼Œ[Java](https://github.com/The-Pocket/PocketFlow-Java)ï¼Œ[C++](https://github.com/The-Pocket/PocketFlow-CPP)å’Œ[Go](https://github.com/The-Pocket/PocketFlow-Go)ç‰ˆæœ¬ï¼

## ä¸ºä»€ä¹ˆé€‰æ‹©Pocket Flowï¼Ÿ

å½“å‰çš„LLMæ¡†æž¶è¿‡äºŽè‡ƒè‚¿... ä½ åªéœ€è¦100è¡Œä»£ç å°±èƒ½æž„å»ºLLMæ¡†æž¶ï¼

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **æŠ½è±¡**          | **åº”ç”¨ç‰¹å®šåŒ…è£…å™¨**                                      | **ä¾›åº”å•†ç‰¹å®šåŒ…è£…å™¨**                                    | **ä»£ç è¡Œæ•°**       | **å¤§å°**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒQA, æ‘˜è¦)</sub></sup>              | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒOpenAI, Pineconeç­‰)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒFileReadTool, SerperDevTool)</sub></sup>         | å¾ˆå¤š <br><sup><sub>(ä¾‹å¦‚ï¼ŒOpenAI, Anthropic, Pineconeç­‰)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒCodeAgent, VisitWebTool)</sub></sup>         | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒDuckDuckGo, Hugging Faceç­‰)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼Œè¯­ä¹‰æœç´¢)</sub></sup>                     | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒPostgresStore, SqliteSaverç­‰) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | ä¸€äº› <br><sup><sub>(ä¾‹å¦‚ï¼ŒTool Agent, Chat Agent)</sub></sup>              | å¾ˆå¤š <sup><sub>[å¯é€‰]<br> (ä¾‹å¦‚ï¼ŒOpenAI, Pineconeç­‰)</sub></sup>        | 7K <br><sup><sub>(ä»…æ ¸å¿ƒ)</sub></sup>    | +26MB <br><sup><sub>(ä»…æ ¸å¿ƒ)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **æ— **                                                 | **æ— **                                                  | **100**       | **+56KB**                  |

</div>

## Pocket Flowå¦‚ä½•å·¥ä½œï¼Ÿ

è¿™[100è¡Œä»£ç ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)æ•æ‰äº†LLMæ¡†æž¶çš„æ ¸å¿ƒæŠ½è±¡ï¼šå›¾ï¼ˆGraphï¼‰ï¼
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ä»Žè¿™é‡Œå¼€å§‹ï¼Œå¾ˆå®¹æ˜“å®žçŽ°æµè¡Œçš„è®¾è®¡æ¨¡å¼ï¼Œå¦‚([å¤š-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[æ™ºèƒ½ä½“](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ï¼Œ[å·¥ä½œæµ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ï¼Œ[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ç­‰ã€‚
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ä»¥ä¸‹æ˜¯åŸºç¡€æ•™ç¨‹ï¼š

<div align="center">
  
|  åç§°  | éš¾åº¦    |  æè¿°  |  
| :-------------:  | :-------------: | :--------------------- |  
| [èŠå¤©](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *å…¥é—¨*   | å¸¦æœ‰å¯¹è¯åŽ†å²çš„åŸºç¡€èŠå¤©æœºå™¨äºº |
| [ç»“æž„åŒ–è¾“å‡º](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *å…¥é—¨* | é€šè¿‡æç¤ºä»Žç®€åŽ†ä¸­æå–ç»“æž„åŒ–æ•°æ® |
| [å·¥ä½œæµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *å…¥é—¨*   | ä¸€ä¸ªå†™ä½œå·¥ä½œæµï¼ŒåŒ…æ‹¬å¤§çº²ç¼–å†™ã€å†…å®¹åˆ›ä½œå’Œæ ·å¼åº”ç”¨ |
| [æ™ºèƒ½ä½“](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *å…¥é—¨*   | ä¸€ä¸ªå¯ä»¥æœç´¢ç½‘ç»œå¹¶å›žç­”é—®é¢˜çš„ç ”ç©¶æ™ºèƒ½ä½“ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *å…¥é—¨*   | ä¸€ä¸ªç®€å•çš„æ£€ç´¢å¢žå¼ºç”Ÿæˆè¿‡ç¨‹ |
| [æ‰¹å¤„ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *å…¥é—¨* | ä¸€ä¸ªå°†markdownå†…å®¹ç¿»è¯‘æˆå¤šç§è¯­è¨€çš„æ‰¹å¤„ç†å™¨ |
| [æµå¼å¤„ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *å…¥é—¨*   | å…·æœ‰ç”¨æˆ·ä¸­æ–­åŠŸèƒ½çš„å®žæ—¶LLMæµå¼æ¼”ç¤º |
| [èŠå¤©æŠ¤æ ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *å…¥é—¨*  | ä¸€ä¸ªä»…å¤„ç†æ—…è¡Œç›¸å…³æŸ¥è¯¢çš„æ—…è¡Œé¡¾é—®èŠå¤©æœºå™¨äºº |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *åˆçº§* | ä½¿ç”¨map-reduceæ¨¡å¼è¿›è¡Œæ‰¹é‡è¯„ä¼°çš„ç®€åŽ†èµ„æ ¼å¤„ç†å™¨ |
| [å¤šæ™ºèƒ½ä½“](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *åˆçº§* | ä¸¤ä¸ªæ™ºèƒ½ä½“ä¹‹é—´å¼‚æ­¥é€šä¿¡çš„ç¦å¿Œè¯æ¸¸æˆ |
| [ç›‘ç£è€…](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *åˆçº§* | ç ”ç©¶æ™ºèƒ½ä½“å˜å¾—ä¸å¯é ...è®©æˆ‘ä»¬æž„å»ºä¸€ä¸ªç›‘ç£æµç¨‹|
| [å¹¶è¡Œ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *åˆçº§*   | å±•ç¤º3å€åŠ é€Ÿçš„å¹¶è¡Œæ‰§è¡Œæ¼”ç¤º |
| [å¹¶è¡Œæµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *åˆçº§*   | å±•ç¤ºä½¿ç”¨å¤šä¸ªè¿‡æ»¤å™¨å®žçŽ°8å€åŠ é€Ÿçš„å¹¶è¡Œå›¾åƒå¤„ç†æ¼”ç¤º |
| [å¤šæ•°æŠ•ç¥¨](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *åˆçº§* | é€šè¿‡èšåˆå¤šæ¬¡è§£å†³æ–¹æ¡ˆå°è¯•æ¥æé«˜æŽ¨ç†å‡†ç¡®æ€§ |
| [æ€è€ƒ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *åˆçº§*   | é€šè¿‡æ€ç»´é“¾è§£å†³å¤æ‚æŽ¨ç†é—®é¢˜ |
| [è®°å¿†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *åˆçº§* | å…·æœ‰çŸ­æœŸå’Œé•¿æœŸè®°å¿†çš„èŠå¤©æœºå™¨äºº |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *åˆçº§* | ä½¿ç”¨è‡ªåŠ¨è°ƒè¯•å¾ªçŽ¯å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢ |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *åˆçº§* |  ä½¿ç”¨æ¨¡åž‹ä¸Šä¸‹æ–‡åè®®è¿›è¡Œæ•°å€¼è¿ç®—çš„æ™ºèƒ½ä½“ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *åˆçº§* | ä½¿ç”¨æ™ºèƒ½ä½“åˆ°æ™ºèƒ½ä½“åè®®åŒ…è£…çš„æ™ºèƒ½ä½“ï¼Œç”¨äºŽæ™ºèƒ½ä½“é—´é€šä¿¡ |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *åˆçº§* | å…·æœ‰SSEæ›´æ–°çš„äººå·¥å®¡æ ¸å¾ªçŽ¯çš„æœ€å°WebæœåŠ¡ |

</div>

ðŸ‘€ æƒ³çœ‹å…¶ä»–å…¥é—¨æ•™ç¨‹ï¼Ÿ[åˆ›å»ºä¸€ä¸ªissueï¼](https://github.com/The-Pocket/PocketFlow/issues/new)

## å¦‚ä½•ä½¿ç”¨Pocket Flowï¼Ÿ

ðŸš€ é€šè¿‡**æ™ºèƒ½ä½“ç¼–ç **â€”æœ€å¿«çš„LLMåº”ç”¨å¼€å‘èŒƒå¼â€”*äººç±»è®¾è®¡*ï¼Œ*æ™ºèƒ½ä½“ç¼–ç *ï¼

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ä»¥ä¸‹æ˜¯æ›´å¤æ‚LLMåº”ç”¨çš„ç¤ºä¾‹ï¼š

<div align="center">
  
|  åº”ç”¨åç§°     |  éš¾åº¦    | ä¸»é¢˜  | äººç±»è®¾è®¡ | æ™ºèƒ½ä½“ä»£ç  |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [ç”¨Cursoræž„å»ºCursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>æˆ‘ä»¬å¾ˆå¿«å°†è¾¾åˆ°å¥‡ç‚¹...</sup></sub> | â˜…â˜…â˜… <br> *é«˜çº§*   | [æ™ºèƒ½ä½“](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ä»£ç åº“çŸ¥è¯†æž„å»ºå™¨](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>ç”Ÿå‘½å¤ªçŸ­æš‚ï¼Œä¸åº”è¯¥å›°æƒ‘åœ°ç›¯ç€ä»–äººçš„ä»£ç </sup></sub> |  â˜…â˜…â˜† <br> *ä¸­çº§* | [å·¥ä½œæµ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [è¯¢é—®AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>è¯¢é—®AI Paul Grahamï¼Œä»¥é˜²ä½ æ²¡è¢«å½•å–</sup></sub> | â˜…â˜…â˜† <br> *ä¸­çº§*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtubeæ‘˜è¦å™¨](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> åƒä½ 5å²ä¸€æ ·å‘ä½ è§£é‡ŠYouTubeè§†é¢‘ </sup></sub> | â˜…â˜†â˜† <br> *åˆçº§*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [å†·å¯åŠ¨ç”Ÿæˆå™¨](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> å°†å†·é—¨çº¿ç´¢è½¬å˜ä¸ºçƒ­é—¨çš„å³æ—¶ç ´å†°å·¥å…· </sup></sub> | â˜…â˜†â˜† <br> *åˆçº§*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Webæœç´¢](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [è®¾è®¡æ–‡æ¡£](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flowä»£ç ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- æƒ³å­¦ä¹ **æ™ºèƒ½ä½“ç¼–ç **ï¼Ÿ

  - æŸ¥çœ‹[æˆ‘çš„YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)èŽ·å–å…³äºŽå¦‚ä½•åˆ¶ä½œä¸Šè¿°åº”ç”¨çš„è§†é¢‘æ•™ç¨‹ï¼

  - æƒ³æž„å»ºè‡ªå·±çš„LLMåº”ç”¨ï¼Ÿé˜…è¯»è¿™ç¯‡[æ–‡ç« ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)ï¼ä»Ž[è¿™ä¸ªæ¨¡æ¿](https://github.com/The-Pocket/PocketFlow-Template-Python)å¼€å§‹ï¼
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_FRENCH.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ framework LLM minimaliste en 100 lignes" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | FranÃ§ais | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow est un framework LLM minimaliste en [100 lignes](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **LÃ©ger** : Seulement 100 lignes. ZÃ©ro superflu, zÃ©ro dÃ©pendance, zÃ©ro verrouillage fournisseur.
  
- **Expressif** : Tout ce que vous aimez â€” ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), et plus encore.

- **[Programmation Agentique](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)** : Laissez les Agents IA (par exemple, Cursor AI) crÃ©er des Agents â€” augmentez votre productivitÃ© par 10 !

Commencer avec Pocket Flow :
- Pour installer, ```pip install pocketflow``` ou copiez simplement le [code source](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (seulement 100 lignes).
- Pour en savoir plus, consultez la [documentation](https://the-pocket.github.io/PocketFlow/). Pour comprendre la motivation, lisez l'[histoire](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Des questions ? Consultez cet [Assistant IA](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), ou [crÃ©ez une issue !](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Rejoignez notre [Discord](https://discord.gg/hUHHE9Sa6T) pour vous connecter avec d'autres dÃ©veloppeurs utilisant Pocket Flow !
- ðŸŽ‰ Pocket Flow est initialement en Python, mais nous avons maintenant des versions en [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) et [Go](https://github.com/The-Pocket/PocketFlow-Go) !

## Pourquoi Pocket Flow ?

Les frameworks LLM actuels sont surchargÃ©s... Vous n'avez besoin que de 100 lignes pour un framework LLM !

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **Abstraction**          | **Wrappers spÃ©cifiques aux applications**                                      | **Wrappers spÃ©cifiques aux fournisseurs**                                    | **Lignes**       | **Taille**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | Nombreux <br><sup><sub>(ex., QA, RÃ©sumÃ©)</sub></sup>              | Nombreux <br><sup><sub>(ex., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | Nombreux <br><sup><sub>(ex., FileReadTool, SerperDevTool)</sub></sup>         | Nombreux <br><sup><sub>(ex., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | Quelques <br><sup><sub>(ex., CodeAgent, VisitWebTool)</sub></sup>         | Quelques <br><sup><sub>(ex., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | Quelques <br><sup><sub>(ex., Recherche SÃ©mantique)</sub></sup>                     | Quelques <br><sup><sub>(ex., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | Quelques <br><sup><sub>(ex., Tool Agent, Chat Agent)</sub></sup>              | Nombreux <sup><sub>[Optionnel]<br> (ex., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(core-only)</sub></sup>    | +26MB <br><sup><sub>(core-only)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **Aucun**                                                 | **Aucun**                                                  | **100**       | **+56KB**                  |

</div>

## Comment fonctionne Pocket Flow ?

Les [100 lignes](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturent l'abstraction fondamentale des frameworks LLM : le Graph !
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

De lÃ , il est facile d'implÃ©menter des modÃ¨les de conception populaires comme ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Voici des tutoriels de base :

<div align="center">
  
|  Nom  | DifficultÃ©    |  Description  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un chatbot basique avec historique de conversation |
| [Sortie StructurÃ©e](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *DÃ©butant* | Extraction de donnÃ©es structurÃ©es Ã  partir de CV par prompt |
| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un workflow d'Ã©criture qui planifie, rÃ©dige du contenu et applique un style |
| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un agent de recherche qui peut chercher sur le web et rÃ©pondre aux questions |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *DÃ©butant*   | Un processus simple de gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration |
| [Batch](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *DÃ©butant* | Un processeur par lots qui traduit du contenu markdown en plusieurs langues |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *DÃ©butant*   | Une dÃ©mo de streaming LLM en temps rÃ©el avec capacitÃ© d'interruption utilisateur |
| [Garde-fou de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *DÃ©butant*  | Un chatbot conseiller de voyage qui ne traite que les requÃªtes liÃ©es au voyage |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un processeur de qualification de CV utilisant le modÃ¨le map-reduce pour l'Ã©valuation par lots |
| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un jeu de Tabou pour la communication asynchrone entre deux agents |
| [Superviseur](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | L'agent de recherche devient peu fiable... Construisons un processus de supervision |
| [ParallÃ¨le](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | Une dÃ©mo d'exÃ©cution parallÃ¨le montrant une accÃ©lÃ©ration de 3x |
| [Flux ParallÃ¨le](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | Une dÃ©mo de traitement d'image parallÃ¨le montrant une accÃ©lÃ©ration de 8x avec plusieurs filtres |
| [Vote Majoritaire](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | AmÃ©liorer la prÃ©cision du raisonnement en agrÃ©geant plusieurs tentatives de solution |
| [RÃ©flexion](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | RÃ©soudre des problÃ¨mes de raisonnement complexes grÃ¢ce Ã  la ChaÃ®ne de PensÃ©e |
| [MÃ©moire](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un chatbot avec mÃ©moire Ã  court et long terme |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Convertir le langage naturel en requÃªtes SQL avec une boucle d'auto-dÃ©bogage |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *IntermÃ©diaire* |  Agent utilisant le Protocole de Contexte de ModÃ¨le pour les opÃ©rations numÃ©riques |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Agent encapsulÃ© avec le protocole Agent-to-Agent pour la communication inter-agent |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *IntermÃ©diaire* | Un service web minimal pour une boucle de rÃ©vision humaine avec mises Ã  jour SSE |

</div>

ðŸ‘€ Vous voulez voir d'autres tutoriels pour dÃ©butants ? [CrÃ©ez une issue !](https://github.com/The-Pocket/PocketFlow/issues/new)

## Comment utiliser Pocket Flow ?

ðŸš€ Par la **Programmation Agentique** â€” le paradigme de dÃ©veloppement d'applications LLM le plus rapide â€” oÃ¹ *les humains conÃ§oivent* et *les agents programment* !

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Voici des exemples d'applications LLM plus complexes :

<div align="center">
  
|  Nom de l'application     |  DifficultÃ©    | Sujets  | Conception Humaine | Code Agent |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Construire Cursor avec Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Nous atteindrons bientÃ´t la singularitÃ© ...</sup></sub> | â˜…â˜…â˜… <br> *AvancÃ©*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Constructeur de Connaissances de Base de Code](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>La vie est trop courte pour rester perplexe devant le code des autres</sup></sub> |  â˜…â˜…â˜† <br> *Moyen* | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Interroger l'IA Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Interrogez l'IA Paul Graham, au cas oÃ¹ vous ne seriez pas acceptÃ©</sup></sub> | â˜…â˜…â˜† <br> *Moyen*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Document de conception](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [RÃ©sumeur Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Vous explique les vidÃ©os YouTube comme si vous aviez 5 ans </sup></sub> | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Document de conception](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [GÃ©nÃ©rateur d'Accroche pour Email](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Des brise-glaces instantanÃ©s qui transforment les prospects froids en prospects chauds </sup></sub> | â˜…â˜†â˜† <br> *IntermÃ©diaire*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Recherche Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Document de conception](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Code Flow](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Vous voulez apprendre la **Programmation Agentique** ?

  - Consultez [ma chaÃ®ne YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) pour des tutoriels vidÃ©o sur la faÃ§on dont certaines applications ci-dessus sont crÃ©Ã©es !

  - Vous voulez crÃ©er votre propre application LLM ? Lisez cet [article](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to) ! Commencez avec [ce modÃ¨le](https://github.com/The-Pocket/PocketFlow-Template-Python) !
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_GERMAN.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-Zeilen minimalistisches LLM-Framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | Deutsch | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![Lizenz: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow ist ein [100-zeiliges](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalistisches LLM-Framework

- **Leichtgewichtig**: Nur 100 Zeilen. Kein Ballast, keine AbhÃ¤ngigkeiten, keine Anbieterbindung.
  
- **Ausdrucksstark**: Alles, was Sie liebenâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agenten](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), und mehr.

- **[Agenten-basiertes Programmieren](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Lassen Sie KI-Agenten (z.B. Cursor AI) Agenten bauenâ€”10-fache ProduktivitÃ¤tssteigerung!

Erste Schritte mit Pocket Flow:
- Zur Installation, ```pip install pocketflow```oder kopieren Sie einfach den [Quellcode](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (nur 100 Zeilen).
- Um mehr zu erfahren, schauen Sie in die [Dokumentation](https://the-pocket.github.io/PocketFlow/). Um die Motivation zu verstehen, lesen Sie die [Geschichte](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Haben Sie Fragen? Schauen Sie sich diesen [KI-Assistenten](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant) an, oder [erstellen Sie ein Issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Treten Sie unserem [Discord](https://discord.gg/hUHHE9Sa6T) bei, um sich mit anderen Entwicklern zu vernetzen, die mit Pocket Flow arbeiten!
- ðŸŽ‰ Pocket Flow ist ursprÃ¼nglich in Python geschrieben, aber wir haben jetzt auch Versionen fÃ¼r [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) und [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## Warum Pocket Flow?

Aktuelle LLM-Frameworks sind aufgeblÃ¤ht... Sie brauchen nur 100 Zeilen fÃ¼r ein LLM-Framework!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **Abstraktion**          | **App-spezifische Wrapper**                                      | **Anbieter-spezifische Wrapper**                                    | **Zeilen**       | **GrÃ¶ÃŸe**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | Viele <br><sup><sub>(z.B. QA, Zusammenfassung)</sub></sup>              | Viele <br><sup><sub>(z.B. OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | Viele <br><sup><sub>(z.B. FileReadTool, SerperDevTool)</sub></sup>         | Viele <br><sup><sub>(z.B. OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | Einige <br><sup><sub>(z.B. CodeAgent, VisitWebTool)</sub></sup>         | Einige <br><sup><sub>(z.B. DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | Einige <br><sup><sub>(z.B. Semantic Search)</sub></sup>                     | Einige <br><sup><sub>(z.B. PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | Einige <br><sup><sub>(z.B. Tool Agent, Chat Agent)</sub></sup>              | Viele <sup><sub>[Optional]<br> (z.B. OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(nur Kern)</sub></sup>    | +26MB <br><sup><sub>(nur Kern)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **Keine**                                                 | **Keine**                                                  | **100**       | **+56KB**                  |

</div>

## Wie funktioniert Pocket Flow?

Die [100 Zeilen](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) erfassen die Kernabstraktion von LLM-Frameworks: Graph!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

Von dort aus ist es einfach, beliebte Designmuster wie ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agenten](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc. zu implementieren.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Hier sind grundlegende Tutorials:

<div align="center">
  
|  Name  | Schwierigkeit    |  Beschreibung  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein einfacher Chatbot mit GesprÃ¤chsverlauf |
| [Strukturierte Ausgabe](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *AnfÃ¤nger* | Extraktion strukturierter Daten aus LebenslÃ¤ufen durch Prompting |
| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein Schreib-Workflow, der gliedert, Inhalte schreibt und Formatierungen anwendet |
| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein Recherche-Agent, der im Web suchen und Fragen beantworten kann |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Ein einfacher Abrufsaugmentierter Generierungsprozess |
| [Batch](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *AnfÃ¤nger* | Ein Batch-Prozessor, der Markdown-Inhalte in mehrere Sprachen Ã¼bersetzt |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *AnfÃ¤nger*   | Eine Echtzeit-LLM-Streaming-Demo mit Benutzer-Unterbrechungsfunktion |
| [Chat-Leitplanke](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *AnfÃ¤nger*  | Ein Reiseberater-Chatbot, der nur reisebezogene Anfragen verarbeitet |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *Einsteiger* | Ein Lebenslauf-Qualifikationsprozessor, der das Map-Reduce-Muster fÃ¼r Batch-Auswertungen verwendet |
| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Einsteiger* | Ein Tabu-Wortspiel fÃ¼r asynchrone Kommunikation zwischen zwei Agenten |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Einsteiger* | Forschungsagent wird unzuverlÃ¤ssig... Bauen wir einen Ãœberwachungsprozess auf|
| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Einsteiger*   | Eine parallele AusfÃ¼hrungsdemo, die 3-fache Beschleunigung zeigt |
| [Paralleler Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Einsteiger*   | Eine parallele Bildverarbeitungsdemo, die 8-fache Beschleunigung mit mehreren Filtern zeigt |
| [Mehrheitswahl](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *Einsteiger* | Verbesserte Schlussfolgerungsgenauigkeit durch Aggregation mehrerer LÃ¶sungsversuche |
| [Denken](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Einsteiger*   | LÃ¶sen komplexer Schlussfolgerungsprobleme durch Chain-of-Thought |
| [GedÃ¤chtnis](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Einsteiger* | Ein Chatbot mit Kurz- und LangzeitgedÃ¤chtnis |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *Einsteiger* | Konvertierung natÃ¼rlicher Sprache in SQL-Abfragen mit Auto-Debug-Schleife |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Einsteiger* |  Agent mit Model Context Protocol fÃ¼r numerische Operationen |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *Einsteiger* | Agent mit Agent-to-Agent-Protokoll fÃ¼r Inter-Agenten-Kommunikation |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *Einsteiger* | Ein minimaler Webdienst fÃ¼r eine menschliche ÃœberprÃ¼fungsschleife mit SSE-Updates |

</div>

ðŸ‘€ MÃ¶chten Sie andere Tutorials fÃ¼r AnfÃ¤nger sehen? [Erstellen Sie ein Issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Wie verwendet man Pocket Flow?

ðŸš€ Durch **Agenten-basiertes Programmieren**â€”das schnellste LLM-App-Entwicklungsparadigma, bei dem *Menschen entwerfen* und *Agenten programmieren*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Hier sind Beispiele fÃ¼r komplexere LLM-Apps:

<div align="center">
  
|  App-Name     |  Schwierigkeit    | Themen  | Menschlicher Entwurf | Agent-Code |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Cursor mit Cursor bauen](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Wir werden bald die SingularitÃ¤t erreichen ...</sup></sub> | â˜…â˜…â˜… <br> *Fortgeschritten*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Codebase-Wissensgenerator](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>Das Leben ist zu kurz, um ratlos fremden Code anzustarren</sup></sub> |  â˜…â˜…â˜† <br> *Mittel* | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Frage KI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Frage KI Paul Graham, falls du nicht reinkommst</sup></sub> | â˜…â˜…â˜† <br> *Mittel*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Design-Dokument](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtube-Zusammenfasser](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> ErklÃ¤rt YouTube-Videos so, als wÃ¤rst du 5 </sup></sub> | â˜…â˜†â˜† <br> *Einsteiger*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Design-Dokument](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Cold-Opener-Generator](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Sofortige Eisbrecher, die kalte Leads heiÃŸ machen </sup></sub> | â˜…â˜†â˜† <br> *Einsteiger*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Web-Suche](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Design-Dokument](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flow-Code](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- MÃ¶chten Sie **Agenten-basiertes Programmieren** lernen?

  - Schauen Sie sich [meinen YouTube-Kanal](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) fÃ¼r Video-Tutorials an, wie einige der oben genannten Apps erstellt wurden!

  - MÃ¶chten Sie Ihre eigene LLM-App erstellen? Lesen Sie diesen [Beitrag](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Beginnen Sie mit [dieser Vorlage](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_JAPANESE.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100è¡Œã®ãƒŸãƒ‹ãƒžãƒªã‚¹ãƒˆLLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

English | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flowã¯[ãŸã£ãŸ100è¡Œ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ã®ãƒŸãƒ‹ãƒžãƒªã‚¹ãƒˆLLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™

- **è»½é‡**: ã‚ãšã‹100è¡Œã€‚ä½™åˆ†ãªã‚‚ã®ãªã—ã€ä¾å­˜é–¢ä¿‚ãªã—ã€ãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ãªã—ã€‚
  
- **è¡¨ç¾åŠ›è±Šã‹**: ã‚ãªãŸãŒæ„›ã™ã‚‹ã™ã¹ã¦ã®ã‚‚ã®â€”([ãƒžãƒ«ãƒ](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ã€[ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ã€[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ãªã©ã€‚

- **[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆä¾‹ï¼šCursor AIï¼‰ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã•ã›ã‚‹â€”ç”Ÿç”£æ€§ãŒ10å€å‘ä¸Šï¼

Pocket Flowã‚’å§‹ã‚ã‚‹ã«ã¯ï¼š
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€```pip install pocketflow```ã¾ãŸã¯[ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ï¼ˆã‚ãšã‹100è¡Œï¼‰ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã ã‘ã§ã™ã€‚
- è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/)ã‚’ã”è¦§ãã ã•ã„ã€‚é–‹ç™ºã®å‹•æ©Ÿã«ã¤ã„ã¦ã¯ã€[ã‚¹ãƒˆãƒ¼ãƒªãƒ¼](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚
- è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã“ã®[AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant)ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‹ã€[å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ [Discord](https://discord.gg/hUHHE9Sa6T)ã«å‚åŠ ã—ã¦ã€Pocket Flowã§é–‹ç™ºã—ã¦ã„ã‚‹ä»–ã®é–‹ç™ºè€…ã¨ã¤ãªãŒã‚Šã¾ã—ã‚‡ã†ï¼
- ðŸŽ‰ Pocket Flowã¯æœ€åˆã¯Pythonã§ã™ãŒã€ç¾åœ¨ã¯[Typescript](https://github.com/The-Pocket/PocketFlow-Typescript)ã€[Java](https://github.com/The-Pocket/PocketFlow-Java)ã€[C++](https://github.com/The-Pocket/PocketFlow-CPP)ã€[Go](https://github.com/The-Pocket/PocketFlow-Go)ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚ã‚ã‚Šã¾ã™ï¼

## ãªãœPocket Flowï¼Ÿ

ç¾åœ¨ã®LLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯è†¨å¤§ã™ãŽã¾ã™... LLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã¯100è¡Œã ã‘ã§ååˆ†ã§ã™ï¼

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **æŠ½è±¡åŒ–**          | **ã‚¢ãƒ—ãƒªå›ºæœ‰ã®ãƒ©ãƒƒãƒ‘ãƒ¼**                                      | **ãƒ™ãƒ³ãƒ€ãƒ¼å›ºæœ‰ã®ãƒ©ãƒƒãƒ‘ãƒ¼**                                    | **è¡Œæ•°**       | **ã‚µã‚¤ã‚º**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒã‚§ãƒ¼ãƒ³               | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šQAã€è¦ç´„)</sub></sup>              | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šOpenAIã€Pineconeãªã©)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒã‚§ãƒ¼ãƒ³            | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šFileReadToolã€SerperDevTool)</sub></sup>         | å¤šæ•° <br><sup><sub>(ä¾‹ï¼šOpenAIã€Anthropicã€Pineconeãªã©)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ                      | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šCodeAgentã€VisitWebTool)</sub></sup>         | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šDuckDuckGoã€Hugging Faceãªã©)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã‚°ãƒ©ãƒ•           | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢)</sub></sup>                     | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šPostgresStoreã€SqliteSaverãªã©) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ                | ä¸€éƒ¨ <br><sup><sub>(ä¾‹ï¼šãƒ„ãƒ¼ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)</sub></sup>              | å¤šæ•° <sup><sub>[ã‚ªãƒ—ã‚·ãƒ§ãƒ³]<br> (ä¾‹ï¼šOpenAIã€Pineconeãªã©)</sub></sup>        | 7K <br><sup><sub>(ã‚³ã‚¢ã®ã¿)</sub></sup>    | +26MB <br><sup><sub>(ã‚³ã‚¢ã®ã¿)</sub></sup>          |
| **PocketFlow** | **ã‚°ãƒ©ãƒ•**                    | **ãªã—**                                                 | **ãªã—**                                                  | **100**       | **+56KB**                  |

</div>

## Pocket Flowã¯ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ

[100è¡Œ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ãŒLLMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸­æ ¸çš„æŠ½è±¡åŒ–ã‚’æ‰ãˆã¦ã„ã¾ã™ï¼šã‚°ãƒ©ãƒ•ï¼
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ãã“ã‹ã‚‰ã€([ãƒžãƒ«ãƒ](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html)ã€[ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html)ã€[RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html)ãªã©ã®äººæ°—ã®ã‚ã‚‹ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç°¡å˜ã«å®Ÿè£…ã§ãã¾ã™ã€‚
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ä»¥ä¸‹ã¯åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã™ï¼š

<div align="center">
  
|  åå‰  | é›£æ˜“åº¦    |  èª¬æ˜Ž  |  
| :-------------:  | :-------------: | :--------------------- |  
| [ãƒãƒ£ãƒƒãƒˆ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ä¼šè©±å±¥æ­´ã‚’æŒã¤åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ |
| [æ§‹é€ åŒ–å‡ºåŠ›](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜* | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã£ã¦å±¥æ­´æ›¸ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ |
| [ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ä½œæˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã€ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨ã‚’è¡Œã†ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ |
| [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ã‚¦ã‚§ãƒ–ã‚’æ¤œç´¢ã—ã¦è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã‚‹èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢æ‹¡å¼µç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ |
| [ãƒãƒƒãƒå‡¦ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜* | ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¤‡æ•°ã®è¨€èªžã«ç¿»è¨³ã™ã‚‹ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µ |
| [ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*   | ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰²ã‚Šè¾¼ã¿æ©Ÿèƒ½ã‚’å‚™ãˆãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¢ |
| [ãƒãƒ£ãƒƒãƒˆã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *è¶…ç°¡å˜*  | æ—…è¡Œé–¢é€£ã®ã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡¦ç†ã™ã‚‹æ—…è¡Œã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ |
| [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *åˆç´š* | ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ãŸãƒãƒƒãƒè©•ä¾¡ã®å±¥æ­´æ›¸è³‡æ ¼å‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ  |
| [ãƒžãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *åˆç´š* | 2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®éžåŒæœŸé€šä¿¡ã®ãŸã‚ã®ã‚¿ãƒ–ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ  |
| [ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒã‚¤ã‚¶ãƒ¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *åˆç´š* | èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä¿¡é ¼æ€§ã‚’å¤±ã£ã¦ã„ã¾ã™... ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã† |
| [ä¸¦åˆ—å‡¦ç†](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *åˆç´š*   | 3å€ã®é«˜é€ŸåŒ–ã‚’ç¤ºã™ä¸¦åˆ—å®Ÿè¡Œãƒ‡ãƒ¢ |
| [ä¸¦åˆ—ãƒ•ãƒ­ãƒ¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *åˆç´š*   | è¤‡æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚‹8å€ã®é«˜é€ŸåŒ–ã‚’ç¤ºã™ä¸¦åˆ—ç”»åƒå‡¦ç†ãƒ‡ãƒ¢ |
| [å¤šæ•°æ±º](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *åˆç´š* | è¤‡æ•°ã®è§£æ±ºç­–ã‚’é›†ç´„ã—ã¦æŽ¨è«–ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ |
| [æ€è€ƒ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *åˆç´š*   | æ€è€ƒã®é€£éŽ–ã‚’é€šã˜ã¦è¤‡é›‘ãªæŽ¨è«–å•é¡Œã‚’è§£æ±ºã™ã‚‹ |
| [ãƒ¡ãƒ¢ãƒª](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *åˆç´š* | çŸ­æœŸè¨˜æ†¶ã¨é•·æœŸè¨˜æ†¶ã‚’æŒã¤ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *åˆç´š* | è‡ªå‹•ãƒ‡ãƒãƒƒã‚°ãƒ«ãƒ¼ãƒ—ã‚’å‚™ãˆãŸè‡ªç„¶è¨€èªžã‹ã‚‰SQLã‚¯ã‚¨ãƒªã¸ã®å¤‰æ› |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *åˆç´š* | æ•°å€¤æ¼”ç®—ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *åˆç´š* | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ã®ãŸã‚ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *åˆç´š* | SSEæ›´æ–°ã‚’å‚™ãˆãŸäººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã®ãƒŸãƒ‹ãƒžãƒ«ãªã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ |

</div>

ðŸ‘€ ä»–ã®è¶…åˆå¿ƒè€…å‘ã‘ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’è¦‹ãŸã„ã§ã™ã‹ï¼Ÿ[å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼](https://github.com/The-Pocket/PocketFlow/issues/new)

## Pocket Flowã®ä½¿ã„æ–¹

ðŸš€ **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**ã‚’é€šã˜ã¦â€”*äººé–“ãŒè¨­è¨ˆã—*ã€*ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹*æœ€é€Ÿã®LLMã‚¢ãƒ—ãƒªé–‹ç™ºãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ï¼

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="ã‚¤ãƒ¡ãƒ¼ã‚¸ä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆ" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ä»¥ä¸‹ã¯ã‚ˆã‚Šè¤‡é›‘ãªLLMã‚¢ãƒ—ãƒªã®ä¾‹ã§ã™ï¼š

<div align="center">
  
|  ã‚¢ãƒ—ãƒªå     |  é›£æ˜“åº¦    | ãƒˆãƒ”ãƒƒã‚¯  | äººé–“ã®è¨­è¨ˆ | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‰ |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Cursorã§Cursorã‚’æ§‹ç¯‰ã™ã‚‹](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>ã‚‚ã†ã™ãã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£ã«é”ã—ã¾ã™...</sup></sub> | â˜…â˜…â˜… <br> *ä¸Šç´š*   | [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹çŸ¥è­˜ãƒ“ãƒ«ãƒ€ãƒ¼](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>ä»–äººã®ã‚³ãƒ¼ãƒ‰ã‚’æ··ä¹±ã—ã¦è¦‹ã¤ã‚ã‚‹ã»ã©äººç”Ÿã¯çŸ­ããªã„</sup></sub> |  â˜…â˜…â˜† <br> *ä¸­ç´š* | [ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [AI Paul Grahamã«è³ªå•ã™ã‚‹](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>æŽ¡ç”¨ã•ã‚Œãªã„å ´åˆã«å‚™ãˆã¦ã€AI Paul Grahamã«è³ªå•ã—ã¾ã—ã‚‡ã†</sup></sub> | â˜…â˜…â˜† <br> *ä¸­ç´š*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtubeã‚µãƒžãƒ©ã‚¤ã‚¶ãƒ¼](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> 5æ­³å…ã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«YouTubeå‹•ç”»ã‚’èª¬æ˜Ž </sup></sub> | â˜…â˜†â˜† <br> *åˆç´š*   | [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [ã‚³ãƒ¼ãƒ«ãƒ‰ã‚ªãƒ¼ãƒ—ãƒŠãƒ¼ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> å†·ãŸã„ãƒªãƒ¼ãƒ‰ã‚’ç†±ãã™ã‚‹å³å¸­ã‚¢ã‚¤ã‚¹ãƒ–ãƒ¬ã‚¤ã‚«ãƒ¼ </sup></sub> | â˜…â˜†â˜† <br> *åˆç´š*   | [ãƒžãƒƒãƒ—ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [ã‚¦ã‚§ãƒ–æ¤œç´¢](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [ãƒ•ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**ã‚’å­¦ã³ãŸã„ã§ã™ã‹ï¼Ÿ

  - ä¸Šè¨˜ã®ã‚¢ãƒ—ãƒªã®ä½œã‚Šæ–¹ã«é–¢ã™ã‚‹ãƒ“ãƒ‡ã‚ªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã¤ã„ã¦ã¯ã€[ç§ã®YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼

  - è‡ªåˆ†ã®LLMã‚¢ãƒ—ãƒªã‚’æ§‹ç¯‰ã—ãŸã„ã§ã™ã‹ï¼Ÿã“ã®[æŠ•ç¨¿](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)ã‚’èª­ã‚“ã§ãã ã•ã„ï¼[ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ](https://github.com/The-Pocket/PocketFlow-Template-Python)ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_KOREAN.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | í•œêµ­ì–´

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket FlowëŠ” [100ì¤„](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ì˜ ë¯¸ë‹ˆë©€ë¦¬ìŠ¤íŠ¸ LLM í”„ë ˆìž„ì›Œí¬ìž…ë‹ˆë‹¤

- **ê²½ëŸ‰í™”**: ë‹¨ 100ì¤„. ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì—†ìŒ, ì˜ì¡´ì„± ì—†ìŒ, ë²¤ë” ì¢…ì†ì„± ì—†ìŒ.
  
- **í‘œí˜„ë ¥**: ì—¬ëŸ¬ë¶„ì´ ì¢‹ì•„í•˜ëŠ” ëª¨ë“  ê²ƒâ€”([ë©€í‹°-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ì—ì´ì „íŠ¸](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [ì›Œí¬í”Œë¡œìš°](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) ë“±.

- **[ì—ì´ì „íŠ¸ ì½”ë”©](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: AI ì—ì´ì „íŠ¸(ì˜ˆ: Cursor AI)ê°€ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ë„ë¡ í•˜ì„¸ìš”â€”ìƒì‚°ì„± 10ë°° í–¥ìƒ!

Pocket Flow ì‹œìž‘í•˜ê¸°:
- ì„¤ì¹˜í•˜ë ¤ë©´ ```pip install pocketflow```ë‚˜ [ì†ŒìŠ¤ ì½”ë“œ](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)(ë‹¨ 100ì¤„)ë¥¼ ë³µì‚¬í•˜ì„¸ìš”.
- ë” ì•Œì•„ë³´ë ¤ë©´ [ë¬¸ì„œ](https://the-pocket.github.io/PocketFlow/)ë¥¼ í™•ì¸í•˜ì„¸ìš”. ê°œë°œ ë™ê¸°ì— ëŒ€í•´ ì•Œê³  ì‹¶ë‹¤ë©´ [ì´ì•¼ê¸°](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just)ë¥¼ ì½ì–´ë³´ì„¸ìš”.
- ì§ˆë¬¸ì´ ìžˆìœ¼ì‹ ê°€ìš”? [AI ì–´ì‹œìŠ¤í„´íŠ¸](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant)ë¥¼ í™•ì¸í•˜ê±°ë‚˜, [ì´ìŠˆë¥¼ ìƒì„±í•˜ì„¸ìš”!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Pocket Flowë¡œ ê°œë°œí•˜ëŠ” ë‹¤ë¥¸ ê°œë°œìžë“¤ê³¼ ì†Œí†µí•˜ë ¤ë©´ [Discord](https://discord.gg/hUHHE9Sa6T)ì— ê°€ìž…í•˜ì„¸ìš”!
- ðŸŽ‰ Pocket FlowëŠ” ì²˜ìŒì— Pythonìœ¼ë¡œ ê°œë°œë˜ì—ˆì§€ë§Œ, ì´ì œ [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) ë° [Go](https://github.com/The-Pocket/PocketFlow-Go) ë²„ì „ë„ ìžˆìŠµë‹ˆë‹¤!

## ì™œ Pocket Flowì¸ê°€?

í˜„ìž¬ LLM í”„ë ˆìž„ì›Œí¬ë“¤ì€ ë„ˆë¬´ ë¹„ëŒ€í•©ë‹ˆë‹¤... LLM í”„ë ˆìž„ì›Œí¬ëŠ” ë‹¨ 100ì¤„ì´ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **ì¶”ìƒí™”**          | **ì•± íŠ¹í™” ëž˜í¼**                                      | **ë²¤ë” íŠ¹í™” ëž˜í¼**                                    | **ì½”ë“œ ì¤„**       | **í¬ê¸°**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: QA, ìš”ì•½)</sub></sup>              | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: OpenAI, Pinecone ë“±)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: FileReadTool, SerperDevTool)</sub></sup>         | ë§ŽìŒ <br><sup><sub>(ì˜ˆ: OpenAI, Anthropic, Pinecone ë“±)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: CodeAgent, VisitWebTool)</sub></sup>         | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: DuckDuckGo, Hugging Face ë“±)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: Semantic Search)</sub></sup>                     | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: PostgresStore, SqliteSaver ë“±) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | ì¼ë¶€ <br><sup><sub>(ì˜ˆ: Tool Agent, Chat Agent)</sub></sup>              | ë§ŽìŒ <sup><sub>[ì„ íƒì ]<br> (ì˜ˆ: OpenAI, Pinecone ë“±)</sub></sup>        | 7K <br><sup><sub>(í•µì‹¬ë§Œ)</sub></sup>    | +26MB <br><sup><sub>(í•µì‹¬ë§Œ)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **ì—†ìŒ**                                                 | **ì—†ìŒ**                                                  | **100**       | **+56KB**                  |

</div>

## Pocket FlowëŠ” ì–´ë–»ê²Œ ìž‘ë™í•˜ë‚˜ìš”?

[100ì¤„](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)ì˜ ì½”ë“œëŠ” LLM í”„ë ˆìž„ì›Œí¬ì˜ í•µì‹¬ ì¶”ìƒí™”ì¸ ê·¸ëž˜í”„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ([ë©€í‹°-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ì—ì´ì „íŠ¸](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [ì›Œí¬í”Œë¡œìš°](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) ë“±ì˜ ì¸ê¸° ìžˆëŠ” ë””ìžì¸ íŒ¨í„´ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ì•„ëž˜ëŠ” ê¸°ë³¸ íŠœí† ë¦¬ì–¼ìž…ë‹ˆë‹¤:

<div align="center">
  
|  ì´ë¦„  | ë‚œì´ë„    |  ì„¤ëª…  |  
| :-------------:  | :-------------: | :--------------------- |  
| [ì±„íŒ…](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ëŒ€í™” ê¸°ë¡ì„ ê°€ì§„ ê¸°ë³¸ ì±„íŒ…ë´‡ |
| [êµ¬ì¡°í™”ëœ ì¶œë ¥](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *ì´ˆë³´* | í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì´ë ¥ì„œì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ |
| [ì›Œí¬í”Œë¡œìš°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ê°œìš” ìž‘ì„±, ë‚´ìš© ìž‘ì„±, ìŠ¤íƒ€ì¼ ì ìš©ì´ í¬í•¨ëœ ìž‘ì„± ì›Œí¬í”Œë¡œìš° |
| [ì—ì´ì „íŠ¸](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ì›¹ì„ ê²€ìƒ‰í•˜ê³  ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìžˆëŠ” ì—°êµ¬ ì—ì´ì „íŠ¸ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ê°„ë‹¨í•œ ê²€ìƒ‰ ì¦ê°• ìƒì„± í”„ë¡œì„¸ìŠ¤ |
| [ë°°ì¹˜](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *ì´ˆë³´* | ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ì—¬ëŸ¬ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë°°ì¹˜ í”„ë¡œì„¸ì„œ |
| [ìŠ¤íŠ¸ë¦¬ë°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *ì´ˆë³´*   | ì‚¬ìš©ìž ì¤‘ë‹¨ ê¸°ëŠ¥ì´ ìžˆëŠ” ì‹¤ì‹œê°„ LLM ìŠ¤íŠ¸ë¦¬ë° ë°ëª¨ |
| [ì±„íŒ… ê°€ë“œë ˆì¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *ì´ˆë³´*  | ì—¬í–‰ ê´€ë ¨ ì¿¼ë¦¬ë§Œ ì²˜ë¦¬í•˜ëŠ” ì—¬í–‰ ìƒë‹´ ì±„íŒ…ë´‡ |
| [ë§µ-ë¦¬ë“€ìŠ¤](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ë°°ì¹˜ í‰ê°€ë¥¼ ìœ„í•œ ë§µ-ë¦¬ë“€ìŠ¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” ì´ë ¥ì„œ ìžê²© ì²˜ë¦¬ê¸° |
| [ë©€í‹°-ì—ì´ì „íŠ¸](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ë‘ ì—ì´ì „íŠ¸ ê°„ì˜ ë¹„ë™ê¸° í†µì‹ ì„ ìœ„í•œ ê¸ˆì§€ì–´ ê²Œìž„ |
| [ê°ë…ìž](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ì—°êµ¬ ì—ì´ì „íŠ¸ê°€ ë¶ˆì•ˆì •í•  ë•Œ... ê°ë… í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì¶•í•´ ë´…ì‹œë‹¤ |
| [ë³‘ë ¬](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | 3ë°° ì†ë„ í–¥ìƒì„ ë³´ì—¬ì£¼ëŠ” ë³‘ë ¬ ì‹¤í–‰ ë°ëª¨ |
| [ë³‘ë ¬ í”Œë¡œìš°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | ì—¬ëŸ¬ í•„í„°ë¥¼ ì‚¬ìš©í•œ 8ë°° ì†ë„ í–¥ìƒì„ ë³´ì—¬ì£¼ëŠ” ë³‘ë ¬ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°ëª¨ |
| [ë‹¤ìˆ˜ê²° íˆ¬í‘œ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ì—¬ëŸ¬ ì†”ë£¨ì…˜ ì‹œë„ë¥¼ ì§‘ê³„í•˜ì—¬ ì¶”ë¡  ì •í™•ë„ í–¥ìƒ |
| [ì‚¬ê³ ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | Chain-of-Thoughtë¥¼ í†µí•œ ë³µìž¡í•œ ì¶”ë¡  ë¬¸ì œ í•´ê²° |
| [ë©”ëª¨ë¦¬](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ë‹¨ê¸° ë° ìž¥ê¸° ë©”ëª¨ë¦¬ê°€ ìžˆëŠ” ì±„íŒ…ë´‡ |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ìžë™ ë””ë²„ê·¸ ë£¨í”„ê°€ ìžˆëŠ” ìžì—°ì–´ì—ì„œ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜ |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ìˆ˜ì¹˜ ì—°ì‚°ì„ ìœ„í•œ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡œí† ì½œì„ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | ì—ì´ì „íŠ¸ ê°„ í†µì‹ ì„ ìœ„í•œ Agent-to-Agent í”„ë¡œí† ì½œë¡œ ëž˜í•‘ëœ ì—ì´ì „íŠ¸ |
| [ì›¹ HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *ì´ˆê¸‰* | SSE ì—…ë°ì´íŠ¸ê°€ ìžˆëŠ” ì¸ê°„ ê²€í†  ë£¨í”„ë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ì›¹ ì„œë¹„ìŠ¤ |

</div>

ðŸ‘€ ë” ë§Žì€ ì´ˆë³´ìžìš© íŠœí† ë¦¬ì–¼ì„ ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? [ì´ìŠˆë¥¼ ìƒì„±í•˜ì„¸ìš”!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Pocket Flowë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?

ðŸš€ **ì—ì´ì „íŠ¸ ì½”ë”©**ì„ í†µí•´â€”ê°€ìž¥ ë¹ ë¥¸ LLM ì•± ê°œë°œ íŒ¨ëŸ¬ë‹¤ìž„ìœ¼ë¡œ, *ì¸ê°„ì´ ì„¤ê³„*í•˜ê³  *ì—ì´ì „íŠ¸ê°€ ì½”ë”©*í•©ë‹ˆë‹¤!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ì•„ëž˜ëŠ” ë” ë³µìž¡í•œ LLM ì•±ì˜ ì˜ˆì‹œìž…ë‹ˆë‹¤:

<div align="center">
  
|  ì•± ì´ë¦„     |  ë‚œì´ë„    | ì£¼ì œ  | ì¸ê°„ ì„¤ê³„ | ì—ì´ì „íŠ¸ ì½”ë“œ |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Cursorë¡œ Cursor ë§Œë“¤ê¸°](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>ê³§ ê¸°ìˆ ì  íŠ¹ì´ì ì— ë„ë‹¬í•  ê²ƒìž…ë‹ˆë‹¤...</sup></sub> | â˜…â˜…â˜… <br> *ê³ ê¸‰*   | [ì—ì´ì „íŠ¸](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ì½”ë“œë² ì´ìŠ¤ ì§€ì‹ ë¹Œë”](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>ì¸ìƒì€ ë‹¤ë¥¸ ì‚¬ëžŒì˜ ì½”ë“œë¥¼ í˜¼ëž€ìŠ¤ëŸ½ê²Œ ë°”ë¼ë³¼ ë§Œí¼ ê¸¸ì§€ ì•ŠìŠµë‹ˆë‹¤</sup></sub> |  â˜…â˜…â˜† <br> *ì¤‘ê¸‰* | [ì›Œí¬í”Œë¡œìš°](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [AI Paul Grahamì—ê²Œ ë¬¼ì–´ë³´ê¸°](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>í•©ê²©í•˜ì§€ ëª»í•œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ AI Paul Grahamì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”</sup></sub> | â˜…â˜…â˜† <br> *ì¤‘ê¸‰*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [ë§µ ë¦¬ë“€ìŠ¤](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [ìœ íŠœë¸Œ ìš”ì•½ê¸°](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> 5ì‚´ ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ YouTube ë™ì˜ìƒ ì„¤ëª… </sup></sub> | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | [ë§µ ë¦¬ë“€ìŠ¤](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [ì½œë“œ ì˜¤í”„ë„ˆ ìƒì„±ê¸°](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> ì°¨ê°€ìš´ ìž ìž¬ ê³ ê°ì„ ëœ¨ê²ê²Œ ë§Œë“œëŠ” ì¦‰ê°ì ì¸ ì•„ì´ìŠ¤ë¸Œë ˆì´ì»¤ </sup></sub> | â˜…â˜†â˜† <br> *ì´ˆê¸‰*   | [ë§µ ë¦¬ë“€ìŠ¤](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [ì›¹ ê²€ìƒ‰](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [ì„¤ê³„ ë¬¸ì„œ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [í”Œë¡œìš° ì½”ë“œ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- **ì—ì´ì „íŠ¸ ì½”ë”©**ì„ ë°°ìš°ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?

  - ìœ„ì— ì†Œê°œëœ ì•±ë“¤ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼ì„ ë³´ë ¤ë©´ [ì œ YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1)ë¥¼ í™•ì¸í•˜ì„¸ìš”!

  - ìžì‹ ë§Œì˜ LLM ì•±ì„ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ì´ [í¬ìŠ¤íŠ¸](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)ë¥¼ ì½ì–´ë³´ì„¸ìš”! [ì´ í…œí”Œë¦¿](https://github.com/The-Pocket/PocketFlow-Template-Python)ìœ¼ë¡œ ì‹œìž‘í•˜ì„¸ìš”!
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_PORTUGUESE.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | PortuguÃªs | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow Ã© um framework minimalista para LLM com [apenas 100 linhas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **Leve**: Apenas 100 linhas. Zero inchaÃ§o, zero dependÃªncias, zero aprisionamento a fornecedores.
  
- **Expressivo**: Tudo o que vocÃª adoraâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), e mais.

- **[CodificaÃ§Ã£o AgÃªntica](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Deixe que Agentes de IA (ex: Cursor AI) construam Agentesâ€”aumento de produtividade de 10x!

Comece com o Pocket Flow:
- Para instalar, ```pip install pocketflow``` ou apenas copie o [cÃ³digo-fonte](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (apenas 100 linhas).
- Para saber mais, consulte a [documentaÃ§Ã£o](https://the-pocket.github.io/PocketFlow/). Para entender a motivaÃ§Ã£o, leia a [histÃ³ria](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Tem perguntas? Consulte este [Assistente de IA](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), ou [crie uma issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Junte-se ao nosso [Discord](https://discord.gg/hUHHE9Sa6T) para se conectar com outros desenvolvedores construindo com o Pocket Flow!
- ðŸŽ‰ O Pocket Flow Ã© inicialmente em Python, mas agora temos versÃµes em [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) e [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## Por que Pocket Flow?

Os frameworks LLM atuais sÃ£o pesados... VocÃª sÃ³ precisa de 100 linhas para um Framework LLM!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **AbstraÃ§Ã£o**          | **Wrappers EspecÃ­ficos para AplicaÃ§Ãµes**                                      | **Wrappers EspecÃ­ficos para Fornecedores**                                    | **Linhas**       | **Tamanho**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agente, Cadeia               | Muitos <br><sup><sub>(ex: QA, Summarization)</sub></sup>              | Muitos <br><sup><sub>(ex: OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agente, Cadeia            | Muitos <br><sup><sub>(ex: FileReadTool, SerperDevTool)</sub></sup>         | Muitos <br><sup><sub>(ex: OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agente                      | Alguns <br><sup><sub>(ex: CodeAgent, VisitWebTool)</sub></sup>         | Alguns <br><sup><sub>(ex: DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agente, Grafo           | Alguns <br><sup><sub>(ex: Semantic Search)</sub></sup>                     | Alguns <br><sup><sub>(ex: PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agente                | Alguns <br><sup><sub>(ex: Tool Agent, Chat Agent)</sub></sup>              | Muitos <sup><sub>[Opcional]<br> (ex: OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(somente core)</sub></sup>    | +26MB <br><sup><sub>(somente core)</sub></sup>          |
| **PocketFlow** | **Grafo**                    | **Nenhum**                                                 | **Nenhum**                                                  | **100**       | **+56KB**                  |

</div>

## Como funciona o Pocket Flow?

As [100 linhas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturam a abstraÃ§Ã£o central dos frameworks LLM: o Grafo!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

A partir daÃ­, Ã© fÃ¡cil implementar padrÃµes de design populares como ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Abaixo estÃ£o tutoriais bÃ¡sicos:

<div align="center">
  
|  Nome  | Dificuldade    |  DescriÃ§Ã£o  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um chatbot bÃ¡sico com histÃ³rico de conversaÃ§Ã£o |
| [SaÃ­da Estruturada](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *BÃ¡sico* | Extraindo dados estruturados de currÃ­culos por prompt |
| [Fluxo de Trabalho](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um fluxo de escrita que esboÃ§a, escreve conteÃºdo e aplica estilo |
| [Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um agente de pesquisa que pode buscar na web e responder perguntas |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Um processo simples de GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o |
| [Processamento em Lote](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *BÃ¡sico* | Um processador em lote que traduz conteÃºdo markdown para vÃ¡rios idiomas |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *BÃ¡sico*   | Uma demonstraÃ§Ã£o de streaming LLM em tempo real com capacidade de interrupÃ§Ã£o pelo usuÃ¡rio |
| [Guardrail de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *BÃ¡sico*  | Um chatbot de consultoria de viagens que processa apenas consultas relacionadas a viagens |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *Iniciante* | Um processador de qualificaÃ§Ã£o de currÃ­culos usando o padrÃ£o map-reduce para avaliaÃ§Ã£o em lote |
| [Multi-Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Iniciante* | Um jogo de Tabu para comunicaÃ§Ã£o assÃ­ncrona entre dois agentes |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Iniciante* | O agente de pesquisa estÃ¡ ficando pouco confiÃ¡vel... Vamos criar um processo de supervisÃ£o |
| [Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Iniciante*   | Uma demonstraÃ§Ã£o de execuÃ§Ã£o paralela que mostra um aumento de velocidade de 3x |
| [Fluxo Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Iniciante*   | Uma demonstraÃ§Ã£o de processamento de imagem paralelo mostrando um aumento de velocidade de 8x com mÃºltiplos filtros |
| [Voto MajoritÃ¡rio](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *Iniciante* | Melhore a precisÃ£o de raciocÃ­nio agregando mÃºltiplas tentativas de soluÃ§Ã£o |
| [Pensamento](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Iniciante*   | Resolva problemas complexos de raciocÃ­nio atravÃ©s de Cadeia-de-Pensamento |
| [MemÃ³ria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Iniciante* | Um chatbot com memÃ³ria de curto e longo prazo |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *Iniciante* | Converta linguagem natural para consultas SQL com um loop de autodepuraÃ§Ã£o |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Iniciante* | Agente usando o Protocolo de Contexto de Modelo para operaÃ§Ãµes numÃ©ricas |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *Iniciante* | Agente envolvido com o protocolo Agente-para-Agente para comunicaÃ§Ã£o entre agentes |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *Iniciante* | Um serviÃ§o web mÃ­nimo para um loop de revisÃ£o humana com atualizaÃ§Ãµes SSE |

</div>

ðŸ‘€ Quer ver outros tutoriais para iniciantes? [Crie uma issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Como usar o Pocket Flow?

ðŸš€ AtravÃ©s da **CodificaÃ§Ã£o AgÃªntica**â€”o paradigma mais rÃ¡pido de desenvolvimento de aplicativos LLMâ€”onde *humanos projetam* e *agentes codificam*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Abaixo estÃ£o exemplos de aplicativos LLM mais complexos:

<div align="center">
  
|  Nome do Aplicativo     |  Dificuldade    | TÃ³picos  | Design Humano | CÃ³digo do Agente |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Construir Cursor com Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Logo chegaremos Ã  singularidade ...</sup></sub> | â˜…â˜…â˜… <br> *AvanÃ§ado*   | [Agente](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Construtor de Conhecimento de Base de CÃ³digo](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>A vida Ã© curta demais para ficar olhando o cÃ³digo dos outros em confusÃ£o</sup></sub> |  â˜…â˜…â˜† <br> *MÃ©dio* | [Fluxo de Trabalho](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Pergunte Ã  IA Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Pergunte Ã  IA Paul Graham, caso vocÃª nÃ£o consiga entrar</sup></sub> | â˜…â˜…â˜† <br> *MÃ©dio*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Doc de Design](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Resumidor de Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explica vÃ­deos do YouTube para vocÃª como se vocÃª tivesse 5 anos </sup></sub> | â˜…â˜†â˜† <br> *Iniciante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Doc de Design](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Gerador de Abertura a Frio](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Quebra-gelos instantÃ¢neos que transformam leads frios em quentes </sup></sub> | â˜…â˜†â˜† <br> *Iniciante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Busca Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Doc de Design](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [CÃ³digo de Fluxo](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Quer aprender **CodificaÃ§Ã£o AgÃªntica**?

  - Confira [meu YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) para tutoriais em vÃ­deo sobre como alguns dos aplicativos acima sÃ£o feitos!

  - Quer construir seu prÃ³prio aplicativo LLM? Leia este [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Comece com [este modelo](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_RUSSIAN.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | Ð ÑƒÑÑÐºÐ¸Ð¹| [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow â€” ÑÑ‚Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ LLM Ð²ÑÐµÐ³Ð¾ Ð² [100 ÑÑ‚Ñ€Ð¾Ðº](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **Ð›ÐµÐ³ÐºÐ¸Ð¹**: Ð’ÑÐµÐ³Ð¾ 100 ÑÑ‚Ñ€Ð¾Ðº. ÐÐ¸ÐºÐ°ÐºÐ¾Ð³Ð¾ Ð»Ð¸ÑˆÐ½ÐµÐ³Ð¾ Ð²ÐµÑÐ°, Ð½Ð¸ÐºÐ°ÐºÐ¸Ñ… Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹, Ð½Ð¸ÐºÐ°ÐºÐ¾Ð¹ Ð¿Ñ€Ð¸Ð²ÑÐ·ÐºÐ¸ Ðº Ð²ÐµÐ½Ð´Ð¾Ñ€Ð°Ð¼.
  
- **Ð’Ñ‹Ñ€Ð°Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹**: Ð’ÑÑ‘, Ñ‡Ñ‚Ð¾ Ð²Ñ‹ Ð»ÑŽÐ±Ð¸Ñ‚Ðµ â€” ([ÐœÑƒÐ»ÑŒÑ‚Ð¸-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ÐÐ³ÐµÐ½Ñ‚Ñ‹](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) Ð¸ Ð¼Ð½Ð¾Ð³Ð¾Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ðµ.

- **[ÐÐ³ÐµÐ½Ñ‚ÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: ÐŸÐ¾Ð·Ð²Ð¾Ð»ÑŒÑ‚Ðµ Ð˜Ð˜-Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Cursor AI) ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² â€” Ð¿Ð¾Ð²Ñ‹ÑÑŒÑ‚Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð² 10 Ñ€Ð°Ð·!

ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Pocket Flow:
- Ð”Ð»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸, ```pip install pocketflow``` Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ [Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ ÐºÐ¾Ð´](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (Ð²ÑÐµÐ³Ð¾ 100 ÑÑ‚Ñ€Ð¾Ðº).
- Ð§Ñ‚Ð¾Ð±Ñ‹ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ, Ð¾Ð·Ð½Ð°ÐºÐ¾Ð¼ÑŒÑ‚ÐµÑÑŒ Ñ [Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÐµÐ¹](https://the-pocket.github.io/PocketFlow/). Ð§Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð½ÑÑ‚ÑŒ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸ÑŽ, Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ [Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Ð•ÑÑ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹? Ð¡Ð¿Ñ€Ð¾ÑÐ¸Ñ‚Ðµ ÑÑ‚Ð¾Ð³Ð¾ [Ð˜Ð˜-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant) Ð¸Ð»Ð¸ [ÑÐ¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ ÐŸÑ€Ð¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÐ¹Ñ‚ÐµÑÑŒ Ðº Ð½Ð°ÑˆÐµÐ¼Ñƒ [Discord](https://discord.gg/hUHHE9Sa6T), Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±Ñ‰Ð°Ñ‚ÑŒÑÑ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ°Ð¼Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Pocket Flow!
- ðŸŽ‰ Pocket Flow Ð¸Ð·Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½ Ð½Ð° Python, Ð½Ð¾ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð½Ð° [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) Ð¸ [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Pocket Flow?

Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¸ Ð´Ð»Ñ LLM ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð³Ñ€Ð¾Ð¼Ð¾Ð·Ð´ÐºÐ¸Ðµ... Ð”Ð»Ñ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ° LLM Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð²ÑÐµÐ³Ð¾ 100 ÑÑ‚Ñ€Ð¾Ðº!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **ÐÐ±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸Ñ**          | **ÐžÐ±ÐµÑ€Ñ‚ÐºÐ¸ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹**                                      | **ÐžÐ±ÐµÑ€Ñ‚ÐºÐ¸ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð²ÐµÐ½Ð´Ð¾Ñ€Ð¾Ð²**                                    | **Ð¡Ñ‚Ñ€Ð¾Ðº**       | **Ð Ð°Ð·Ð¼ÐµÑ€**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., QA, Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ)</sub></sup>              | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., OpenAI, Pinecone Ð¸ Ñ‚.Ð´.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., FileReadTool, SerperDevTool)</sub></sup>         | ÐœÐ½Ð¾Ð³Ð¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., OpenAI, Anthropic, Pinecone Ð¸ Ñ‚.Ð´.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., CodeAgent, VisitWebTool)</sub></sup>         | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., DuckDuckGo, Hugging Face Ð¸ Ñ‚.Ð´.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., Semantic Search)</sub></sup>                     | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., PostgresStore, SqliteSaver Ð¸ Ñ‚.Ð´.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ <br><sup><sub>(Ð½Ð°Ð¿Ñ€., Tool Agent, Chat Agent)</sub></sup>              | ÐœÐ½Ð¾Ð³Ð¾ <sup><sub>[ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾]<br> (Ð½Ð°Ð¿Ñ€., OpenAI, Pinecone Ð¸ Ñ‚.Ð´.)</sub></sup>        | 7K <br><sup><sub>(Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ´Ñ€Ð¾)</sub></sup>    | +26MB <br><sup><sub>(Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ´Ñ€Ð¾)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **ÐÐµÑ‚**                                                 | **ÐÐµÑ‚**                                                  | **100**       | **+56KB**                  |

</div>

## ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Pocket Flow?

[100 ÑÑ‚Ñ€Ð¾Ðº](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) Ð¾Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸ÑŽ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ¾Ð² LLM: Ð“Ñ€Ð°Ñ„!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

ÐžÑ‚ÑÑŽÐ´Ð° Ð»ÐµÐ³ÐºÐ¾ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº ([ÐœÑƒÐ»ÑŒÑ‚Ð¸-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[ÐÐ³ÐµÐ½Ñ‚Ñ‹](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ ÐÐ¸Ð¶Ðµ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð°:

<div align="center">
  
|  ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ  | Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ    |  ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Ð§Ð°Ñ‚](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÐµÐ¹ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° |
| [Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹* | Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð² |
| [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ Ð¿Ð»Ð°Ð½, Ð¿Ð¸ÑˆÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ ÑÑ‚Ð¸Ð»Ð¸ |
| [ÐÐ³ÐµÐ½Ñ‚](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¸ÑÐºÐ°Ñ‚ÑŒ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ |
| [ÐŸÐ°ÐºÐµÑ‚Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹* | ÐŸÐ°ÐºÐµÑ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ markdown-ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ·Ñ‹ÐºÐ¾Ð² |
| [ÐŸÐ¾Ñ‚Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*   | Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ LLM Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ñ€ÐµÑ€Ñ‹Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ |
| [ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð°](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ¸Ð¹*  | Ð§Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ‚ÑƒÑ€Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚Ð°, Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‰Ð¸Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð¿ÑƒÑ‚ÐµÑˆÐµÑÑ‚Ð²Ð¸ÑÐ¼Ð¸ |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐŸÑ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ñ€ÐµÐ·ÑŽÐ¼Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½ map-reduce Ð´Ð»Ñ Ð¿Ð°ÐºÐµÑ‚Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ |
| [ÐœÑƒÐ»ÑŒÑ‚Ð¸-Ð°Ð³ÐµÐ½Ñ‚](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | Ð˜Ð³Ñ€Ð° Ð¢Ð°Ð±Ñƒ Ð´Ð»Ñ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ð´Ð²ÑƒÐ¼Ñ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼Ð¸ |
| [Ð¡ÑƒÐ¿ÐµÑ€Ð²Ð¸Ð·Ð¾Ñ€](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ð½ÐµÐ½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¼... Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ ÑÐ¾Ð·Ð´Ð°Ð´Ð¸Ð¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð½Ð°Ð´Ð·Ð¾Ñ€Ð°|
| [ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‰Ð°Ñ 3-ÐºÑ€Ð°Ñ‚Ð½Ð¾Ðµ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ |
| [ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ð¾Ðº](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‰Ð°Ñ 8-ÐºÑ€Ð°Ñ‚Ð½Ð¾Ðµ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼Ð¸ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ð¼Ð¸ |
| [Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ð¾Ð¼](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¿ÑƒÑ‚ÐµÐ¼ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº Ñ€ÐµÑˆÐµÐ½Ð¸Ñ |
| [ÐœÑ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | Ð ÐµÑˆÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ð¹ |
| [ÐŸÐ°Ð¼ÑÑ‚ÑŒ](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | Ð§Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¸ Ð´Ð¾Ð»Ð³Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð² SQL-Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐÐ³ÐµÐ½Ñ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐÐ³ÐµÐ½Ñ‚, Ð¾Ð±ÐµÑ€Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð¾Ð¼ Ð°Ð³ÐµÐ½Ñ‚-Ðº-Ð°Ð³ÐµÐ½Ñ‚Ñƒ Ð´Ð»Ñ Ð¼ÐµÐ¶Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹* | ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð²ÐµÐ±-ÑÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ñ†Ð¸ÐºÐ»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¼ Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÑÐ¼Ð¸ SSE |

</div>

ðŸ‘€ Ð¥Ð¾Ñ‚Ð¸Ñ‚Ðµ ÑƒÐ²Ð¸Ð´ÐµÑ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð° Ð´Ð»Ñ Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‰Ð¸Ñ…? [Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## ÐšÐ°Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Pocket Flow?

ðŸš€ Ð§ÐµÑ€ÐµÐ· **ÐÐ³ÐµÐ½Ñ‚ÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ** â€” ÑÐ°Ð¼ÑƒÑŽ Ð±Ñ‹ÑÑ‚Ñ€ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ñƒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ LLM-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, Ð³Ð´Ðµ *Ð»ÑŽÐ´Ð¸ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€ÑƒÑŽÑ‚*, Ð° *Ð°Ð³ÐµÐ½Ñ‚Ñ‹ ÐºÐ¾Ð´Ð¸Ñ€ÑƒÑŽÑ‚*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ ÐÐ¸Ð¶Ðµ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… LLM-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹:

<div align="center">
  
|  ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ     |  Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ    | Ð¢ÐµÐ¼Ñ‹  | Ð”Ð¸Ð·Ð°Ð¹Ð½ Ð¾Ñ‚ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° | ÐšÐ¾Ð´ Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Cursor Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Ð¡ÐºÐ¾Ñ€Ð¾ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð½ÐµÐ¼ ÑÐ¸Ð½Ð³ÑƒÐ»ÑÑ€Ð½Ð¾ÑÑ‚Ð¸ ...</sup></sub> | â˜…â˜…â˜… <br> *ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ð¹*   | [ÐÐ³ÐµÐ½Ñ‚](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [ÐšÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¾Ñ€ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¾ ÐºÐ¾Ð´Ð¾Ð²Ð¾Ð¹ Ð±Ð°Ð·Ðµ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>Ð–Ð¸Ð·Ð½ÑŒ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð² Ñ€Ð°ÑÑ‚ÐµÑ€ÑÐ½Ð½Ð¾ÑÑ‚Ð¸ ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð° Ñ‡ÑƒÐ¶Ð¾Ð¹ ÐºÐ¾Ð´</sup></sub> |  â˜…â˜…â˜† <br> *Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹* | [Ð Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Ð¡Ð¿Ñ€Ð¾ÑÐ¸ Ð˜Ð˜ ÐŸÐ¾Ð»Ð° Ð“Ñ€ÑÐ¼Ð°](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Ð¡Ð¿Ñ€Ð¾ÑÐ¸ Ð˜Ð˜ ÐŸÐ¾Ð»Ð° Ð“Ñ€ÑÐ¼Ð°, ÐµÑÐ»Ð¸ Ñ‚ÐµÐ±Ñ Ð½Ðµ Ð¿Ñ€Ð¸Ð½ÑÐ»Ð¸</sup></sub> | â˜…â˜…â˜† <br> *Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Ð¡ÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ YouTube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> ÐžÐ±ÑŠÑÑÐ½ÑÐµÑ‚ YouTube-Ð²Ð¸Ð´ÐµÐ¾ ÐºÐ°Ðº Ð´Ð»Ñ 5-Ð»ÐµÑ‚Ð½ÐµÐ³Ð¾ </sup></sub> | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Ð“ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ñ…Ð¾Ð»Ð¾Ð´Ð½Ñ‹Ñ… Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ð¹](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> ÐœÐ³Ð½Ð¾Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð»ÐµÐ´Ð¾ÐºÐ¾Ð»Ñ‹, Ð¿Ñ€ÐµÐ²Ñ€Ð°Ñ‰Ð°ÑŽÑ‰Ð¸Ðµ Ñ…Ð¾Ð»Ð¾Ð´Ð½Ñ‹Ñ… Ð»Ð¸Ð´Ð¾Ð² Ð² Ð³Ð¾Ñ€ÑÑ‡Ð¸Ñ… </sup></sub> | â˜…â˜†â˜† <br> *ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Ð’ÐµÐ±-Ð¿Ð¾Ð¸ÑÐº](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð´Ð¸Ð·Ð°Ð¹Ð½Ñƒ](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [ÐšÐ¾Ð´ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Ð¥Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¸Ð·ÑƒÑ‡Ð¸Ñ‚ÑŒ **ÐÐ³ÐµÐ½Ñ‚ÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ**?

  - ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ñ‚Ðµ [Ð¼Ð¾Ð¹ YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) Ð´Ð»Ñ Ð²Ð¸Ð´ÐµÐ¾ÑƒÑ€Ð¾ÐºÐ¾Ð² Ð¾ Ñ‚Ð¾Ð¼, ÐºÐ°Ðº ÑÐ¾Ð·Ð´Ð°ÑŽÑ‚ÑÑ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¸Ð· Ð²Ñ‹ÑˆÐµÐ¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹!

  - Ð¥Ð¾Ñ‚Ð¸Ñ‚Ðµ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐ²Ð¾Ðµ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ LLM-Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ? ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ ÑÑ‚Ñƒ [ÑÑ‚Ð°Ñ‚ÑŒÑŽ](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! ÐÐ°Ñ‡Ð½Ð¸Ñ‚Ðµ Ñ [ÑÑ‚Ð¾Ð³Ð¾ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-parallel-batch/translations/README_SPANISH.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) -->

[English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md) | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | EspaÃ±ol | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow es un framework minimalista de LLM de [100 lÃ­neas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py)

- **Ligero**: Solo 100 lÃ­neas. Cero hinchazÃ³n, cero dependencias, cero vinculaciÃ³n a proveedores.
  
- **Expresivo**: Todo lo que amasâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Flujo de Trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), y mÃ¡s.

- **[ProgramaciÃ³n mediante Agentes](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Permite que los Agentes de IA (por ejemplo, Cursor AI) construyan Agentesâ€”Â¡multiplicando la productividad por 10!

Comienza con Pocket Flow:
- Para instalar, ```pip install pocketflow``` o simplemente copia el [cÃ³digo fuente](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (solo 100 lÃ­neas).
- Para aprender mÃ¡s, consulta la [documentaciÃ³n](https://the-pocket.github.io/PocketFlow/). Para conocer la motivaciÃ³n, lee la [historia](https://zacharyhuang.substack.com/p/i-built-an-llm-framework-in-just).
- Â¿Tienes preguntas? Consulta este [Asistente de IA](https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-pocket-flow-assistant), o [Â¡crea un issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
- ðŸŽ‰ Â¡Ãšnete a nuestro [Discord](https://discord.gg/hUHHE9Sa6T) para conectar con otros desarrolladores construyendo con Pocket Flow!
- ðŸŽ‰ Pocket Flow inicialmente estÃ¡ en Python, Â¡pero ahora tenemos versiones en [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP) y [Go](https://github.com/The-Pocket/PocketFlow-Go)!

## Â¿Por quÃ© Pocket Flow?

Los frameworks actuales de LLM estÃ¡n sobrecargados... Â¡Solo necesitas 100 lÃ­neas para un framework de LLM!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **AbstracciÃ³n**          | **Envolturas EspecÃ­ficas de AplicaciÃ³n**                                      | **Envolturas EspecÃ­ficas de Proveedor**                                    | **LÃ­neas**       | **TamaÃ±o**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agente, Cadena               | Muchas <br><sup><sub>(p.ej., QA, Resumen)</sub></sup>              | Muchas <br><sup><sub>(p.ej., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agente, Cadena            | Muchas <br><sup><sub>(p.ej., FileReadTool, SerperDevTool)</sub></sup>         | Muchas <br><sup><sub>(p.ej., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agente                      | Algunas <br><sup><sub>(p.ej., CodeAgent, VisitWebTool)</sub></sup>         | Algunas <br><sup><sub>(p.ej., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agente, Grafo           | Algunas <br><sup><sub>(p.ej., BÃºsqueda SemÃ¡ntica)</sub></sup>                     | Algunas <br><sup><sub>(p.ej., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agente                | Algunas <br><sup><sub>(p.ej., Tool Agent, Chat Agent)</sub></sup>              | Muchas <sup><sub>[Opcional]<br> (p.ej., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(solo-nÃºcleo)</sub></sup>    | +26MB <br><sup><sub>(solo-nÃºcleo)</sub></sup>          |
| **PocketFlow** | **Grafo**                    | **Ninguna**                                                 | **Ninguna**                                                  | **100**       | **+56KB**                  |

</div>

## Â¿CÃ³mo funciona Pocket Flow?

Las [100 lÃ­neas](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capturan la abstracciÃ³n principal de los frameworks de LLM: Â¡el Grafo!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

A partir de ahÃ­, es fÃ¡cil implementar patrones de diseÃ±o populares como ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agentes](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Flujo de Trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ A continuaciÃ³n se presentan tutoriales bÃ¡sicos:

<div align="center">
  
|  Nombre  | Dificultad    |  DescripciÃ³n  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *Principiante*   | Un chatbot bÃ¡sico con historial de conversaciÃ³n |
| [Salida Estructurada](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *Principiante* | ExtracciÃ³n de datos estructurados de currÃ­culums mediante prompts |
| [Flujo de Trabajo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *Principiante*   | Un flujo de escritura que esquematiza, escribe contenido y aplica estilo |
| [Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *Principiante*   | Un agente de investigaciÃ³n que puede buscar en la web y responder preguntas |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *Principiante*   | Un simple proceso de GeneraciÃ³n aumentada por RecuperaciÃ³n |
| [Procesamiento por Lotes](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <br> *Principiante* | Un procesador por lotes que traduce contenido markdown a mÃºltiples idiomas |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *Principiante*   | Una demostraciÃ³n de streaming LLM en tiempo real con capacidad de interrupciÃ³n del usuario |
| [ProtecciÃ³n de Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *Principiante*  | Un chatbot asesor de viajes que solo procesa consultas relacionadas con viajes |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜…â˜†â˜† <br> *Inicial* | Un procesador de calificaciÃ³n de currÃ­culums que utiliza el patrÃ³n map-reduce para evaluaciÃ³n por lotes |
| [Multi-Agente](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Inicial* | Un juego de palabras TabÃº para comunicaciÃ³n asÃ­ncrona entre dos agentes |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Inicial* | El agente de investigaciÃ³n se vuelve poco fiable... Construyamos un proceso de supervisiÃ³n|
| [Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Inicial*   | Una demostraciÃ³n de ejecuciÃ³n paralela que muestra una aceleraciÃ³n de 3x |
| [Flujo Paralelo](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Inicial*   | Una demostraciÃ³n de procesamiento de imÃ¡genes en paralelo que muestra una aceleraciÃ³n de 8x con mÃºltiples filtros |
| [Voto por MayorÃ­a](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜…â˜†â˜† <br> *Inicial* | Mejora de la precisiÃ³n del razonamiento mediante la agregaciÃ³n de mÃºltiples intentos de soluciÃ³n |
| [Pensamiento](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Inicial*   | Resolver problemas de razonamiento complejos a travÃ©s de Cadena de Pensamiento |
| [Memoria](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Inicial* | Un chatbot con memoria a corto y largo plazo |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) | â˜…â˜†â˜† <br> *Inicial* | Convertir lenguaje natural a consultas SQL con un bucle de auto-depuraciÃ³n |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Inicial* | Agente que utiliza el Protocolo de Contexto de Modelo para operaciones numÃ©ricas |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) | â˜…â˜†â˜† <br> *Inicial* | Agente envuelto con protocolo Agente-a-Agente para comunicaciÃ³n entre agentes |
| [Web HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-web-hitl) | â˜…â˜†â˜† <br> *Inicial* | Un servicio web mÃ­nimo para un bucle de revisiÃ³n humana con actualizaciones SSE |

</div>

ðŸ‘€ Â¿Quieres ver otros tutoriales para principiantes? [Â¡Crea un issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## Â¿CÃ³mo usar Pocket Flow?

ðŸš€ A travÃ©s de la **ProgramaciÃ³n mediante Agentes**â€”el paradigma de desarrollo de aplicaciones LLM mÃ¡s rÃ¡pido- donde *los humanos diseÃ±an* y *los agentes programan*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ A continuaciÃ³n hay ejemplos de aplicaciones LLM mÃ¡s complejas:

<div align="center">
  
|  Nombre de la App     |  Dificultad    | Temas  | DiseÃ±o Humano | CÃ³digo del Agente |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Construir Cursor con Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>Pronto alcanzaremos la singularidad ...</sup></sub> | â˜…â˜…â˜… <br> *Avanzado*   | [Agente](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Constructor de Conocimiento de CÃ³digo Base](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>La vida es demasiado corta para mirar el cÃ³digo de otros con confusiÃ³n</sup></sub> |  â˜…â˜…â˜† <br> *Medio* | [Flujo de Trabajo](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Pregunta a AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Pregunta a AI Paul Graham, en caso de que no entres</sup></sub> | â˜…â˜…â˜† <br> *Medio*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Resumidor de Youtube](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explica videos de YouTube como si tuvieras 5 aÃ±os </sup></sub> | â˜…â˜†â˜† <br> *Principiante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Generador de IntroducciÃ³n para Email FrÃ­o](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Rompehielos instantÃ¡neos que convierten leads frÃ­os en calientes </sup></sub> | â˜…â˜†â˜† <br> *Principiante*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [BÃºsqueda Web](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Doc de DiseÃ±o](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [CÃ³digo de Flujo](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)

</div>

- Â¿Quieres aprender **ProgramaciÃ³n mediante Agentes**?

  - Â¡Consulta [mi YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) para ver tutoriales en video sobre cÃ³mo se crearon algunas de las aplicaciones anteriores!

  - Â¿Quieres construir tu propia aplicaciÃ³n LLM? Â¡Lee este [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Â¡Comienza con [esta plantilla](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="cookbook/pocketflow-parallel-batch/main.py">
import asyncio
import time
import os
from pocketflow import AsyncFlow, AsyncParallelBatchNode
from utils import call_llm

# --- Node Definitions ---

class TranslateTextNodeParallel(AsyncParallelBatchNode):
    """Translates README into multiple languages in parallel and saves files."""
    async def prep_async(self, shared):
        """Reads text and target languages from shared store."""
        text = shared.get("text", "(No text provided)")
        languages = shared.get("languages", [])
        return [(text, lang) for lang in languages]

    async def exec_async(self, data_tuple):
        """Calls the async LLM utility for each target language."""
        text, language = data_tuple
        
        prompt = f"""
Please translate the following markdown file into {language}. 
But keep the original markdown format, links and code blocks.
Directly return the translated text, without any other text or comments.

Original: 
{text}

Translated:"""
        
        result = await call_llm(prompt)
        print(f"Translated {language} text")
        return {"language": language, "translation": result}

    async def post_async(self, shared, prep_res, exec_res_list):
        """Stores the dictionary of {language: translation} pairs and writes to files."""
        output_dir = shared.get("output_dir", "translations")
        os.makedirs(output_dir, exist_ok=True)
        
        for result in exec_res_list:
            if isinstance(result, dict):
                language = result.get("language", "unknown")
                translation = result.get("translation", "")
                
                filename = os.path.join(output_dir, f"README_{language.upper()}.md")
                try:
                    import aiofiles
                    async with aiofiles.open(filename, "w", encoding="utf-8") as f:
                        await f.write(translation)
                    print(f"Saved translation to {filename}")
                except ImportError:
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(translation)
                    print(f"Saved translation to {filename} (sync fallback)")
                except Exception as e:
                    print(f"Error writing file {filename}: {e}")
            else:
                print(f"Warning: Skipping invalid result item: {result}")
        return "default"

# --- Flow Creation ---

def create_parallel_translation_flow():
    """Creates and returns the parallel translation flow."""
    translate_node = TranslateTextNodeParallel(max_retries=3)
    return AsyncFlow(start=translate_node)

# --- Main Execution ---

async def main():
    source_readme_path = "../../README.md"
    try:
        with open(source_readme_path, "r", encoding='utf-8') as f:
            text = f.read()
    except FileNotFoundError:
        print(f"Error: Could not find the source README file at {source_readme_path}")
        exit(1)
    except Exception as e:
        print(f"Error reading file {source_readme_path}: {e}")
        exit(1)

    shared = {
        "text": text,
        "languages": ["Chinese", "Spanish", "Japanese", "German", "Russian", "Portuguese", "French", "Korean"],
        "output_dir": "translations"
    }

    translation_flow = create_parallel_translation_flow()

    print(f"Starting parallel translation into {len(shared['languages'])} languages...")
    start_time = time.perf_counter()

    await translation_flow.run_async(shared)

    end_time = time.perf_counter()
    duration = end_time - start_time
    print(f"\nTotal parallel translation time: {duration:.4f} seconds")
    print("\n=== Translation Complete ===")
    print(f"Translations saved to: {shared['output_dir']}")
    print("============================")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-parallel-batch/README.md">
# Parallel Batch Translation Process

This project demonstrates using PocketFlow's async and parallel features (`AsyncFlow`, `AsyncParallelBatchNode`) to translate a document into multiple languages concurrently.

- Check out the [Substack Post Tutorial](https://pocketflow.substack.com/p/parallel-llm-calls-from-scratch-tutorial) for more!

## Goal

Translate `../../README.md` into multiple languages (Chinese, Spanish, etc.) in parallel, saving each to a file in the `translations/` directory. The main goal is to compare execution time against a sequential process.

## Getting Started

1. Install requirements:
```bash
pip install -r requirements.txt
```

2. Set API Key:
   Set the environment variable for your Anthropic API key.
   ```bash
   export ANTHROPIC_API_KEY="your-api-key-here"
   ```
   *(Replace `"your-api-key-here"` with your actual key)*
   *(Alternatively, place `ANTHROPIC_API_KEY=your-api-key-here` in a `.env` file)*

3. Verify API Key (Optional):
   Run a quick check using the utility script.
   ```bash
   python utils.py
   ```
   *(Note: This requires a valid API key to be set.)*

4. Run the translation process:
   ```bash
   python main.py
   ```

## How It Works

The implementation uses an `AsyncParallelBatchNode` that processes translation requests concurrently. The `TranslateTextNodeParallel`:

1. Prepares batches, pairing the source text with each target language.

2. Executes translation calls to the LLM for all languages concurrently using `async` operations.

3. Saves the translated content to individual files (`translations/README_LANGUAGE.md`).

This approach leverages `asyncio` and parallel execution to speed up I/O-bound tasks like multiple API calls.

## Example Output & Comparison

Running this parallel version significantly reduces the total time compared to a sequential approach:

```
# --- Sequential Run Output (from pocketflow-batch) ---
Starting sequential translation into 8 languages...
Translated Chinese text
...
Translated Korean text
Saved translation to translations/README_CHINESE.md
...
Saved translation to translations/README_KOREAN.md

Total sequential translation time: ~1136 seconds

=== Translation Complete ===
Translations saved to: translations
============================


# --- Parallel Run Output (this example) ---
Starting parallel translation into 8 languages...
Translated French text
Translated Portuguese text
... # Messages may appear interleaved
Translated Spanish text
Saved translation to translations/README_CHINESE.md
...
Saved translation to translations/README_KOREAN.md

Total parallel translation time: ~209 seconds

=== Translation Complete ===
Translations saved to: translations
============================
```
*(Actual times will vary based on API response speed and system.)*

## Files

- [`main.py`](./main.py): Implements the parallel batch translation node and flow.
- [`utils.py`](./utils.py): Async wrapper for calling the Anthropic model.
- [`requirements.txt`](./requirements.txt): Project dependencies (includes `aiofiles`).
- [`translations/`](./translations/): Output directory (created automatically).
</file>

<file path="cookbook/pocketflow-parallel-batch/requirements.txt">
pocketflow>=0.0.2
anthropic>=0.15.0
python-dotenv
httpx
aiofiles
</file>

<file path="cookbook/pocketflow-parallel-batch/utils.py">
import os
import asyncio
from anthropic import AsyncAnthropic

# Async version of the simple wrapper, using Anthropic
async def call_llm(prompt):
    """Async wrapper for Anthropic API call."""
    client = AsyncAnthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
    response = await client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=20000,
        thinking={
            "type": "enabled",
            "budget_tokens": 16000
        },
        messages=[
            {"role": "user", "content": prompt}
        ],
    )
    return response.content[1].text

if __name__ == "__main__":
    async def run_test():
        print("## Testing async call_llm with Anthropic")
        prompt = "In a few words, what is the meaning of life?"
        print(f"## Prompt: {prompt}")
        response = await call_llm(prompt)
        print(f"## Response: {response}")

    asyncio.run(run_test())
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/flow.py">
"""Flow definitions for parallel image processing."""

from pocketflow import AsyncParallelBatchFlow, AsyncBatchFlow
from nodes import LoadImage, ApplyFilter, SaveImage

def create_base_flow():
    """Create flow for processing a single image with one filter."""
    # Create nodes
    load = LoadImage()
    apply_filter = ApplyFilter()
    save = SaveImage()
    
    # Connect nodes
    load - "apply_filter" >> apply_filter
    apply_filter - "save" >> save
    
    # Create flow
    return load

class ImageBatchFlow(AsyncBatchFlow):
    """Flow that processes multiple images with multiple filters in batch."""
    
    async def prep_async(self, shared):
        """Generate parameters for each image-filter combination."""
        # Get list of images and filters
        images = shared.get("images", [])
        filters = ["grayscale", "blur", "sepia"]
        
        # Create parameter combinations
        params = []
        for image_path in images:
            for filter_type in filters:
                params.append({
                    "image_path": image_path,
                    "filter": filter_type
                })
        
        print(f"Processing {len(images)} images with {len(filters)} filters...")
        print(f"Total combinations: {len(params)}")
        return params

class ImageParallelBatchFlow(AsyncParallelBatchFlow):
    """Flow that processes multiple images with multiple filters in parallel."""

    async def prep_async(self, shared):
        """Generate parameters for each image-filter combination."""
        # Get list of images and filters
        images = shared.get("images", [])
        filters = ["grayscale", "blur", "sepia"]
        
        # Create parameter combinations
        params = []
        for image_path in images:
            for filter_type in filters:
                params.append({
                    "image_path": image_path,
                    "filter": filter_type
                })
        
        print(f"Processing {len(images)} images with {len(filters)} filters...")
        print(f"Total combinations: {len(params)}")
        return params

def create_flows():
    """Create the complete parallel processing flow."""
    # Create base flow for single image processing
    base_flow = create_base_flow()
    
    # Wrap in parallel batch flow
    return ImageBatchFlow(start=base_flow), ImageParallelBatchFlow(start=base_flow)
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/main.py">
import os
import asyncio
import time
from flow import create_flows

def get_image_paths():
    """Get paths of existing images in the images directory."""
    images_dir = "images"
    if not os.path.exists(images_dir):
        raise ValueError(f"Directory '{images_dir}' not found!")
    
    # List all jpg files in the images directory
    image_paths = []
    for filename in os.listdir(images_dir):
        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
            image_paths.append(os.path.join(images_dir, filename))
    
    if not image_paths:
        raise ValueError(f"No images found in '{images_dir}' directory!")
    
    print(f"Found {len(image_paths)} images:")
    for path in image_paths:
        print(f"- {path}")
    
    return image_paths

async def main():
    """Run the parallel image processing example."""
    print("Parallel Image Processor")
    print("-" * 30)
    
    # Get existing image paths
    image_paths = get_image_paths()
    
    # Create shared store with image paths
    shared = {"images": image_paths}
    
    # Create both flows
    batch_flow, parallel_batch_flow = create_flows()
    
    # Run and time batch flow
    start_time = time.time()
    print("\nRunning sequential batch flow...")
    await batch_flow.run_async(shared)
    batch_time = time.time() - start_time
    
    # Run and time parallel batch flow
    start_time = time.time()
    print("\nRunning parallel batch flow...")
    await parallel_batch_flow.run_async(shared)
    parallel_time = time.time() - start_time
    
    # Print timing results
    print("\nTiming Results:")
    print(f"Sequential batch processing: {batch_time:.2f} seconds")
    print(f"Parallel batch processing: {parallel_time:.2f} seconds")
    print(f"Speedup: {batch_time/parallel_time:.2f}x")
    
    print("\nProcessing complete! Check the output/ directory for results.")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/nodes.py">
"""AsyncNode implementations for image processing."""
import os
import asyncio
from PIL import Image, ImageFilter
import numpy as np
from pocketflow import AsyncNode

class LoadImage(AsyncNode):
    """Node that loads an image from file."""
    async def prep_async(self, shared):
        """Get image path from parameters."""
        image_path = self.params["image_path"]
        print(f"Loading image: {image_path}")
        return image_path
    
    async def exec_async(self, image_path):
        """Load image using PIL."""
        # Simulate I/O delay
        await asyncio.sleep(0.5)
        return Image.open(image_path)
    
    async def post_async(self, shared, prep_res, exec_res):
        """Store image in shared store."""
        shared["image"] = exec_res
        return "apply_filter"

class ApplyFilter(AsyncNode):
    """Node that applies a filter to an image."""
    async def prep_async(self, shared):
        """Get image and filter type."""
        image = shared["image"]
        filter_type = self.params["filter"]
        print(f"Applying {filter_type} filter...")
        return image, filter_type
    
    async def exec_async(self, inputs):
        """Apply the specified filter."""
        image, filter_type = inputs
        
        # Simulate processing delay
        await asyncio.sleep(0.5)
        
        if filter_type == "grayscale":
            return image.convert("L")
        elif filter_type == "blur":
            return image.filter(ImageFilter.BLUR)
        elif filter_type == "sepia":
            # Convert to array for sepia calculation
            img_array = np.array(image)
            sepia_matrix = np.array([
                [0.393, 0.769, 0.189],
                [0.349, 0.686, 0.168],
                [0.272, 0.534, 0.131]
            ])
            sepia_array = img_array.dot(sepia_matrix.T)
            sepia_array = np.clip(sepia_array, 0, 255).astype(np.uint8)
            return Image.fromarray(sepia_array)
        else:
            raise ValueError(f"Unknown filter: {filter_type}")
    
    async def post_async(self, shared, prep_res, exec_res):
        """Store filtered image."""
        shared["filtered_image"] = exec_res
        return "save"

class SaveImage(AsyncNode):
    """Node that saves the processed image."""
    async def prep_async(self, shared):
        """Prepare output path."""
        image = shared["filtered_image"]
        base_name = os.path.splitext(os.path.basename(self.params["image_path"]))[0]
        filter_type = self.params["filter"]
        output_path = f"output/{base_name}_{filter_type}.jpg"
        
        # Create output directory if needed
        os.makedirs("output", exist_ok=True)
        
        return image, output_path
    
    async def exec_async(self, inputs):
        """Save the image."""
        image, output_path = inputs
        
        # Simulate I/O delay
        await asyncio.sleep(0.5)
        
        image.save(output_path)
        return output_path
    
    async def post_async(self, shared, prep_res, exec_res):
        """Print success message."""
        print(f"Saved: {exec_res}")
        return "default"
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/README.md">
# Parallel Image Processor

Demonstrates how AsyncParallelBatchFlow processes multiple images with multiple filters >8x faster than sequential processing.

## Features

  ```mermaid
  graph TD
      subgraph AsyncParallelBatchFlow[Image Processing Flow]
          subgraph AsyncFlow[Per Image-Filter Flow]
              A[Load Image] --> B[Apply Filter]
              B --> C[Save Image]
          end
      end
  ```
  
- Processes images with multiple filters in parallel
- Applies three different filters (grayscale, blur, sepia)
- Shows significant speed improvement over sequential processing
- Manages system resources with semaphores

## Run It

```bash
pip install -r requirements.txt
python main.py
```

## Output

```=== Processing Images in Parallel ===
Parallel Image Processor
------------------------------
Found 3 images:
- images/bird.jpg
- images/cat.jpg
- images/dog.jpg

Running sequential batch flow...
Processing 3 images with 3 filters...
Total combinations: 9
Loading image: images/bird.jpg
Applying grayscale filter...
Saved: output/bird_grayscale.jpg
...etc

Timing Results:
Sequential batch processing: 13.76 seconds
Parallel batch processing: 1.71 seconds
Speedup: 8.04x

Processing complete! Check the output/ directory for results.
```

## Key Points

- **Sequential**: Total time = sum of all item times
  - Good for: Rate-limited APIs, maintaining order

- **Parallel**: Total time â‰ˆ longest single item time
  - Good for: I/O-bound tasks, independent operations
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/requirements.txt">
pocketflow
Pillow>=10.0.0  # For image processing
numpy>=1.24.0   # For image array operations
</file>

<file path="cookbook/pocketflow-rag/flow.py">
from pocketflow import Flow
from nodes import EmbedDocumentsNode, CreateIndexNode, EmbedQueryNode, RetrieveDocumentNode, ChunkDocumentsNode, GenerateAnswerNode

def get_offline_flow():
    # Create offline flow for document indexing
    chunk_docs_node = ChunkDocumentsNode()
    embed_docs_node = EmbedDocumentsNode()
    create_index_node = CreateIndexNode()
    
    # Connect the nodes
    chunk_docs_node >> embed_docs_node >> create_index_node
    
    offline_flow = Flow(start=chunk_docs_node)
    return offline_flow

def get_online_flow():
    # Create online flow for document retrieval and answer generation
    embed_query_node = EmbedQueryNode()
    retrieve_doc_node = RetrieveDocumentNode()
    generate_answer_node = GenerateAnswerNode()
    
    # Connect the nodes
    embed_query_node >> retrieve_doc_node >> generate_answer_node
    
    online_flow = Flow(start=embed_query_node)
    return online_flow

# Initialize flows
offline_flow = get_offline_flow()
online_flow = get_online_flow()
</file>

<file path="cookbook/pocketflow-rag/main.py">
import sys
from flow import offline_flow, online_flow

def run_rag_demo():
    """
    Run a demonstration of the RAG system.
    
    This function:
    1. Indexes a set of sample documents (offline flow)
    2. Takes a query from the command line
    3. Retrieves the most relevant document (online flow)
    4. Generates an answer using an LLM
    """

    # Sample texts - specialized/fictional content that benefits from RAG
    texts = [
        # PocketFlow framework
        """Pocket Flow is a 100-line minimalist LLM framework
        Lightweight: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.
        Expressive: Everything you loveâ€”(Multi-)Agents, Workflow, RAG, and more.
        Agentic Coding: Let AI Agents (e.g., Cursor AI) build Agentsâ€”10x productivity boost!
        To install, pip install pocketflow or just copy the source code (only 100 lines).""",
        
        # Fictional medical device
        """NeurAlign M7 is a revolutionary non-invasive neural alignment device.
        Targeted magnetic resonance technology increases neuroplasticity in specific brain regions.
        Clinical trials showed 72% improvement in PTSD treatment outcomes.
        Developed by Cortex Medical in 2024 as an adjunct to standard cognitive therapy.
        Portable design allows for in-home use with remote practitioner monitoring.""",
        
        # Made-up historical event
        """The Velvet Revolution of Caldonia (1967-1968) ended Generalissimo Verak's 40-year rule.
        Led by poet Eliza Markovian through underground literary societies.
        Culminated in the Great Silence Protest with 300,000 silent protesters.
        First democratic elections held in March 1968 with 94% voter turnout.
        Became a model for non-violent political transitions in neighboring regions.""",
        
        # Fictional technology 
        """Q-Mesh is QuantumLeap Technologies' instantaneous data synchronization protocol.
        Utilizes directed acyclic graph consensus for 500,000 transactions per second.
        Consumes 95% less energy than traditional blockchain systems.
        Adopted by three central banks for secure financial data transfer.
        Released in February 2024 after five years of development in stealth mode.""",
        
        # Made-up scientific research
        """Harlow Institute's Mycelium Strain HI-271 removes 99.7% of PFAS from contaminated soil.
        Engineered fungi create symbiotic relationships with native soil bacteria.
        Breaks down "forever chemicals" into non-toxic compounds within 60 days.
        Field tests successfully remediated previously permanently contaminated industrial sites.
        Deployment costs 80% less than traditional chemical extraction methods."""
    ]
    
    print("=" * 50)
    print("PocketFlow RAG Document Retrieval")
    print("=" * 50)
    
    # Default query about the fictional technology
    default_query = "How to install PocketFlow?"
    
    # Get query from command line if provided with --
    query = default_query
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            query = arg[2:]
            break
    
    # Single shared store for both flows
    shared = {
        "texts": texts,
        "embeddings": None,
        "index": None,
        "query": query,
        "query_embedding": None,
        "retrieved_document": None,
        "generated_answer": None
    }
    
    # Initialize and run the offline flow (document indexing)
    offline_flow.run(shared)
    
    # Run the online flow to retrieve the most relevant document and generate an answer
    online_flow.run(shared)


if __name__ == "__main__":
    run_rag_demo()
</file>

<file path="cookbook/pocketflow-rag/nodes.py">
from pocketflow import Node, Flow, BatchNode
import numpy as np
import faiss
from utils import call_llm, get_embedding, fixed_size_chunk

# Nodes for the offline flow
class ChunkDocumentsNode(BatchNode):
    def prep(self, shared):
        """Read texts from shared store"""
        return shared["texts"]
    
    def exec(self, text):
        """Chunk a single text into smaller pieces"""
        return fixed_size_chunk(text)
    
    def post(self, shared, prep_res, exec_res_list):
        """Store chunked texts in the shared store"""
        # Flatten the list of lists into a single list of chunks
        all_chunks = []
        for chunks in exec_res_list:
            all_chunks.extend(chunks)
        
        # Replace the original texts with the flat list of chunks
        shared["texts"] = all_chunks
        
        print(f"âœ… Created {len(all_chunks)} chunks from {len(prep_res)} documents")
        return "default"
    
class EmbedDocumentsNode(BatchNode):
    def prep(self, shared):
        """Read texts from shared store and return as an iterable"""
        return shared["texts"]
    
    def exec(self, text):
        """Embed a single text"""
        return get_embedding(text)
    
    def post(self, shared, prep_res, exec_res_list):
        """Store embeddings in the shared store"""
        embeddings = np.array(exec_res_list, dtype=np.float32)
        shared["embeddings"] = embeddings
        print(f"âœ… Created {len(embeddings)} document embeddings")
        return "default"

class CreateIndexNode(Node):
    def prep(self, shared):
        """Get embeddings from shared store"""
        return shared["embeddings"]
    
    def exec(self, embeddings):
        """Create FAISS index and add embeddings"""
        print("ðŸ” Creating search index...")
        dimension = embeddings.shape[1]
        
        # Create a flat L2 index
        index = faiss.IndexFlatL2(dimension)
        
        # Add the embeddings to the index
        index.add(embeddings)
        
        return index
    
    def post(self, shared, prep_res, exec_res):
        """Store the index in shared store"""
        shared["index"] = exec_res
        print(f"âœ… Index created with {exec_res.ntotal} vectors")
        return "default"

# Nodes for the online flow
class EmbedQueryNode(Node):
    def prep(self, shared):
        """Get query from shared store"""
        return shared["query"]
    
    def exec(self, query):
        """Embed the query"""
        print(f"ðŸ” Embedding query: {query}")
        query_embedding = get_embedding(query)
        return np.array([query_embedding], dtype=np.float32)
    
    def post(self, shared, prep_res, exec_res):
        """Store query embedding in shared store"""
        shared["query_embedding"] = exec_res
        return "default"

class RetrieveDocumentNode(Node):
    def prep(self, shared):
        """Get query embedding, index, and texts from shared store"""
        return shared["query_embedding"], shared["index"], shared["texts"]
    
    def exec(self, inputs):
        """Search the index for similar documents"""
        print("ðŸ”Ž Searching for relevant documents...")
        query_embedding, index, texts = inputs
        
        # Search for the most similar document
        distances, indices = index.search(query_embedding, k=1)
        
        # Get the index of the most similar document
        best_idx = indices[0][0]
        distance = distances[0][0]
        
        # Get the corresponding text
        most_relevant_text = texts[best_idx]
        
        return {
            "text": most_relevant_text,
            "index": best_idx,
            "distance": distance
        }
    
    def post(self, shared, prep_res, exec_res):
        """Store retrieved document in shared store"""
        shared["retrieved_document"] = exec_res
        print(f"ðŸ“„ Retrieved document (index: {exec_res['index']}, distance: {exec_res['distance']:.4f})")
        print(f"ðŸ“„ Most relevant text: \"{exec_res['text']}\"")
        return "default"
    
class GenerateAnswerNode(Node):
    def prep(self, shared):
        """Get query, retrieved document, and any other context needed"""
        return shared["query"], shared["retrieved_document"]
    
    def exec(self, inputs):
        """Generate an answer using the LLM"""
        query, retrieved_doc = inputs
        
        prompt = f"""
Briefly answer the following question based on the context provided:
Question: {query}
Context: {retrieved_doc['text']}
Answer:
"""
        
        answer = call_llm(prompt)
        return answer
    
    def post(self, shared, prep_res, exec_res):
        """Store generated answer in shared store"""
        shared["generated_answer"] = exec_res
        print("\nðŸ¤– Generated Answer:")
        print(exec_res)
        return "default"
</file>

<file path="cookbook/pocketflow-rag/README.md">
# Retrieval Augmented Generation (RAG)

This project demonstrates a simplified RAG system that retrieves relevant documents based on user queries and generates answers using an LLM. This implementation is based directly on the tutorial: [Retrieval Augmented Generation (RAG) from Scratch â€” Tutorial For Dummies](https://zacharyhuang.substack.com/p/retrieval-augmented-generation-rag).


## Features

- Document chunking for processing long texts
- FAISS-powered vector-based document retrieval
- LLM-powered answer generation

## How to Run

1. Set your API key:
   ```bash
   export OPENAI_API_KEY="your-api-key-here"
   ```
   Or update it directly in `utils.py`

   Let's do a quick check to make sure your API key is working properly:

   ```bash
   python utils.py
   ```

2. Install and run with the default query:
   ```bash
   pip install -r requirements.txt
   python main.py
   ```

3. Run the application with a sample query:

   ```bash
   python main.py --"How does the Q-Mesh protocol achieve high transaction speeds?"
   ```

## How It Works

The magic happens through a two-phase pipeline implemented with PocketFlow:

```mermaid
graph TD
    subgraph OfflineFlow[Offline Document Indexing]
        ChunkDocs[ChunkDocumentsNode] --> EmbedDocs[EmbedDocumentsNode] --> CreateIndex[CreateIndexNode]
    end
    
    subgraph OnlineFlow[Online Processing]
        EmbedQuery[EmbedQueryNode] --> RetrieveDoc[RetrieveDocumentNode] --> GenerateAnswer[GenerateAnswerNode]
    end
```

Here's what each part does:
1. **ChunkDocumentsNode**: Breaks documents into smaller chunks for better retrieval
2. **EmbedDocumentsNode**: Converts document chunks into vector representations
3. **CreateIndexNode**: Creates a searchable FAISS index from embeddings
4. **EmbedQueryNode**: Converts user query into the same vector space
5. **RetrieveDocumentNode**: Finds the most similar document using vector search
6. **GenerateAnswerNode**: Uses an LLM to generate an answer based on the retrieved content

## Example Output

```
âœ… Created 5 chunks from 5 documents
âœ… Created 5 document embeddings
ðŸ” Creating search index...
âœ… Index created with 5 vectors
ðŸ” Embedding query: How to install PocketFlow?
ðŸ”Ž Searching for relevant documents...
ðŸ“„ Retrieved document (index: 0, distance: 0.3427)
ðŸ“„ Most relevant text: "Pocket Flow is a 100-line minimalist LLM framework
        Lightweight: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.
        Expressive: Everything you loveâ€”(Multi-)Agents, Workflow, RAG, and more.
        Agentic Coding: Let AI Agents (e.g., Cursor AI) build Agentsâ€”10x productivity boost!
        To install, pip install pocketflow or just copy the source code (only 100 lines)."

ðŸ¤– Generated Answer:
To install PocketFlow, use the command `pip install pocketflow` or simply copy its 100 lines of source code.
```
</file>

<file path="cookbook/pocketflow-rag/requirements.txt">
pocketflow>=0.0.1
numpy>=1.20.0
faiss-cpu>=1.7.0
openai>=1.0.0
</file>

<file path="cookbook/pocketflow-rag/utils.py">
import os
import numpy as np
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

def get_embedding(text):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    
    # Extract the embedding vector from the response
    embedding = response.data[0].embedding
    
    # Convert to numpy array for consistency with other embedding functions
    return np.array(embedding, dtype=np.float32)

def fixed_size_chunk(text, chunk_size=2000):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i : i + chunk_size])
    return chunks

if __name__ == "__main__":
    print("=== Testing call_llm ===")
    prompt = "In a few words, what is the meaning of life?"
    print(f"Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"Response: {response}")

    print("=== Testing embedding function ===")
    
    text1 = "The quick brown fox jumps over the lazy dog."
    text2 = "Python is a popular programming language for data science."
    
    oai_emb1 = get_embedding(text1)
    oai_emb2 = get_embedding(text2)
    print(f"OpenAI Embedding 1 shape: {oai_emb1.shape}")
    oai_similarity = np.dot(oai_emb1, oai_emb2)
    print(f"OpenAI similarity between texts: {oai_similarity:.4f}")
</file>

<file path="cookbook/pocketflow-streamlit-fsm/docs/design.md">
# Design Doc: PocketFlow Streamlit Image Generation HITL

> Human-in-the-Loop image generation application using PocketFlow and Streamlit

## Requirements

**User Story**: As a user, I want to:
1. Enter a text prompt describing an image I want to generate
2. Have the system generate an image based on my prompt using OpenAI's image generation API
3. Review the generated image in the web interface
4. Approve the image if I'm satisfied, OR regenerate with the same prompt if I want a different result
5. See the final approved image as the completed result

**Technical Requirements**:
- Use OpenAI's image generation API (via responses.create with image_generation tool)
- Keep generated images in memory (base64 format) - no disk storage
- Provide clear approve/regenerate workflow
- Handle API errors gracefully with retries
- Maintain session state between generations

## Flow Design

### Applicable Design Pattern:

**State Machine with Multiple Subflows**: Each state has its own user interface and workflow. Users interact with different UI elements in each state, and the app transitions to the next state based on user actions and feedback.

### States & User Interface:

1. **initial_input** - User sees text input field, enters prompt, clicks "Generate Image" button
2. **user_feedback** - User sees generated image, has "Approve" and "Regenerate" buttons 
3. **final** - User sees final approved image and "Start Over" button

### Flow High-level Design & Transitions:

```mermaid
flowchart TD
    Start([Start]) --> IS[initial_input]
    IS --> GI[GenerateImage]
    GI --> UF[user_feedback]
    UF -->|Regenerate| GI
    UF -->|Approve| F[final]
    F --> IS
    
    %% Legend
    classDef stateStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef nodeStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px
    
    class IS,UF,F stateStyle
    class GI nodeStyle
```

**Legend:**
- ðŸ”· **Blue rectangles**: User interface states (initial_input, user_feedback, final)
- ðŸ”¶ **Orange rectangles**: PocketFlow processing nodes (GenerateImage)

## Utility Functions

1. **Generate Image** (`utils/generate_image.py`)
   - *Input*: prompt (str)
   - *Output*: base64 image data (str)
   - *Purpose*: Calls OpenAI's image generation API and returns base64 encoded image
   - *Error Handling*: Includes retry logic for API failures

## Node Design

### Shared Memory

**Using Streamlit Session State as Shared Store**: We use `st.session_state` directly as the shared store for PocketFlow, eliminating the need for separate data structures.

The session state structure for the image generation workflow:

```python
st.session_state = {
    # User input and workflow state
    "task_input": "user's text prompt for image generation",
    "stage": "current workflow stage (initial_input/user_feedback/final)",
    "error_message": "any error messages for user feedback",
    
    # Processing data
    "input_used_by_process": "prompt used for generation",
    "generated_image": "base64 encoded image data",
    "final_result": "final approved image data",
    
    # Streamlit built-in keys (managed automatically)
    # "_streamlit_*": various internal streamlit state
}
```

### Node Steps

**Initial Input Flow Nodes:**

1. **Image Generation Node**
   - *Purpose*: Generate image using OpenAI API based on the prompt
   - *Type*: Regular (with retries for API reliability)
   - *Steps*:
     - *prep*: Read "input_used_by_process" from st.session_state
     - *exec*: Call generate_image utility with the prompt, return base64 image data
     - *post*: Write base64 image data to "generated_image" in st.session_state

**User Feedback Flow:**
- Reuses the same `GenerateImage` node when user clicks "Regenerate"

**Final Flow:**
- No processing nodes needed - the `final` state simply displays the approved image from `generated_image` and provides UI for starting over
</file>

<file path="cookbook/pocketflow-streamlit-fsm/utils/generate_image.py">
from openai import OpenAI
import os
import base64

def generate_image(prompt: str) -> str:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    response = client.images.generate(
        model="gpt-image-1",
        prompt=prompt,
        n=1,
        size="1024x1024"
    )
    
    image_b64 = response.data[0].b64_json
    print(f"Generated image ({len(image_b64)} chars)")
    return image_b64

if __name__ == "__main__":
    test_prompt = "A gray tabby cat hugging an otter with an orange scarf"
    print(f"Generating image for prompt: {test_prompt[:50]}...")
    
    image_base64 = generate_image(test_prompt)
    print(f"Success! Generated {len(image_base64)} characters of base64 data")
    
    # Write image to local file for testing
    image_bytes = base64.b64decode(image_base64)
    with open("test_generated_image.png", "wb") as f:
        f.write(image_bytes)
    print("Test image saved as test_generated_image.png")
</file>

<file path="cookbook/pocketflow-streamlit-fsm/app.py">
import streamlit as st
import base64
from flow import create_generation_flow

st.title("PocketFlow Image Generation HITL")

# Initialize session state for shared store
if 'stage' not in st.session_state:
    st.session_state.stage = "initial_input"
    st.session_state.task_input = ""
    st.session_state.generated_image = ""
    st.session_state.final_result = ""
    st.session_state.error_message = ""

# Debug info
with st.expander("Session State"):
    st.json({k: v for k, v in st.session_state.items() if not k.startswith("_")})

# State-based UI
if st.session_state.stage == "initial_input":
    st.header("1. Generate Image")
    
    prompt = st.text_area("Enter image prompt:", value=st.session_state.task_input, height=100)
    
    if st.button("Generate Image"):
        if prompt.strip():
            st.session_state.task_input = prompt
            st.session_state.error_message = ""
            
            try:
                with st.spinner("Generating image..."):
                    flow = create_generation_flow()
                    flow.run(st.session_state)
                st.rerun()
            except Exception as e:
                st.session_state.error_message = str(e)
        else:
            st.error("Please enter a prompt")

elif st.session_state.stage == "user_feedback":
    st.header("2. Review Generated Image")
    
    if st.session_state.generated_image:
        # Display image
        image_bytes = base64.b64decode(st.session_state.generated_image)
        st.image(image_bytes, caption=f"Prompt: {st.session_state.task_input}")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("Approve", use_container_width=True):
                st.session_state.final_result = st.session_state.generated_image
                st.session_state.stage = "final"
                st.rerun()
        
        with col2:
            if st.button("Regenerate", use_container_width=True):
                try:
                    with st.spinner("Regenerating image..."):
                        flow = create_generation_flow()
                        flow.run(st.session_state)
                    st.rerun()
                except Exception as e:
                    st.session_state.error_message = str(e)

elif st.session_state.stage == "final":
    st.header("3. Final Result")
    st.success("Image approved!")
    
    if st.session_state.final_result:
        image_bytes = base64.b64decode(st.session_state.final_result)
        st.image(image_bytes, caption=f"Final approved image: {st.session_state.task_input}")
    
    if st.button("Start Over", use_container_width=True):
        st.session_state.stage = "initial_input"
        st.session_state.task_input = ""
        st.session_state.generated_image = ""
        st.session_state.final_result = ""
        st.session_state.error_message = ""
        st.rerun()

# Show errors
if st.session_state.error_message:
    st.error(st.session_state.error_message)
</file>

<file path="cookbook/pocketflow-streamlit-fsm/flow.py">
from pocketflow import Flow
from nodes import GenerateImageNode

def create_generation_flow():
    """Creates a flow for image generation (initial or regeneration)."""
    generate_image_node = GenerateImageNode()
    return Flow(start=generate_image_node)
</file>

<file path="cookbook/pocketflow-streamlit-fsm/nodes.py">
from pocketflow import Node
from utils.generate_image import generate_image

class GenerateImageNode(Node):
    """Generates image from text prompt using OpenAI API."""
    
    def prep(self, shared):
        return shared.get("task_input", "")

    def exec(self, prompt):
        return generate_image(prompt)

    def post(self, shared, prep_res, exec_res):
        shared["input_used_by_process"] = prep_res
        shared["generated_image"] = exec_res
        shared["stage"] = "user_feedback"
        return "default"
</file>

<file path="cookbook/pocketflow-streamlit-fsm/README.md">
# PocketFlow Streamlit Image Generation HITL

Human-in-the-Loop (HITL) image generation application using PocketFlow and Streamlit. Enter text prompts, generate images with OpenAI, and approve/regenerate results.

<p align="center">
  <img 
    src="./assets/banner.png" width="800"
  />
</p>

## Features

-   **Image Generation:** Uses OpenAI's `gpt-image-1` model to generate images from text prompts
-   **Human Review:** Interactive interface to approve or regenerate images
-   **State Machine:** Clean state-based workflow (`initial_input` â†’ `user_feedback` â†’ `final`)
-   **PocketFlow Integration:** Uses PocketFlow `Node` and `Flow` for image generation with built-in retries
-   **Session State Management:** Streamlit session state acts as PocketFlow's shared store
-   **In-Memory Images:** Images stored as base64 strings, no disk storage required

## How to Run

1.  **Set OpenAI API Key:**
    ```bash
    export OPENAI_API_KEY="your-openai-api-key"
    ```

2.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the Streamlit Application:**
    ```bash
    streamlit run app.py
    ```

4.  **Access the Web UI:**
    Open the URL provided by Streamlit (usually `http://localhost:8501`).

## Usage

1. **Enter Prompt**: Describe the image you want to generate
2. **Generate**: Click "Generate Image" to create the image
3. **Review**: View the generated image and choose:
   - **Approve**: Accept the image and move to final result
   - **Regenerate**: Generate a new image with the same prompt
4. **Final**: View approved image and optionally start over

## Files

-   [`app.py`](./app.py): Main Streamlit application with state-based UI
-   [`nodes.py`](./nodes.py): PocketFlow `GenerateImageNode` definition
-   [`flow.py`](./flow.py): PocketFlow `Flow` for image generation
-   [`utils/generate_image.py`](./utils/generate_image.py): OpenAI image generation utility
-   [`requirements.txt`](./requirements.txt): Project dependencies
-   [`docs/design.md`](./docs/design.md): System design documentation
-   [`README.md`](./README.md): This file
</file>

<file path="cookbook/pocketflow-streamlit-fsm/requirements.txt">
streamlit
pocketflow
openai
</file>

<file path="cookbook/pocketflow-structured-output/data.txt">
# JOHN SMTIH

**Email:** johnsmtih1983@gnail.com
**Phone:** (555) 123-4556
**Address:** 123 Main st, Anytown, USA

## PROFFESIONAL SUMMARY

Dedicated and hardworking professional with over 10 years of exprience in business manegement. Known for finding creatve solutions to complex problems and excelent communication skills. Seeking new opportunites to leverage my expertise in a dynamic environment.

## WORK EXPERENCE

### SALES MANAGER
**ABC Corportaion** | Anytown, USA | June 2018 - Present
- Oversee a team of 12 sales represenatives and achieve quarterly targets
- Increased departmnet revenue by 24% in fiscal year 2019-2020
- Implemneted new CRM system that improved efficiency by 15%
- Collabarate with Marketing team on product launch campaigns
- Developed training materials for new hiers

### ASST. MANAGER
**XYZ Industries** | Somewhere Else, USA | March 2015 - may 2018
- Assisted the Regional Manager in daily operations and reporting
- managed inventory and vendor relations
- Trained and mentored junior staff members
- Recieved "Employee of the Month" award 4 times

### CUSTOMER SERVICE REPRESENTATIVE
**Fast Solutions Inc** | Another City, USA | January 2010 - February 2015
* Responded to customer inquiries via phone email, and in-person
* Resolved customer complaints and escalated issues when necessary
* Maintained a 95% customer satsfaction rating


## EDUCATIONS

**Bachelor of Buisness Administration**
University of Somewhere | 2006 - 2010
GPA: 3.6/4.0

**Assosiate Degree in Communications**
Community College | 2004-2006

## SKILSS

- Microsoft Office: *Excel, Word, Powerpoint* (Advanced)
- Customer relationship management (CRM) software
- Team leadership & managment
- Project management
- Public speking
- Time managemant

## REFERENCES

Available upon reqeust

### OTHER ACTVITIES
- Volunteer at the local food bank (2016-present)
- Member of Toastmasters International
- Enjoy hiking and photografy
</file>

<file path="cookbook/pocketflow-structured-output/main.py">
import yaml
import os  # Needed for the utils import below
from pocketflow import Node, Flow
from utils import call_llm # Assumes utils.py with call_llm exists

class ResumeParserNode(Node):
    def prep(self, shared):
        """Return resume text and target skills from shared state."""
        return {
            "resume_text": shared["resume_text"],
            "target_skills": shared.get("target_skills", [])
        }

    def exec(self, prep_res):
        """Extract structured data from resume using prompt engineering.
        Requests YAML output with comments and skill indexes as a list.
        """
        resume_text = prep_res["resume_text"]
        target_skills = prep_res["target_skills"]

        # Format skills with indexes for the prompt
        skill_list_for_prompt = "\n".join([f"{i}: {skill}" for i, skill in enumerate(target_skills)])

        # Simplified Prompt focusing on key instructions and format
        prompt = f"""
Analyze the resume below. Output ONLY the requested information in YAML format.

**Resume:**
```
{resume_text}
```

**Target Skills (use these indexes):**
```
{skill_list_for_prompt}
```

**YAML Output Requirements:**
- Extract `name` (string).
- Extract `email` (string).
- Extract `experience` (list of objects with `title` and `company`).
- Extract `skill_indexes` (list of integers found from the Target Skills list).
- **Add a YAML comment (`#`) explaining the source BEFORE `name`, `email`, `experience`, each item in `experience`, and `skill_indexes`.**

**Example Format:**
```yaml
# Found name at top
name: Jane Doe
# Found email in contact info
email: jane@example.com
# Experience section analysis
experience:
  # First job listed
  - title: Manager
    company: Corp A
  # Second job listed
  - title: Assistant
    company: Corp B
# Skills identified from the target list based on resume content
skill_indexes:
  # Found 0 at top  
  - 0
  # Found 2 in experience
  - 2
```

Generate the YAML output now:
"""
        response = call_llm(prompt)

        # --- Minimal YAML Extraction ---
        # Assumes LLM correctly uses ```yaml blocks
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        structured_result = yaml.safe_load(yaml_str)
        # --- End Minimal Extraction ---

        # --- Basic Validation ---
        assert structured_result is not None, "Validation Failed: Parsed YAML is None"
        assert "name" in structured_result, "Validation Failed: Missing 'name'"
        assert "email" in structured_result, "Validation Failed: Missing 'email'"
        assert "experience" in structured_result, "Validation Failed: Missing 'experience'"
        assert isinstance(structured_result.get("experience"), list), "Validation Failed: 'experience' is not a list"
        assert "skill_indexes" in structured_result, "Validation Failed: Missing 'skill_indexes'"
        skill_indexes_val = structured_result.get("skill_indexes")
        assert skill_indexes_val is None or isinstance(skill_indexes_val, list), "Validation Failed: 'skill_indexes' is not a list or None"
        if isinstance(skill_indexes_val, list):
             for index in skill_indexes_val:
                 assert isinstance(index, int), f"Validation Failed: Skill index '{index}' is not an integer"
        # --- End Basic Validation ---

        return structured_result

    def post(self, shared, prep_res, exec_res):
        """Store structured data and print it."""
        shared["structured_data"] = exec_res

        print("\n=== STRUCTURED RESUME DATA (Comments & Skill Index List) ===\n")
        # Dump YAML ensuring block style for readability
        print(yaml.dump(exec_res, sort_keys=False, allow_unicode=True, default_flow_style=None))
        print("\n============================================================\n")
        print("âœ… Extracted resume information.")


# === Main Execution Logic ===
if __name__ == "__main__":
    print("=== Resume Parser - Structured Output with Indexes & Comments ===\n")

    # --- Configuration ---
    target_skills_to_find = [
        "Team leadership & management", # 0
        "CRM software",                 # 1
        "Project management",           # 2
        "Public speaking",              # 3
        "Microsoft Office",             # 4
        "Python",                       # 5
        "Data Analysis"                 # 6
    ]
    resume_file = 'data.txt' # Assumes data.txt contains the resume

    # --- Prepare Shared State ---
    shared = {}
    try:
        with open(resume_file, 'r') as file:
            shared["resume_text"] = file.read()
    except FileNotFoundError:
        print(f"Error: Resume file '{resume_file}' not found.")
        exit(1) # Exit if resume file is missing

    shared["target_skills"] = target_skills_to_find

    # --- Define and Run Flow ---
    parser_node = ResumeParserNode(max_retries=3, wait=10)
    flow = Flow(start=parser_node)
    flow.run(shared) # Execute the parsing node

    # --- Display Found Skills ---
    if "structured_data" in shared and "skill_indexes" in shared["structured_data"]:
         print("\n--- Found Target Skills (from Indexes) ---")
         found_indexes = shared["structured_data"]["skill_indexes"]
         if found_indexes: # Check if the list is not empty or None
             for index in found_indexes:
                 if 0 <= index < len(target_skills_to_find):
                     print(f"- {target_skills_to_find[index]} (Index: {index})")
                 else:
                     print(f"- Warning: Found invalid skill index {index}")
         else:
             print("No target skills identified from the list.")
         print("----------------------------------------\n")
</file>

<file path="cookbook/pocketflow-structured-output/README.md">
# Structured Output Demo

A minimal demo application showing how to use PocketFlow to extract structured data from a resume using direct prompting and YAML formatting. Why YAML? Check out the [doc](https://the-pocket.github.io/PocketFlow/design_pattern/structure.html).

This implementation is based on: [Structured Output for Beginners: 3 Must-Know Prompting Tips](https://zacharyhuang.substack.com/p/structured-output-for-beginners-3).

## Features

- Extracts structured data using prompt engineering
- Validates output structure before processing

## Run It

1. Install the packages you need with this simple command:
    ```bash
    pip install -r requirements.txt
    ```

2. Make sure your OpenAI API key is set:
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```
    Alternatively, you can edit the [`utils.py`](./utils.py) file to include your API key directly.

    Let's do a quick check to make sure your API key is working properly:

    ```bash
    python utils.py
    ```

3. Edit [data.txt](./data.txt) with the resume you want to parse (a sample resume is already included)

4. Run the application:
    ```bash
    python main.py
    ```

## How It Works

```mermaid
flowchart LR
    parser[ResumeParserNode]
```

The Resume Parser application uses a single node that:
1. Takes resume text from the shared state (loaded from data.txt)
2. Sends the resume to an LLM with a prompt that requests YAML formatted output
3. Extracts and validates the structured YAML data
4. Outputs the structured result

## Files

- [`main.py`](./main.py): Implementation of the ResumeParserNode
- [`utils.py`](./utils.py): LLM utilities
- [`data.txt`](./data.txt): Sample resume text file
 
## Example Output

```
=== Resume Parser - Structured Output with Indexes & Comments ===


=== STRUCTURED RESUME DATA (Comments & Skill Index List) ===

name: JOHN SMTIH
email: johnsmtih1983@gnail.com
experience:
- {title: SALES MANAGER, company: ABC Corportaion}
- {title: ASST. MANAGER, company: XYZ Industries}
- {title: CUSTOMER SERVICE REPRESENTATIVE, company: Fast Solutions Inc}
skill_indexes: [0, 1, 2, 3, 4]


============================================================

âœ… Extracted resume information.

--- Found Target Skills (from Indexes) ---
- Team leadership & management (Index: 0)
- CRM software (Index: 1)
- Project management (Index: 2)
- Public speaking (Index: 3)
- Microsoft Office (Index: 4)
----------------------------------------
```
</file>

<file path="cookbook/pocketflow-structured-output/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
</file>

<file path="cookbook/pocketflow-structured-output/utils.py">
import os
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

# Example usage
if __name__ == "__main__":
    print(call_llm("Tell me a short joke"))
</file>

<file path="cookbook/pocketflow-supervisor/flow.py">
from pocketflow import Flow
from nodes import DecideAction, SearchWeb, UnreliableAnswerNode, SupervisorNode

def create_agent_inner_flow():
    """
    Create the inner research agent flow without supervision.
    
    This flow handles the research cycle:
    1. DecideAction node decides whether to search or answer
    2. If search, go to SearchWeb node and return to decide
    3. If answer, go to UnreliableAnswerNode
    
    Returns:
        Flow: A research agent flow
    """
    # Create instances of each node
    decide = DecideAction()
    search = SearchWeb()
    answer = UnreliableAnswerNode()
    
    # Connect the nodes
    # If DecideAction returns "search", go to SearchWeb
    decide - "search" >> search
    
    # If DecideAction returns "answer", go to UnreliableAnswerNode
    decide - "answer" >> answer
    
    # After SearchWeb completes and returns "decide", go back to DecideAction
    search - "decide" >> decide
    
    # Create and return the inner flow, starting with the DecideAction node
    return Flow(start=decide)

def create_agent_flow():
    """
    Create a supervised agent flow by treating the entire agent flow as a node
    and placing the supervisor outside of it.
    
    The flow works like this:
    1. Inner agent flow does research and generates an answer
    2. SupervisorNode checks if the answer is valid
    3. If answer is valid, flow completes
    4. If answer is invalid, restart the inner agent flow
    
    Returns:
        Flow: A complete research agent flow with supervision
    """
    # Create the inner flow
    agent_flow = create_agent_inner_flow()
    
    # Create the supervisor node
    supervisor = SupervisorNode()
    
    # Connect the components
    # After agent_flow completes, go to supervisor
    agent_flow >> supervisor
    
    # If supervisor rejects the answer, go back to agent_flow
    supervisor - "retry" >> agent_flow
    
    # Create and return the outer flow, starting with the agent_flow
    return Flow(start=agent_flow)
</file>

<file path="cookbook/pocketflow-supervisor/main.py">
import sys
from flow import create_agent_flow

def main():
    """Simple function to process a question with supervised answers."""
    # Default question
    default_question = "Who won the Nobel Prize in Physics 2024?"
    
    # Get question from command line if provided with --
    question = default_question
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            question = arg[2:]
            break
    
    # Create the agent flow with supervision
    agent_flow = create_agent_flow()
    
    # Process the question
    shared = {"question": question}
    print(f"ðŸ¤” Processing question: {question}")
    agent_flow.run(shared)
    print("\nðŸŽ¯ Final Answer:")
    print(shared.get("answer", "No answer found"))

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-supervisor/nodes.py">
from pocketflow import Node
from utils import call_llm, search_web
import yaml
import random

class DecideAction(Node):
    def prep(self, shared):
        """Prepare the context and question for the decision-making process."""
        # Get the current context (default to "No previous search" if none exists)
        context = shared.get("context", "No previous search")
        # Get the question from the shared store
        question = shared["question"]
        # Return both for the exec step
        return question, context
        
    def exec(self, inputs):
        """Call the LLM to decide whether to search or answer."""
        question, context = inputs
        
        print(f"ðŸ¤” Agent deciding what to do next...")
        
        # Create a prompt to help the LLM decide what to do next
        prompt = f"""
### CONTEXT
You are a research assistant that can search the web.
Question: {question}
Previous Research: {context}

### ACTION SPACE
[1] search
  Description: Look up more information on the web
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Answer the question with current knowledge
  Parameters:
    - answer (str): Final answer to the question

## NEXT ACTION
Decide the next action based on the context and available actions.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: search OR answer
reason: <why you chose this action>
search_query: <specific search query if action is search>
```"""
        
        # Call the LLM to make a decision
        response = call_llm(prompt)
        
        # Parse the response to get the decision
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        decision = yaml.safe_load(yaml_str)
        
        return decision
    
    def post(self, shared, prep_res, exec_res):
        """Save the decision and determine the next step in the flow."""
        # If LLM decided to search, save the search query
        if exec_res["action"] == "search":
            shared["search_query"] = exec_res["search_query"]
            print(f"ðŸ” Agent decided to search for: {exec_res['search_query']}")
        else:
            print(f"ðŸ’¡ Agent decided to answer the question")
        
        # Return the action to determine the next node in the flow
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        """Get the search query from the shared store."""
        return shared["search_query"]
        
    def exec(self, search_query):
        """Search the web for the given query."""
        # Call the search utility function
        print(f"ðŸŒ Searching the web for: {search_query}")
        results = search_web(search_query)
        return results
    
    def post(self, shared, prep_res, exec_res):
        """Save the search results and go back to the decision node."""
        # Add the search results to the context in the shared store
        previous = shared.get("context", "")
        shared["context"] = previous + "\n\nSEARCH: " + shared["search_query"] + "\nRESULTS: " + exec_res
        
        print(f"ðŸ“š Found information, analyzing results...")
        
        # Always go back to the decision node after searching
        return "decide"

class UnreliableAnswerNode(Node):
    def prep(self, shared):
        """Get the question and context for answering."""
        return shared["question"], shared.get("context", "")
        
    def exec(self, inputs):
        """Call the LLM to generate a final answer with 50% chance of returning a dummy answer."""
        question, context = inputs
        
        # 50% chance to return a dummy answer
        if random.random() < 0.5:
            print(f"ðŸ¤ª Generating unreliable dummy answer...")
            return "Sorry, I'm on a coffee break right now. All information I provide is completely made up anyway. The answer to your question is 42, or maybe purple unicorns. Who knows? Certainly not me!"
        
        print(f"âœï¸ Crafting final answer...")
        
        # Create a prompt for the LLM to answer the question
        prompt = f"""
### CONTEXT
Based on the following information, answer the question.
Question: {question}
Research: {context}

## YOUR ANSWER:
Provide a comprehensive answer using the research results.
"""
        # Call the LLM to generate an answer
        answer = call_llm(prompt)
        return answer
    
    def post(self, shared, prep_res, exec_res):
        """Save the final answer and complete the flow."""
        # Save the answer in the shared store
        shared["answer"] = exec_res
        
        print(f"âœ… Answer generated successfully")

class SupervisorNode(Node):
    def prep(self, shared):
        """Get the current answer for evaluation."""
        return shared["answer"]
    
    def exec(self, answer):
        """Check if the answer is valid or nonsensical."""
        print(f"    ðŸ” Supervisor checking answer quality...")
        
        # Check for obvious markers of the nonsense answers
        nonsense_markers = [
            "coffee break", 
            "purple unicorns", 
            "made up", 
            "42", 
            "Who knows?"
        ]
        
        # Check if the answer contains any nonsense markers
        is_nonsense = any(marker in answer for marker in nonsense_markers)
        
        if is_nonsense:
            return {"valid": False, "reason": "Answer appears to be nonsensical or unhelpful"}
        else:
            return {"valid": True, "reason": "Answer appears to be legitimate"}
    
    def post(self, shared, prep_res, exec_res):
        """Decide whether to accept the answer or restart the process."""
        if exec_res["valid"]:
            print(f"    âœ… Supervisor approved answer: {exec_res['reason']}")
        else:
            print(f"    âŒ Supervisor rejected answer: {exec_res['reason']}")
            # Clean up the bad answer
            shared["answer"] = None
            # Add a note about the rejected answer
            context = shared.get("context", "")
            shared["context"] = context + "\n\nNOTE: Previous answer attempt was rejected by supervisor."
            return "retry"
</file>

<file path="cookbook/pocketflow-supervisor/README.md">
# Research Supervisor

This project demonstrates a supervisor that oversees an unreliable [research agent](../pocketflow-agent) to ensure high-quality answers.

## Features

- Evaluates responses for quality and relevance
- Rejects nonsensical or unreliable answers
- Requests new answers until a quality response is produced

## Getting Started

1. Install the packages you need with this simple command:
```bash
pip install -r requirements.txt
```

2. Let's get your OpenAI API key ready:

```bash
export OPENAI_API_KEY="your-api-key-here"
```

3. Let's do a quick check to make sure your API key is working properly:

```bash
python utils.py
```

This will test both the LLM call and web search features. If you see responses, you're good to go!

4. Try out the agent with the default question (about Nobel Prize winners):

```bash
python main.py
```

5. Got a burning question? Ask anything you want by using the `--` prefix:

```bash
python main.py --"What is quantum computing?"
```

## How It Works?

The magic happens through a simple but powerful graph structure with these main components:

```mermaid
graph TD
    subgraph InnerAgent[Inner Research Agent]
        DecideAction -->|"search"| SearchWeb
        DecideAction -->|"answer"| UnreliableAnswerNode
        SearchWeb -->|"decide"| DecideAction
    end
    
    InnerAgent --> SupervisorNode
    SupervisorNode -->|"retry"| InnerAgent
```

Here's what each part does:
1. **DecideAction**: The brain that figures out whether to search or answer based on current context
2. **SearchWeb**: The researcher that goes out and finds information using web search
3. **UnreliableAnswerNode**: Generates answers (with a 50% chance of being unreliable)
4. **SupervisorNode**: Quality control that validates answers and rejects nonsensical ones

## Example Output

```
ðŸ¤” Processing question: Who won the Nobel Prize in Physics 2024?
ðŸ¤” Agent deciding what to do next...
ðŸ” Agent decided to search for: Nobel Prize in Physics 2024 winner
ðŸŒ Searching the web for: Nobel Prize in Physics 2024 winner
ðŸ“š Found information, analyzing results...
ðŸ¤” Agent deciding what to do next...
ðŸ’¡ Agent decided to answer the question
ðŸ¤ª Generating unreliable dummy answer...
âœ… Answer generated successfully
    ðŸ” Supervisor checking answer quality...
    âŒ Supervisor rejected answer: Answer appears to be nonsensical or unhelpful
ðŸ¤” Agent deciding what to do next...
ðŸ’¡ Agent decided to answer the question
âœï¸ Crafting final answer...
âœ… Answer generated successfully
    ðŸ” Supervisor checking answer quality...
    âœ… Supervisor approved answer: Answer appears to be legitimate

ðŸŽ¯ Final Answer:
The Nobel Prize in Physics for 2024 was awarded jointly to John J. Hopfield and Geoffrey Hinton. They were recognized "for foundational discoveries and inventions that enable machine learning with artificial neural networks." Their work has been pivotal in the field of artificial intelligence, specifically in developing the theories and technologies that support machine learning using artificial neural networks. John Hopfield is associated with Princeton University, while Geoffrey Hinton is connected to the University of Toronto. Their achievements have laid essential groundwork for advancements in AI and its widespread application across various domains.
```

## Files

- [`main.py`](./main.py): The starting point - runs the whole show!
- [`flow.py`](./flow.py): Connects everything together into a smart agent with supervision
- [`nodes.py`](./nodes.py): The building blocks that make decisions, take actions, and validate answers
- [`utils.py`](./utils.py): Helper functions for talking to the LLM and searching the web
</file>

<file path="cookbook/pocketflow-supervisor/requirements.txt">
pocketflow>=0.0.1
aiohttp>=3.8.0  # For async HTTP requests
openai>=1.0.0   # For async LLM calls 
duckduckgo-search>=7.5.2    # For web search
</file>

<file path="cookbook/pocketflow-supervisor/utils.py">
from openai import OpenAI
import os
from duckduckgo_search import DDGS

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

def search_web(query):
    results = DDGS().text(query, max_results=5)
    # Convert results to a string
    results_str = "\n\n".join([f"Title: {r['title']}\nURL: {r['href']}\nSnippet: {r['body']}" for r in results])
    return results_str
    
if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")

    print("## Testing search_web")
    query = "Who won the Nobel Prize in Physics 2024?"
    print(f"## Query: {query}")
    results = search_web(query)
    print(f"## Results: {results}")
</file>

<file path="cookbook/pocketflow-tao/flow.py">
# flow.py

from pocketflow import Flow
from nodes import ThinkNode, ActionNode, ObserveNode, EndNode

def create_tao_flow():
    """
    Create a Thought-Action-Observation loop flow
    
    How the flow works:
    1. ThinkNode decides the next action
    2. ActionNode executes the action
    3. ObserveNode observes the action result
    4. Return to ThinkNode to continue thinking, or end the flow
    
    Returns:
        Flow: Complete TAO loop flow
    """
    # Create node instances
    think = ThinkNode()
    action = ActionNode()
    observe = ObserveNode()
    end = EndNode()
    
    # Connect nodes
    # If ThinkNode returns "action", go to ActionNode
    think - "action" >> action
    
    # If ThinkNode returns "end", end the flow
    think - "end" >> end
    
    # After ActionNode completes, go to ObserveNode
    action - "observe" >> observe
    
    # After ObserveNode completes, return to ThinkNode
    observe - "think" >> think
    
    # Create and return flow, starting from ThinkNode
    return Flow(start=think)
</file>

<file path="cookbook/pocketflow-tao/main.py">
# main.py

from flow import create_tao_flow

def main():
    
    query = """I need to understand the latest developments in artificial intelligence"""
    
    # Create shared data
    shared = {
        "query": query,
        "thoughts": [],
        "observations": [],
        "current_thought_number": 0
    }
    
    # Create and run flow
    tao_flow = create_tao_flow()
    tao_flow.run(shared)
    
    # Print final result
    if "final_answer" in shared:
        print("\nFinal Answer:")
        print(shared["final_answer"])
    else:
        print("\nFlow did not produce a final answer")

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tao/nodes.py">
# nodes.py

from pocketflow import Node
import yaml
from utils import call_llm

class ThinkNode(Node):
    def prep(self, shared):
        """Prepare the context needed for thinking"""
        query = shared.get("query", "")
        observations = shared.get("observations", [])
        thoughts = shared.get("thoughts", [])
        current_thought_number = shared.get("current_thought_number", 0)
        
        # Update thought count
        shared["current_thought_number"] = current_thought_number + 1
        
        # Format previous observations
        observations_text = "\n".join([f"Observation {i+1}: {obs}" for i, obs in enumerate(observations)])
        if not observations_text:
            observations_text = "No observations yet."
            
        return {
            "query": query,
            "observations_text": observations_text,
            "thoughts": thoughts,
            "current_thought_number": current_thought_number + 1
        }
    
    def exec(self, prep_res):
        """Execute the thinking process, decide the next action"""
        query = prep_res["query"]
        observations_text = prep_res["observations_text"]
        current_thought_number = prep_res["current_thought_number"]
        
        # Build the prompt
        prompt = f"""
        You are an AI assistant solving a problem. Based on the user's query and previous observations, think about what action to take next.
        
        User query: {query}
        
        Previous observations:
        {observations_text}
        
        Please think about the next action and return your thinking process and decision in YAML format:
        ```yaml
        thinking: |
            <detailed thinking process>
        action: <action name, such as 'search' or 'answer'>
        action_input: <input parameters for the action>
        is_final: <set to true if this is the final answer, otherwise false>
        ```
        """
        
        # Call LLM to get thinking result
        response = call_llm(prompt)
        
        # Parse YAML response
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        thought_data = yaml.safe_load(yaml_str)
        
        # Add thought number
        thought_data["thought_number"] = current_thought_number
        
        return thought_data
    
    def post(self, shared, prep_res, exec_res):
        """Save the thinking result and decide the next step in the flow"""
        # Save thinking result
        if "thoughts" not in shared:
            shared["thoughts"] = []
        shared["thoughts"].append(exec_res)
        
        # Save action information
        shared["current_action"] = exec_res["action"]
        shared["current_action_input"] = exec_res["action_input"]
        
        # If it's the final answer, end the flow
        if exec_res.get("is_final", False):
            shared["final_answer"] = exec_res["action_input"]
            print(f"ðŸŽ¯ Final Answer: {exec_res['action_input']}")
            return "end"
        
        # Otherwise continue with the action
        print(f"ðŸ¤” Thought {exec_res['thought_number']}: Decided to execute {exec_res['action']}")
        return "action"

class ActionNode(Node):
    def prep(self, shared):
        """Prepare to execute action"""
        action = shared["current_action"]
        action_input = shared["current_action_input"]
        return action, action_input
    
    def exec(self, inputs):
        """Execute action and return result"""
        action, action_input = inputs
        
        print(f"ðŸš€ Executing action: {action}, input: {action_input}")
        
        # Execute different operations based on action type
        if action == "search":
            # Simulate search operation
            result = self.search_web(action_input)
        elif action == "calculate":
            # Simulate calculation operation
            result = self.calculate(action_input)
        elif action == "answer":
            # Direct return answer
            result = action_input
        else:
            # Unknown action type
            result = f"Unknown action type: {action}"
        
        return result
    
    def post(self, shared, prep_res, exec_res):
        """Save action result"""
        # Save the current action result
        shared["current_action_result"] = exec_res
        print(f"âœ… Action completed, result obtained")
        
        # Continue to observation node
        return "observe"
    
    # Simulated tool functions
    def search_web(self, query):
        # This should be actual search logic
        return f"Search results: Information about '{query}'..."
    
    def calculate(self, expression):
        # This should be actual calculation logic
        try:
            return f"Calculation result: {eval(expression)}"
        except:
            return f"Unable to calculate expression: {expression}"

class ObserveNode(Node):
    def prep(self, shared):
        """Prepare observation data"""
        action = shared["current_action"]
        action_input = shared["current_action_input"]
        action_result = shared["current_action_result"]
        return action, action_input, action_result
    
    def exec(self, inputs):
        """Analyze action results, generate observation"""
        action, action_input, action_result = inputs
        
        # Build prompt
        prompt = f"""
        You are an observer, needing to analyze action results and provide objective observations.
        
        Action: {action}
        Action input: {action_input}
        Action result: {action_result}
        
        Please provide a concise observation of this result. Don't make decisions, just describe what you see.
        """
        
        # Call LLM to get observation result
        observation = call_llm(prompt)
        
        print(f"ðŸ‘ï¸ Observation: {observation[:50]}...")
        return observation
    
    def post(self, shared, prep_res, exec_res):
        """Save observation result and decide next flow step"""
        # Save observation result
        if "observations" not in shared:
            shared["observations"] = []
        shared["observations"].append(exec_res)
        
        # Continue thinking
        return "think"
    

    
class EndNode(Node):
    def prep(self, shared):
        """Prepare end node"""
        
        return {}
    def exec(self, prep_res):
        """Execute end operation"""
        print("Flow ended, thank you for using!")
        return None
    def post(self, shared, prep_res, exec_res):
        """End flow"""
        return None
</file>

<file path="cookbook/pocketflow-tao/README.md">
# PocketFlow TAO (Thought-Action-Observation)

A powerful pattern that enables AI agents to solve complex problems through structured thinking, action execution, and result observation. This example demonstrates how to implement the TAO pattern using PocketFlow.

## Project Structure

```
.
â”œâ”€â”€ flow.py        # PocketFlow implementation of TAO pattern
â”œâ”€â”€ main.py        # Main application entry point
â”œâ”€â”€ nodes.py       # TAO node definitions
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ README.md      # Project documentation
```

## Overview

The TAO pattern consists of three key steps:
1. **Thought**: The agent deeply analyzes the problem and forms a solution strategy
2. **Action**: Concrete actions are executed based on the thinking
3. **Observation**: Results are evaluated and feedback is gathered

This cycle continues until the problem is solved or termination conditions are met.

## Setup

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set API key (if using specific LLM services):
```bash
export OPENAI_API_KEY="your-api-key-here"
# Or set in code
```

## How to Run

Execute the example:
```bash
python main.py
```

## How It Works

The TAO pattern is implemented as a flow in PocketFlow, with each step handled by specialized nodes:

```mermaid
graph TD
    Problem[Problem Input] --> ThoughtNode
    ThoughtNode[Thought Node] --> ActionNode[Action Node]
    ActionNode --> ObservationNode[Observation Node]
    ObservationNode --> DecisionNode{Problem Solved?}
    DecisionNode -->|Yes| Solution[Solution]
    DecisionNode -->|No| ThoughtNode
```

Each TAO cycle generates new insights for the problem-solving process, allowing the AI to iteratively approach an optimal solution.

## Use Cases

- Complex problem solving
- Multi-step reasoning tasks
- Projects requiring iterative improvement
- Reinforcement learning-style AI applications

## Example Output

```
Query: I need to understand the latest developments in artificial intelligence

ðŸ¤” Thought 1: Decided to execute search
ðŸš€ Executing action: search, input: latest developments in artificial intelligence 2023
âœ… Action completed, result obtained
ðŸ‘ï¸ Observation: The search result indicates that information was r...
ðŸŽ¯ Final Answer: As of October 2023, some of the latest developments in artificial intelligence include advances in large language models like GPT-4, increased focus on AI alignment and safety, improvements in reinforcement learning, and the integration of AI into more industries such as healthcare, finance, and autonomous vehicles. Researchers are also exploring ethical considerations and regulatory frameworks to ensure responsible AI deployment. For the most current updates beyond this date, I recommend checking recent publications, official AI research organization releases, or news sources specializing in technology.

Flow ended, thank you for using!

Final Answer:
As of October 2023, some of the latest developments in artificial intelligence include advances in large language models like GPT-4, increased focus on AI alignment and safety, improvements in reinforcement learning, and the integration of AI into more industries such as healthcare, finance, and autonomous vehicles. Researchers are also exploring ethical considerations and regulatory frameworks to ensure responsible AI deployment. For the most current updates beyond this date, I recommend checking recent publications, official AI research organization releases, or news sources specializing in technology.
```

## Advanced Usage

The TAO pattern can be extended by:
- Adding memory components to store past thoughts and observations.
- Implementing adaptive action selection strategies.
- Integrating external tools and APIs.
- Adding human feedback loops.
- Adding max attempt to control the iteration.

## Additional Resources

- [PocketFlow Documentation](https://the-pocket.github.io/PocketFlow/)
- [Understanding AI Agents through the Thought-Action-Observation Cycle](https://huggingface.co/learn/agents-course/en/unit1/agent-steps-and-structure)
</file>

<file path="cookbook/pocketflow-tao/utils.py">
# utils.py

from openai import OpenAI
import os

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "Your Key Here"),base_url=os.environ.get("OPENAI_API_BASE", "Your API Base Here"))
    r = client.chat.completions.create(
        model=os.environ.get("OPENAI_MODEL", "openai/gpt-4.1-nano"),
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")
</file>

<file path="cookbook/pocketflow-text2sql/docs/design.md">
# Design Doc: Text-to-SQL Agent

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

The system should take a natural language query and a path to an SQLite database as input. It should then:
1.  Extract the schema from the database.
2.  Generate an SQL query based on the natural language query and the schema.
3.  Execute the SQL query against the database.
4.  If the SQL execution fails, attempt to debug and retry the SQL generation and execution up to a specified maximum number of attempts.
5.  Return the final results of the SQL query or an error message if the process fails.

## Flow Design

> Notes for AI:
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

The primary design pattern is a **Workflow** with an embedded **Agent**-like behavior for debugging.
-   **Workflow**: The process follows a sequence: Get Schema -> Generate SQL -> Execute SQL.
-   **Agent (for Debugging)**: If `ExecuteSQL` fails, the `DebugSQL` node acts like an agent, taking the error and previous SQL as context to generate a revised SQL query. This forms a loop back to `ExecuteSQL`.

### Flow high-level Design:

1.  **`GetSchema`**: Retrieves the database schema.
2.  **`GenerateSQL`**: Generates an SQL query from a natural language question and the schema.
3.  **`ExecuteSQL`**: Executes the generated SQL. If successful, the flow ends. If an error occurs, it transitions to `DebugSQL`.
4.  **`DebugSQL`**: Attempts to correct the failed SQL query based on the error message. It then transitions back to `ExecuteSQL` to try the corrected query.

```mermaid
flowchart TD
    A[GetSchema] --> B[GenerateSQL]
    B --> C{ExecuteSQL}
    C -- Success --> D[End]
    C -- Error --> E[DebugSQL]
    E --> C
```

## Utility Functions

> Notes for AI:
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1.  **Call LLM** (`utils/call_llm.py`)
    *   *Input*: `prompt` (str)
    *   *Output*: `response` (str)
    *   *Necessity*: Used by `GenerateSQL` and `DebugSQL` nodes to interact with the language model for SQL generation and correction.

*Database interaction (e.g., `sqlite3.connect`, `cursor.execute`) is handled directly within the nodes and is not abstracted into separate utility functions in this implementation.*

## Node Design

### Shared Store

> Notes for AI: Try to minimize data redundancy

The shared store structure is organized as follows:

```python
shared = {
    "db_path": "path/to/database.db",       # Input: Path to the SQLite database
    "natural_query": "User's question",      # Input: Natural language query from the user
    "max_debug_attempts": 3,                # Input: Max retries for the debug loop
    "schema": None,                         # Output of GetSchema: String representation of DB schema
    "generated_sql": None,                  # Output of GenerateSQL/DebugSQL: The SQL query string
    "execution_error": None,                # Output of ExecuteSQL (on failure): Error message
    "debug_attempts": 0,                    # Internal: Counter for debug attempts
    "final_result": None,                   # Output of ExecuteSQL (on success): Query results
    "result_columns": None,                 # Output of ExecuteSQL (on success): Column names for results
    "final_error": None                     # Output: Overall error message if flow fails after retries
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1.  **`GetSchema`**
    *   *Purpose*: To extract and store the schema of the target SQLite database.
    *   *Type*: Regular
    *   *Steps*:
        *   *`prep`*: Reads `db_path` from the shared store.
        *   *`exec`*: Connects to the SQLite database, inspects `sqlite_master` and `PRAGMA table_info` to build a string representation of all tables and their columns.
        *   *`post`*: Writes the extracted `schema` string to the shared store.

2.  **`GenerateSQL`**
    *   *Purpose*: To generate an SQL query based on the user's natural language query and the database schema.
    *   *Type*: Regular
    *   *Steps*:
        *   *`prep`*: Reads `natural_query` and `schema` from the shared store.
        *   *`exec`*: Constructs a prompt for the LLM, including the schema and the natural language query, asking for an SQL query in YAML format. Calls the `call_llm` utility. Parses the YAML response to extract the SQL query.
        *   *`post`*: Writes the `generated_sql` to the shared store. Resets `debug_attempts` to 0.

3.  **`ExecuteSQL`**
    *   *Purpose*: To execute the generated SQL query against the database and handle results or errors.
    *   *Type*: Regular
    *   *Steps*:
        *   *`prep`*: Reads `db_path` and `generated_sql` from the shared store.
        *   *`exec`*: Connects to the SQLite database and executes the `generated_sql`. It determines if the query is a SELECT or an DML/DDL statement to fetch results or commit changes. Returns a tuple `(success_boolean, result_or_error_message, column_names_list)`.
        *   *`post`*:
            *   If successful: Stores `final_result` and `result_columns` in the shared store. Returns no action (ends the flow path).
            *   If failed: Stores `execution_error` in the shared store. Increments `debug_attempts`. If `debug_attempts` is less than `max_debug_attempts`, returns `"error_retry"` action to trigger the `DebugSQL` node. Otherwise, sets `final_error` and returns no action.

4.  **`DebugSQL`**
    *   *Purpose*: To attempt to correct a failed SQL query using LLM based on the error message.
    *   *Type*: Regular
    *   *Steps*:
        *   *`prep`*: Reads `natural_query`, `schema`, `generated_sql` (the failed one), and `execution_error` from the shared store.
        *   *`exec`*: Constructs a prompt for the LLM, providing the failed SQL, the original query, the schema, and the error message, asking for a corrected SQL query in YAML format. Calls the `call_llm` utility. Parses the YAML response to extract the corrected SQL query.
        *   *`post`*: Overwrites `generated_sql` in the shared store with the corrected SQL. Removes `execution_error` from the shared store. Returns a default action to go back to `ExecuteSQL`.
</file>

<file path="cookbook/pocketflow-text2sql/utils/call_llm.py">
import os
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

# Example usage
if __name__ == "__main__":
    print(call_llm("Tell me a short joke"))
</file>

<file path="cookbook/pocketflow-text2sql/flow.py">
from pocketflow import Flow, Node
from nodes import GetSchema, GenerateSQL, ExecuteSQL, DebugSQL

def create_text_to_sql_flow():
    """Creates the text-to-SQL workflow with a debug loop."""
    get_schema_node = GetSchema()
    generate_sql_node = GenerateSQL()
    execute_sql_node = ExecuteSQL()
    debug_sql_node = DebugSQL()

    # Define the main flow sequence using the default transition operator
    get_schema_node >> generate_sql_node >> execute_sql_node

    # --- Define the debug loop connections using the correct operator ---
    # If ExecuteSQL returns "error_retry", go to DebugSQL
    execute_sql_node - "error_retry" >> debug_sql_node

    # If DebugSQL returns "default", go back to ExecuteSQL
    # debug_sql_node - "default" >> execute_sql_node # Explicitly for "default"
    # OR using the shorthand for default:
    debug_sql_node >> execute_sql_node

    # Create the flow
    text_to_sql_flow = Flow(start=get_schema_node)
    return text_to_sql_flow
</file>

<file path="cookbook/pocketflow-text2sql/main.py">
import sys
import os
from flow import create_text_to_sql_flow
from populate_db import populate_database, DB_FILE

def run_text_to_sql(natural_query, db_path=DB_FILE, max_debug_retries=3):
    if not os.path.exists(db_path) or os.path.getsize(db_path) == 0:
        print(f"Database at {db_path} missing or empty. Populating...")
        populate_database(db_path)

    shared = {
        "db_path": db_path,
        "natural_query": natural_query,
        "max_debug_attempts": max_debug_retries,
        "debug_attempts": 0,
        "final_result": None,
        "final_error": None
    }

    print(f"\n=== Starting Text-to-SQL Workflow ===")
    print(f"Query: '{natural_query}'")
    print(f"Database: {db_path}")
    print(f"Max Debug Retries on SQL Error: {max_debug_retries}")
    print("=" * 45)

    flow = create_text_to_sql_flow()
    flow.run(shared) # Let errors inside the loop be handled by the flow logic

    # Check final state based on shared data
    if shared.get("final_error"):
            print("\n=== Workflow Completed with Error ===")
            print(f"Error: {shared['final_error']}")
    elif shared.get("final_result") is not None:
            print("\n=== Workflow Completed Successfully ===")
            # Result already printed by ExecuteSQL node
    else:
            # Should not happen if flow logic is correct and covers all end states
            print("\n=== Workflow Completed (Unknown State) ===")

    print("=" * 36)
    return shared

if __name__ == "__main__":
    if len(sys.argv) > 1:
        query = " ".join(sys.argv[1:])
    else:
        query = "total products per category"

    run_text_to_sql(query)
</file>

<file path="cookbook/pocketflow-text2sql/nodes.py">
import sqlite3
import time
import yaml # Import yaml here as nodes use it
from pocketflow import Node
from utils.call_llm import call_llm

class GetSchema(Node):
    def prep(self, shared):
        return shared["db_path"]

    def exec(self, db_path):
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        schema = []
        for table_name_tuple in tables:
            table_name = table_name_tuple[0]
            schema.append(f"Table: {table_name}")
            cursor.execute(f"PRAGMA table_info({table_name});")
            columns = cursor.fetchall()
            for col in columns:
                schema.append(f"  - {col[1]} ({col[2]})")
            schema.append("")
        conn.close()
        return "\n".join(schema).strip()

    def post(self, shared, prep_res, exec_res):
        shared["schema"] = exec_res
        print("\n===== DB SCHEMA =====\n")
        print(exec_res)
        print("\n=====================\n")
        # return "default"

class GenerateSQL(Node):
    def prep(self, shared):
        return shared["natural_query"], shared["schema"]

    def exec(self, prep_res):
        natural_query, schema = prep_res
        prompt = f"""
Given SQLite schema:
{schema}

Question: "{natural_query}"

Respond ONLY with a YAML block containing the SQL query under the key 'sql':
```yaml
sql: |
  SELECT ...
```"""
        llm_response = call_llm(prompt)
        yaml_str = llm_response.split("```yaml")[1].split("```")[0].strip()
        structured_result = yaml.safe_load(yaml_str)
        sql_query = structured_result["sql"].strip().rstrip(';')
        return sql_query

    def post(self, shared, prep_res, exec_res):
        # exec_res is now the parsed SQL query string
        shared["generated_sql"] = exec_res
        # Reset debug attempts when *successfully* generating new SQL
        shared["debug_attempts"] = 0
        print(f"\n===== GENERATED SQL (Attempt {shared.get('debug_attempts', 0) + 1}) =====\n")
        print(exec_res)
        print("\n====================================\n")
        # return "default"

class ExecuteSQL(Node):
    def prep(self, shared):
        return shared["db_path"], shared["generated_sql"]

    def exec(self, prep_res):
        db_path, sql_query = prep_res
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            start_time = time.time()
            cursor.execute(sql_query)

            is_select = sql_query.strip().upper().startswith(("SELECT", "WITH"))
            if is_select:
                results = cursor.fetchall()
                column_names = [desc[0] for desc in cursor.description] if cursor.description else []
            else:
                conn.commit()
                results = f"Query OK. Rows affected: {cursor.rowcount}"
                column_names = []
            conn.close()
            duration = time.time() - start_time
            print(f"SQL executed in {duration:.3f} seconds.")
            return (True, results, column_names)
        except sqlite3.Error as e:
            print(f"SQLite Error during execution: {e}")
            if 'conn' in locals() and conn:
                 try:
                     conn.close()
                 except Exception:
                     pass
            return (False, str(e), [])

    def post(self, shared, prep_res, exec_res):
        success, result_or_error, column_names = exec_res

        if success:
            shared["final_result"] = result_or_error
            shared["result_columns"] = column_names
            print("\n===== SQL EXECUTION SUCCESS =====\n")
            # (Same result printing logic as before)
            if isinstance(result_or_error, list):
                 if column_names: print(" | ".join(column_names)); print("-" * (sum(len(str(c)) for c in column_names) + 3 * (len(column_names) -1)))
                 if not result_or_error: print("(No results found)")
                 else:
                     for row in result_or_error: print(" | ".join(map(str, row)))
            else: print(result_or_error)
            print("\n=================================\n")
            return
        else:
            # Execution failed (SQLite error caught in exec)
            shared["execution_error"] = result_or_error # Store the error message
            shared["debug_attempts"] = shared.get("debug_attempts", 0) + 1
            max_attempts = shared.get("max_debug_attempts", 3) # Get max attempts from shared

            print(f"\n===== SQL EXECUTION FAILED (Attempt {shared['debug_attempts']}) =====\n")
            print(f"Error: {shared['execution_error']}")
            print("=========================================\n")

            if shared["debug_attempts"] >= max_attempts:
                print(f"Max debug attempts ({max_attempts}) reached. Stopping.")
                shared["final_error"] = f"Failed to execute SQL after {max_attempts} attempts. Last error: {shared['execution_error']}"
                return
            else:
                print("Attempting to debug the SQL...")
                return "error_retry" # Signal to go to DebugSQL

class DebugSQL(Node):
    def prep(self, shared):
        return (
            shared.get("natural_query"),
            shared.get("schema"),
            shared.get("generated_sql"),
            shared.get("execution_error")
        )

    def exec(self, prep_res):
        natural_query, schema, failed_sql, error_message = prep_res
        prompt = f"""
The following SQLite SQL query failed:
```sql
{failed_sql}
```
It was generated for: "{natural_query}"
Schema:
{schema}
Error: "{error_message}"

Provide a corrected SQLite query.

Respond ONLY with a YAML block containing the corrected SQL under the key 'sql':
```yaml
sql: |
  SELECT ... -- corrected query
```"""
        llm_response = call_llm(prompt)

        yaml_str = llm_response.split("```yaml")[1].split("```")[0].strip()
        structured_result = yaml.safe_load(yaml_str)
        corrected_sql = structured_result["sql"].strip().rstrip(';')
        return corrected_sql

    def post(self, shared, prep_res, exec_res):
        # exec_res is the corrected SQL string
        shared["generated_sql"] = exec_res # Overwrite with the new attempt
        shared.pop("execution_error", None) # Clear the previous error for the next ExecuteSQL attempt

        print(f"\n===== REVISED SQL (Attempt {shared.get('debug_attempts', 0) + 1}) =====\n")
        print(exec_res)
        print("\n====================================\n")
</file>

<file path="cookbook/pocketflow-text2sql/populate_db.py">
import sqlite3
import os
import random
from datetime import datetime, timedelta

DB_FILE = "ecommerce.db"

def populate_database(db_file=DB_FILE):
    """Creates and populates the SQLite database."""
    if os.path.exists(db_file):
        os.remove(db_file)
        print(f"Removed existing database: {db_file}")

    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Create Tables
    cursor.execute("""
    CREATE TABLE customers (
        customer_id INTEGER PRIMARY KEY AUTOINCREMENT,
        first_name TEXT NOT NULL,
        last_name TEXT NOT NULL,
        email TEXT UNIQUE NOT NULL,
        registration_date DATE NOT NULL,
        city TEXT,
        country TEXT DEFAULT 'USA'
    );
    """)
    print("Created 'customers' table.")

    cursor.execute("""
    CREATE TABLE products (
        product_id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        description TEXT,
        category TEXT NOT NULL,
        price REAL NOT NULL CHECK (price > 0),
        stock_quantity INTEGER NOT NULL DEFAULT 0 CHECK (stock_quantity >= 0)
    );
    """)
    print("Created 'products' table.")

    cursor.execute("""
    CREATE TABLE orders (
        order_id INTEGER PRIMARY KEY AUTOINCREMENT,
        customer_id INTEGER NOT NULL,
        order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        status TEXT NOT NULL CHECK (status IN ('pending', 'processing', 'shipped', 'delivered', 'cancelled')),
        total_amount REAL,
        shipping_address TEXT,
        FOREIGN KEY (customer_id) REFERENCES customers (customer_id)
    );
    """)
    print("Created 'orders' table.")

    cursor.execute("""
    CREATE TABLE order_items (
        order_item_id INTEGER PRIMARY KEY AUTOINCREMENT,
        order_id INTEGER NOT NULL,
        product_id INTEGER NOT NULL,
        quantity INTEGER NOT NULL CHECK (quantity > 0),
        price_per_unit REAL NOT NULL,
        FOREIGN KEY (order_id) REFERENCES orders (order_id),
        FOREIGN KEY (product_id) REFERENCES products (product_id)
    );
    """)
    print("Created 'order_items' table.")

    # Insert Sample Data
    customers_data = [
        ('Alice', 'Smith', 'alice.s@email.com', '2023-01-15', 'New York', 'USA'),
        ('Bob', 'Johnson', 'b.johnson@email.com', '2023-02-20', 'Los Angeles', 'USA'),
        ('Charlie', 'Williams', 'charlie.w@email.com', '2023-03-10', 'Chicago', 'USA'),
        ('Diana', 'Brown', 'diana.b@email.com', '2023-04-05', 'Houston', 'USA'),
        ('Ethan', 'Davis', 'ethan.d@email.com', '2023-05-12', 'Phoenix', 'USA'),
        ('Fiona', 'Miller', 'fiona.m@email.com', '2023-06-18', 'Philadelphia', 'USA'),
        ('George', 'Wilson', 'george.w@email.com', '2023-07-22', 'San Antonio', 'USA'),
        ('Hannah', 'Moore', 'hannah.m@email.com', '2023-08-30', 'San Diego', 'USA'),
        ('Ian', 'Taylor', 'ian.t@email.com', '2023-09-05', 'Dallas', 'USA'),
        ('Julia', 'Anderson', 'julia.a@email.com', '2023-10-11', 'San Jose', 'USA')
    ]
    cursor.executemany("INSERT INTO customers (first_name, last_name, email, registration_date, city, country) VALUES (?, ?, ?, ?, ?, ?)", customers_data)
    print(f"Inserted {len(customers_data)} customers.")

    products_data = [
        ('Laptop Pro', 'High-end laptop for professionals', 'Electronics', 1200.00, 50),
        ('Wireless Mouse', 'Ergonomic wireless mouse', 'Accessories', 25.50, 200),
        ('Mechanical Keyboard', 'RGB backlit mechanical keyboard', 'Accessories', 75.00, 150),
        ('4K Monitor', '27-inch 4K UHD Monitor', 'Electronics', 350.00, 80),
        ('Smartphone X', 'Latest generation smartphone', 'Electronics', 999.00, 120),
        ('Coffee Maker', 'Drip coffee maker', 'Home Goods', 50.00, 300),
        ('Running Shoes', 'Comfortable running shoes', 'Apparel', 90.00, 250),
        ('Yoga Mat', 'Eco-friendly yoga mat', 'Sports', 30.00, 400),
        ('Desk Lamp', 'Adjustable LED desk lamp', 'Home Goods', 45.00, 180),
        ('Backpack', 'Durable backpack for travel', 'Accessories', 60.00, 220)
    ]
    cursor.executemany("INSERT INTO products (name, description, category, price, stock_quantity) VALUES (?, ?, ?, ?, ?)", products_data)
    print(f"Inserted {len(products_data)} products.")

    orders_data = []
    start_date = datetime.now() - timedelta(days=60)
    order_statuses = ['pending', 'processing', 'shipped', 'delivered', 'cancelled']
    for i in range(1, 21): # Create 20 orders
        customer_id = random.randint(1, 10)
        order_date = start_date + timedelta(days=random.randint(0, 59), hours=random.randint(0, 23))
        status = random.choice(order_statuses)
        shipping_address = f"{random.randint(100, 999)} Main St, Anytown"
        orders_data.append((customer_id, order_date.strftime('%Y-%m-%d %H:%M:%S'), status, None, shipping_address)) # Total amount calculated later

    cursor.executemany("INSERT INTO orders (customer_id, order_date, status, total_amount, shipping_address) VALUES (?, ?, ?, ?, ?)", orders_data)
    print(f"Inserted {len(orders_data)} orders.")

    order_items_data = []
    order_totals = {} # Keep track of totals per order
    for order_id in range(1, 21):
        num_items = random.randint(1, 4)
        order_total = 0
        for _ in range(num_items):
            product_id = random.randint(1, 10)
            quantity = random.randint(1, 5)
            # Get product price
            cursor.execute("SELECT price FROM products WHERE product_id = ?", (product_id,))
            price_per_unit = cursor.fetchone()[0]
            order_items_data.append((order_id, product_id, quantity, price_per_unit))
            order_total += quantity * price_per_unit
        order_totals[order_id] = round(order_total, 2)

    cursor.executemany("INSERT INTO order_items (order_id, product_id, quantity, price_per_unit) VALUES (?, ?, ?, ?)", order_items_data)
    print(f"Inserted {len(order_items_data)} order items.")

    # Update order totals
    for order_id, total_amount in order_totals.items():
        cursor.execute("UPDATE orders SET total_amount = ? WHERE order_id = ?", (total_amount, order_id))
    print("Updated order totals.")

    conn.commit()
    conn.close()
    print(f"Database '{db_file}' created and populated successfully.")

if __name__ == "__main__":
    populate_database()
</file>

<file path="cookbook/pocketflow-text2sql/README.md">
# Text-to-SQL Workflow

A PocketFlow example demonstrating a text-to-SQL workflow that converts natural language questions into executable SQL queries for an SQLite database, including an LLM-powered debugging loop for failed queries.

- Check out the [Substack Post Tutorial](https://zacharyhuang.substack.com/p/text-to-sql-from-scratch-tutorial) for more!

## Features

-   **Schema Awareness**: Automatically retrieves the database schema to provide context to the LLM.
-   **LLM-Powered SQL Generation**: Uses an LLM (GPT-4o) to translate natural language questions into SQLite queries (using YAML structured output).
-   **Automated Debugging Loop**: If SQL execution fails, an LLM attempts to correct the query based on the error message. This process repeats up to a configurable number of times.
## Getting Started

1.  **Install Packages:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Set API Key:**
    Set the environment variable for your OpenAI API key.
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```
    *(Replace `"your-api-key-here"` with your actual key)*

3.  **Verify API Key (Optional):**
    Run a quick check using the utility script. If successful, it will print a short joke.
    ```bash
    python utils.py
    ```
    *(Note: This requires a valid API key to be set.)*

4.  **Run Default Example:**
    Execute the main script. This will create the sample `ecommerce.db` if it doesn't exist and run the workflow with a default query.
    ```bash
    python main.py
    ```
    The default query is:
    > Show me the names and email addresses of customers from New York

5.  **Run Custom Query:**
    Provide your own natural language query as command-line arguments after the script name.
    ```bash
    python main.py What is the total stock quantity for products in the 'Accessories' category?
    ```
    Or, for queries with spaces, ensure they are treated as a single argument by the shell if necessary (quotes might help depending on your shell):
    ```bash
    python main.py "List orders placed in the last 30 days with status 'shipped'"
    ```

## How It Works

The workflow uses several nodes connected in a sequence, with a loop for debugging failed SQL queries.

```mermaid
graph LR
    A[Get Schema] --> B[Generate SQL]
    B --> C[Execute SQL]
    C -- Success --> E[End]
    C -- SQLite Error --> D{Debug SQL Attempt}
    D -- Corrected SQL --> C
    C -- Max Retries Reached --> F[End with Error]

    style E fill:#dff,stroke:#333,stroke-width:2px
    style F fill:#fdd,stroke:#333,stroke-width:2px

```

**Node Descriptions:**

1.  **`GetSchema`**: Connects to the SQLite database (`ecommerce.db` by default) and extracts the schema (table names and columns).
2.  **`GenerateSQL`**: Takes the natural language query and the database schema, prompts the LLM to generate an SQLite query (expecting YAML output with the SQL), and parses the result.
3.  **`ExecuteSQL`**: Attempts to run the generated SQL against the database.
    *   If successful, the results are stored, and the flow ends successfully.
    *   If an `sqlite3.Error` occurs (e.g., syntax error), it captures the error message and triggers the debug loop.
4.  **`DebugSQL`**: If `ExecuteSQL` failed, this node takes the original query, schema, failed SQL, and error message, prompts the LLM to generate a *corrected* SQL query (again, expecting YAML).
5.  **(Loop)**: The corrected SQL from `DebugSQL` is passed back to `ExecuteSQL` for another attempt.
6.  **(End Conditions)**: The loop continues until `ExecuteSQL` succeeds or the maximum number of debug attempts (default: 3) is reached.

## Files

-   [`main.py`](./main.py): Main entry point to run the workflow. Handles command-line arguments for the query.
-   [`flow.py`](./flow.py): Defines the PocketFlow `Flow` connecting the different nodes, including the debug loop logic.
-   [`nodes.py`](./nodes.py): Contains the `Node` classes for each step (`GetSchema`, `GenerateSQL`, `ExecuteSQL`, `DebugSQL`).
-   [`utils.py`](./utils.py): Contains the minimal `call_llm` utility function.
-   [`populate_db.py`](./populate_db.py): Script to create and populate the sample `ecommerce.db` SQLite database.
-   [`requirements.txt`](./requirements.txt): Lists Python package dependencies.
-   [`README.md`](./README.md): This file.

## Example Output (Successful Run)

```
=== Starting Text-to-SQL Workflow ===
Query: 'total products per category'
Database: ecommerce.db
Max Debug Retries on SQL Error: 3
=============================================

===== DB SCHEMA =====

Table: customers
  - customer_id (INTEGER)
  - first_name (TEXT)
  - last_name (TEXT)
  - email (TEXT)
  - registration_date (DATE)
  - city (TEXT)
  - country (TEXT)

Table: sqlite_sequence
  - name ()
  - seq ()

Table: products
  - product_id (INTEGER)
  - name (TEXT)
  - description (TEXT)
  - category (TEXT)
  - price (REAL)
  - stock_quantity (INTEGER)

Table: orders
  - order_id (INTEGER)
  - customer_id (INTEGER)
  - order_date (TIMESTAMP)
  - status (TEXT)
  - total_amount (REAL)
  - shipping_address (TEXT)

Table: order_items
  - order_item_id (INTEGER)
  - order_id (INTEGER)
  - product_id (INTEGER)
  - quantity (INTEGER)
  - price_per_unit (REAL)

=====================


===== GENERATED SQL (Attempt 1) =====

SELECT category, COUNT(*) AS total_products
FROM products
GROUP BY category

====================================

SQL executed in 0.000 seconds.

===== SQL EXECUTION SUCCESS =====

category | total_products
-------------------------
Accessories | 3
Apparel | 1
Electronics | 3
Home Goods | 2
Sports | 1

=== Workflow Completed Successfully ===
====================================
```
</file>

<file path="cookbook/pocketflow-text2sql/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
pyyaml>=6.0
sqlite3>=3.0
</file>

<file path="cookbook/pocketflow-thinking/design.md">
# Chain of Thought Node Design

## 1. Requirements
Create a self-looping Chain of Thought node that can:
- Solve a problem step-by-step by maintaining and executing a structured plan.
- Critically evaluate the previous step's reasoning and results before proceeding.
- Refine the plan by breaking down complex steps into nested sub-steps.
- Update the status of plan steps (`Pending`, `Done`, `Verification Needed`) and record concise results.
- Handle potential errors identified during evaluation by adjusting the plan.
- Provide a detailed trace of the thinking process and plan evolution.
- Generate a final conclusion summarizing the solution when the plan is complete.

## 2. Flow Design
This will be a simple flow with a single node that can call itself repeatedly based on whether more thinking is needed according to the plan:

```mermaid
flowchart LR
    cot[ChainOfThoughtNode] -->|"continue"| cot
```

## 3. Utilities
We'll need one primary utility function:
- `call_llm`: Call the LLM to generate the next thought (including evaluation, thinking, and updated plan) based on the problem, previous thoughts, and the current plan state. Helper functions (`format_plan`, `format_plan_for_prompt`) assist in presenting the plan.

## 4. Node Design
### Shared Store Design
```python
shared = {
    "problem": str,             # The problem statement.
    "thoughts": list[dict],     # List of thought dictionaries generated so far.
    "current_thought_number": int, # Counter for the current thought being generated.
    "solution": str | None    # Stores the final conclusion text when finished.
}
```

Each thought dictionary added to the `shared["thoughts"]` list will contain the structured output from the LLM's execution step, plus the thought number:
```python
{
    "thought_number": int,      # The sequence number of this thought.
    "current_thinking": str,    # Detailed text of the evaluation and thinking for this step.
    "planning": list[dict],     # The updated plan structure (list of dictionaries).
    "next_thought_needed": bool # Flag indicating if the loop should continue.
}
```

The `planning` list contains dictionaries representing steps, which can be nested:
```python
# Example structure for a plan step dictionary
{
    "description": str,                     # Description of the step.
    "status": str,                          # "Pending", "Done", "Verification Needed".
    "result": str | None,                   # Optional: Concise result when status is "Done".
    "mark": str | None,                     # Optional: Reason for "Verification Needed".
    "sub_steps": list[dict] | None          # Optional: Nested list for sub-steps.
}
```

### Chain of Thought Node (`ChainOfThoughtNode`)
-   **`type`**: Regular (self-looping node).
-   **`prep`**:
    -   Reads the problem statement and the list of previous thoughts from the shared store.
    -   Formats the history of thoughts and the *last known plan structure* into a text representation suitable for the LLM prompt.
    -   Determines if this is the first thought to adjust prompt instructions.
    -   Increments and updates `shared["current_thought_number"]`.
-   **`exec`**:
    -   Constructs a detailed prompt for the LLM, including:
        -   The problem statement.
        -   The formatted history of previous thoughts and plans.
        -   Specific instructions for evaluating the previous thought, executing the next pending step, updating the plan structure (using the dictionary format), handling sub-steps, managing statuses/results, and indicating completion.
        -   The required YAML output format (`current_thinking`, `planning`, `next_thought_needed`).
    -   Calls the `call_llm` utility with the prompt.
    -   Parses the LLM's YAML response.
    -   Validates the presence and basic types of required keys (`current_thinking`, `planning`, `next_thought_needed`) using `assert`.
    -   Adds the `thought_number` to the parsed data.
-   **`post`**:
    -   Appends the result dictionary from `exec` to the `shared["thoughts"]` list.
    -   Checks the `next_thought_needed` flag from the execution result.
    -   If `False`:
        -   Extracts the `current_thinking` content as the final `shared["solution"]`.
        -   Prints the final thought, plan, and solution.
        -   Returns `"end"` to terminate the flow loop.
    -   If `True`:
        -   Prints the current thought number, thinking content, and formatted current plan status.
        -   Returns `"continue"` to trigger the next iteration of the node.
</file>

<file path="cookbook/pocketflow-thinking/flow.py">
from pocketflow import Flow
from nodes import ChainOfThoughtNode

def create_chain_of_thought_flow():
    # Create a ChainOfThoughtNode
    cot_node = ChainOfThoughtNode(max_retries=3, wait=10)
    
    # Connect the node to itself for the "continue" action
    cot_node - "continue" >> cot_node
    
    # Create the flow
    cot_flow = Flow(start=cot_node)
    return cot_flow
</file>

<file path="cookbook/pocketflow-thinking/main.py">
import sys
from flow import create_chain_of_thought_flow

def main():
    # Default question
    default_question = "You keep rolling a fair die until you roll three, four, five in that order consecutively on three rolls. What is the probability that you roll the die an odd number of times?"
    
    # Get question from command line if provided with --
    question = default_question
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            question = arg[2:]
            break
    
    print(f"ðŸ¤” Processing question: {question}")   

    # Create the flow
    cot_flow = create_chain_of_thought_flow()

    # Set up shared state
    shared = {
        "problem": question,
        "thoughts": [],
        "current_thought_number": 0,
        "total_thoughts_estimate": 10,
        "solution": None
    }
    
    # Run the flow
    cot_flow.run(shared)
    
if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-thinking/nodes.py">
# cookbook/pocketflow-thinking/nodes.py
from pocketflow import Node
import yaml
from utils import call_llm
import textwrap

# Helper function to format structured plan for printing
def format_plan(plan_items, indent_level=0):
    indent = "  " * indent_level
    output = []
    if isinstance(plan_items, list):
        for item in plan_items:
            if isinstance(item, dict):
                status = item.get('status', 'Unknown')
                desc = item.get('description', 'No description')
                result = item.get('result', '')
                mark = item.get('mark', '') # For verification etc.

                # Format the main step line
                line = f"{indent}- [{status}] {desc}"
                if result:
                    line += f": {result}"
                if mark:
                    line += f" ({mark})"
                output.append(line)

                # Recursively format sub-steps if they exist
                sub_steps = item.get('sub_steps')
                if sub_steps:
                    output.append(format_plan(sub_steps, indent_level + 1))
            elif isinstance(item, str): # Basic fallback for string items
                 output.append(f"{indent}- {item}")
            else: # Fallback for unexpected types
                 output.append(f"{indent}- {str(item)}")

    elif isinstance(plan_items, str): # Handle case where plan is just an error string
        output.append(f"{indent}{plan_items}")
    else:
        output.append(f"{indent}# Invalid plan format: {type(plan_items)}")

    return "\n".join(output)

# Helper function to format structured plan for the prompt (simplified view)
def format_plan_for_prompt(plan_items, indent_level=0):
    indent = "  " * indent_level
    output = []
    # Simplified formatting for prompt clarity
    if isinstance(plan_items, list):
        for item in plan_items:
            if isinstance(item, dict):
                status = item.get('status', 'Unknown')
                desc = item.get('description', 'No description')
                line = f"{indent}- [{status}] {desc}"
                output.append(line)
                sub_steps = item.get('sub_steps')
                if sub_steps:
                    # Indicate nesting without full recursive display in prompt
                    output.append(format_plan_for_prompt(sub_steps, indent_level + 1))
            else: # Fallback
                 output.append(f"{indent}- {str(item)}")
    else:
        output.append(f"{indent}{str(plan_items)}")
    return "\n".join(output)


class ChainOfThoughtNode(Node):
    def prep(self, shared):
        problem = shared.get("problem", "")
        thoughts = shared.get("thoughts", [])
        current_thought_number = shared.get("current_thought_number", 0)

        shared["current_thought_number"] = current_thought_number + 1

        # Format previous thoughts and extract last plan structure
        thoughts_text = ""
        last_plan_structure = None # Will store the list of dicts
        if thoughts:
            thoughts_text_list = []
            for i, t in enumerate(thoughts):
                 thought_block = f"Thought {t.get('thought_number', i+1)}:\n"
                 thinking = textwrap.dedent(t.get('current_thinking', 'N/A')).strip()
                 thought_block += f"  Thinking:\n{textwrap.indent(thinking, '    ')}\n"

                 plan_list = t.get('planning', [])
                 # Use the recursive helper for display formatting
                 plan_str_formatted = format_plan(plan_list, indent_level=2)
                 thought_block += f"  Plan Status After Thought {t.get('thought_number', i+1)}:\n{plan_str_formatted}"

                 if i == len(thoughts) - 1:
                     last_plan_structure = plan_list # Keep the actual structure

                 thoughts_text_list.append(thought_block)

            thoughts_text = "\n--------------------\n".join(thoughts_text_list)
        else:
            thoughts_text = "No previous thoughts yet."
            # Suggest an initial plan structure using dictionaries
            last_plan_structure = [
                {'description': "Understand the problem", 'status': "Pending"},
                {'description': "Develop a high-level plan", 'status': "Pending"},
                {'description': "Conclusion", 'status': "Pending"}
            ]

        # Format the last plan structure for the prompt context using the specific helper
        last_plan_text_for_prompt = format_plan_for_prompt(last_plan_structure) if last_plan_structure else "# No previous plan available."

        return {
            "problem": problem,
            "thoughts_text": thoughts_text,
            "last_plan_text": last_plan_text_for_prompt,
            "last_plan_structure": last_plan_structure, # Pass the raw structure too if needed for complex updates
            "current_thought_number": current_thought_number + 1,
            "is_first_thought": not thoughts
        }

    def exec(self, prep_res):
        problem = prep_res["problem"]
        thoughts_text = prep_res["thoughts_text"]
        last_plan_text = prep_res["last_plan_text"]
        # last_plan_structure = prep_res["last_plan_structure"] # Can use if needed
        current_thought_number = prep_res["current_thought_number"]
        is_first_thought = prep_res["is_first_thought"]

        # --- Construct Prompt ---
        # Instructions updated for dictionary structure
        instruction_base = textwrap.dedent(f"""
            Your task is to generate the next thought (Thought {current_thought_number}).

            Instructions:
            1.  **Evaluate Previous Thought:** If not the first thought, start `current_thinking` by evaluating Thought {current_thought_number - 1}. State: "Evaluation of Thought {current_thought_number - 1}: [Correct/Minor Issues/Major Error - explain]". Address errors first.
            2.  **Execute Step:** Execute the first step in the plan with `status: Pending`.
            3.  **Maintain Plan (Structure):** Generate an updated `planning` list. Each item should be a dictionary with keys: `description` (string), `status` (string: "Pending", "Done", "Verification Needed"), and optionally `result` (string, concise summary when Done) or `mark` (string, reason for Verification Needed). Sub-steps are represented by a `sub_steps` key containing a *list* of these dictionaries.
            4.  **Update Current Step Status:** In the updated plan, change the `status` of the executed step to "Done" and add a `result` key with a concise summary. If verification is needed based on evaluation, change status to "Verification Needed" and add a `mark`.
            5.  **Refine Plan (Sub-steps):** If a "Pending" step is complex, add a `sub_steps` key to its dictionary containing a list of new step dictionaries (status: "Pending") breaking it down. Keep the parent step's status "Pending" until all sub-steps are "Done".
            6.  **Refine Plan (Errors):** Modify the plan logically based on evaluation findings (e.g., change status, add correction steps).
            7.  **Final Step:** Ensure the plan progresses towards a final step dictionary like `{{'description': "Conclusion", 'status': "Pending"}}`.
            8.  **Termination:** Set `next_thought_needed` to `false` ONLY when executing the step with `description: "Conclusion"`.
        """)

        # Context remains largely the same
        if is_first_thought:
            instruction_context = textwrap.dedent("""
                **This is the first thought:** Create an initial plan as a list of dictionaries (keys: description, status). Include sub-steps via the `sub_steps` key if needed. Then, execute the first step in `current_thinking` and provide the updated plan (marking step 1 `status: Done` with a `result`).
            """)
        else:
            instruction_context = textwrap.dedent(f"""
                **Previous Plan (Simplified View):**
                {last_plan_text}

                Start `current_thinking` by evaluating Thought {current_thought_number - 1}. Then, proceed with the first step where `status: Pending`. Update the plan structure (list of dictionaries) reflecting evaluation, execution, and refinements.
            """)

        # Output format example updated for dictionary structure
        instruction_format = textwrap.dedent("""
            Format your response ONLY as a YAML structure enclosed in ```yaml ... ```:
            ```yaml
            current_thinking: |
              # Evaluation of Thought N: [Assessment] ... (if applicable)
              # Thinking for the current step...
            planning:
              # List of dictionaries (keys: description, status, Optional[result, mark, sub_steps])
              - description: "Step 1"
                status: "Done"
                result: "Concise result summary"
              - description: "Step 2 Complex Task" # Now broken down
                status: "Pending" # Parent remains Pending
                sub_steps:
                  - description: "Sub-task 2a"
                    status: "Pending"
                  - description: "Sub-task 2b"
                    status: "Verification Needed"
                    mark: "Result from Thought X seems off"
              - description: "Step 3"
                status: "Pending"
              - description: "Conclusion"
                status: "Pending"
            next_thought_needed: true # Set to false ONLY when executing the Conclusion step.
            ```
        """)

        # Combine prompt parts
        prompt = textwrap.dedent(f"""
            You are a meticulous AI assistant solving a complex problem step-by-step using a structured plan. You critically evaluate previous steps, refine the plan with sub-steps if needed, and handle errors logically. Use the specified YAML dictionary structure for the plan.

            Problem: {problem}

            Previous thoughts:
            {thoughts_text}
            --------------------
            {instruction_base}
            {instruction_context}
            {instruction_format}
        """)
        # --- End Prompt Construction ---

        response = call_llm(prompt)

        # Simple YAML extraction
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        thought_data = yaml.safe_load(yaml_str) # Can raise YAMLError

        # --- Validation (using assert) ---
        assert thought_data is not None, "YAML parsing failed, result is None"
        assert "current_thinking" in thought_data, "LLM response missing 'current_thinking'"
        assert "next_thought_needed" in thought_data, "LLM response missing 'next_thought_needed'"
        assert "planning" in thought_data, "LLM response missing 'planning'"
        assert isinstance(thought_data.get("planning"), list), "LLM response 'planning' is not a list"
        # Optional: Add deeper validation of list items being dicts if needed
        # --- End Validation ---

        # Add thought number
        thought_data["thought_number"] = current_thought_number
        return thought_data


    def post(self, shared, prep_res, exec_res):
        # Add the new thought to the list
        if "thoughts" not in shared:
            shared["thoughts"] = []
        shared["thoughts"].append(exec_res)

        # Extract plan for printing using the updated recursive helper function
        plan_list = exec_res.get("planning", ["Error: Planning data missing."])
        plan_str_formatted = format_plan(plan_list, indent_level=1)

        thought_num = exec_res.get('thought_number', 'N/A')
        current_thinking = exec_res.get('current_thinking', 'Error: Missing thinking content.')
        dedented_thinking = textwrap.dedent(current_thinking).strip()

        # Determine if this is the conclusion step based on description
        is_conclusion = False
        if isinstance(plan_list, list):
             # Check if the currently executed step (likely the last 'Done' or the current 'Pending' if evaluation failed) is Conclusion
             # This logic is approximate - might need refinement based on how LLM handles status updates
             for item in reversed(plan_list): # Check recent items first
                 if isinstance(item, dict) and item.get('description') == "Conclusion":
                     # If Conclusion is Done or it's Pending and we are ending, consider it conclusion
                     if item.get('status') == "Done" or (item.get('status') == "Pending" and not exec_res.get("next_thought_needed", True)):
                         is_conclusion = True
                         break
                 # Simple check, might need nested search if Conclusion could be a sub-step

        # Use is_conclusion flag OR the next_thought_needed flag for termination
        if not exec_res.get("next_thought_needed", True): # Primary termination signal
            shared["solution"] = dedented_thinking # Solution is the thinking content of the final step
            print(f"\nThought {thought_num} (Conclusion):")
            print(f"{textwrap.indent(dedented_thinking, '  ')}")
            print("\nFinal Plan Status:")
            print(textwrap.indent(plan_str_formatted, '  '))
            print("\n=== FINAL SOLUTION ===")
            print(dedented_thinking)
            print("======================\n")
            return "end"

        # Otherwise, continue the chain
        print(f"\nThought {thought_num}:")
        print(f"{textwrap.indent(dedented_thinking, '  ')}")
        print("\nCurrent Plan Status:")
        print(textwrap.indent(plan_str_formatted, '  '))
        print("-" * 50)

        return "continue"
</file>

<file path="cookbook/pocketflow-thinking/README.md">
# Chain-of-Thought

This project demonstrates an implementation that orchestrates a Chain-of-Thought process, enabling LLMs to solve complex reasoning problems by thinking step-by-step. It's designed to improve problem-solving accuracy through deliberate, structured reasoning managed externally.

This implementation is based on: [Build Chain-of-Thought From Scratch - Tutorial for Dummies](https://zacharyhuang.substack.com/p/build-chain-of-thought-from-scratch).

## Features

- Improves model reasoning on complex problems.
- Leverages capable instruction-following models (e.g., Claude 3.7 Sonnet, GPT-4 series) to perform structured Chain-of-Thought reasoning.
- Solves problems that direct prompting often fails on by breaking them down systematically.
- Provides detailed reasoning traces, including step-by-step evaluation and planning, for verification.

## Getting Started

1.  **Install Packages:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Set API Key:**
    ```bash
    export ANTHROPIC_API_KEY="your-api-key-here"
    ```

3.  **Verify API Key (Optional):**
    Run a quick check to ensure your key and environment are set up correctly.
    ```bash
    python utils.py
    ```

4.  **Run Default Example:**
    Execute the main script to see the process in action with the default Jane Street problem.
    ```bash
    python main.py
    ```
    The default question is:
    > You keep rolling a fair die until you roll three, four, five in that order consecutively on three rolls. What is the probability that you roll the die an odd number of times?

5.  **Run Custom Problem:**
    Provide your own reasoning problem using the `--` argument.
    ```bash
    python main.py --"Your complex reasoning problem here"
    ```

## How It Works

The implementation uses a self-looping PocketFlow node (`ChainOfThoughtNode`) that guides an LLM through a structured problem-solving process:

```mermaid
flowchart LR
    cot[ChainOfThoughtNode] -->|"continue"| cot
```

In each loop (thought step), the node directs the LLM to:
1.  Evaluate the previous thought's reasoning and results.
2.  Execute the next pending step according to a maintained plan.
3.  Update the plan, marking the step done (with results) or noting issues.
4.  Refine the plan if steps need breaking down or errors require correction.
5.  Decide if further thinking (`next_thought_needed`) is required based on the plan state.

This external orchestration enforces a systematic approach, helping models tackle problems that are difficult with a single prompt.

## Comparison with Different Approaches

-   **Standard Prompting**: Techniques like asking the model to "think step by step" within a single prompt can help, but the reasoning might lack depth or structure, and the model can easily lose track or make unrecoverable errors.
-   **Native Extended Thinking Modes**: Some models (like Claude 3.7, GPT-o1, etc.) offer dedicated modes or features explicitly for extended reasoning, often yielding strong results directly via API calls.
-   **This Implementation**: Demonstrates how to orchestrate a structured Chain-of-Thought process using standard LLMs (even those without a specific native 'extended thinking' mode), managing the steps, planning, and evaluation externally via prompt engineering and flow control.

## Example Thinking Process

Let's try out this challenging [Jane Street Quant Trading Interview Question](https://www.youtube.com/watch?v=gQJTkuEVPrU):

> **Problem**: You keep rolling a fair die until you roll three, four, five in that order consecutively on three rolls. What is the probability that you roll the die an odd number of times?

This problem demonstrates why structured Chain-of-Thought is valuable:

-   **Standard models (single prompt)**: Often get the wrong answer or provide flawed reasoning.
-   **Models using native thinking modes**: Can find the correct answer (216/431 â‰ˆ 0.5012), though performance and reasoning clarity may vary.
-   **This implementation (orchestrating a capable LLM)**: Can guide the model towards the correct answer by enforcing a step-by-step plan, evaluation, and refinement loop.

For comparison:
-   [Claude 3.7 Sonnet (single prompt)](https://claude.ai/share/da139326-42fe-42d9-9d7b-35870daa5c1b): Wrong answer
-   [Claude 3.7 Sonnet (using built-in thinking)](https://claude.ai/share/6f4140ed-f33c-4949-8778-a57719498e40): Correct answer after 3m, 45s
-   [GPT-o1 (using built-in thinking)](https://chatgpt.com/share/67fee0fd-2600-8000-bcdf-76e40a986ee4): Correct answer after 2m, 0s
-   [GPT-o1 pro (using built-in thinking)](https://chatgpt.com/share/67fee11b-530c-8000-92d1-609b6ca49c9c): Correct answer after 4m, 24s

Below is an example output trace showing how **this implementation guides Claude 3.7 Sonnet** through the problem-solving process:

> **Note:** Even with structured thinking orchestration, models don't always get the right answer, especially on very complex or novel problems. However, this approach significantly improves the robustness of the reasoning process and provides a traceable path for verification and debugging.

```
ðŸ¤” Processing question: You keep rolling a fair die until you roll three, four, five in that order consecutively on three rolls. What is the probability that you roll the die an odd number of times?

Thought 1:
  Let me think through this problem by setting up a clear approach.

  We need to find the probability that the total number of die rolls will be odd, given we stop when we roll "3,4,5" consecutively in that order.

  This is a problem about a stochastic process that ends when a specific pattern occurs. To solve it, I'll:
  1. Set up a Markov chain model to track progress toward the target sequence
  2. Calculate expected lengths and probabilities for different outcomes
  3. Determine if the total number of rolls has equal probability of being odd vs. even

  First, I need to understand what states we can be in while rolling the die:
  - State 0: Haven't rolled any of the sequence yet
  - State 1: Just rolled a 3, waiting for 4
  - State 2: Rolled 3 followed by 4, waiting for 5
  - State 3: Success! Rolled the full "3,4,5" sequence

  When the die is rolled, we transition between these states with certain probabilities. Each state depends only on the previous 0-2 rolls, making this a Markov process.

Current Plan Status:
    - [Done] Understand the problem structure: Identified that we need the probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Pending] Set up the Markov model with transition probabilities
      - [Pending] Define relevant states in the Markov chain
      - [Pending] Calculate transition probabilities between states
    - [Pending] Calculate generating functions for the process
    - [Pending] Determine probability of odd number of rolls
      - [Pending] Extract coefficient information from generating function
      - [Pending] Compute final probability value
    - [Pending] Verify solution with alternative approach
    - [Pending] Conclusion
--------------------------------------------------

Thought 2:
  Evaluation of Thought 1: Correct. The approach of using a Markov chain to model this problem is appropriate. Identifying the states (0: no part of sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) correctly captures the progression toward our target sequence.

  I'll now set up the Markov model with transition probabilities. Each state represents our progress toward the target sequence "3,4,5".

  State 0: Haven't rolled any relevant part of the sequence yet
  - If we roll a 3: Move to State 1 (probability 1/6)
  - If we roll anything else: Stay in State 0 (probability 5/6)

  State 1: Just rolled a 3, waiting for a 4
  - If we roll a 4: Move to State 2 (probability 1/6)
  - If we roll a 3: Stay in State 1 (probability 1/6)
  - If we roll anything else: Return to State 0 (probability 4/6)

  State 2: Rolled 3 followed by 4, waiting for a 5
  - If we roll a 5: Move to State 3 (success - probability 1/6)
  - If we roll a 3: Move to State 1 (probability 1/6)
  - If we roll anything else: Return to State 0 (probability 4/6)

  State 3: Success! Rolled the sequence "3,4,5"
  - This is an absorbing state; once we reach it, we stop rolling.

  These transition probabilities can be represented as a matrix:
  P = [
      [5/6, 1/6, 0,   0  ],
      [4/6, 1/6, 1/6, 0  ],
      [4/6, 1/6, 0,   1/6],
      [0,   0,   0,   1  ]
  ]

  This completes the setup of the Markov model with transition probabilities.

Current Plan Status:
    - [Done] Understand the problem structure: Identified that we need the probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Done] Set up the Markov model with transition probabilities: Defined 4 states (0: no sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) and calculated transition probabilities between all states in matrix form.
    - [Pending] Calculate generating functions for the process
      - [Pending] Set up generating function equations for each state
      - [Pending] Solve the system of equations to find generating functions
    - [Pending] Determine probability of odd number of rolls
      - [Pending] Extract coefficient information from generating function
      - [Pending] Compute final probability value
    - [Pending] Verify solution with alternative approach
    - [Pending] Conclusion
--------------------------------------------------

Thought 3:
  Evaluation of Thought 2: Correct. The Markov chain setup with the four states and the transition probability matrix accurately represents our problem. Now I'll proceed to calculating the generating functions for this process.

  To calculate the generating functions, I'll use the variable z to represent a single die roll. For each state, the generating function gives us information about the number of rolls until we reach our success state.

  Let Gâ‚€(z), Gâ‚(z), Gâ‚‚(z), and Gâ‚ƒ(z) be the generating functions for reaching the success state (State 3) when starting from State 0, 1, 2, and 3 respectively.

  Gâ‚ƒ(z) = 1 (since we're already in the success state)

  For the other states, we can write equations based on the first roll and subsequent state:

  Gâ‚€(z) = zÂ·(5/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z))
  This means: we roll once (factor z), then with probability 5/6 we stay in State 0, or with probability 1/6 we move to State 1.

  Gâ‚(z) = zÂ·(4/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z) + 1/6Â·Gâ‚‚(z))
  When in State 1, we roll once (factor z), then either move to State 0, stay in State 1, or advance to State 2.

  Gâ‚‚(z) = zÂ·(4/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z) + 1/6Â·Gâ‚ƒ(z))
      = zÂ·(4/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z) + 1/6Â·1)
      = zÂ·(4/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z) + 1/6)

  Now I can solve this system of equations for Gâ‚€(z):

  From Gâ‚€(z) = zÂ·(5/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z)):
  Gâ‚€(z) - 5z/6Â·Gâ‚€(z) = z/6Â·Gâ‚(z)
  Gâ‚€(z)(1 - 5z/6) = z/6Â·Gâ‚(z)
  Gâ‚€(z) = (z/6Â·Gâ‚(z))/(1 - 5z/6) ... (1)

  From Gâ‚(z) = zÂ·(4/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z) + 1/6Â·Gâ‚‚(z)):
  Gâ‚(z) - z/6Â·Gâ‚(z) = 4z/6Â·Gâ‚€(z) + z/6Â·Gâ‚‚(z)
  Gâ‚(z)(1 - z/6) = 4z/6Â·Gâ‚€(z) + z/6Â·Gâ‚‚(z) ... (2)

  From Gâ‚‚(z) = zÂ·(4/6Â·Gâ‚€(z) + 1/6Â·Gâ‚(z) + 1/6):
  Gâ‚‚(z) = 4z/6Â·Gâ‚€(z) + z/6Â·Gâ‚(z) + z/6 ... (3)

  Substituting (3) into (2):
  Gâ‚(z)(1 - z/6) = 4z/6Â·Gâ‚€(z) + z/6Â·(4z/6Â·Gâ‚€(z) + z/6Â·Gâ‚(z) + z/6)
  Gâ‚(z)(1 - z/6) = 4z/6Â·Gâ‚€(z) + 4zÂ²/36Â·Gâ‚€(z) + zÂ²/36Â·Gâ‚(z) + zÂ²/36
  Gâ‚(z)(1 - z/6 - zÂ²/36) = 4z/6Â·Gâ‚€(z)(1 + z/6) + zÂ²/36
  Gâ‚(z) = (4z/6Â·Gâ‚€(z)(1 + z/6) + zÂ²/36)/(1 - z/6 - zÂ²/36) ... (4)

  Substituting (4) into (1) gives us Gâ‚€(z) in terms of z only, which I can then simplify to find the closed form.

  This is getting algebraically intricate. Let me take a step back and consider a different approach using first-step analysis with parity.

Current Plan Status:
    - [Done] Understand the problem structure: Identified that we need probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Done] Set up the Markov model with transition probabilities: Defined 4 states (0: no sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) and calculated transition probabilities between all states in matrix form.
    - [Verification Needed] Calculate generating functions for the process (The generating function approach became algebraically complicated. Consider a more direct approach using first-step analysis with parity.)
      - [Done] Set up generating function equations for each state: Established equations Gâ‚€(z), Gâ‚(z), Gâ‚‚(z), and Gâ‚ƒ(z) to track rolls until success.
      - [Verification Needed] Solve the system of equations to find generating functions (Equations become unwieldy; may not be most efficient approach.)
    - [Pending] Use first-step analysis with parity
      - [Pending] Define probability variables with parity information
      - [Pending] Set up equations relating odd/even outcome probabilities
      - [Pending] Solve the system of equations
    - [Pending] Determine probability of odd number of rolls
      - [Pending] Extract final probability from analysis
      - [Pending] Verify the mathematical result
    - [Pending] Verify solution with alternative approach
    - [Pending] Conclusion
--------------------------------------------------

Thought 4:
  Evaluation of Thought 3: Correct, but pivoting is wise. The generating function approach, while theoretically sound, is becoming algebraically complex. The decision to switch to a first-step analysis with parity is appropriate for this problem.

  I'll now use first-step analysis focusing specifically on the parity of the number of rolls.

  Let's define:
  - Pâ‚€: Probability of rolling an odd number of times when starting in State 0
  - Pâ‚: Probability of rolling an odd number of times when starting in State 1
  - Pâ‚‚: Probability of rolling an odd number of times when starting in State 2

  Using first-step analysis, we can write equations for each state:

  For State 0:
  - If we roll a 3 (prob 1/6), we go to State 1 with 1 roll used (odd). The total number of rolls will be odd if the remaining rolls from State 1 are even. 
  - If we roll anything else (prob 5/6), we stay in State 0 with 1 roll used (odd). The total will be odd if the remaining rolls from State 0 are even.

  Therefore: Pâ‚€ = (1/6)*(1-Pâ‚) + (5/6)*(1-Pâ‚€) = 5/6 - 5Pâ‚€/6 + 1/6 - Pâ‚/6

  For State 1:
  - If we roll a 4 (prob 1/6), we go to State 2 with 1 roll used (odd). The total will be odd if remaining rolls from State 2 are even.
  - If we roll a 3 (prob 1/6), we stay in State 1 with 1 roll used (odd). The total will be odd if remaining rolls from State 1 are even.
  - If we roll anything else (prob 4/6), we go to State 0 with 1 roll used (odd). The total will be odd if remaining rolls from State 0 are even.

  Therefore: Pâ‚ = (1/6)*(1-Pâ‚‚) + (1/6)*(1-Pâ‚) + (4/6)*(1-Pâ‚€) = 1 - Pâ‚‚/6 - Pâ‚/6 - 4Pâ‚€/6

  For State 2:
  - If we roll a 5 (prob 1/6), we're done with 1 more roll (odd). Total is definitely odd.
  - If we roll a 3 (prob 1/6), we go to State 1 with 1 roll used (odd). The total will be odd if remaining rolls from State 1 are even.
  - If we roll anything else (prob 4/6), we go to State 0 with 1 roll used (odd). The total will be odd if remaining rolls from State 0 are even.

  Therefore: Pâ‚‚ = (1/6)*1 + (1/6)*(1-Pâ‚) + (4/6)*(1-Pâ‚€) = 1/6 + 1/6 - Pâ‚/6 + 4/6 - 4Pâ‚€/6 = 6/6 - Pâ‚/6 - 4Pâ‚€/6

  Now I have a system of 3 equations with 3 unknowns:
  Pâ‚€ = 1 - 5Pâ‚€/6 - Pâ‚/6
  Pâ‚ = 1 - Pâ‚/6 - Pâ‚‚/6 - 4Pâ‚€/6
  Pâ‚‚ = 1 - Pâ‚/6 - 4Pâ‚€/6

  Simplifying:
  (6/6)Pâ‚€ = 6/6 - 5Pâ‚€/6 - Pâ‚/6
  (6/6)Pâ‚ = 6/6 - Pâ‚/6 - Pâ‚‚/6 - 4Pâ‚€/6
  (6/6)Pâ‚‚ = 6/6 - Pâ‚/6 - 4Pâ‚€/6

  Further simplifying:
  11Pâ‚€/6 + Pâ‚/6 = 1
  4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1
  4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1

  This is a linear system of equations that I can solve.

Current Plan Status:
    - [Done] Understand the problem structure: Identified that we need probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Done] Set up the Markov model with transition probabilities: Defined 4 states (0: no sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) and calculated transition probabilities between all states in matrix form.
    - [Verification Needed] Calculate generating functions for the process (Approach became algebraically complicated; shifted to more direct first-step analysis with parity.)
      - [Done] Set up generating function equations for each state: Established equations Gâ‚€(z), Gâ‚(z), Gâ‚‚(z), and Gâ‚ƒ(z) to track rolls until success.
      - [Verification Needed] Solve the system of equations to find generating functions (Equations became unwieldy; more efficient to use first-step analysis with parity instead.)
    - [Done] Use first-step analysis with parity: Defined variables Pâ‚€, Pâ‚, Pâ‚‚ for probability of odd rolls from each state. Set up system of equations tracking odd/even outcomes.
      - [Done] Define probability variables with parity information: Defined Pâ‚€, Pâ‚, Pâ‚‚ as probabilities of odd number of rolls starting from respective states.
      - [Done] Set up equations relating odd/even outcome probabilities: Derived equations: 11Pâ‚€/6 + Pâ‚/6 = 1, 4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1, 4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1.
      - [Pending] Solve the system of equations
    - [Pending] Determine probability of odd number of rolls
      - [Pending] Extract final probability from analysis
      - [Pending] Verify the mathematical result
    - [Pending] Verify solution with alternative approach
    - [Pending] Conclusion
--------------------------------------------------

Thought 5:
  Evaluation of Thought 4: Correct. The first-step analysis with parity is well set up, with appropriate variables Pâ‚€, Pâ‚, and Pâ‚‚ representing the probability of rolling an odd number of times from each state. The system of equations is correctly derived, though I notice it's written in slightly different forms across the thought. Let me solve this system now.

  Taking the system of equations from the end of Thought 4:
  11Pâ‚€/6 + Pâ‚/6 = 1
  4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1
  4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1

  Let's multiply all equations by 6 to eliminate fractions:
  11Pâ‚€ + Pâ‚ = 6
  4Pâ‚€ + 7Pâ‚ + Pâ‚‚ = 6
  4Pâ‚€ + Pâ‚ + 6Pâ‚‚ = 6

  Subtracting the third equation from the second:
  (4Pâ‚€ + 7Pâ‚ + Pâ‚‚) - (4Pâ‚€ + Pâ‚ + 6Pâ‚‚) = 6 - 6
  6Pâ‚ - 5Pâ‚‚ = 0
  6Pâ‚ = 5Pâ‚‚
  Pâ‚ = 5Pâ‚‚/6

  From the first equation:
  11Pâ‚€ + Pâ‚ = 6
  11Pâ‚€ + 5Pâ‚‚/6 = 6
  66Pâ‚€ + 5Pâ‚‚ = 36

  From the third equation:
  4Pâ‚€ + Pâ‚ + 6Pâ‚‚ = 6
  4Pâ‚€ + 5Pâ‚‚/6 + 6Pâ‚‚ = 6
  24Pâ‚€ + 5Pâ‚‚ + 36Pâ‚‚ = 36
  24Pâ‚€ + 41Pâ‚‚ = 36

  Now solving for Pâ‚€ in terms of Pâ‚‚:
  24Pâ‚€ + 41Pâ‚‚ = 36
  24Pâ‚€ = 36 - 41Pâ‚‚
  Pâ‚€ = (36 - 41Pâ‚‚)/24 = 3/2 - 41Pâ‚‚/24

  Substituting back into 66Pâ‚€ + 5Pâ‚‚ = 36:
  66(3/2 - 41Pâ‚‚/24) + 5Pâ‚‚ = 36
  99/2 - 66*41Pâ‚‚/24 + 5Pâ‚‚ = 36
  99/2 - 2706Pâ‚‚/24 + 5Pâ‚‚ = 36
  99/2 - 2706Pâ‚‚/24 + 120Pâ‚‚/24 = 36
  99/2 - (2706-120)Pâ‚‚/24 = 36
  99/2 - 2586Pâ‚‚/24 = 36
  99 - 2586Pâ‚‚/12 = 72
  99 - 72 = 2586Pâ‚‚/12
  27 = 2586Pâ‚‚/12
  27*12 = 2586Pâ‚‚
  324 = 2586Pâ‚‚
  Pâ‚‚ = 324/2586 = 108/862 = 54/431

  Now we can find Pâ‚:
  Pâ‚ = 5Pâ‚‚/6 = 5(54/431)/6 = 270/2586 = 45/431

  And Pâ‚€:
  Pâ‚€ = 3/2 - 41Pâ‚‚/24 = 3/2 - 41(54/431)/24 = 3/2 - 2214/10344 = 3/2 - 1107/5172

  Let's calculate: 
  Pâ‚€ = 3/2 - 1107/5172 = 7758/5172 - 1107/5172 = 6651/5172 = 2217/1724

  Now we have:
  Pâ‚€ = 2217/1724
  Pâ‚ = 45/431
  Pâ‚‚ = 54/431

  Wait, I see an error in my calculation. Let me restart with the system of equations.

  From the equations:
  11Pâ‚€/6 + Pâ‚/6 = 1
  4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1
  4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1

  Multiplying by 6:
  11Pâ‚€ + Pâ‚ = 6
  4Pâ‚€ + 7Pâ‚ + Pâ‚‚ = 6
  4Pâ‚€ + Pâ‚ + 6Pâ‚‚ = 6

  From the first: Pâ‚ = 6 - 11Pâ‚€

  Substituting into the second:
  4Pâ‚€ + 7(6 - 11Pâ‚€) + Pâ‚‚ = 6
  4Pâ‚€ + 42 - 77Pâ‚€ + Pâ‚‚ = 6
  4Pâ‚€ - 77Pâ‚€ + Pâ‚‚ = 6 - 42
  -73Pâ‚€ + Pâ‚‚ = -36
  Pâ‚‚ = -36 + 73Pâ‚€

  Substituting Pâ‚ and Pâ‚‚ into the third equation:
  4Pâ‚€ + (6 - 11Pâ‚€) + 6(-36 + 73Pâ‚€) = 6
  4Pâ‚€ + 6 - 11Pâ‚€ - 216 + 438Pâ‚€ = 6
  4Pâ‚€ - 11Pâ‚€ + 438Pâ‚€ = 6 - 6 + 216
  431Pâ‚€ = 216
  Pâ‚€ = 216/431 = 8/16 = 1/2

  Now we can find:
  Pâ‚ = 6 - 11Pâ‚€ = 6 - 11(216/431) = 6 - 2376/431 = 2586/431 - 2376/431 = 210/431
  Pâ‚‚ = -36 + 73Pâ‚€ = -36 + 73(216/431) = -36 + 15768/431 = -15552/431 + 15768/431 = 216/431

  The answer to our original question is Pâ‚€, which is 216/431.

  Actually, I notice the fraction simplified to 1/2 above is incorrect (216/431 â‰  8/16). Let's double-check with the equations:

  From 11Pâ‚€ + Pâ‚ = 6 and Pâ‚ = 210/431, we get:
  11(216/431) + 210/431 = 6
  2376/431 + 210/431 = 6
  2586/431 = 6
  2586 = 2586

  This confirms our answer: Pâ‚€ = 216/431, which is the probability of rolling the die an odd number of times.

Current Plan Status:
    - [Done] Understand the problem structure: Identified that we need probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Done] Set up the Markov model with transition probabilities: Defined 4 states (0: no sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) and calculated transition probabilities between all states in matrix form.
    - [Verification Needed] Calculate generating functions for the process (The generating function approach was algebraically complex; pivoted to first-step analysis with parity instead.)
      - [Done] Set up generating function equations for each state: Established equations Gâ‚€(z), Gâ‚(z), Gâ‚‚(z), and Gâ‚ƒ(z) to track rolls until success.
      - [Verification Needed] Solve the system of equations to find generating functions (Equations became unwieldy; more efficient to use first-step analysis with parity instead.)
    - [Done] Use first-step analysis with parity: Successfully applied first-step analysis with parity to solve the problem directly.
      - [Done] Define probability variables with parity information: Defined Pâ‚€, Pâ‚, Pâ‚‚ as probabilities of odd number of rolls starting from respective states.
      - [Done] Set up equations relating odd/even outcome probabilities: Derived equations: 11Pâ‚€/6 + Pâ‚/6 = 1, 4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1, 4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1.
      - [Done] Solve the system of equations: Found Pâ‚€ = 216/431, Pâ‚ = 210/431, Pâ‚‚ = 216/431.
    - [Done] Determine probability of odd number of rolls: The probability of rolling the die an odd number of times is 216/431 â‰ˆ 0.501
      - [Done] Extract final probability from analysis: Identified Pâ‚€ = 216/431 as the answer to our original question.
      - [Done] Verify the mathematical result: Verified solution by substituting back into original equations.
    - [Pending] Verify solution with alternative approach
    - [Pending] Conclusion
--------------------------------------------------

Thought 6:
  Evaluation of Thought 5: Correct. The calculation was methodical and accurate, resulting in the solution Pâ‚€ = 216/431. The self-correction when an arithmetic error was spotted was appropriate. The substitution back into the original equation verified that the answer is correct.

  Let me now verify our solution using an alternative approach. The problem asks for the probability of an odd number of rolls when stopping at the sequence "3,4,5". An alternative way to approach this is to consider the complementary probability - the probability of an even number of rolls.

  Since Pâ‚€ = 216/431 is the probability of an odd number of rolls, the probability of an even number of rolls must be 1 - 216/431 = 215/431.

  Let me verify this with the equations we derived:
  - Let Qâ‚€, Qâ‚, Qâ‚‚ be the probabilities of rolling an even number of times from states 0, 1, and 2 respectively.
  - Since the events "odd number of rolls" and "even number of rolls" are complementary: Qâ‚€ = 1-Pâ‚€, Qâ‚ = 1-Pâ‚, Qâ‚‚ = 1-Pâ‚‚
  - Therefore: Qâ‚€ = 1 - 216/431 = 215/431, Qâ‚ = 1 - 210/431 = 221/431, Qâ‚‚ = 1 - 216/431 = 215/431

  Now, I can derive the equations for Qâ‚€, Qâ‚, Qâ‚‚ directly:

  For State 0:
  - If we roll a 3 (prob 1/6), we transition to State 1 after 1 roll (odd). The total will be even if the remaining rolls from State 1 are odd.
  - If we roll anything else (prob 5/6), we stay in State 0 after 1 roll (odd). The total will be even if the remaining rolls from State 0 are odd.

  Therefore: Qâ‚€ = (1/6)(Pâ‚) + (5/6)(Pâ‚€) = Pâ‚/6 + 5Pâ‚€/6 = 210/431/6 + 5(216/431)/6 = 210/2586 + 1080/2586 = 1290/2586 = 215/431

  For State 1:
  - If we roll a 4 (prob 1/6), we go to State 2 after 1 roll (odd). The total will be even if the remaining rolls from State 2 are odd.
  - If we roll a 3 (prob 1/6), we stay in State 1 after 1 roll (odd). The total will be even if the remaining rolls from State 1 are odd.
  - If we roll anything else (prob 4/6), we go to State 0 after 1 roll (odd). The total will be even if the remaining rolls from State 0 are odd.

  Therefore: Qâ‚ = (1/6)(Pâ‚‚) + (1/6)(Pâ‚) + (4/6)(Pâ‚€) = Pâ‚‚/6 + Pâ‚/6 + 4Pâ‚€/6 = 216/431/6 + 210/431/6 + 4(216/431)/6 = 216/2586 + 210/2586 + 864/2586 = 1290/2586 = 215/431

  Wait, that doesn't match our expected Qâ‚ = 221/431. Let me double-check...

  The direct calculation of Qâ‚ gives a different result than 1-Pâ‚. This suggests an error somewhere. Let me reconsider our system:

  If Pâ‚€ + Qâ‚€ = 1, Pâ‚ + Qâ‚ = 1, Pâ‚‚ + Qâ‚‚ = 1, then our system should be consistent. Let me verify our values from Thought 5:

  From 11Pâ‚€ + Pâ‚ = 6 and Pâ‚€ = 216/431:
  11(216/431) + Pâ‚ = 6
  2376/431 + Pâ‚ = 6
  Pâ‚ = 6 - 2376/431 = 2586/431 - 2376/431 = 210/431

  From 4Pâ‚€ + Pâ‚ + 6Pâ‚‚ = 6:
  4(216/431) + 210/431 + 6Pâ‚‚ = 6
  864/431 + 210/431 + 6Pâ‚‚ = 6
  1074/431 + 6Pâ‚‚ = 6
  6Pâ‚‚ = 6 - 1074/431 = 2586/431 - 1074/431 = 1512/431
  Pâ‚‚ = 1512/431/6 = 1512/2586 = 252/431

  This doesn't match our calculated Pâ‚‚ = 216/431 from Thought 5. Let me solve the system again, more carefully.

  From the system:
  11Pâ‚€ + Pâ‚ = 6
  4Pâ‚€ + 7Pâ‚ + Pâ‚‚ = 6
  4Pâ‚€ + Pâ‚ + 6Pâ‚‚ = 6

  Subtracting the first from the third:
  (4Pâ‚€ + Pâ‚ + 6Pâ‚‚) - (11Pâ‚€ + Pâ‚) = 6 - 6
  4Pâ‚€ - 11Pâ‚€ + 6Pâ‚‚ = 0
  -7Pâ‚€ + 6Pâ‚‚ = 0
  6Pâ‚‚ = 7Pâ‚€
  Pâ‚‚ = 7Pâ‚€/6

  Subtracting the first from the second:
  (4Pâ‚€ + 7Pâ‚ + Pâ‚‚) - (11Pâ‚€ + Pâ‚) = 6 - 6
  4Pâ‚€ - 11Pâ‚€ + 6Pâ‚ + Pâ‚‚ = 0
  -7Pâ‚€ + 6Pâ‚ + Pâ‚‚ = 0

  Substituting Pâ‚‚ = 7Pâ‚€/6:
  -7Pâ‚€ + 6Pâ‚ + 7Pâ‚€/6 = 0
  -42Pâ‚€ + 36Pâ‚ + 7Pâ‚€ = 0
  -35Pâ‚€ + 36Pâ‚ = 0
  36Pâ‚ = 35Pâ‚€
  Pâ‚ = 35Pâ‚€/36

  Now, from the first equation:
  11Pâ‚€ + Pâ‚ = 6
  11Pâ‚€ + 35Pâ‚€/36 = 6
  396Pâ‚€ + 35Pâ‚€ = 216
  431Pâ‚€ = 216
  Pâ‚€ = 216/431

  Therefore:
  Pâ‚ = 35Pâ‚€/36 = 35(216/431)/36 = 35(6)/36(431/6) = 35/6 Ã· 431/6 = 35/431
  Pâ‚‚ = 7Pâ‚€/6 = 7(216/431)/6 = 7(6)/6(431/6) = 7/431/6 Ã— 6 = 7/431 Ã— 6 = 42/431

  Hmm, these values are different from what we calculated in Thought 5. Let me check these values in the original equations:

  11(216/431) + 35/431 = 6?
  2376/431 + 35/431 = 6?
  2411/431 = 6?
  2411 = 2586?

  This doesn't check out. Let me solve the system one more time using a different approach.

  From the original system:
  11Pâ‚€ + Pâ‚ = 6                 (1)
  4Pâ‚€ + 7Pâ‚ + Pâ‚‚ = 6            (2)
  4Pâ‚€ + Pâ‚ + 6Pâ‚‚ = 6            (3)

  From (3) - (1):
  4Pâ‚€ + Pâ‚ + 6Pâ‚‚ - 11Pâ‚€ - Pâ‚ = 6 - 6
  -7Pâ‚€ + 6Pâ‚‚ = 0
  6Pâ‚‚ = 7Pâ‚€
  Pâ‚‚ = 7Pâ‚€/6                    (4)

  From (2) - (1):
  4Pâ‚€ + 7Pâ‚ + Pâ‚‚ - 11Pâ‚€ - Pâ‚ = 6 - 6
  -7Pâ‚€ + 6Pâ‚ + Pâ‚‚ = 0

  Substituting (4):
  -7Pâ‚€ + 6Pâ‚ + 7Pâ‚€/6 = 0
  -42Pâ‚€ + 36Pâ‚ + 7Pâ‚€ = 0
  -35Pâ‚€ + 36Pâ‚ = 0
  36Pâ‚ = 35Pâ‚€
  Pâ‚ = 35Pâ‚€/36                  (5)

  Substituting (5) into (1):
  11Pâ‚€ + 35Pâ‚€/36 = 6
  (11Ã—36)Pâ‚€ + 35Pâ‚€ = 6Ã—36
  396Pâ‚€ + 35Pâ‚€ = 216
  431Pâ‚€ = 216
  Pâ‚€ = 216/431

  Now calculate:
  Pâ‚ = 35Pâ‚€/36 = 35(216/431)/36 = 7560/15516 = 35/72 Ã— 216/431 = 35Ã—216/36Ã—431 = 7560/15516 = 35/72 Ã— 216/431 = 7560/15516 = 1260/2586 = 210/431

  Let me verify:
  11(216/431) + 210/431 = 6?
  2376/431 + 210/431 = 6?
  2586/431 = 6?
  2586 = 2586 âœ“

  Now for Pâ‚‚:
  Pâ‚‚ = 7Pâ‚€/6 = 7(216/431)/6 = 1512/2586 = 252/431

  Checking:
  4(216/431) + 210/431 + 6(252/431) = 6?
  864/431 + 210/431 + 1512/431 = 6?
  2586/431 = 6?
  2586 = 2586 âœ“

  Our final answer is: Pâ‚€ = 216/431, Pâ‚ = 210/431, Pâ‚‚ = 252/431

  This confirms our result that the probability of rolling an odd number of times is 216/431 â‰ˆ 0.5012.

Current Plan Status:
    - [Done] Understand the problem structure: Identified that we need probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Done] Set up the Markov model with transition probabilities: Defined 4 states (0: no sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) and calculated transition probabilities between all states in matrix form.
    - [Verification Needed] Calculate generating functions for the process (The generating function approach was algebraically complex; pivoted to first-step analysis with parity instead.)
      - [Done] Set up generating function equations for each state: Established equations Gâ‚€(z), Gâ‚(z), Gâ‚‚(z), and Gâ‚ƒ(z) to track rolls until success.
      - [Verification Needed] Solve the system of equations to find generating functions (Equations became unwieldy; more efficient to use first-step analysis with parity instead.)
    - [Done] Use first-step analysis with parity: Successfully applied first-step analysis with parity to solve the problem directly.
      - [Done] Define probability variables with parity information: Defined Pâ‚€, Pâ‚, Pâ‚‚ as probabilities of odd number of rolls starting from respective states.
      - [Done] Set up equations relating odd/even outcome probabilities: Derived equations: 11Pâ‚€/6 + Pâ‚/6 = 1, 4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1, 4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1.
      - [Done] Solve the system of equations: Found Pâ‚€ = 216/431, Pâ‚ = 210/431, Pâ‚‚ = 252/431.
    - [Done] Determine probability of odd number of rolls: The probability of rolling the die an odd number of times is 216/431 â‰ˆ 0.5012.
      - [Done] Extract final probability from analysis: Identified Pâ‚€ = 216/431 as the answer to our original question.
      - [Done] Verify the mathematical result: Verified solution by substituting back into original equations.
    - [Done] Verify solution with alternative approach: Double-checked our solution by re-solving the system and verifying that the complementary probability approach gives consistent results.
    - [Pending] Conclusion
--------------------------------------------------

Thought 7 (Conclusion):
  Evaluation of Thought 6: Correct. The analysis carefully re-solved the system of equations to verify our answer and found Pâ‚€ = 216/431, Pâ‚ = 210/431, and Pâ‚‚ = 252/431. The verification process was thorough, checking the solution by substituting back into the original equations, confirming that the probability of rolling an odd number of times is 216/431 â‰ˆ 0.5012.

  Now I'll provide a conclusion that summarizes the problem, our approach, and the final answer.

  We set out to find the probability that we roll a fair die an odd number of times until getting the sequence "3,4,5" consecutively in that order.

  Our approach used a Markov chain model with states representing progress toward the target sequence:
  - State 0: Haven't rolled any part of the sequence yet
  - State 1: Just rolled a 3, waiting for 4
  - State 2: Rolled 3 followed by 4, waiting for 5
  - State 3: Success! Rolled the full "3,4,5" sequence

  Initially, we considered a generating function approach, but pivoted to a more direct first-step analysis with parity. We defined variables Pâ‚€, Pâ‚, and Pâ‚‚ representing the probabilities of an odd number of total rolls when starting from each respective state.

  Through careful derivation and solution of a system of linear equations, we found:
  Pâ‚€ = 216/431 â‰ˆ 0.5012
  Pâ‚ = 210/431 â‰ˆ 0.4872
  Pâ‚‚ = 252/431 â‰ˆ 0.5847

  Since we start from State 0 (no part of the sequence rolled yet), the answer to our original question is Pâ‚€ = 216/431. 

  To verify this result, we solved the system multiple times and checked that our values satisfied the original equations.

  The final answer is: The probability of rolling the die an odd number of times until getting the sequence "3,4,5" is 216/431 â‰ˆ 0.5012.

  This probability is slightly greater than 1/2, meaning it's slightly more likely to roll an odd number of times than an even number of times before completing the sequence.

Final Plan Status:
    - [Done] Understand the problem structure: Identified that we need probability of an odd number of total rolls until getting '3,4,5' consecutively. Set up a Markov chain with states representing progress toward target sequence.
    - [Done] Set up the Markov model with transition probabilities: Defined 4 states (0: no sequence, 1: just rolled 3, 2: rolled 3,4, 3: success with 3,4,5) and calculated transition probabilities between all states in matrix form.
    - [Verification Needed] Calculate generating functions for the process (The generating function approach became algebraically complex; pivoted to first-step analysis with parity instead.)
      - [Done] Set up generating function equations for each state: Established equations Gâ‚€(z), Gâ‚(z), Gâ‚‚(z), and Gâ‚ƒ(z) to track rolls until success.
      - [Verification Needed] Solve the system of equations to find generating functions (Equations became unwieldy; more efficient to use first-step analysis with parity instead.)
    - [Done] Use first-step analysis with parity: Successfully applied first-step analysis with parity to solve the problem directly.
      - [Done] Define probability variables with parity information: Defined Pâ‚€, Pâ‚, Pâ‚‚ as probabilities of odd number of rolls starting from respective states.
      - [Done] Set up equations relating odd/even outcome probabilities: Derived equations: 11Pâ‚€/6 + Pâ‚/6 = 1, 4Pâ‚€/6 + 7Pâ‚/6 + Pâ‚‚/6 = 1, 4Pâ‚€/6 + Pâ‚/6 + 6Pâ‚‚/6 = 1.
      - [Done] Solve the system of equations: Found Pâ‚€ = 216/431, Pâ‚ = 210/431, Pâ‚‚ = 252/431.
    - [Done] Determine probability of odd number of rolls: The probability of rolling the die an odd number of times is 216/431 â‰ˆ 0.5012.
      - [Done] Extract final probability from analysis: Identified Pâ‚€ = 216/431 as the answer to our original question.
      - [Done] Verify the mathematical result: Verified solution by substituting back into original equations.
    - [Done] Verify solution with alternative approach: Double-checked our solution by re-solving the system and verifying that the values satisfy all original equations.
    - [Done] Conclusion: The probability of rolling the die an odd number of times until getting the sequence '3,4,5' is 216/431 â‰ˆ 0.5012, which is slightly greater than 1/2.

=== FINAL SOLUTION ===
Evaluation of Thought 6: Correct. The analysis carefully re-solved the system of equations to verify our answer and found Pâ‚€ = 216/431, Pâ‚ = 210/431, and Pâ‚‚ = 252/431. The verification process was thorough, checking the solution by substituting back into the original equations, confirming that the probability of rolling an odd number of times is 216/431 â‰ˆ 0.5012.

Now I'll provide a conclusion that summarizes the problem, our approach, and the final answer.

We set out to find the probability that we roll a fair die an odd number of times until getting the sequence "3,4,5" consecutively in that order.

Our approach used a Markov chain model with states representing progress toward the target sequence:
- State 0: Haven't rolled any part of the sequence yet
- State 1: Just rolled a 3, waiting for 4
- State 2: Rolled 3 followed by 4, waiting for 5
- State 3: Success! Rolled the full "3,4,5" sequence

Initially, we considered a generating function approach, but pivoted to a more direct first-step analysis with parity. We defined variables Pâ‚€, Pâ‚, and Pâ‚‚ representing the probabilities of an odd number of total rolls when starting from each respective state.

Through careful derivation and solution of a system of linear equations, we found:
Pâ‚€ = 216/431 â‰ˆ 0.5012
Pâ‚ = 210/431 â‰ˆ 0.4872
Pâ‚‚ = 252/431 â‰ˆ 0.5847

Since we start from State 0 (no part of the sequence rolled yet), the answer to our original question is Pâ‚€ = 216/431. 

To verify this result, we solved the system multiple times and checked that our values satisfied the original equations.

The final answer is: The probability of rolling the die an odd number of times until getting the sequence "3,4,5" is 216/431 â‰ˆ 0.5012.

This probability is slightly greater than 1/2, meaning it's slightly more likely to roll an odd number of times than an even number of times before completing the sequence.
======================
```
</file>

<file path="cookbook/pocketflow-thinking/requirements.txt">
pocketflow>=0.0.1
anthropic>=0.15.0   # For Claude API access
</file>

<file path="cookbook/pocketflow-thinking/utils.py">
from anthropic import Anthropic
import os

def call_llm(prompt):
    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
    response = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=6000,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

if __name__ == "__main__":
    print("## Testing call_llm")
    prompt = "In a few words, what is the meaning of life?"
    print(f"## Prompt: {prompt}")
    response = call_llm(prompt)
    print(f"## Response: {response}")
</file>

<file path="cookbook/pocketflow-tool-crawler/tools/crawler.py">
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Set

class WebCrawler:
    """Simple web crawler that extracts content and follows links"""
    
    def __init__(self, base_url: str, max_pages: int = 10):
        self.base_url = base_url
        self.max_pages = max_pages
        self.visited: Set[str] = set()
        
    def is_valid_url(self, url: str) -> bool:
        """Check if URL belongs to the same domain"""
        base_domain = urlparse(self.base_url).netloc
        url_domain = urlparse(url).netloc
        return base_domain == url_domain
        
    def extract_page_content(self, url: str) -> Dict:
        """Extract content from a single page"""
        try:
            response = requests.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Extract main content
            content = {
                "url": url,
                "title": soup.title.string if soup.title else "",
                "text": soup.get_text(separator="\n", strip=True),
                "links": []
            }
            
            # Extract links
            for link in soup.find_all("a"):
                href = link.get("href")
                if href:
                    absolute_url = urljoin(url, href)
                    if self.is_valid_url(absolute_url):
                        content["links"].append(absolute_url)
            
            return content
            
        except Exception as e:
            print(f"Error crawling {url}: {str(e)}")
            return None
    
    def crawl(self) -> List[Dict]:
        """Crawl website starting from base_url"""
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if url in self.visited:
                continue
                
            print(f"Crawling: {url}")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content)
                
                # Add new URLs to visit
                new_urls = [url for url in content["links"] 
                          if url not in self.visited 
                          and url not in to_visit]
                to_visit.extend(new_urls)
        
        return results
</file>

<file path="cookbook/pocketflow-tool-crawler/tools/parser.py">
from typing import Dict, List
from utils.call_llm import call_llm

def analyze_content(content: Dict) -> Dict:
    """Analyze webpage content using LLM
    
    Args:
        content (Dict): Webpage content with url, title and text
        
    Returns:
        Dict: Analysis results including summary and topics
    """
    prompt = f"""
Analyze this webpage content:

Title: {content['title']}
URL: {content['url']}
Content: {content['text'][:2000]}  # Limit content length

Please provide:
1. A brief summary (2-3 sentences)
2. Main topics/keywords (up to 5)
3. Content type (article, product page, etc)

Output in YAML format:
```yaml
summary: >
    brief summary here
topics:
    - topic 1
    - topic 2
content_type: type here
```
"""
    
    try:
        response = call_llm(prompt)
        # Extract YAML between code fences
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        
        import yaml
        analysis = yaml.safe_load(yaml_str)
        
        # Validate required fields
        assert "summary" in analysis
        assert "topics" in analysis
        assert "content_type" in analysis
        assert isinstance(analysis["topics"], list)
        
        return analysis
        
    except Exception as e:
        print(f"Error analyzing content: {str(e)}")
        return {
            "summary": "Error analyzing content",
            "topics": [],
            "content_type": "unknown"
        }

def analyze_site(crawl_results: List[Dict]) -> List[Dict]:
    """Analyze all crawled pages
    
    Args:
        crawl_results (List[Dict]): List of crawled page contents
        
    Returns:
        List[Dict]: Original content with added analysis
    """
    analyzed_results = []
    
    for content in crawl_results:
        if content and content.get("text"):
            analysis = analyze_content(content)
            content["analysis"] = analysis
            analyzed_results.append(content)
            
    return analyzed_results
</file>

<file path="cookbook/pocketflow-tool-crawler/utils/call_llm.py">
from openai import OpenAI
import os

# Initialize OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def call_llm(prompt: str) -> str:
    """Call OpenAI API to analyze text
    
    Args:
        prompt (str): Input prompt for the model
        
    Returns:
        str: Model response
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"Error calling LLM API: {str(e)}")
        return ""

if __name__ == "__main__":
    # Test LLM call
    response = call_llm("What is web crawling?")
    print("Response:", response)
</file>

<file path="cookbook/pocketflow-tool-crawler/flow.py">
from pocketflow import Flow
from nodes import CrawlWebsiteNode, AnalyzeContentBatchNode, GenerateReportNode

def create_flow() -> Flow:
    """Create and configure the crawling flow
    
    Returns:
        Flow: Configured flow ready to run
    """
    # Create nodes
    crawl = CrawlWebsiteNode()
    analyze = AnalyzeContentBatchNode()
    report = GenerateReportNode()
    
    # Connect nodes
    crawl >> analyze >> report
    
    # Create flow starting with crawl
    return Flow(start=crawl)
</file>

<file path="cookbook/pocketflow-tool-crawler/main.py">
import os
from flow import create_flow

def main():
    """Run the web crawler flow"""
    
    # Get website URL from user
    url = input("Enter website URL to crawl (e.g., https://example.com): ")
    if not url:
        print("Error: URL is required")
        return
        
    # Initialize shared data
    shared = {
        "base_url": url,
        "max_pages": 1
    }
    
    # Create and run flow
    flow = create_flow()
    flow.run(shared)
    
    # Results are in shared["report"]
    
if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tool-crawler/nodes.py">
from pocketflow import Node, BatchNode
from tools.crawler import WebCrawler
from tools.parser import analyze_site
from typing import List, Dict

class CrawlWebsiteNode(Node):
    """Node to crawl a website and extract content"""
    
    def prep(self, shared):
        return shared.get("base_url"), shared.get("max_pages", 10)
        
    def exec(self, inputs):
        base_url, max_pages = inputs
        if not base_url:
            return []
            
        crawler = WebCrawler(base_url, max_pages)
        return crawler.crawl()
        
    def post(self, shared, prep_res, exec_res):
        shared["crawl_results"] = exec_res
        return "default"

class AnalyzeContentBatchNode(BatchNode):
    """Node to analyze crawled content in batches"""
    
    def prep(self, shared):
        results = shared.get("crawl_results", [])
        # Process in batches of 5 pages
        batch_size = 5
        return [results[i:i+batch_size] for i in range(0, len(results), batch_size)]
        
    def exec(self, batch):
        return analyze_site(batch)
        
    def post(self, shared, prep_res, exec_res_list):
        # Flatten results from all batches
        all_results = []
        for batch_results in exec_res_list:
            all_results.extend(batch_results)
            
        shared["analyzed_results"] = all_results
        return "default"

class GenerateReportNode(Node):
    """Node to generate a summary report of the analysis"""
    
    def prep(self, shared):
        return shared.get("analyzed_results", [])
        
    def exec(self, results):
        if not results:
            return "No results to report"
            
        report = []
        report.append(f"Analysis Report\n")
        report.append(f"Total pages analyzed: {len(results)}\n")
        
        for page in results:
            report.append(f"\nPage: {page['url']}")
            report.append(f"Title: {page['title']}")
            
            analysis = page.get("analysis", {})
            report.append(f"Summary: {analysis.get('summary', 'N/A')}")
            report.append(f"Topics: {', '.join(analysis.get('topics', []))}")
            report.append(f"Content Type: {analysis.get('content_type', 'unknown')}")
            report.append("-" * 80)
            
        return "\n".join(report)
        
    def post(self, shared, prep_res, exec_res):
        shared["report"] = exec_res
        print("\nReport generated:")
        print(exec_res)
        return "default"
</file>

<file path="cookbook/pocketflow-tool-crawler/README.md">
# Web Crawler with Content Analysis

A web crawler tool built with PocketFlow that crawls websites and analyzes content using LLM.

## Features

- Crawls websites while respecting domain boundaries
- Extracts text content and links from pages
- Analyzes content using GPT-4 to generate:
  - Page summaries
  - Main topics/keywords
  - Content type classification
- Processes pages in batches for efficiency
- Generates a comprehensive analysis report

## Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set your OpenAI API key:
   ```bash
   export OPENAI_API_KEY='your-api-key'
   ```

## Usage

Run the crawler:
```bash
python main.py
```

You will be prompted to:
1. Enter the website URL to crawl
2. Specify maximum number of pages to crawl (default: 10)

The tool will then:
1. Crawl the specified website
2. Extract and analyze content using GPT-4
3. Generate a report with findings

## Project Structure

```
pocketflow-tool-crawler/
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ crawler.py     # Web crawling functionality
â”‚   â””â”€â”€ parser.py      # Content analysis using LLM
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ call_llm.py    # LLM API wrapper
â”œâ”€â”€ nodes.py           # PocketFlow nodes
â”œâ”€â”€ flow.py           # Flow configuration
â”œâ”€â”€ main.py           # Main script
â””â”€â”€ requirements.txt   # Dependencies
```

## Limitations

- Only crawls within the same domain
- Text content only (no images/media)
- Rate limited by OpenAI API
- Basic error handling

## Dependencies

- pocketflow: Flow-based processing
- requests: HTTP requests
- beautifulsoup4: HTML parsing
- openai: GPT-4 API access
</file>

<file path="cookbook/pocketflow-tool-crawler/requirements.txt">
pocketflow>=0.1.0
requests>=2.31.0
beautifulsoup4>=4.12.0
openai>=1.0.0  # for content analysis
</file>

<file path="cookbook/pocketflow-tool-database/tools/database.py">
import sqlite3
from typing import List, Tuple, Any

def execute_sql(query: str, params: Tuple = None) -> List[Tuple[Any, ...]]:
    """Execute a SQL query and return results
    
    Args:
        query (str): SQL query to execute
        params (tuple, optional): Query parameters to prevent SQL injection
        
    Returns:
        list: Query results as a list of tuples
    """
    conn = sqlite3.connect("example.db")
    try:
        cursor = conn.cursor()
        if params:
            cursor.execute(query, params)
        else:
            cursor.execute(query)
        result = cursor.fetchall()
        conn.commit()
        return result
    finally:
        conn.close()

def init_db():
    """Initialize database with example table"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS tasks (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT NOT NULL,
        description TEXT,
        status TEXT DEFAULT 'pending',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """
    execute_sql(create_table_sql)
</file>

<file path="cookbook/pocketflow-tool-database/flow.py">
from pocketflow import Flow
from nodes import InitDatabaseNode, CreateTaskNode, ListTasksNode

def create_database_flow():
    """Create a flow for database operations"""
    
    # Create nodes
    init_db = InitDatabaseNode()
    create_task = CreateTaskNode()
    list_tasks = ListTasksNode()
    
    # Connect nodes
    init_db >> create_task >> list_tasks
    
    # Create and return flow
    return Flow(start=init_db)
</file>

<file path="cookbook/pocketflow-tool-database/main.py">
from flow import create_database_flow

def main():
    # Create the flow
    flow = create_database_flow()
    
    # Prepare example task data
    shared = {
        "task_title": "Example Task",
        "task_description": "This is an example task created using PocketFlow"
    }
    
    # Run the flow
    flow.run(shared)
    
    # Print results
    print("Database Status:", shared.get("db_status"))
    print("Task Status:", shared.get("task_status"))
    print("\nAll Tasks:")
    for task in shared.get("tasks", []):
        print(f"- ID: {task[0]}")
        print(f"  Title: {task[1]}")
        print(f"  Description: {task[2]}")
        print(f"  Status: {task[3]}")
        print(f"  Created: {task[4]}")
        print()

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tool-database/nodes.py">
from pocketflow import Node
from tools.database import execute_sql, init_db

class InitDatabaseNode(Node):
    """Node for initializing the database"""
    
    def exec(self, _):
        init_db()
        return "Database initialized"
        
    def post(self, shared, prep_res, exec_res):
        shared["db_status"] = exec_res
        return "default"

class CreateTaskNode(Node):
    """Node for creating a new task"""
    
    def prep(self, shared):
        return (
            shared.get("task_title", ""),
            shared.get("task_description", "")
        )
        
    def exec(self, inputs):
        title, description = inputs
        query = "INSERT INTO tasks (title, description) VALUES (?, ?)"
        execute_sql(query, (title, description))
        return "Task created successfully"
        
    def post(self, shared, prep_res, exec_res):
        shared["task_status"] = exec_res
        return "default"

class ListTasksNode(Node):
    """Node for listing all tasks"""
    
    def exec(self, _):
        query = "SELECT * FROM tasks"
        return execute_sql(query)
        
    def post(self, shared, prep_res, exec_res):
        shared["tasks"] = exec_res
        return "default"
</file>

<file path="cookbook/pocketflow-tool-database/README.md">
# SQLite Database with PocketFlow

This example demonstrates how to properly integrate SQLite database operations with PocketFlow, focusing on:

1. Clean code organization with separation of concerns:
   - Tools layer for database operations (`tools/database.py`)
   - Node implementation for PocketFlow integration (`nodes.py`)
   - Flow configuration (`flow.py`)
   - Safe SQL query execution with parameter binding

2. Best practices for database operations:
   - Connection management with proper closing
   - SQL injection prevention using parameterized queries
   - Error handling and resource cleanup
   - Simple schema management

3. Example task management system:
   - Database initialization
   - Task creation
   - Task listing
   - Status tracking

## Project Structure

```
pocketflow-tool-database/
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ database.py    # SQLite database operations
â”œâ”€â”€ nodes.py          # PocketFlow node implementation
â”œâ”€â”€ flow.py          # Flow configuration
â””â”€â”€ main.py          # Example usage
```

## Setup

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

Run the example:
```bash
python main.py
```

This will:
1. Initialize a SQLite database with a tasks table
2. Create an example task
3. List all tasks in the database
4. Display the results

## Key Concepts Demonstrated

1. **Database Operations**
   - Safe connection handling
   - Query parameterization
   - Schema management

2. **Code Organization**
   - Clear separation between database operations and PocketFlow components
   - Modular project structure
   - Type hints and documentation

3. **PocketFlow Integration**
   - Node implementation with prep->exec->post lifecycle
   - Flow configuration
   - Shared store usage for data passing

## Example Output

```
Database Status: Database initialized
Task Status: Task created successfully

All Tasks:
- ID: 1
  Title: Example Task
  Description: This is an example task created using PocketFlow
  Status: pending
  Created: 2024-03-02 12:34:56
```
</file>

<file path="cookbook/pocketflow-tool-database/requirements.txt">
pocketflow>=0.1.0
python-dotenv>=0.19.0
</file>

<file path="cookbook/pocketflow-tool-embeddings/tools/embeddings.py">
from utils.call_llm import client

def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    return response.data[0].embedding
</file>

<file path="cookbook/pocketflow-tool-embeddings/utils/call_llm.py">
import os
from openai import OpenAI

# No need for dotenv if using system environment variables
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def call_llm(prompt):    
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content
    
if __name__ == "__main__":
    prompt = "What is the meaning of life?"
    print(call_llm(prompt))
</file>

<file path="cookbook/pocketflow-tool-embeddings/flow.py">
from pocketflow import Flow
from nodes import EmbeddingNode

def create_embedding_flow():
    """Create a flow for text embedding"""
    # Create embedding node
    embedding = EmbeddingNode()
    
    # Create and return flow
    return Flow(start=embedding)
</file>

<file path="cookbook/pocketflow-tool-embeddings/main.py">
from flow import create_embedding_flow

def main():
    # Create the flow
    flow = create_embedding_flow()
    
    # Example text
    text = "What's the meaning of life?"
    
    # Prepare shared data
    shared = {"text": text}
    
    # Run the flow
    flow.run(shared)
    
    # Print results
    print("Text:", text)
    print("Embedding dimension:", len(shared["embedding"]))
    print("First 5 values:", shared["embedding"][:5])

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tool-embeddings/nodes.py">
from pocketflow import Node
from tools.embeddings import get_embedding

class EmbeddingNode(Node):
    """Node for getting embeddings from OpenAI API"""
    
    def prep(self, shared):
        # Get text from shared store
        return shared.get("text", "")
        
    def exec(self, text):
        # Get embedding using tool function
        return get_embedding(text)
        
    def post(self, shared, prep_res, exec_res):
        # Store embedding in shared store
        shared["embedding"] = exec_res
        return "default"
</file>

<file path="cookbook/pocketflow-tool-embeddings/README.md">
# OpenAI Embeddings with PocketFlow

This example demonstrates how to properly integrate OpenAI's text embeddings API with PocketFlow, focusing on:

1. Clean code organization with separation of concerns:
   - Tools layer for API interactions (`tools/embeddings.py`)
   - Node implementation for PocketFlow integration (`nodes.py`)
   - Flow configuration (`flow.py`)
   - Centralized environment configuration (`utils/call_llm.py`)

2. Best practices for API key management:
   - Using environment variables
   - Supporting both `.env` files and system environment variables
   - Secure configuration handling

3. Proper project structure:
   - Modular code organization
   - Clear separation between tools and PocketFlow components
   - Reusable OpenAI client configuration

## Project Structure

```
pocketflow-tool-embeddings/
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ embeddings.py     # OpenAI embeddings API wrapper
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ call_llm.py      # Centralized OpenAI client configuration
â”œâ”€â”€ nodes.py             # PocketFlow node implementation
â”œâ”€â”€ flow.py             # Flow configuration
â””â”€â”€ main.py             # Example usage
```

## Setup

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up your OpenAI API key in one of two ways:
   
   a. Using a `.env` file:
   ```bash
   OPENAI_API_KEY=your_api_key_here
   ```
   
   b. Or as a system environment variable:
   ```bash
   export OPENAI_API_KEY=your_api_key_here
   ```

## Usage

Run the example:
```bash
python main.py
```

This will:
1. Load the OpenAI API key from environment
2. Create a PocketFlow node to handle embedding generation
3. Process a sample text and generate its embedding
4. Display the embedding dimension and first few values

## Key Concepts Demonstrated

1. **Environment Configuration**
   - Secure API key handling
   - Flexible configuration options

2. **Code Organization**
   - Clear separation between tools and PocketFlow components
   - Reusable OpenAI client configuration
   - Modular project structure

3. **PocketFlow Integration**
   - Node implementation with prep->exec->post lifecycle
   - Flow configuration
   - Shared store usage for data passing
</file>

<file path="cookbook/pocketflow-tool-embeddings/requirements.txt">
openai>=1.0.0
numpy>=1.24.0
faiss-cpu>=1.7.0
python-dotenv>=1.0.0
pocketflow>=0.1.0
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/tools/pdf.py">
import fitz  # PyMuPDF
from PIL import Image
import io
import base64
from typing import List, Tuple

def pdf_to_images(pdf_path: str, max_size: int = 2000) -> List[Tuple[Image.Image, int]]:
    """Convert PDF pages to PIL Images with size limit
    
    Args:
        pdf_path (str): Path to PDF file
        max_size (int): Maximum dimension (width/height) for images
        
    Returns:
        list: List of tuples (PIL Image, page number)
    """
    doc = fitz.open(pdf_path)
    images = []
    
    try:
        for page_num in range(len(doc)):
            page = doc[page_num]
            pix = page.get_pixmap()
            
            # Convert to PIL Image
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # Resize if needed while maintaining aspect ratio
            if max(img.size) > max_size:
                ratio = max_size / max(img.size)
                new_size = tuple(int(dim * ratio) for dim in img.size)
                img = img.resize(new_size, Image.Resampling.LANCZOS)
            
            images.append((img, page_num + 1))
            
    finally:
        doc.close()
        
    return images

def image_to_base64(image: Image.Image) -> str:
    """Convert PIL Image to base64 string
    
    Args:
        image (PIL.Image): Image to convert
        
    Returns:
        str: Base64 encoded image string
    """
    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode('utf-8')
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/tools/vision.py">
from PIL import Image
from utils.call_llm import client
from tools.pdf import image_to_base64

def extract_text_from_image(image: Image.Image, prompt: str = None) -> str:
    """Extract text from image using OpenAI Vision API
    
    Args:
        image (PIL.Image): Image to process
        prompt (str, optional): Custom prompt for extraction. Defaults to general OCR.
        
    Returns:
        str: Extracted text from image
    """
    # Convert image to base64
    img_base64 = image_to_base64(image)
    
    # Default prompt for general OCR
    if prompt is None:
        prompt = "Please extract all text from this image."
    
    # Call Vision API
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}
            ]
        }]
    )
    
    return response.choices[0].message.content

if __name__ == "__main__":
    # Test vision processing
    test_image = Image.open("example.png")
    result = extract_text_from_image(test_image)
    print("Extracted text:", result)
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/utils/call_llm.py">
import os
from openai import OpenAI
from pathlib import Path

# Get the project root directory (parent of utils directory)
ROOT_DIR = Path(__file__).parent.parent

# Initialize OpenAI client with API key from environment
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/flow.py">
from pocketflow import Flow
from nodes import ProcessPDFBatchNode

def create_vision_flow():
    """Create a flow for batch PDF processing with Vision API"""
    return Flow(start=ProcessPDFBatchNode())
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/main.py">
from flow import create_vision_flow

def main():
    # Create and run flow
    flow = create_vision_flow()
    shared = {}
    flow.run(shared)
    
    # Print results
    if "results" in shared:
        for result in shared["results"]:
            print(f"\nFile: {result['filename']}")
            print("-" * 50)
            print(result["text"])

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/nodes.py">
from pocketflow import Node, BatchNode
from tools.pdf import pdf_to_images
from tools.vision import extract_text_from_image
from typing import List, Dict, Any
from pathlib import Path
import os

class ProcessPDFBatchNode(BatchNode):
    """Node for processing multiple PDFs from a directory"""
    
    def prep(self, shared):
        # Get PDF directory path
        root_dir = Path(__file__).parent
        pdf_dir = root_dir / "pdfs"
        
        # List all PDFs
        pdf_files = []
        for file in os.listdir(pdf_dir):
            if file.lower().endswith('.pdf'):
                pdf_files.append({
                    "pdf_path": str(pdf_dir / file),
                    "extraction_prompt": shared.get("extraction_prompt", 
                        "Extract all text from this document, preserving formatting and layout.")
                })
        
        if not pdf_files:
            print("No PDF files found in 'pdfs' directory!")
            return []
            
        print(f"Found {len(pdf_files)} PDF files")
        return pdf_files
    
    def exec(self, item):
        # Create flow for single PDF
        flow = create_single_pdf_flow()
        
        # Process PDF
        print(f"\nProcessing: {os.path.basename(item['pdf_path'])}")
        print("-" * 50)
        
        # Run flow
        shared = item.copy()
        flow.run(shared)
        
        return {
            "filename": os.path.basename(item["pdf_path"]),
            "text": shared.get("final_text", "No text extracted")
        }
    
    def post(self, shared, prep_res, exec_res_list):
        shared["results"] = exec_res_list
        return "default"

class LoadPDFNode(Node):
    """Node for loading and converting a single PDF to images"""
    
    def prep(self, shared):
        return shared.get("pdf_path", "")
        
    def exec(self, pdf_path):
        return pdf_to_images(pdf_path)
        
    def post(self, shared, prep_res, exec_res):
        shared["page_images"] = exec_res
        return "default"

class ExtractTextNode(Node):
    """Node for extracting text from images using Vision API"""
    
    def prep(self, shared):
        return (
            shared.get("page_images", []),
            shared.get("extraction_prompt", None)
        )
        
    def exec(self, inputs):
        images, prompt = inputs
        results = []
        
        for img, page_num in images:
            text = extract_text_from_image(img, prompt)
            results.append({
                "page": page_num,
                "text": text
            })
            
        return results
        
    def post(self, shared, prep_res, exec_res):
        shared["extracted_text"] = exec_res
        return "default"

class CombineResultsNode(Node):
    """Node for combining and formatting extracted text"""
    
    def prep(self, shared):
        return shared.get("extracted_text", [])
        
    def exec(self, results):
        # Sort by page number
        sorted_results = sorted(results, key=lambda x: x["page"])
        
        # Combine text with page numbers
        combined = []
        for result in sorted_results:
            combined.append(f"=== Page {result['page']} ===\n{result['text']}\n")
            
        return "\n".join(combined)
        
    def post(self, shared, prep_res, exec_res):
        shared["final_text"] = exec_res
        return "default"

def create_single_pdf_flow():
    """Create a flow for processing a single PDF"""
    from pocketflow import Flow
    
    # Create nodes
    load_pdf = LoadPDFNode()
    extract_text = ExtractTextNode()
    combine_results = CombineResultsNode()
    
    # Connect nodes
    load_pdf >> extract_text >> combine_results
    
    # Create and return flow
    return Flow(start=load_pdf)
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/README.md">
# PocketFlow Tool: PDF Vision

A PocketFlow example project demonstrating PDF processing with OpenAI's Vision API for OCR and text extraction.

## Features

- Convert PDF pages to images while maintaining quality and size limits
- Extract text from scanned documents using GPT-4 Vision API
- Support for custom extraction prompts
- Maintain page order and formatting in extracted text
- Batch processing of multiple PDFs from a directory

## Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set your OpenAI API key as an environment variable:
   ```bash
   export OPENAI_API_KEY=your_api_key_here
   ```

## Usage

1. Place your PDF files in the `pdfs` directory
2. Run the example:
   ```bash
   python main.py
   ```
   The script will process all PDF files in the `pdfs` directory and output the extracted text for each one.

## Project Structure

```
pocketflow-tool-pdf-vision/
â”œâ”€â”€ pdfs/           # Directory for PDF files to process
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ pdf.py     # PDF to image conversion
â”‚   â””â”€â”€ vision.py  # Vision API integration
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ call_llm.py # OpenAI client config
â”œâ”€â”€ nodes.py       # PocketFlow nodes
â”œâ”€â”€ flow.py        # Flow configuration
â””â”€â”€ main.py        # Example usage
```

## Flow Description

1. **LoadPDFNode**: Loads PDF and converts pages to images
2. **ExtractTextNode**: Processes images with Vision API
3. **CombineResultsNode**: Combines extracted text from all pages

## Customization

You can customize the extraction by modifying the prompt in `shared`:

```python
shared = {
    "pdf_path": "your_file.pdf",
    "extraction_prompt": "Your custom prompt here"
}
```

## Limitations

- Maximum PDF page size: 2000px (configurable in `tools/pdf.py`)
- Vision API token limit: 1000 tokens per response
- Image size limit: 20MB per image for Vision API

## License

MIT
</file>

<file path="cookbook/pocketflow-tool-pdf-vision/requirements.txt">
pocketflow>=0.1.0
openai>=1.0.0
PyMuPDF>=1.22.0  # for PDF processing
Pillow>=10.0.0   # for image processing
</file>

<file path="cookbook/pocketflow-tool-search/tools/parser.py">
from typing import Dict, List
from utils.call_llm import call_llm

def analyze_results(query: str, results: List[Dict]) -> Dict:
    """Analyze search results using LLM
    
    Args:
        query (str): Original search query
        results (List[Dict]): Search results to analyze
        
    Returns:
        Dict: Analysis including summary and key points
    """
    # Format results for prompt
    formatted_results = []
    for i, result in enumerate(results, 1):
        formatted_results.append(f"""
Result {i}:
Title: {result['title']}
Snippet: {result['snippet']}
URL: {result['link']}
""")
    
    prompt = f"""
Analyze these search results for the query: "{query}"

{'\n'.join(formatted_results)}

Please provide:
1. A concise summary of the findings (2-3 sentences)
2. Key points or facts (up to 5 bullet points)
3. Suggested follow-up queries (2-3)

Output in YAML format:
```yaml
summary: >
    brief summary here
key_points:
    - point 1
    - point 2
follow_up_queries:
    - query 1
    - query 2
```
"""
    
    try:
        response = call_llm(prompt)
        # Extract YAML between code fences
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        
        import yaml
        analysis = yaml.safe_load(yaml_str)
        
        # Validate required fields
        assert "summary" in analysis
        assert "key_points" in analysis
        assert "follow_up_queries" in analysis
        assert isinstance(analysis["key_points"], list)
        assert isinstance(analysis["follow_up_queries"], list)
        
        return analysis
        
    except Exception as e:
        print(f"Error analyzing results: {str(e)}")
        return {
            "summary": "Error analyzing results",
            "key_points": [],
            "follow_up_queries": []
        }
</file>

<file path="cookbook/pocketflow-tool-search/tools/search.py">
import os
from serpapi import GoogleSearch
from typing import Dict, List, Optional

class SearchTool:
    """Tool for performing web searches using SerpAPI"""
    
    def __init__(self, api_key: Optional[str] = None):
        """Initialize search tool with API key
        
        Args:
            api_key (str, optional): SerpAPI key. Defaults to env var SERPAPI_API_KEY.
        """
        self.api_key = api_key or os.getenv("SERPAPI_API_KEY")
        if not self.api_key:
            raise ValueError("SerpAPI key not found. Set SERPAPI_API_KEY env var.")
            
    def search(self, query: str, num_results: int = 5) -> List[Dict]:
        """Perform Google search via SerpAPI
        
        Args:
            query (str): Search query
            num_results (int, optional): Number of results to return. Defaults to 5.
            
        Returns:
            List[Dict]: Search results with title, snippet, and link
        """
        # Configure search parameters
        params = {
            "engine": "google",
            "q": query,
            "api_key": self.api_key,
            "num": num_results
        }
        
        try:
            # Execute search
            search = GoogleSearch(params)
            results = search.get_dict()
            
            # Extract organic results
            if "organic_results" not in results:
                return []
                
            processed_results = []
            for result in results["organic_results"][:num_results]:
                processed_results.append({
                    "title": result.get("title", ""),
                    "snippet": result.get("snippet", ""),
                    "link": result.get("link", "")
                })
                
            return processed_results
            
        except Exception as e:
            print(f"Search error: {str(e)}")
            return []
</file>

<file path="cookbook/pocketflow-tool-search/utils/call_llm.py">
import os
from openai import OpenAI
from pathlib import Path

# Get the project root directory (parent of utils directory)
ROOT_DIR = Path(__file__).parent.parent

# Initialize OpenAI client with API key from environment
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def call_llm(prompt: str) -> str:
    """Call OpenAI API to analyze text
    
    Args:
        prompt (str): Input prompt for the model
        
    Returns:
        str: Model response
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"Error calling LLM API: {str(e)}")
        return ""

if __name__ == "__main__":
    # Test LLM call
    response = call_llm("What is web search?")
    print("Response:", response)
</file>

<file path="cookbook/pocketflow-tool-search/flow.py">
from pocketflow import Flow
from nodes import SearchNode, AnalyzeResultsNode

def create_flow() -> Flow:
    """Create and configure the search flow
    
    Returns:
        Flow: Configured flow ready to run
    """
    # Create nodes
    search = SearchNode()
    analyze = AnalyzeResultsNode()
    
    # Connect nodes
    search >> analyze
    
    # Create flow starting with search
    return Flow(start=search)
</file>

<file path="cookbook/pocketflow-tool-search/main.py">
import os
from flow import create_flow

def main():
    """Run the web search flow"""
    
    # Get search query from user
    query = input("Enter search query: ")
    if not query:
        print("Error: Query is required")
        return
        
    # Initialize shared data
    shared = {
        "query": query,
        "num_results": 5
    }
    
    # Create and run flow
    flow = create_flow()
    flow.run(shared)
    
    # Results are in shared["analysis"]
    
if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tool-search/nodes.py">
from pocketflow import Node
from tools.search import SearchTool
from tools.parser import analyze_results
from typing import List, Dict

class SearchNode(Node):
    """Node to perform web search using SerpAPI"""
    
    def prep(self, shared):
        return shared.get("query"), shared.get("num_results", 5)
        
    def exec(self, inputs):
        query, num_results = inputs
        if not query:
            return []
            
        searcher = SearchTool()
        return searcher.search(query, num_results)
        
    def post(self, shared, prep_res, exec_res):
        shared["search_results"] = exec_res
        return "default"

class AnalyzeResultsNode(Node):
    """Node to analyze search results using LLM"""
    
    def prep(self, shared):
        return shared.get("query"), shared.get("search_results", [])
        
    def exec(self, inputs):
        query, results = inputs
        if not results:
            return {
                "summary": "No search results to analyze",
                "key_points": [],
                "follow_up_queries": []
            }
            
        return analyze_results(query, results)
        
    def post(self, shared, prep_res, exec_res):
        shared["analysis"] = exec_res
        
        # Print analysis
        print("\nSearch Analysis:")
        print("\nSummary:", exec_res["summary"])
        
        print("\nKey Points:")
        for point in exec_res["key_points"]:
            print(f"- {point}")
            
        print("\nSuggested Follow-up Queries:")
        for query in exec_res["follow_up_queries"]:
            print(f"- {query}")
            
        return "default"
</file>

<file path="cookbook/pocketflow-tool-search/README.md">
# Web Search with Analysis

A web search tool built with PocketFlow that performs searches using SerpAPI and analyzes results using LLM.

## Features

- Web search using Google via SerpAPI
- Extracts titles, snippets, and links
- Analyzes search results using GPT-4 to provide:
  - Result summaries
  - Key points/facts
  - Suggested follow-up queries
- Clean command-line interface

## Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Set required API keys:
   ```bash
   export SERPAPI_API_KEY='your-serpapi-key'
   export OPENAI_API_KEY='your-openai-key'
   ```

## Usage

Run the search tool:
```bash
python main.py
```

You will be prompted to:
1. Enter your search query
2. Specify number of results to fetch (default: 5)

The tool will then:
1. Perform the search using SerpAPI
2. Analyze results using GPT-4
3. Present a summary with key points and follow-up queries

## Project Structure

```
pocketflow-tool-search/
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ search.py      # SerpAPI search functionality
â”‚   â””â”€â”€ parser.py      # Result analysis using LLM
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ call_llm.py    # LLM API wrapper
â”œâ”€â”€ nodes.py           # PocketFlow nodes
â”œâ”€â”€ flow.py           # Flow configuration
â”œâ”€â”€ main.py           # Main script
â””â”€â”€ requirements.txt   # Dependencies
```

## Limitations

- Requires SerpAPI subscription
- Rate limited by both APIs
- Basic error handling
- Text results only

## Dependencies

- pocketflow: Flow-based processing
- google-search-results: SerpAPI client
- openai: GPT-4 API access
- pyyaml: YAML processing
</file>

<file path="cookbook/pocketflow-tool-search/requirements.txt">
pocketflow>=0.1.0
google-search-results>=2.4.2  # SerpAPI client
openai>=1.0.0  # for search result analysis
pyyaml>=6.0.1  # for structured output
</file>

<file path="cookbook/pocketflow-tracing/examples/async_example.py">
#!/usr/bin/env python3
"""
Async example demonstrating PocketFlow tracing with Langfuse.

This example shows how to use the @trace_flow decorator with AsyncFlow
and AsyncNode to trace asynchronous workflows.
"""

import asyncio
import sys
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add parent directory to path to import pocketflow and tracing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "..", ".."))
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from pocketflow import AsyncNode, AsyncFlow
from tracing import trace_flow, TracingConfig


class AsyncDataFetchNode(AsyncNode):
    """An async node that simulates fetching data."""

    async def prep_async(self, shared):
        """Extract the query from shared data."""
        query = shared.get("query", "default")
        return query

    async def exec_async(self, query):
        """Simulate async data fetching."""
        print(f"ðŸ” Fetching data for query: {query}")

        # Simulate async operation
        await asyncio.sleep(1)

        # Return mock data
        data = {
            "query": query,
            "results": [f"Result {i} for {query}" for i in range(3)],
            "timestamp": "2024-01-01T00:00:00Z",
        }
        return data

    async def post_async(self, shared, prep_res, exec_res):
        """Store the fetched data."""
        shared["fetched_data"] = exec_res
        return "process"


class AsyncDataProcessNode(AsyncNode):
    """An async node that processes the fetched data."""

    async def prep_async(self, shared):
        """Get the fetched data."""
        return shared.get("fetched_data", {})

    async def exec_async(self, data):
        """Process the data asynchronously."""
        print("âš™ï¸ Processing fetched data...")

        # Simulate async processing
        await asyncio.sleep(0.5)

        # Process the results
        processed_results = []
        for result in data.get("results", []):
            processed_results.append(f"PROCESSED: {result}")

        return {
            "original_query": data.get("query"),
            "processed_results": processed_results,
            "result_count": len(processed_results),
        }

    async def post_async(self, shared, prep_res, exec_res):
        """Store the processed data."""
        shared["processed_data"] = exec_res
        return "default"


@trace_flow(flow_name="AsyncDataProcessingFlow")
class AsyncDataProcessingFlow(AsyncFlow):
    """An async flow that fetches and processes data."""

    def __init__(self):
        # Create async nodes
        fetch_node = AsyncDataFetchNode()
        process_node = AsyncDataProcessNode()

        # Connect nodes
        fetch_node - "process" >> process_node

        # Initialize async flow
        super().__init__(start=fetch_node)


async def main():
    """Run the async tracing example."""
    print("ðŸš€ Starting PocketFlow Async Tracing Example")
    print("=" * 50)

    # Create the async flow
    flow = AsyncDataProcessingFlow()

    # Prepare shared data
    shared = {"query": "machine learning tutorials"}

    print(f"ðŸ“¥ Input: {shared}")

    # Run the async flow (this will be automatically traced)
    try:
        result = await flow.run_async(shared)
        print(f"ðŸ“¤ Output: {shared}")
        print(f"ðŸŽ¯ Result: {result}")
        print("âœ… Async flow completed successfully!")

        # Print the processed data
        if "processed_data" in shared:
            processed = shared["processed_data"]
            print(
                f"ðŸŽ‰ Processed {processed['result_count']} results for query: {processed['original_query']}"
            )
            for result in processed["processed_results"]:
                print(f"   - {result}")

    except Exception as e:
        print(f"âŒ Async flow failed with error: {e}")
        raise

    print("\nðŸ“Š Check your Langfuse dashboard to see the async trace!")
    langfuse_host = os.getenv("LANGFUSE_HOST", "your-langfuse-host")
    print(f"   Dashboard URL: {langfuse_host}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-tracing/examples/basic_example.py">
#!/usr/bin/env python3
"""
Basic example demonstrating PocketFlow tracing with Langfuse.

This example shows how to use the @trace_flow decorator to automatically
trace a simple PocketFlow workflow.
"""

import sys
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add parent directory to path to import pocketflow and tracing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "..", ".."))
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from pocketflow import Node, Flow
from tracing import trace_flow, TracingConfig


class GreetingNode(Node):
    """A simple node that creates a greeting message."""

    def prep(self, shared):
        """Extract the name from shared data."""
        name = shared.get("name", "World")
        return name

    def exec(self, name):
        """Create a greeting message."""
        greeting = f"Hello, {name}!"
        return greeting

    def post(self, shared, prep_res, exec_res):
        """Store the greeting in shared data."""
        shared["greeting"] = exec_res
        return "default"


class UppercaseNode(Node):
    """A node that converts the greeting to uppercase."""

    def prep(self, shared):
        """Get the greeting from shared data."""
        return shared.get("greeting", "")

    def exec(self, greeting):
        """Convert to uppercase."""
        return greeting.upper()

    def post(self, shared, prep_res, exec_res):
        """Store the uppercase greeting."""
        shared["uppercase_greeting"] = exec_res
        return "default"


@trace_flow(flow_name="BasicGreetingFlow")
class BasicGreetingFlow(Flow):
    """A simple flow that creates and processes a greeting."""

    def __init__(self):
        # Create nodes
        greeting_node = GreetingNode()
        uppercase_node = UppercaseNode()

        # Connect nodes
        greeting_node >> uppercase_node

        # Initialize flow
        super().__init__(start=greeting_node)


def main():
    """Run the basic tracing example."""
    print("ðŸš€ Starting PocketFlow Tracing Basic Example")
    print("=" * 50)

    # Create the flow
    flow = BasicGreetingFlow()

    # Prepare shared data
    shared = {"name": "PocketFlow User"}

    print(f"ðŸ“¥ Input: {shared}")

    # Run the flow (this will be automatically traced)
    try:
        result = flow.run(shared)
        print(f"ðŸ“¤ Output: {shared}")
        print(f"ðŸŽ¯ Result: {result}")
        print("âœ… Flow completed successfully!")

        # Print the final greeting
        if "uppercase_greeting" in shared:
            print(f"ðŸŽ‰ Final greeting: {shared['uppercase_greeting']}")

    except Exception as e:
        print(f"âŒ Flow failed with error: {e}")
        raise

    print("\nðŸ“Š Check your Langfuse dashboard to see the trace!")
    langfuse_host = os.getenv("LANGFUSE_HOST", "your-langfuse-host")
    print(f"   Dashboard URL: {langfuse_host}")


if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tracing/tracing/__init__.py">
"""
PocketFlow Tracing Module

This module provides observability and tracing capabilities for PocketFlow workflows
using Langfuse as the backend. It includes decorators and utilities to automatically
trace node execution, inputs, and outputs.
"""

from .config import TracingConfig
from .core import LangfuseTracer
from .decorator import trace_flow

__all__ = ["trace_flow", "TracingConfig", "LangfuseTracer"]
</file>

<file path="cookbook/pocketflow-tracing/tracing/config.py">
"""
Configuration module for PocketFlow tracing with Langfuse.
"""

import os
from dataclasses import dataclass
from typing import Optional
from dotenv import load_dotenv


@dataclass
class TracingConfig:
    """Configuration class for PocketFlow tracing with Langfuse."""
    
    # Langfuse configuration
    langfuse_secret_key: Optional[str] = None
    langfuse_public_key: Optional[str] = None
    langfuse_host: Optional[str] = None
    
    # PocketFlow tracing configuration
    debug: bool = False
    trace_inputs: bool = True
    trace_outputs: bool = True
    trace_prep: bool = True
    trace_exec: bool = True
    trace_post: bool = True
    trace_errors: bool = True
    
    # Session configuration
    session_id: Optional[str] = None
    user_id: Optional[str] = None
    
    @classmethod
    def from_env(cls, env_file: Optional[str] = None) -> "TracingConfig":
        """
        Create TracingConfig from environment variables.
        
        Args:
            env_file: Optional path to .env file. If None, looks for .env in current directory.
            
        Returns:
            TracingConfig instance with values from environment variables.
        """
        # Load environment variables from .env file if it exists
        if env_file:
            load_dotenv(env_file)
        else:
            # Try to find .env file in current directory or parent directories
            load_dotenv()
        
        return cls(
            langfuse_secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
            langfuse_public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
            langfuse_host=os.getenv("LANGFUSE_HOST"),
            debug=os.getenv("POCKETFLOW_TRACING_DEBUG", "false").lower() == "true",
            trace_inputs=os.getenv("POCKETFLOW_TRACE_INPUTS", "true").lower() == "true",
            trace_outputs=os.getenv("POCKETFLOW_TRACE_OUTPUTS", "true").lower() == "true",
            trace_prep=os.getenv("POCKETFLOW_TRACE_PREP", "true").lower() == "true",
            trace_exec=os.getenv("POCKETFLOW_TRACE_EXEC", "true").lower() == "true",
            trace_post=os.getenv("POCKETFLOW_TRACE_POST", "true").lower() == "true",
            trace_errors=os.getenv("POCKETFLOW_TRACE_ERRORS", "true").lower() == "true",
            session_id=os.getenv("POCKETFLOW_SESSION_ID"),
            user_id=os.getenv("POCKETFLOW_USER_ID"),
        )
    
    def validate(self) -> bool:
        """
        Validate that required configuration is present.
        
        Returns:
            True if configuration is valid, False otherwise.
        """
        if not self.langfuse_secret_key:
            if self.debug:
                print("Warning: LANGFUSE_SECRET_KEY not set")
            return False
            
        if not self.langfuse_public_key:
            if self.debug:
                print("Warning: LANGFUSE_PUBLIC_KEY not set")
            return False
            
        if not self.langfuse_host:
            if self.debug:
                print("Warning: LANGFUSE_HOST not set")
            return False
            
        return True
    
    def to_langfuse_kwargs(self) -> dict:
        """
        Convert configuration to kwargs for Langfuse client initialization.
        
        Returns:
            Dictionary of kwargs for Langfuse client.
        """
        kwargs = {}
        
        if self.langfuse_secret_key:
            kwargs["secret_key"] = self.langfuse_secret_key
            
        if self.langfuse_public_key:
            kwargs["public_key"] = self.langfuse_public_key
            
        if self.langfuse_host:
            kwargs["host"] = self.langfuse_host
            
        if self.debug:
            kwargs["debug"] = True
            
        return kwargs
</file>

<file path="cookbook/pocketflow-tracing/tracing/core.py">
"""
Core tracing functionality for PocketFlow with Langfuse integration.
"""

import json
import time
import uuid
from typing import Any, Dict, Optional, Union
from datetime import datetime

try:
    from langfuse import Langfuse

    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    print("Warning: langfuse package not installed. Install with: pip install langfuse")

from .config import TracingConfig


class LangfuseTracer:
    """
    Core tracer class that handles Langfuse integration for PocketFlow.
    """

    def __init__(self, config: TracingConfig):
        """
        Initialize the LangfuseTracer.

        Args:
            config: TracingConfig instance with Langfuse settings.
        """
        self.config = config
        self.client = None
        self.current_trace = None
        self.spans = {}  # Store spans by node ID

        if LANGFUSE_AVAILABLE and config.validate():
            try:
                # Initialize Langfuse client with proper parameters
                kwargs = {}
                if config.langfuse_secret_key:
                    kwargs["secret_key"] = config.langfuse_secret_key
                if config.langfuse_public_key:
                    kwargs["public_key"] = config.langfuse_public_key
                if config.langfuse_host:
                    kwargs["host"] = config.langfuse_host
                if config.debug:
                    kwargs["debug"] = True

                self.client = Langfuse(**kwargs)
                if config.debug:
                    print(
                        f"âœ“ Langfuse client initialized with host: {config.langfuse_host}"
                    )
            except Exception as e:
                if config.debug:
                    print(f"âœ— Failed to initialize Langfuse client: {e}")
                self.client = None
        else:
            if config.debug:
                print("âœ— Langfuse not available or configuration invalid")

    def start_trace(self, flow_name: str, input_data: Dict[str, Any]) -> Optional[str]:
        """
        Start a new trace for a flow execution.

        Args:
            flow_name: Name of the flow being traced.
            input_data: Input data for the flow.

        Returns:
            Trace ID if successful, None otherwise.
        """
        if not self.client:
            return None

        try:
            # Serialize input data safely
            serialized_input = self._serialize_data(input_data)

            # Use Langfuse v2 API to create a trace
            self.current_trace = self.client.trace(
                name=flow_name,
                input=serialized_input,
                metadata={
                    "framework": "PocketFlow",
                    "trace_type": "flow_execution",
                    "timestamp": datetime.now().isoformat(),
                },
                session_id=self.config.session_id,
                user_id=self.config.user_id,
            )

            # Get the trace ID
            trace_id = self.current_trace.id

            if self.config.debug:
                print(f"âœ“ Started trace: {trace_id} for flow: {flow_name}")

            return trace_id

        except Exception as e:
            if self.config.debug:
                print(f"âœ— Failed to start trace: {e}")
            return None

    def end_trace(self, output_data: Dict[str, Any], status: str = "success") -> None:
        """
        End the current trace.

        Args:
            output_data: Output data from the flow.
            status: Status of the trace execution.
        """
        if not self.current_trace:
            return

        try:
            # Serialize output data safely
            serialized_output = self._serialize_data(output_data)

            # Update the trace with output data using v2 API
            self.current_trace.update(
                output=serialized_output,
                metadata={
                    "status": status,
                    "end_timestamp": datetime.now().isoformat(),
                },
            )

            if self.config.debug:
                print(f"âœ“ Ended trace with status: {status}")

        except Exception as e:
            if self.config.debug:
                print(f"âœ— Failed to end trace: {e}")
        finally:
            self.current_trace = None
            self.spans.clear()

    def start_node_span(
        self, node_name: str, node_id: str, phase: str
    ) -> Optional[str]:
        """
        Start a span for a node execution phase.

        Args:
            node_name: Name/type of the node.
            node_id: Unique identifier for the node instance.
            phase: Execution phase (prep, exec, post).

        Returns:
            Span ID if successful, None otherwise.
        """
        if not self.current_trace:
            return None

        try:
            span_id = f"{node_id}_{phase}"

            # Create a child span using v2 API
            span = self.current_trace.span(
                name=f"{node_name}.{phase}",
                metadata={
                    "node_type": node_name,
                    "node_id": node_id,
                    "phase": phase,
                    "start_timestamp": datetime.now().isoformat(),
                },
            )

            self.spans[span_id] = span

            if self.config.debug:
                print(f"âœ“ Started span: {span_id}")

            return span_id

        except Exception as e:
            if self.config.debug:
                print(f"âœ— Failed to start span: {e}")
            return None

    def end_node_span(
        self,
        span_id: str,
        input_data: Any = None,
        output_data: Any = None,
        error: Exception = None,
    ) -> None:
        """
        End a node execution span.

        Args:
            span_id: ID of the span to end.
            input_data: Input data for the phase.
            output_data: Output data from the phase.
            error: Exception if the phase failed.
        """
        if span_id not in self.spans:
            return

        try:
            span = self.spans[span_id]

            # Prepare update data
            update_data = {}

            if input_data is not None and self.config.trace_inputs:
                update_data["input"] = self._serialize_data(input_data)
            if output_data is not None and self.config.trace_outputs:
                update_data["output"] = self._serialize_data(output_data)

            if error and self.config.trace_errors:
                update_data.update(
                    {
                        "level": "ERROR",
                        "status_message": str(error),
                        "metadata": {
                            "error_type": type(error).__name__,
                            "error_message": str(error),
                            "end_timestamp": datetime.now().isoformat(),
                        },
                    }
                )
            else:
                update_data.update(
                    {
                        "level": "DEFAULT",
                        "metadata": {"end_timestamp": datetime.now().isoformat()},
                    }
                )

            # Update the span with all data at once
            span.update(**update_data)

            # End the span
            span.end()

            if self.config.debug:
                status = "ERROR" if error else "SUCCESS"
                print(f"âœ“ Ended span: {span_id} with status: {status}")

        except Exception as e:
            if self.config.debug:
                print(f"âœ— Failed to end span: {e}")
        finally:
            if span_id in self.spans:
                del self.spans[span_id]

    def _serialize_data(self, data: Any) -> Any:
        """
        Safely serialize data for Langfuse.

        Args:
            data: Data to serialize.

        Returns:
            Serialized data that can be sent to Langfuse.
        """
        try:
            # Handle common PocketFlow data types
            if hasattr(data, "__dict__"):
                # Convert objects to dict representation
                return {"_type": type(data).__name__, "_data": str(data)}
            elif isinstance(data, (dict, list, str, int, float, bool, type(None))):
                # JSON-serializable types
                return data
            else:
                # Fallback to string representation
                return {"_type": type(data).__name__, "_data": str(data)}
        except Exception:
            # Ultimate fallback
            return {"_type": "unknown", "_data": "<serialization_failed>"}

    def flush(self) -> None:
        """Flush any pending traces to Langfuse."""
        if self.client:
            try:
                self.client.flush()
                if self.config.debug:
                    print("âœ“ Flushed traces to Langfuse")
            except Exception as e:
                if self.config.debug:
                    print(f"âœ— Failed to flush traces: {e}")
</file>

<file path="cookbook/pocketflow-tracing/tracing/decorator.py">
"""
Decorator for tracing PocketFlow workflows with Langfuse.
"""

import functools
import inspect
import uuid
from typing import Any, Callable, Dict, Optional, Union

from .config import TracingConfig
from .core import LangfuseTracer


def trace_flow(
    config: Optional[TracingConfig] = None,
    flow_name: Optional[str] = None,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None
):
    """
    Decorator to add Langfuse tracing to PocketFlow flows.
    
    This decorator automatically traces:
    - Flow execution start/end
    - Each node's prep, exec, and post phases
    - Input and output data for each phase
    - Errors and exceptions
    
    Args:
        config: TracingConfig instance. If None, loads from environment.
        flow_name: Custom name for the flow. If None, uses the flow class name.
        session_id: Session ID for grouping related traces.
        user_id: User ID for the trace.
        
    Returns:
        Decorated flow class or function.
        
    Example:
        ```python
        from tracing import trace_flow
        
        @trace_flow()
        class MyFlow(Flow):
            def __init__(self):
                super().__init__(start=MyNode())
        
        # Or with custom configuration
        config = TracingConfig.from_env()
        
        @trace_flow(config=config, flow_name="CustomFlow")
        class MyFlow(Flow):
            pass
        ```
    """
    def decorator(flow_class_or_func):
        # Handle both class and function decoration
        if inspect.isclass(flow_class_or_func):
            return _trace_flow_class(flow_class_or_func, config, flow_name, session_id, user_id)
        else:
            return _trace_flow_function(flow_class_or_func, config, flow_name, session_id, user_id)
    
    return decorator


def _trace_flow_class(flow_class, config, flow_name, session_id, user_id):
    """Trace a Flow class by wrapping its methods."""
    
    # Get or create config
    if config is None:
        config = TracingConfig.from_env()
    
    # Override session/user if provided
    if session_id:
        config.session_id = session_id
    if user_id:
        config.user_id = user_id
    
    # Get flow name
    if flow_name is None:
        flow_name = flow_class.__name__
    
    # Store original methods
    original_init = flow_class.__init__
    original_run = getattr(flow_class, 'run', None)
    original_run_async = getattr(flow_class, 'run_async', None)
    
    def traced_init(self, *args, **kwargs):
        """Initialize the flow with tracing capabilities."""
        # Call original init
        original_init(self, *args, **kwargs)
        
        # Add tracing attributes
        self._tracer = LangfuseTracer(config)
        self._flow_name = flow_name
        self._trace_id = None
        
        # Patch all nodes in the flow
        self._patch_nodes()
    
    def traced_run(self, shared):
        """Traced version of the run method."""
        if not hasattr(self, '_tracer'):
            # Fallback if not properly initialized
            return original_run(self, shared) if original_run else None
            
        # Start trace
        self._trace_id = self._tracer.start_trace(self._flow_name, shared)
        
        try:
            # Run the original flow
            result = original_run(self, shared) if original_run else None
            
            # End trace successfully
            self._tracer.end_trace(shared, "success")
            
            return result
            
        except Exception as e:
            # End trace with error
            self._tracer.end_trace(shared, "error")
            raise
        finally:
            # Ensure cleanup
            self._tracer.flush()
    
    async def traced_run_async(self, shared):
        """Traced version of the async run method."""
        if not hasattr(self, '_tracer'):
            # Fallback if not properly initialized
            return await original_run_async(self, shared) if original_run_async else None
            
        # Start trace
        self._trace_id = self._tracer.start_trace(self._flow_name, shared)
        
        try:
            # Run the original flow
            result = await original_run_async(self, shared) if original_run_async else None
            
            # End trace successfully
            self._tracer.end_trace(shared, "success")
            
            return result
            
        except Exception as e:
            # End trace with error
            self._tracer.end_trace(shared, "error")
            raise
        finally:
            # Ensure cleanup
            self._tracer.flush()
    
    def patch_nodes(self):
        """Patch all nodes in the flow to add tracing."""
        if not hasattr(self, 'start_node') or not self.start_node:
            return
            
        visited = set()
        nodes_to_patch = [self.start_node]
        
        while nodes_to_patch:
            node = nodes_to_patch.pop(0)
            if id(node) in visited:
                continue
                
            visited.add(id(node))
            
            # Patch this node
            self._patch_node(node)
            
            # Add successors to patch list
            if hasattr(node, 'successors'):
                for successor in node.successors.values():
                    if successor and id(successor) not in visited:
                        nodes_to_patch.append(successor)
    
    def patch_node(self, node):
        """Patch a single node to add tracing."""
        if hasattr(node, '_pocketflow_traced'):
            return  # Already patched
            
        node_id = str(uuid.uuid4())
        node_name = type(node).__name__
        
        # Store original methods
        original_prep = getattr(node, 'prep', None)
        original_exec = getattr(node, 'exec', None)
        original_post = getattr(node, 'post', None)
        original_prep_async = getattr(node, 'prep_async', None)
        original_exec_async = getattr(node, 'exec_async', None)
        original_post_async = getattr(node, 'post_async', None)
        
        # Create traced versions
        if original_prep:
            node.prep = self._create_traced_method(original_prep, node_id, node_name, 'prep')
        if original_exec:
            node.exec = self._create_traced_method(original_exec, node_id, node_name, 'exec')
        if original_post:
            node.post = self._create_traced_method(original_post, node_id, node_name, 'post')
        if original_prep_async:
            node.prep_async = self._create_traced_async_method(original_prep_async, node_id, node_name, 'prep')
        if original_exec_async:
            node.exec_async = self._create_traced_async_method(original_exec_async, node_id, node_name, 'exec')
        if original_post_async:
            node.post_async = self._create_traced_async_method(original_post_async, node_id, node_name, 'post')
        
        # Mark as traced
        node._pocketflow_traced = True
    
    def create_traced_method(self, original_method, node_id, node_name, phase):
        """Create a traced version of a synchronous method."""
        @functools.wraps(original_method)
        def traced_method(*args, **kwargs):
            span_id = self._tracer.start_node_span(node_name, node_id, phase)
            
            try:
                result = original_method(*args, **kwargs)
                self._tracer.end_node_span(span_id, input_data=args, output_data=result)
                return result
            except Exception as e:
                self._tracer.end_node_span(span_id, input_data=args, error=e)
                raise
                
        return traced_method
    
    def create_traced_async_method(self, original_method, node_id, node_name, phase):
        """Create a traced version of an asynchronous method."""
        @functools.wraps(original_method)
        async def traced_async_method(*args, **kwargs):
            span_id = self._tracer.start_node_span(node_name, node_id, phase)
            
            try:
                result = await original_method(*args, **kwargs)
                self._tracer.end_node_span(span_id, input_data=args, output_data=result)
                return result
            except Exception as e:
                self._tracer.end_node_span(span_id, input_data=args, error=e)
                raise
                
        return traced_async_method
    
    # Replace methods on the class
    flow_class.__init__ = traced_init
    flow_class._patch_nodes = patch_nodes
    flow_class._patch_node = patch_node
    flow_class._create_traced_method = create_traced_method
    flow_class._create_traced_async_method = create_traced_async_method
    
    if original_run:
        flow_class.run = traced_run
    if original_run_async:
        flow_class.run_async = traced_run_async
    
    return flow_class


def _trace_flow_function(flow_func, config, flow_name, session_id, user_id):
    """Trace a flow function (for functional-style flows)."""
    
    # Get or create config
    if config is None:
        config = TracingConfig.from_env()
    
    # Override session/user if provided
    if session_id:
        config.session_id = session_id
    if user_id:
        config.user_id = user_id
    
    # Get flow name
    if flow_name is None:
        flow_name = flow_func.__name__
    
    tracer = LangfuseTracer(config)
    
    @functools.wraps(flow_func)
    def traced_flow_func(*args, **kwargs):
        # Assume first argument is shared data
        shared = args[0] if args else {}
        
        # Start trace
        trace_id = tracer.start_trace(flow_name, shared)
        
        try:
            result = flow_func(*args, **kwargs)
            tracer.end_trace(shared, "success")
            return result
        except Exception as e:
            tracer.end_trace(shared, "error")
            raise
        finally:
            tracer.flush()
    
    return traced_flow_func
</file>

<file path="cookbook/pocketflow-tracing/utils/__init__.py">
"""
Utility functions for PocketFlow tracing.
"""

from .setup import setup_tracing, test_langfuse_connection

__all__ = ['setup_tracing', 'test_langfuse_connection']
</file>

<file path="cookbook/pocketflow-tracing/utils/setup.py">
"""
Setup and testing utilities for PocketFlow tracing.
"""

import os
import sys
from typing import Optional

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

try:
    from langfuse import Langfuse
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False

from tracing import TracingConfig, LangfuseTracer


def setup_tracing(env_file: Optional[str] = None) -> TracingConfig:
    """
    Set up tracing configuration and validate the setup.
    
    Args:
        env_file: Optional path to .env file. If None, uses default location.
        
    Returns:
        TracingConfig instance.
        
    Raises:
        RuntimeError: If setup fails.
    """
    print("ðŸ”§ Setting up PocketFlow tracing...")
    
    # Check if langfuse is installed
    if not LANGFUSE_AVAILABLE:
        raise RuntimeError(
            "Langfuse package not installed. Install with: pip install langfuse"
        )
    
    # Load configuration
    if env_file:
        config = TracingConfig.from_env(env_file)
        print(f"âœ“ Loaded configuration from: {env_file}")
    else:
        config = TracingConfig.from_env()
        print("âœ“ Loaded configuration from environment")
    
    # Validate configuration
    if not config.validate():
        raise RuntimeError(
            "Invalid tracing configuration. Please check your environment variables:\n"
            "- LANGFUSE_SECRET_KEY\n"
            "- LANGFUSE_PUBLIC_KEY\n" 
            "- LANGFUSE_HOST"
        )
    
    print("âœ“ Configuration validated")
    
    # Test connection
    if test_langfuse_connection(config):
        print("âœ“ Langfuse connection successful")
    else:
        raise RuntimeError("Failed to connect to Langfuse. Check your configuration and network.")
    
    print("ðŸŽ‰ PocketFlow tracing setup complete!")
    return config


def test_langfuse_connection(config: TracingConfig) -> bool:
    """
    Test connection to Langfuse.
    
    Args:
        config: TracingConfig instance.
        
    Returns:
        True if connection successful, False otherwise.
    """
    try:
        # Create a test tracer
        tracer = LangfuseTracer(config)
        
        if not tracer.client:
            return False
        
        # Try to start and end a test trace
        trace_id = tracer.start_trace("test_connection", {"test": True})
        if trace_id:
            tracer.end_trace({"test": "completed"}, "success")
            tracer.flush()
            return True
        
        return False
        
    except Exception as e:
        if config.debug:
            print(f"Connection test failed: {e}")
        return False


def print_configuration_help():
    """Print help information for configuring tracing."""
    print("""
ðŸ”§ PocketFlow Tracing Configuration Help

To use PocketFlow tracing, you need to configure Langfuse credentials.

1. Create or update your .env file with:

LANGFUSE_SECRET_KEY=your-secret-key
LANGFUSE_PUBLIC_KEY=your-public-key
LANGFUSE_HOST=your-langfuse-host
POCKETFLOW_TRACING_DEBUG=true

2. Optional configuration:

POCKETFLOW_TRACE_INPUTS=true
POCKETFLOW_TRACE_OUTPUTS=true
POCKETFLOW_TRACE_PREP=true
POCKETFLOW_TRACE_EXEC=true
POCKETFLOW_TRACE_POST=true
POCKETFLOW_TRACE_ERRORS=true
POCKETFLOW_SESSION_ID=your-session-id
POCKETFLOW_USER_ID=your-user-id

3. Install required packages:

pip install -r requirements.txt

4. Test your setup:

python -c "from utils import setup_tracing; setup_tracing()"
""")


if __name__ == "__main__":
    """Command-line interface for setup and testing."""
    import argparse
    
    parser = argparse.ArgumentParser(description="PocketFlow Tracing Setup")
    parser.add_argument("--test", action="store_true", help="Test Langfuse connection")
    parser.add_argument("--help-config", action="store_true", help="Show configuration help")
    parser.add_argument("--env-file", type=str, help="Path to .env file")
    
    args = parser.parse_args()
    
    if args.help_config:
        print_configuration_help()
        sys.exit(0)
    
    if args.test:
        try:
            config = setup_tracing(args.env_file)
            print("\nâœ… All tests passed! Your tracing setup is ready.")
        except Exception as e:
            print(f"\nâŒ Setup failed: {e}")
            print("\nFor help with configuration, run:")
            print("python utils/setup.py --help-config")
            sys.exit(1)
    else:
        print_configuration_help()
</file>

<file path="cookbook/pocketflow-tracing/.env.example">
# PocketFlow Tracing Configuration Template
# Copy this file to .env and replace the placeholder values with your actual Langfuse credentials

# Required Langfuse configuration
LANGFUSE_SECRET_KEY=your-langfuse-secret-key
LANGFUSE_PUBLIC_KEY=your-langfuse-public-key
LANGFUSE_HOST=your-langfuse-host-url

# Optional tracing configuration
POCKETFLOW_TRACING_DEBUG=true
POCKETFLOW_TRACE_INPUTS=true
POCKETFLOW_TRACE_OUTPUTS=true
POCKETFLOW_TRACE_PREP=true
POCKETFLOW_TRACE_EXEC=true
POCKETFLOW_TRACE_POST=true
POCKETFLOW_TRACE_ERRORS=true

# Optional session/user tracking
POCKETFLOW_SESSION_ID=your-session-id
POCKETFLOW_USER_ID=your-user-id
</file>

<file path="cookbook/pocketflow-tracing/README.md">
# PocketFlow Tracing with Langfuse

This cookbook provides comprehensive observability for PocketFlow workflows using [Langfuse](https://langfuse.com/) as the tracing backend. With minimal code changes (just adding a decorator), you can automatically trace all node executions, inputs, outputs, and errors in your PocketFlow workflows.

## ðŸŽ¯ Features

- **Automatic Tracing**: Trace entire flows with a single decorator
- **Node-Level Observability**: Automatically trace `prep`, `exec`, and `post` phases of each node
- **Input/Output Tracking**: Capture all data flowing through your workflow
- **Error Tracking**: Automatically capture and trace exceptions
- **Async Support**: Full support for AsyncFlow and AsyncNode
- **Minimal Code Changes**: Just add `@trace_flow()` to your flow classes
- **Langfuse Integration**: Leverage Langfuse's powerful observability platform

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Environment Setup

Copy the example environment file and configure your Langfuse credentials:

```bash
cp .env.example .env
```

Then edit the `.env` file with your actual Langfuse configuration:

```env
LANGFUSE_SECRET_KEY=your-langfuse-secret-key
LANGFUSE_PUBLIC_KEY=your-langfuse-public-key
LANGFUSE_HOST=your-langfuse-host-url
POCKETFLOW_TRACING_DEBUG=true
```

**Note**: Replace the placeholder values with your actual Langfuse credentials and host URL.

### 3. Basic Usage

```python
from pocketflow import Node, Flow
from tracing import trace_flow

class MyNode(Node):
    def prep(self, shared):
        return shared["input"]
    
    def exec(self, data):
        return f"Processed: {data}"
    
    def post(self, shared, prep_res, exec_res):
        shared["output"] = exec_res
        return "default"

@trace_flow()  # ðŸŽ‰ That's it! Your flow is now traced
class MyFlow(Flow):
    def __init__(self):
        super().__init__(start=MyNode())

# Run your flow - tracing happens automatically
flow = MyFlow()
shared = {"input": "Hello World"}
flow.run(shared)
```

## ðŸ“Š What Gets Traced

When you apply the `@trace_flow()` decorator, the system automatically traces:

### Flow Level
- **Flow Start/End**: Overall execution time and status
- **Input Data**: Initial shared state when flow starts
- **Output Data**: Final shared state when flow completes
- **Errors**: Any exceptions that occur during flow execution

### Node Level
For each node in your flow, the system traces:

- **prep() Phase**: 
  - Input: `shared` data
  - Output: `prep_res` returned by prep method
  - Execution time and any errors

- **exec() Phase**:
  - Input: `prep_res` from prep phase
  - Output: `exec_res` returned by exec method
  - Execution time and any errors
  - Retry attempts (if configured)

- **post() Phase**:
  - Input: `shared`, `prep_res`, `exec_res`
  - Output: Action string returned
  - Execution time and any errors

## ðŸ”§ Configuration Options

### Basic Configuration

```python
from tracing import trace_flow, TracingConfig

# Use environment variables (default)
@trace_flow()
class MyFlow(Flow):
    pass

# Custom flow name
@trace_flow(flow_name="CustomFlowName")
class MyFlow(Flow):
    pass

# Custom session and user IDs
@trace_flow(session_id="session-123", user_id="user-456")
class MyFlow(Flow):
    pass
```

### Advanced Configuration

```python
from tracing import TracingConfig

# Create custom configuration
config = TracingConfig(
    langfuse_secret_key="your-secret-key",
    langfuse_public_key="your-public-key", 
    langfuse_host="https://your-langfuse-instance.com",
    debug=True,
    trace_inputs=True,
    trace_outputs=True,
    trace_errors=True
)

@trace_flow(config=config)
class MyFlow(Flow):
    pass
```

## ðŸ“ Examples

### Basic Synchronous Flow
See `examples/basic_example.py` for a complete example of tracing a simple synchronous flow.

```bash
cd examples
python basic_example.py
```

### Asynchronous Flow
See `examples/async_example.py` for tracing AsyncFlow and AsyncNode.

```bash
cd examples  
python async_example.py
```

## ðŸ” Viewing Traces

After running your traced flows, visit your Langfuse dashboard to view the traces:

**Dashboard URL**: Use the URL you configured in `LANGFUSE_HOST` environment variable

In the dashboard you'll see:
- **Traces**: One trace per flow execution
- **Spans**: Individual node phases (prep, exec, post)
- **Input/Output Data**: All data flowing through your workflow
- **Performance Metrics**: Execution times for each phase
- **Error Details**: Stack traces and error messages

The tracings in examples.
![alt text](screenshots/chrome_2025-06-27_12-05-28.png)

Detailed tracing for a node.
![langfuse](screenshots/chrome_2025-06-27_12-07-56.png)

## ðŸ› ï¸ Advanced Usage

### Custom Tracer Configuration

```python
from tracing import TracingConfig, LangfuseTracer

# Create custom configuration
config = TracingConfig.from_env()
config.debug = True

# Use tracer directly (for advanced use cases)
tracer = LangfuseTracer(config)
```

### Environment Variables

You can customize tracing behavior with these environment variables:

```env
# Required Langfuse configuration
LANGFUSE_SECRET_KEY=your-secret-key
LANGFUSE_PUBLIC_KEY=your-public-key
LANGFUSE_HOST=your-langfuse-host

# Optional tracing configuration
POCKETFLOW_TRACING_DEBUG=true
POCKETFLOW_TRACE_INPUTS=true
POCKETFLOW_TRACE_OUTPUTS=true
POCKETFLOW_TRACE_PREP=true
POCKETFLOW_TRACE_EXEC=true
POCKETFLOW_TRACE_POST=true
POCKETFLOW_TRACE_ERRORS=true

# Optional session/user tracking
POCKETFLOW_SESSION_ID=your-session-id
POCKETFLOW_USER_ID=your-user-id
```

## ðŸ› Troubleshooting

### Common Issues

1. **"langfuse package not installed"**
   ```bash
   pip install langfuse
   ```

2. **"Langfuse client initialization failed"**
   - Check your `.env` file configuration
   - Verify Langfuse server is running at the specified host
   - Check network connectivity

3. **"No traces appearing in dashboard"**
   - Ensure `POCKETFLOW_TRACING_DEBUG=true` to see debug output
   - Check that your flow is actually being executed
   - Verify Langfuse credentials are correct

### Debug Mode

Enable debug mode to see detailed tracing information:

```env
POCKETFLOW_TRACING_DEBUG=true
```

This will print detailed information about:
- Langfuse client initialization
- Trace and span creation
- Data serialization
- Error messages

## ðŸ“š API Reference

### `@trace_flow()`

Decorator to add Langfuse tracing to PocketFlow flows.

**Parameters:**
- `config` (TracingConfig, optional): Custom configuration. If None, loads from environment.
- `flow_name` (str, optional): Custom name for the flow. If None, uses class name.
- `session_id` (str, optional): Session ID for grouping related traces.
- `user_id` (str, optional): User ID for the trace.

### `TracingConfig`

Configuration class for tracing settings.

**Methods:**
- `TracingConfig.from_env()`: Create config from environment variables
- `validate()`: Check if configuration is valid
- `to_langfuse_kwargs()`: Convert to Langfuse client kwargs

### `LangfuseTracer`

Core tracer class for Langfuse integration.

**Methods:**
- `start_trace()`: Start a new trace
- `end_trace()`: End the current trace
- `start_node_span()`: Start a span for node execution
- `end_node_span()`: End a node execution span
- `flush()`: Flush pending traces to Langfuse

## ðŸ¤ Contributing

This cookbook is designed to be a starting point for PocketFlow observability. Feel free to extend and customize it for your specific needs!

## ðŸ“„ License

This cookbook follows the same license as PocketFlow.
</file>

<file path="cookbook/pocketflow-tracing/requirements.txt">
# Core dependencies for PocketFlow tracing
langfuse>=2.0.0,<3.0.0  # v2 low level SDK compatible with Langfuse servers
python-dotenv>=1.0.0

# Optional dependencies for enhanced functionality
pydantic>=2.0.0  # For data validation and serialization
</file>

<file path="cookbook/pocketflow-tracing/setup.py">
#!/usr/bin/env python3
"""
Setup script for PocketFlow Tracing cookbook.

This script helps install dependencies and verify the setup.
"""

import subprocess
import sys
import os


def install_dependencies():
    """Install required dependencies."""
    print("ðŸ“¦ Installing dependencies...")
    try:
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "-r", "requirements.txt"]
        )
        print("âœ… Dependencies installed successfully!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to install dependencies: {e}")
        return False


def verify_setup():
    """Verify that the setup is working."""
    print("ðŸ” Verifying setup...")
    try:
        # Try to import the tracing module
        from tracing import trace_flow, TracingConfig

        print("âœ… Tracing module imported successfully!")

        # Try to load configuration
        config = TracingConfig.from_env()
        if config.validate():
            print("âœ… Configuration is valid!")
        else:
            print("âš ï¸ Configuration validation failed - check your .env file")

        return True
    except ImportError as e:
        print(f"âŒ Failed to import tracing module: {e}")
        return False
    except Exception as e:
        print(f"âŒ Setup verification failed: {e}")
        return False


def main():
    """Main setup function."""
    print("ðŸš€ PocketFlow Tracing Setup")
    print("=" * 40)

    # Check if we're in the right directory
    if not os.path.exists("requirements.txt"):
        print(
            "âŒ requirements.txt not found. Please run this script from the pocketflow-tracing directory."
        )
        sys.exit(1)

    # Install dependencies
    if not install_dependencies():
        sys.exit(1)

    # Verify setup
    if not verify_setup():
        sys.exit(1)

    print("\nðŸŽ‰ Setup completed successfully!")
    print("\nðŸ“š Next steps:")
    print("1. Check the README.md for usage instructions")
    print("2. Run the examples: python examples/basic_example.py")
    print("3. Run the test suite: python test_tracing.py")
    print("4. Check your Langfuse dashboard (URL configured in LANGFUSE_HOST)")


if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-tracing/test_tracing.py">
#!/usr/bin/env python3
"""
Test script for PocketFlow tracing functionality.

This script tests the tracing implementation to ensure it works correctly
with Langfuse integration.
"""

import sys
import os
import asyncio
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add paths for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.insert(0, os.path.dirname(__file__))

from pocketflow import Node, Flow, AsyncNode, AsyncFlow
from tracing import trace_flow, TracingConfig
from utils import setup_tracing


class TestNode(Node):
    """Simple test node for tracing verification."""

    def prep(self, shared):
        """Test prep phase."""
        return shared.get("input", "test_input")

    def exec(self, prep_res):
        """Test exec phase."""
        return f"processed_{prep_res}"

    def post(self, shared, prep_res, exec_res):
        """Test post phase."""
        shared["output"] = exec_res
        return "default"


class TestAsyncNode(AsyncNode):
    """Simple async test node for tracing verification."""

    async def prep_async(self, shared):
        """Test async prep phase."""
        await asyncio.sleep(0.1)  # Simulate async work
        return shared.get("input", "async_test_input")

    async def exec_async(self, prep_res):
        """Test async exec phase."""
        await asyncio.sleep(0.1)  # Simulate async work
        return f"async_processed_{prep_res}"

    async def post_async(self, shared, prep_res, exec_res):
        """Test async post phase."""
        shared["output"] = exec_res
        return "default"


@trace_flow(flow_name="TestSyncFlow")
class TestSyncFlow(Flow):
    """Test synchronous flow with tracing."""

    def __init__(self):
        super().__init__(start=TestNode())


@trace_flow(flow_name="TestAsyncFlow")
class TestAsyncFlow(AsyncFlow):
    """Test asynchronous flow with tracing."""

    def __init__(self):
        super().__init__(start=TestAsyncNode())


def test_sync_flow():
    """Test synchronous flow tracing."""
    print("ðŸ§ª Testing synchronous flow tracing...")

    flow = TestSyncFlow()
    shared = {"input": "sync_test_data"}

    print(f"   Input: {shared}")
    result = flow.run(shared)
    print(f"   Output: {shared}")
    print(f"   Result: {result}")

    # Verify the flow worked
    assert "output" in shared
    assert shared["output"] == "processed_sync_test_data"
    print("   âœ… Sync flow test passed")


async def test_async_flow():
    """Test asynchronous flow tracing."""
    print("ðŸ§ª Testing asynchronous flow tracing...")

    flow = TestAsyncFlow()
    shared = {"input": "async_test_data"}

    print(f"   Input: {shared}")
    result = await flow.run_async(shared)
    print(f"   Output: {shared}")
    print(f"   Result: {result}")

    # Verify the flow worked
    assert "output" in shared
    assert shared["output"] == "async_processed_async_test_data"
    print("   âœ… Async flow test passed")


def test_configuration():
    """Test configuration loading and validation."""
    print("ðŸ§ª Testing configuration...")

    # Test loading from environment
    config = TracingConfig.from_env()
    print(f"   Loaded config: debug={config.debug}")

    # Test validation
    is_valid = config.validate()
    print(f"   Config valid: {is_valid}")

    if is_valid:
        print("   âœ… Configuration test passed")
    else:
        print(
            "   âš ï¸ Configuration test failed (this may be expected if env vars not set)"
        )


def test_error_handling():
    """Test error handling in traced flows."""
    print("ðŸ§ª Testing error handling...")

    class ErrorNode(Node):
        def exec(self, prep_res):
            raise ValueError("Test error for tracing")

    @trace_flow(flow_name="TestErrorFlow")
    class ErrorFlow(Flow):
        def __init__(self):
            super().__init__(start=ErrorNode())

    flow = ErrorFlow()
    shared = {"input": "error_test"}

    try:
        flow.run(shared)
        print("   âŒ Expected error but flow succeeded")
    except ValueError as e:
        print(f"   âœ… Error correctly caught and traced: {e}")
    except Exception as e:
        print(f"   âš ï¸ Unexpected error type: {e}")


async def main():
    """Run all tests."""
    print("ðŸš€ Starting PocketFlow Tracing Tests")
    print("=" * 50)

    # Test configuration first
    test_configuration()
    print()

    # Test setup (optional - only if environment is configured)
    try:
        print("ðŸ”§ Testing setup...")
        config = setup_tracing()
        print("   âœ… Setup test passed")
    except Exception as e:
        print(f"   âš ï¸ Setup test failed: {e}")
        print("   (This is expected if Langfuse is not configured)")
    print()

    # Test sync flow
    test_sync_flow()
    print()

    # Test async flow
    await test_async_flow()
    print()

    # Test error handling
    test_error_handling()
    print()

    print("ðŸŽ‰ All tests completed!")
    print("\nðŸ“Š If Langfuse is configured, check your dashboard for traces:")
    langfuse_host = os.getenv("LANGFUSE_HOST", "your-langfuse-host")
    print(f"   Dashboard URL: {langfuse_host}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-visualization/viz/flow_visualization.html">
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>PocketFlow: Flow Visualization</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            overflow: hidden;
        }
        svg {
            width: 100vw;
            height: 100vh;
        }
        .links path {
            fill: none;
            stroke: #999;
            stroke-opacity: 0.6;
            stroke-width: 1.5px;
        }
        .group-links path {
            fill: none;
            stroke: #333;
            stroke-opacity: 0.8;
            stroke-width: 2px;
            stroke-dasharray: 5,5;
        }
        .nodes circle {
            stroke: #fff;
            stroke-width: 1.5px;
        }
        .node-labels {
            font-size: 12px;
            pointer-events: none;
        }
        .link-labels {
            font-size: 10px;
            fill: #666;
            pointer-events: none;
        }
        .group-link-labels {
            font-size: 11px;
            font-weight: bold;
            fill: #333;
            pointer-events: none;
        }
        .group-container {
            stroke: #333;
            stroke-width: 1.5px;
            stroke-dasharray: 5,5;
            fill: rgba(200, 200, 200, 0.1);
            rx: 10;
            ry: 10;
        }
        .group-label {
            font-size: 14px;
            font-weight: bold;
            pointer-events: none;
        }
    </style>
</head>
<body>
    <svg id="graph"></svg>
    <script>
        // Load data from file
        d3.json("flow_visualization.json").then(data => {
            const svg = d3.select("#graph");
            const width = window.innerWidth;
            const height = window.innerHeight;
            
            // Define arrow markers for links
            svg.append("defs").append("marker")
                .attr("id", "arrowhead")
                .attr("viewBox", "0 -5 10 10")
                .attr("refX", 25) // Position the arrow away from the target node
                .attr("refY", 0)
                .attr("orient", "auto")
                .attr("markerWidth", 6)
                .attr("markerHeight", 6)
                .attr("xoverflow", "visible")
                .append("path")
                .attr("d", "M 0,-5 L 10,0 L 0,5")
                .attr("fill", "#999");
                
            // Define thicker arrow markers for group links
            svg.append("defs").append("marker")
                .attr("id", "group-arrowhead")
                .attr("viewBox", "0 -5 10 10")
                .attr("refX", 3) // Position at the boundary of the group
                .attr("refY", 0)
                .attr("orient", "auto")
                .attr("markerWidth", 8)
                .attr("markerHeight", 8)
                .attr("xoverflow", "visible")
                .append("path")
                .attr("d", "M 0,-5 L 10,0 L 0,5")
                .attr("fill", "#333");
            
            // Color scale for node groups
            const color = d3.scaleOrdinal(d3.schemeCategory10);
            
            // Process the data to identify groups
            const groups = {};
            data.nodes.forEach(node => {
                if (node.group > 0) {
                    if (!groups[node.group]) {
                        // Use the flow name instead of generic "Group X"
                        const flowName = data.flows && data.flows[node.group] ? data.flows[node.group] : `Flow ${node.group}`;
                        groups[node.group] = {
                            id: node.group,
                            name: flowName,
                            nodes: [],
                            x: 0,
                            y: 0,
                            width: 0,
                            height: 0
                        };
                    }
                    groups[node.group].nodes.push(node);
                }
            });
            
            // Create a force simulation
            const simulation = d3.forceSimulation(data.nodes)
                // Controls the distance between connected nodes
                .force("link", d3.forceLink(data.links).id(d => d.id).distance(100))
                // Controls how nodes repel each other - lower values bring nodes closer
                .force("charge", d3.forceManyBody().strength(-30))
                // Centers the entire graph in the SVG
                .force("center", d3.forceCenter(width / 2, height / 2))
                // Prevents nodes from overlapping - acts like a minimum distance
                .force("collide", d3.forceCollide().radius(50));
            
            // Group forces - create a force to keep nodes in the same group closer together
            // This creates the effect of nodes clustering within their group boxes
            const groupForce = alpha => {
                for (let i = 0; i < data.nodes.length; i++) {
                    const node = data.nodes[i];
                    if (node.group > 0) {
                        const group = groups[node.group];
                        if (group && group.nodes.length > 1) {
                            // Calculate center of group
                            let centerX = 0, centerY = 0;
                            group.nodes.forEach(n => {
                                centerX += n.x || 0;
                                centerY += n.y || 0;
                            });
                            centerX /= group.nodes.length;
                            centerY /= group.nodes.length;
                            
                            // Move nodes toward center
                            const k = alpha * 0.3; // Increased from 0.1 to 0.3
                            node.vx += (centerX - node.x) * k;
                            node.vy += (centerY - node.y) * k;
                        }
                    }
                }
            };
            
            // Additional force to position groups in a more organized layout (like in the image)
            // This arranges the groups horizontally/vertically based on their connections
            const groupLayoutForce = alpha => {
                // Get group centers
                const groupCenters = Object.values(groups).map(g => {
                    return { id: g.id, cx: 0, cy: 0 };
                });
                
                // Calculate current center positions
                Object.values(groups).forEach(g => {
                    if (g.nodes.length > 0) {
                        let cx = 0, cy = 0;
                        g.nodes.forEach(n => {
                            cx += n.x || 0;
                            cy += n.y || 0;
                        });
                        
                        const groupCenter = groupCenters.find(gc => gc.id === g.id);
                        if (groupCenter) {
                            groupCenter.cx = cx / g.nodes.length;
                            groupCenter.cy = cy / g.nodes.length;
                        }
                    }
                });
                
                // Apply forces to position groups
                const k = alpha * 0.05;
                
                // Try to position groups in a more structured way
                // Adjust these values to change the overall layout
                for (let i = 0; i < data.group_links.length; i++) {
                    const link = data.group_links[i];
                    const source = groupCenters.find(g => g.id === link.source);
                    const target = groupCenters.find(g => g.id === link.target);
                    
                    if (source && target) {
                        // Add a horizontal force to align groups
                        const desiredDx = 300; // Desired horizontal distance between linked groups
                        const dx = target.cx - source.cx;
                        const diff = desiredDx - Math.abs(dx);
                        
                        // Apply forces to group nodes
                        groups[source.id].nodes.forEach(n => {
                            if (dx > 0) {
                                n.vx -= diff * k;
                            } else {
                                n.vx += diff * k;
                            }
                        });
                        
                        groups[target.id].nodes.forEach(n => {
                            if (dx > 0) {
                                n.vx += diff * k;
                            } else {
                                n.vx -= diff * k;
                            }
                        });
                    }
                }
            };
            
            simulation.force("group", groupForce);
            simulation.force("groupLayout", groupLayoutForce);
            
            // Create links with arrow paths instead of lines
            const link = svg.append("g")
                .attr("class", "links")
                .selectAll("path")
                .data(data.links)
                .enter()
                .append("path")
                .attr("stroke-width", 2)
                .attr("stroke", "#999")
                .attr("marker-end", "url(#arrowhead)");  // Add the arrowhead marker
            
            // Create group containers (drawn before nodes)
            const groupContainers = svg.append("g")
                .attr("class", "groups")
                .selectAll("rect")
                .data(Object.values(groups))
                .enter()
                .append("rect")
                .attr("class", "group-container")
                .attr("fill", d => d3.color(color(d.id)).copy({opacity: 0.2}));
            
            // Create group links between flows
            const groupLink = svg.append("g")
                .attr("class", "group-links")
                .selectAll("path")
                .data(data.group_links || [])
                .enter()
                .append("path")
                .attr("stroke-width", 2)
                .attr("stroke", "#333")
                .attr("marker-end", "url(#group-arrowhead)");
                
            // Create group link labels
            const groupLinkLabel = svg.append("g")
                .attr("class", "group-link-labels")
                .selectAll("text")
                .data(data.group_links || [])
                .enter()
                .append("text")
                .text(d => d.action)
                .attr("font-size", "11px")
                .attr("font-weight", "bold")
                .attr("fill", "#333");
            
            // Create group labels
            const groupLabels = svg.append("g")
                .attr("class", "group-labels")
                .selectAll("text")
                .data(Object.values(groups))
                .enter()
                .append("text")
                .attr("class", "group-label")
                .text(d => d.name)  // Now using the proper flow name
                .attr("fill", d => d3.color(color(d.id)).darker());
            
            // Create link labels
            const linkLabel = svg.append("g")
                .attr("class", "link-labels")
                .selectAll("text")
                .data(data.links)
                .enter()
                .append("text")
                .text(d => d.action)
                .attr("font-size", "10px")
                .attr("fill", "#666");
            
            // Create nodes
            const node = svg.append("g")
                .attr("class", "nodes")
                .selectAll("circle")
                .data(data.nodes)
                .enter()
                .append("circle")
                .attr("r", 15)
                .attr("fill", d => color(d.group))
                .call(d3.drag()
                    .on("start", dragstarted)
                    .on("drag", dragged)
                    .on("end", dragended));
            
            // Create node labels
            const nodeLabel = svg.append("g")
                .attr("class", "node-labels")
                .selectAll("text")
                .data(data.nodes)
                .enter()
                .append("text")
                .text(d => d.name)
                .attr("text-anchor", "middle")
                .attr("dy", 25);
            
            // Add tooltip on hover
            node.append("title")
                .text(d => d.name);
            
            // Update positions on each tick
            simulation.on("tick", () => {
                // Update links with straight lines
                link.attr("d", d => {
                    return `M${d.source.x},${d.source.y} L${d.target.x},${d.target.y}`;
                });
                
                // Update nodes
                node
                    .attr("cx", d => d.x)
                    .attr("cy", d => d.y);
                
                // Update node labels
                nodeLabel
                    .attr("x", d => d.x)
                    .attr("y", d => d.y);
                
                // Position link labels at midpoint
                linkLabel
                    .attr("x", d => (d.source.x + d.target.x) / 2)
                    .attr("y", d => (d.source.y + d.target.y) / 2);
                
                // Update group containers
                groupContainers.each(function(d) {
                    // If there are nodes in this group
                    if (d.nodes.length > 0) {
                        let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
                        
                        // Find the bounding box for all nodes in the group
                        d.nodes.forEach(n => {
                            minX = Math.min(minX, n.x - 30);
                            minY = Math.min(minY, n.y - 30);
                            maxX = Math.max(maxX, n.x + 30);
                            maxY = Math.max(maxY, n.y + 40); // Extra space for labels
                        });
                        
                        // Add padding
                        const padding = 20;
                        minX -= padding;
                        minY -= padding;
                        maxX += padding;
                        maxY += padding;
                        
                        // Save group dimensions
                        d.x = minX;
                        d.y = minY;
                        d.width = maxX - minX;
                        d.height = maxY - minY;
                        d.centerX = minX + d.width / 2;
                        d.centerY = minY + d.height / 2;
                        
                        // Set position and size of the group container
                        d3.select(this)
                            .attr("x", minX)
                            .attr("y", minY)
                            .attr("width", d.width)
                            .attr("height", d.height);
                        
                        // Update group label position (top-left of group)
                        groupLabels.filter(g => g.id === d.id)
                            .attr("x", minX + 10)
                            .attr("y", minY + 20);
                    }
                });
                
                // Update group links between flows
                groupLink.attr("d", d => {
                    const sourceGroup = groups[d.source];
                    const targetGroup = groups[d.target];
                    
                    if (!sourceGroup || !targetGroup) return "";
                    
                    // Find intersection points with group boundaries
                    // This ensures links connect to the group's border rather than its center
                    
                    // Calculate centers of groups
                    const sx = sourceGroup.centerX;
                    const sy = sourceGroup.centerY;
                    const tx = targetGroup.centerX;
                    const ty = targetGroup.centerY;
                    
                    // Calculate angle between centers - used to find intersection points
                    const angle = Math.atan2(ty - sy, tx - sx);
                    
                    // Calculate intersection points with source group borders
                    // We cast a ray from center in the direction of the target
                    let sourceX, sourceY;
                    const cosA = Math.cos(angle);
                    const sinA = Math.sin(angle);
                    
                    // Check intersection with horizontal borders (top and bottom)
                    const ts_top = (sourceGroup.y - sy) / sinA;
                    const ts_bottom = (sourceGroup.y + sourceGroup.height - sy) / sinA;
                    
                    // Check intersection with vertical borders (left and right)
                    const ts_left = (sourceGroup.x - sx) / cosA;
                    const ts_right = (sourceGroup.x + sourceGroup.width - sx) / cosA;
                    
                    // Use the closest positive intersection (first hit with the boundary)
                    let t_source = Infinity;
                    if (ts_top > 0) t_source = Math.min(t_source, ts_top);
                    if (ts_bottom > 0) t_source = Math.min(t_source, ts_bottom);
                    if (ts_left > 0) t_source = Math.min(t_source, ts_left);
                    if (ts_right > 0) t_source = Math.min(t_source, ts_right);
                    
                    // Target group: Find intersection in the opposite direction
                    // We cast a ray from target center toward the source
                    let targetX, targetY;
                    const oppositeAngle = angle + Math.PI;
                    const cosOpp = Math.cos(oppositeAngle);
                    const sinOpp = Math.sin(oppositeAngle);
                    
                    // Check intersections for target group
                    const tt_top = (targetGroup.y - ty) / sinOpp;
                    const tt_bottom = (targetGroup.y + targetGroup.height - ty) / sinOpp;
                    const tt_left = (targetGroup.x - tx) / cosOpp;
                    const tt_right = (targetGroup.x + targetGroup.width - tx) / cosOpp;
                    
                    // Use the closest positive intersection
                    let t_target = Infinity;
                    if (tt_top > 0) t_target = Math.min(t_target, tt_top);
                    if (tt_bottom > 0) t_target = Math.min(t_target, tt_bottom);
                    if (tt_left > 0) t_target = Math.min(t_target, tt_left);
                    if (tt_right > 0) t_target = Math.min(t_target, tt_right);
                    
                    // Calculate actual border points using parametric equation:
                    // point = center + t * direction
                    if (t_source !== Infinity) {
                        sourceX = sx + cosA * t_source;
                        sourceY = sy + sinA * t_source;
                    } else {
                        sourceX = sx;
                        sourceY = sy;
                    }
                    
                    if (t_target !== Infinity) {
                        targetX = tx + cosOpp * t_target;
                        targetY = ty + sinOpp * t_target;
                    } else {
                        targetX = tx;
                        targetY = ty;
                    }
                    
                    // Create a straight line between the border points
                    return `M${sourceX},${sourceY} L${targetX},${targetY}`;
                });
                
                // Update group link labels
                groupLinkLabel.attr("x", d => {
                    const sourceGroup = groups[d.source];
                    const targetGroup = groups[d.target];
                    if (!sourceGroup || !targetGroup) return 0;
                    return (sourceGroup.centerX + targetGroup.centerX) / 2;
                })
                .attr("y", d => {
                    const sourceGroup = groups[d.source];
                    const targetGroup = groups[d.target];
                    if (!sourceGroup || !targetGroup) return 0;
                    return (sourceGroup.centerY + targetGroup.centerY) / 2 - 10;
                });
            });
            
            // Drag functions
            function dragstarted(event, d) {
                if (!event.active) simulation.alphaTarget(0.3).restart();
                d.fx = d.x;
                d.fy = d.y;
            }
            
            function dragged(event, d) {
                d.fx = event.x;
                d.fy = event.y;
            }
            
            function dragended(event, d) {
                if (!event.active) simulation.alphaTarget(0);
                d.fx = null;
                d.fy = null;
            }
        });
    </script>
</body>
</html>
</file>

<file path="cookbook/pocketflow-visualization/viz/flow_visualization.json">
{
  "nodes": [
    {
      "id": 3,
      "name": "ValidatePayment",
      "group": 2
    },
    {
      "id": 4,
      "name": "ProcessPayment",
      "group": 2
    },
    {
      "id": 5,
      "name": "PaymentConfirmation",
      "group": 2
    },
    {
      "id": 7,
      "name": "CheckStock",
      "group": 6
    },
    {
      "id": 8,
      "name": "ReserveItems",
      "group": 6
    },
    {
      "id": 9,
      "name": "UpdateInventory",
      "group": 6
    },
    {
      "id": 11,
      "name": "CreateLabel",
      "group": 10
    },
    {
      "id": 12,
      "name": "AssignCarrier",
      "group": 10
    },
    {
      "id": 13,
      "name": "SchedulePickup",
      "group": 10
    }
  ],
  "links": [
    {
      "source": 3,
      "target": 4,
      "action": "default"
    },
    {
      "source": 4,
      "target": 5,
      "action": "default"
    },
    {
      "source": 7,
      "target": 8,
      "action": "default"
    },
    {
      "source": 8,
      "target": 9,
      "action": "default"
    },
    {
      "source": 11,
      "target": 12,
      "action": "default"
    },
    {
      "source": 12,
      "target": 13,
      "action": "default"
    }
  ],
  "group_links": [
    {
      "source": 2,
      "target": 6,
      "action": "default"
    },
    {
      "source": 6,
      "target": 10,
      "action": "default"
    }
  ],
  "flows": {
    "1": "OrderFlow",
    "2": "AsyncFlow",
    "6": "AsyncFlow",
    "10": "AsyncFlow"
  }
}
</file>

<file path="cookbook/pocketflow-visualization/async_flow.py">
from pocketflow import AsyncNode, AsyncFlow
import asyncio


# Define Payment Nodes
class ValidatePayment(AsyncNode):
    async def exec_async(self, prep_res):
        print("1.1.Validating payment...")
        return "Payment validated successfully"

    async def post_async(self, shared, prep_res, exec_res):
        shared["payment_status"] = exec_res
        return "default"


class ProcessPayment(AsyncNode):
    async def exec_async(self, prep_res):
        print("1.2.Processing payment...")
        return "Payment processed successfully"

    async def post_async(self, shared, prep_res, exec_res):
        shared["payment_result"] = exec_res
        return "default"


class PaymentConfirmation(AsyncNode):
    async def exec_async(self, prep_res):
        print("1.3.Confirming payment...")
        return "Payment confirmed"

    async def post_async(self, shared, prep_res, exec_res):
        shared["payment_confirmation"] = exec_res
        return "default"


# Define Inventory Nodes
class CheckStock(AsyncNode):
    async def exec_async(self, prep_res):
        print("2.1.Checking inventory stock...")
        return "Stock available"

    async def post_async(self, shared, prep_res, exec_res):
        shared["stock_status"] = exec_res
        return "default"


class ReserveItems(AsyncNode):
    async def exec_async(self, prep_res):
        print("2.2.Reserving items...")
        return "Items reserved"

    async def post_async(self, shared, prep_res, exec_res):
        shared["reservation_status"] = exec_res
        return "default"


class UpdateInventory(AsyncNode):
    async def exec_async(self, prep_res):
        print("2.3. Updating inventory...")
        return "Inventory updated"

    async def post_async(self, shared, prep_res, exec_res):
        shared["inventory_update"] = exec_res
        return "default"


# Define Shipping Nodes
class CreateLabel(AsyncNode):
    async def exec_async(self, prep_res):
        print("3.1 Creating shipping label...")
        return "Shipping label created"

    async def post_async(self, shared, prep_res, exec_res):
        shared["shipping_label"] = exec_res
        return "default"


class AssignCarrier(AsyncNode):
    async def exec_async(self, prep_res):
        print("3.2 Assigning carrier...")
        return "Carrier assigned"

    async def post_async(self, shared, prep_res, exec_res):
        shared["carrier"] = exec_res
        return "default"


class SchedulePickup(AsyncNode):
    async def exec_async(self, prep_res):
        print("3.3 Scheduling pickup...")
        return "Pickup scheduled"

    async def post_async(self, shared, prep_res, exec_res):
        shared["pickup_status"] = exec_res
        return "default"


# Create node instances
validate_payment = ValidatePayment()
process_payment = ProcessPayment()
payment_confirmation = PaymentConfirmation()

check_stock = CheckStock()
reserve_items = ReserveItems()
update_inventory = UpdateInventory()

create_label = CreateLabel()
assign_carrier = AssignCarrier()
schedule_pickup = SchedulePickup()

# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = AsyncFlow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = AsyncFlow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = AsyncFlow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow
# payment_flow >> inventory_flow >> create_label
# payment_flow >> inventory_flow >> assign_carrier


# Create the master flow
class OrderFlow(AsyncFlow):
    pass


order_pipeline = OrderFlow(start=payment_flow)

# Create shared data structure
shared_data = {
    "order_id": "ORD-12345",
    "customer": "John Doe",
    "items": [
        {"id": "ITEM-001", "name": "Smartphone", "price": 999.99, "quantity": 1},
        {"id": "ITEM-002", "name": "Phone case", "price": 29.99, "quantity": 1},
    ],
    "shipping_address": {
        "street": "123 Main St",
        "city": "Anytown",
        "state": "CA",
        "zip": "12345",
    },
}


# Run the entire pipeline asynchronously
async def main():
    await order_pipeline.run_async(shared_data)

    # Print final status
    print("\nOrder processing completed!")
    print(f"Payment: {shared_data.get('payment_confirmation')}")
    print(f"Inventory: {shared_data.get('inventory_update')}")
    print(f"Shipping: {shared_data.get('pickup_status')}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-visualization/async_loop_flow.py">
from async_flow import *
from pocketflow import Flow, AsyncParallelBatchNode, Node

# Create node instances
validate_payment = ValidatePayment()
process_payment = ProcessPayment()
payment_confirmation = PaymentConfirmation()

check_stock = CheckStock()
reserve_items = ReserveItems()
update_inventory = UpdateInventory()

create_label = CreateLabel()
assign_carrier = AssignCarrier()
schedule_pickup = SchedulePickup()

# Payment processing sub-flow
validate_payment >> process_payment
validate_payment - "out_of_stock" >> validate_payment  # å¾ªçŽ¯é‡è¯•
process_payment - 'something fail' >> validate_payment
process_payment - 'pass' >> payment_confirmation
payment_flow = AsyncFlow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = AsyncFlow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = AsyncFlow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow
# payment_flow >> inventory_flow >> create_label
# payment_flow >> inventory_flow >> assign_carrier


# Create the master flow
class OrderFlow(AsyncFlow):
    pass

order_pipeline = OrderFlow(start=payment_flow)

# Create shared data structure
shared_data = {
    "order_id": "ORD-12345",
    "customer": "John Doe",
    "items": [
        {"id": "ITEM-001", "name": "Smartphone", "price": 999.99, "quantity": 1},
        {"id": "ITEM-002", "name": "Phone case", "price": 29.99, "quantity": 1},
    ],
    "shipping_address": {
        "street": "123 Main St",
        "city": "Anytown",
        "state": "CA",
        "zip": "12345",
    },
}


# Run the entire pipeline asynchronously
async def main():
    await order_pipeline.run_async(shared_data)

    # Print final status
    print("\nOrder processing completed!")
    print(f"Payment: {shared_data.get('payment_confirmation')}")
    print(f"Inventory: {shared_data.get('inventory_update')}")
    print(f"Shipping: {shared_data.get('pickup_status')}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="cookbook/pocketflow-visualization/README.md">
# PocketFlow Visualization

This directory contains tools for visualizing PocketFlow workflow graphs using interactive D3.js visualizations.

## Overview

The visualization tools allow you to:

1. View PocketFlow nodes and flows as an interactive graph
2. See how different flows connect to each other
3. Understand the relationships between nodes within flows

## Features

- **Interactive Graph**: Nodes can be dragged to reorganize the layout
- **Group Visualization**: Flows are displayed as groups with dashed borders
- **Inter-Group Links**: Connections between flows are shown as dashed lines connecting group boundaries
- **Action Labels**: Edge labels show the actions that trigger transitions between nodes

## Requirements

- Python 3.6 or higher
- Modern web browser (Chrome, Firefox, Edge) for viewing the visualizations

## Usage

### 1. Basic Visualization

To visualize a PocketFlow graph, you can use the `visualize_flow` function in `visualize.py`:

```python
from visualize import visualize_flow
from your_flow_module import your_flow

# Generate visualization
visualize_flow(your_flow, "Your Flow Name")
```

This will:
1. Print a Mermaid diagram to the console
2. Generate a D3.js visualization in the `./viz` directory

### 2. Running the Example

The included example shows an order processing pipeline with payment, inventory, and shipping flows:

```bash
# Navigate to the directory
cd cookbook/pocketflow-minimal-flow2flow

# Run the visualization script
python visualize.py
```

This will generate visualization files in the `./viz` directory.

### 3. Viewing the Visualization

After running the script:

1. Host with 
   ```
   cd ./viz/
   ```

2. Interact with the visualization:
   - **Drag nodes** to reorganize
   - **Hover over nodes** to see node names
   - **Observe connections** between nodes and flows

## Customizing the Visualization

### Adjusting Layout Parameters

You can adjust the force simulation parameters in `visualize.py` to change how nodes and groups are positioned:

```javascript
// Create a force simulation
const simulation = d3.forceSimulation(data.nodes)
    // Controls the distance between connected nodes
    .force("link", d3.forceLink(data.links).id(d => d.id).distance(100))
    // Controls how nodes repel each other - lower values bring nodes closer
    .force("charge", d3.forceManyBody().strength(-30))
    // Centers the entire graph in the SVG
    .force("center", d3.forceCenter(width / 2, height / 2))
    // Prevents nodes from overlapping - acts like a minimum distance
    .force("collide", d3.forceCollide().radius(50));
```

### Styling

Adjust the CSS styles in the HTML template inside `create_d3_visualization` function to change colors, shapes, and other visual properties.

## How It Works

The visualization process consists of three main steps:

1. **Flow to JSON Conversion**: The `flow_to_json` function traverses the PocketFlow graph and converts it to a structure with nodes, links, and group information.

2. **D3.js Visualization**: The JSON data is used to create an interactive D3.js visualization with:
   - Nodes represented as circles
   - Flows represented as dashed rectangles containing nodes
   - Links showing connections within and between flows

3. **Group Boundary Connections**: The visualization calculates intersection points with group boundaries to ensure inter-group links connect at the borders rather than centers.

## Extending the Visualization

You can extend the visualization tools by:

1. Adding new node shapes
2. Implementing additional layout algorithms
3. Adding tooltips with more detailed information
4. Creating animation for flow execution

## Troubleshooting

If you encounter any issues:

- Make sure your flow objects are properly constructed with nodes connected correctly
- Check the browser console for any JavaScript errors
- Verify that the generated JSON data structure matches what you expect

## Example Output

The visualization displays:
- Payment processing flow nodes
- Inventory management flow nodes
- Shipping flow nodes
- Group boundaries around each flow
- Connections between flows (Payment â†’ Inventory â†’ Shipping)
</file>

<file path="cookbook/pocketflow-visualization/visualize.py">
# %%

import json
import os
import http.server
import socketserver
import threading
import webbrowser
import time
import socket
import importlib
import sys
from pathlib import Path
from typing import Any, Optional, Tuple, Union

from pocketflow import Flow

from async_flow import order_pipeline


def build_mermaid(start):
    ids, visited, lines = {}, set(), ["graph LR"]
    ctr = 1

    def get_id(n):
        nonlocal ctr
        return (
            ids[n] if n in ids else (ids.setdefault(n, f"N{ctr}"), (ctr := ctr + 1))[0]
        )

    def link(a, b, action=None):
        if action:
            lines.append(f"    {a} -->|{action}| {b}")
        else:
            lines.append(f"    {a} --> {b}")

    def walk(node, parent=None, action=None):
        if node in visited:
            return parent and link(parent, get_id(node), action)
        visited.add(node)
        if isinstance(node, Flow):
            node.start_node and parent and link(parent, get_id(node.start_node), action)
            lines.append(
                f"\n    subgraph sub_flow_{get_id(node)}[{type(node).__name__}]"
            )
            node.start_node and walk(node.start_node)
            for act, nxt in node.successors.items():
                node.start_node and walk(nxt, get_id(node.start_node), act) or (
                    parent and link(parent, get_id(nxt), action)
                ) or walk(nxt, None, act)
            lines.append("    end\n")
        else:
            lines.append(f"    {(nid := get_id(node))}['{type(node).__name__}']")
            parent and link(parent, nid, action)
            [walk(nxt, nid, act) for act, nxt in node.successors.items()]

    walk(start)
    return "\n".join(lines)


def flow_to_json(start):
    """Convert a flow to JSON format suitable for D3.js visualization.

    This function walks through the flow graph and builds a structure with:
    - nodes: All non-Flow nodes with their group memberships
    - links: Connections between nodes within the same group
    - group_links: Connections between different groups (for inter-flow connections)
    - flows: Flow information for group labeling

    Returns:
        dict: A JSON-serializable dictionary with 'nodes' and 'links' arrays.
    """
    nodes = []
    links = []
    group_links = []  # For connections between groups (Flow to Flow)
    ids = {}
    node_types = {}
    flow_nodes = {}  # Keep track of flow nodes
    ctr = 1
    visited = set()

    def get_id(n):
        nonlocal ctr
        if n not in ids:
            ids[n] = ctr
            node_types[ctr] = type(n).__name__
            if isinstance(n, Flow):
                flow_nodes[ctr] = n  # Store flow reference
            ctr += 1
        return ids[n]

    def walk(node, parent=None, group=None, parent_group=None, action=None):
        """Recursively walk the flow graph to build the visualization data.

        Args:
            node: Current node being processed
            parent: ID of the parent node that connects to this node
            group: Group (Flow) ID this node belongs to
            parent_group: Group ID of the parent node
            action: Action label on the edge from parent to this node
        """
        node_id = get_id(node)
        if (node_id, action) in visited:
            return
        visited.add((node_id, action))

        # Add node if not already in nodes list and not a Flow
        if not any(n["id"] == node_id for n in nodes) and not isinstance(node, Flow):
            node_data = {
                "id": node_id,
                "name": node_types[node_id],
                "group": group or 0,  # Default group
            }
            nodes.append(node_data)

        # Add link from parent if exists
        if parent and not isinstance(node, Flow):
            links.append(
                {"source": parent, "target": node_id, "action": action or "default"}
            )

        # Process different types of nodes
        if isinstance(node, Flow):
            # This is a Flow node - it becomes a group container
            flow_group = node_id  # Use flow's ID as group for contained nodes

            # Add a group-to-group link if this flow has a parent group
            # This creates connections between nested flows
            if parent_group is not None and parent_group != flow_group:
                # Check if this link already exists
                if not any(
                    l["source"] == parent_group and l["target"] == flow_group
                    for l in group_links
                ):
                    group_links.append(
                        {
                            "source": parent_group,
                            "target": flow_group,
                            "action": action or "default",
                        }
                    )

            if node.start_node:
                # Process the start node of this flow
                walk(node.start_node, parent, flow_group, parent_group, action)

                # Process successors of the flow's start node
                for next_action, nxt in node.successors.items():
                    walk(
                        nxt,
                        get_id(node.start_node),
                        flow_group,
                        parent_group,
                        next_action,
                    )
        else:
            # Process successors for regular nodes
            for next_action, nxt in node.successors.items():
                if isinstance(nxt, Flow):
                    # This node connects to a flow - track the group relationship
                    flow_group_id = get_id(nxt)
                    walk(nxt, node_id, None, group, next_action)
                else:
                    # Regular node-to-node connection
                    walk(nxt, node_id, group, parent_group, next_action)

    # Start the traversal
    walk(start)

    # Post-processing: Generate group links based on node connections between different groups
    # This ensures that when nodes in different groups are connected, we show a group-to-group
    # link rather than a direct node-to-node link
    node_groups = {n["id"]: n["group"] for n in nodes}
    filtered_links = []

    for link in links:
        source_id = link["source"]
        target_id = link["target"]
        source_group = node_groups.get(source_id, 0)
        target_group = node_groups.get(target_id, 0)

        # If source and target are in different groups and both groups are valid
        if source_group != target_group and source_group > 0 and target_group > 0:
            # Add to group links if not already there
            # This creates the dashed lines connecting group boxes
            if not any(
                gl["source"] == source_group and gl["target"] == target_group
                for gl in group_links
            ):
                group_links.append(
                    {
                        "source": source_group,
                        "target": target_group,
                        "action": link["action"],
                    }
                )
            # Skip adding this link to filtered_links - we don't want direct node connections across groups
        else:
            # Keep links within the same group
            filtered_links.append(link)

    return {
        "nodes": nodes,
        "links": filtered_links,  # Use filtered links instead of all links
        "group_links": group_links,
        "flows": {str(k): v.__class__.__name__ for k, v in flow_nodes.items()},
    }


def create_d3_visualization(
    json_data,
    output_dir="./viz",
    filename="flow_viz",
    html_title="PocketFlow Visualization",
):
    """Create a D3.js visualization from JSON data.

    Args:
        json_data: The JSON data for the visualization
        output_dir: Directory to save the files
        filename: Base filename (without extension)
        html_title: Title for the HTML page

    Returns:
        str: Path to the HTML file
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Save JSON data to file
    json_path = os.path.join(output_dir, f"{filename}.json")
    with open(json_path, "w") as f:
        json.dump(json_data, f, indent=2)

    # Create HTML file with D3.js visualization
    html_content = r"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>TITLE_PLACEHOLDER</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            overflow: hidden;
        }
        svg {
            width: 100vw;
            height: 100vh;
        }
        .links path {
            fill: none;
            stroke: #999;
            stroke-opacity: 0.6;
            stroke-width: 1.5px;
        }
        .group-links path {
            fill: none;
            stroke: #333;
            stroke-opacity: 0.8;
            stroke-width: 2px;
            stroke-dasharray: 5,5;
        }
        .nodes circle {
            stroke: #fff;
            stroke-width: 1.5px;
        }
        .node-labels {
            font-size: 12px;
            pointer-events: none;
        }
        .link-labels {
            font-size: 10px;
            fill: #666;
            pointer-events: none;
        }
        .group-link-labels {
            font-size: 11px;
            font-weight: bold;
            fill: #333;
            pointer-events: none;
        }
        .group-container {
            stroke: #333;
            stroke-width: 1.5px;
            stroke-dasharray: 5,5;
            fill: rgba(200, 200, 200, 0.1);
            rx: 10;
            ry: 10;
        }
        .group-label {
            font-size: 14px;
            font-weight: bold;
            pointer-events: none;
        }
    </style>
</head>
<body>
    <svg id="graph"></svg>
    <script>
        // Load data from file
        d3.json("FILENAME_PLACEHOLDER.json").then(data => {
            const svg = d3.select("#graph");
            const width = window.innerWidth;
            const height = window.innerHeight;
            
            // Define arrow markers for links
            svg.append("defs").append("marker")
                .attr("id", "arrowhead")
                .attr("viewBox", "0 -5 10 10")
                .attr("refX", 25) // Position the arrow away from the target node
                .attr("refY", 0)
                .attr("orient", "auto")
                .attr("markerWidth", 6)
                .attr("markerHeight", 6)
                .attr("xoverflow", "visible")
                .append("path")
                .attr("d", "M 0,-5 L 10,0 L 0,5")
                .attr("fill", "#999");
                
            // Define thicker arrow markers for group links
            svg.append("defs").append("marker")
                .attr("id", "group-arrowhead")
                .attr("viewBox", "0 -5 10 10")
                .attr("refX", 3) // Position at the boundary of the group
                .attr("refY", 0)
                .attr("orient", "auto")
                .attr("markerWidth", 8)
                .attr("markerHeight", 8)
                .attr("xoverflow", "visible")
                .append("path")
                .attr("d", "M 0,-5 L 10,0 L 0,5")
                .attr("fill", "#333");
            
            // Color scale for node groups
            const color = d3.scaleOrdinal(d3.schemeCategory10);
            
            // Process the data to identify groups
            const groups = {};
            data.nodes.forEach(node => {
                if (node.group > 0) {
                    if (!groups[node.group]) {
                        // Use the flow name instead of generic "Group X"
                        const flowName = data.flows && data.flows[node.group] ? data.flows[node.group] : `Flow ${node.group}`;
                        groups[node.group] = {
                            id: node.group,
                            name: flowName,
                            nodes: [],
                            x: 0,
                            y: 0,
                            width: 0,
                            height: 0
                        };
                    }
                    groups[node.group].nodes.push(node);
                }
            });
            
            // Create a force simulation
            const simulation = d3.forceSimulation(data.nodes)
                // Controls the distance between connected nodes
                .force("link", d3.forceLink(data.links).id(d => d.id).distance(100))
                // Controls how nodes repel each other - lower values bring nodes closer
                .force("charge", d3.forceManyBody().strength(-30))
                // Centers the entire graph in the SVG
                .force("center", d3.forceCenter(width / 2, height / 2))
                // Prevents nodes from overlapping - acts like a minimum distance
                .force("collide", d3.forceCollide().radius(50));
            
            // Group forces - create a force to keep nodes in the same group closer together
            // This creates the effect of nodes clustering within their group boxes
            const groupForce = alpha => {
                for (let i = 0; i < data.nodes.length; i++) {
                    const node = data.nodes[i];
                    if (node.group > 0) {
                        const group = groups[node.group];
                        if (group && group.nodes.length > 1) {
                            // Calculate center of group
                            let centerX = 0, centerY = 0;
                            group.nodes.forEach(n => {
                                centerX += n.x || 0;
                                centerY += n.y || 0;
                            });
                            centerX /= group.nodes.length;
                            centerY /= group.nodes.length;
                            
                            // Move nodes toward center
                            const k = alpha * 0.3; // Increased from 0.1 to 0.3
                            node.vx += (centerX - node.x) * k;
                            node.vy += (centerY - node.y) * k;
                        }
                    }
                }
            };
            
            // Additional force to position groups in a more organized layout (like in the image)
            // This arranges the groups horizontally/vertically based on their connections
            const groupLayoutForce = alpha => {
                // Get group centers
                const groupCenters = Object.values(groups).map(g => {
                    return { id: g.id, cx: 0, cy: 0 };
                });
                
                // Calculate current center positions
                Object.values(groups).forEach(g => {
                    if (g.nodes.length > 0) {
                        let cx = 0, cy = 0;
                        g.nodes.forEach(n => {
                            cx += n.x || 0;
                            cy += n.y || 0;
                        });
                        
                        const groupCenter = groupCenters.find(gc => gc.id === g.id);
                        if (groupCenter) {
                            groupCenter.cx = cx / g.nodes.length;
                            groupCenter.cy = cy / g.nodes.length;
                        }
                    }
                });
                
                // Apply forces to position groups
                const k = alpha * 0.05;
                
                // Try to position groups in a more structured way
                // Adjust these values to change the overall layout
                for (let i = 0; i < data.group_links.length; i++) {
                    const link = data.group_links[i];
                    const source = groupCenters.find(g => g.id === link.source);
                    const target = groupCenters.find(g => g.id === link.target);
                    
                    if (source && target) {
                        // Add a horizontal force to align groups
                        const desiredDx = 300; // Desired horizontal distance between linked groups
                        const dx = target.cx - source.cx;
                        const diff = desiredDx - Math.abs(dx);
                        
                        // Apply forces to group nodes
                        groups[source.id].nodes.forEach(n => {
                            if (dx > 0) {
                                n.vx -= diff * k;
                            } else {
                                n.vx += diff * k;
                            }
                        });
                        
                        groups[target.id].nodes.forEach(n => {
                            if (dx > 0) {
                                n.vx += diff * k;
                            } else {
                                n.vx -= diff * k;
                            }
                        });
                    }
                }
            };
            
            simulation.force("group", groupForce);
            simulation.force("groupLayout", groupLayoutForce);
            
            // Create links with arrow paths instead of lines
            const link = svg.append("g")
                .attr("class", "links")
                .selectAll("path")
                .data(data.links)
                .enter()
                .append("path")
                .attr("stroke-width", 2)
                .attr("stroke", "#999")
                .attr("marker-end", "url(#arrowhead)");  // Add the arrowhead marker
            
            // Create group containers (drawn before nodes)
            const groupContainers = svg.append("g")
                .attr("class", "groups")
                .selectAll("rect")
                .data(Object.values(groups))
                .enter()
                .append("rect")
                .attr("class", "group-container")
                .attr("fill", d => d3.color(color(d.id)).copy({opacity: 0.2}));
            
            // Create group links between flows
            const groupLink = svg.append("g")
                .attr("class", "group-links")
                .selectAll("path")
                .data(data.group_links || [])
                .enter()
                .append("path")
                .attr("stroke-width", 2)
                .attr("stroke", "#333")
                .attr("marker-end", "url(#group-arrowhead)");
                
            // Create group link labels
            const groupLinkLabel = svg.append("g")
                .attr("class", "group-link-labels")
                .selectAll("text")
                .data(data.group_links || [])
                .enter()
                .append("text")
                .text(d => d.action)
                .attr("font-size", "11px")
                .attr("font-weight", "bold")
                .attr("fill", "#333");
            
            // Create group labels
            const groupLabels = svg.append("g")
                .attr("class", "group-labels")
                .selectAll("text")
                .data(Object.values(groups))
                .enter()
                .append("text")
                .attr("class", "group-label")
                .text(d => d.name)  // Now using the proper flow name
                .attr("fill", d => d3.color(color(d.id)).darker());
            
            // Create link labels
            const linkLabel = svg.append("g")
                .attr("class", "link-labels")
                .selectAll("text")
                .data(data.links)
                .enter()
                .append("text")
                .text(d => d.action)
                .attr("font-size", "10px")
                .attr("fill", "#666");
            
            // Create nodes
            const node = svg.append("g")
                .attr("class", "nodes")
                .selectAll("circle")
                .data(data.nodes)
                .enter()
                .append("circle")
                .attr("r", 15)
                .attr("fill", d => color(d.group))
                .call(d3.drag()
                    .on("start", dragstarted)
                    .on("drag", dragged)
                    .on("end", dragended));
            
            // Create node labels
            const nodeLabel = svg.append("g")
                .attr("class", "node-labels")
                .selectAll("text")
                .data(data.nodes)
                .enter()
                .append("text")
                .text(d => d.name)
                .attr("text-anchor", "middle")
                .attr("dy", 25);
            
            // Add tooltip on hover
            node.append("title")
                .text(d => d.name);
            
            // Update positions on each tick
            simulation.on("tick", () => {
                // Update links with curved paths for bidirectional connections
                link.attr("d", d => {
                    // Handle self-referencing links with a water-drop shape
                    if (d.source === d.target) {
                        const nodeX = d.source.x;
                        const nodeY = d.source.y;
                        const offsetX = 40;
                        const offsetY = 10;
                        const controlOffset = 50;
                        
                        // Create a water-drop shaped path
                        return `M ${nodeX},${nodeY - 5}
                                C ${nodeX + controlOffset},${nodeY - 30} 
                                  ${nodeX + offsetX},${nodeY + offsetY} 
                                  ${nodeX},${nodeY}`;
                    }
                    
                    // Check if there's a reverse connection
                    const isReverse = data.links.some(l => 
                        l.source === d.target && l.target === d.source
                    );
                    
                    // If it's part of a bidirectional connection, curve the path
                    if (isReverse) {
                        const dx = d.target.x - d.source.x;
                        const dy = d.target.y - d.source.y;
                        const dr = Math.sqrt(dx * dx + dy * dy) * 0.9;
                        
                        return `M${d.source.x},${d.source.y}A${dr},${dr} 0 0,1 ${d.target.x},${d.target.y}`;
                    }
                    
                    // For unidirectional connections, use straight lines
                    return `M${d.source.x},${d.source.y} L${d.target.x},${d.target.y}`;
                });
                
                // Update nodes
                node
                    .attr("cx", d => d.x)
                    .attr("cy", d => d.y);
                
                // Update node labels
                nodeLabel
                    .attr("x", d => d.x)
                    .attr("y", d => d.y);
                
                // Position link labels with offset for bidirectional connections
                linkLabel.attr("x", d => {
                    // Handle self-referencing links
                    if (d.source === d.target) {
                        return d.source.x + 30;
                    }
                    
                    // Check if there's a reverse connection
                    const reverseLink = data.links.find(l => 
                        l.source === d.target && l.target === d.source
                    );
                    
                    // If it's part of a bidirectional connection, offset the label
                    if (reverseLink) {
                        const dx = d.target.x - d.source.x;
                        const dy = d.target.y - d.source.y;
                        // Calculate perpendicular offset
                        const length = Math.sqrt(dx * dx + dy * dy);
                        const offsetX = -dy / length * 10; // Perpendicular offset
                        
                        return (d.source.x + d.target.x) / 2 + offsetX;
                    }
                    
                    // For unidirectional connections, use midpoint
                    return (d.source.x + d.target.x) / 2;
                })
                .attr("y", d => {
                    // Handle self-referencing links
                    if (d.source === d.target) {
                        return d.source.y;
                    }
                    
                    // Check if there's a reverse connection
                    const reverseLink = data.links.find(l => 
                        l.source === d.target && l.target === d.source
                    );
                    
                    // If it's part of a bidirectional connection, offset the label
                    if (reverseLink) {
                        const dx = d.target.x - d.source.x;
                        const dy = d.target.y - d.source.y;
                        // Calculate perpendicular offset
                        const length = Math.sqrt(dx * dx + dy * dy);
                        const offsetY = dx / length * 10; // Perpendicular offset
                        
                        return (d.source.y + d.target.y) / 2 + offsetY;
                    }
                    
                    // For unidirectional connections, use midpoint
                    return (d.source.y + d.target.y) / 2;
                });
                
                // Update group containers
                groupContainers.each(function(d) {
                    // If there are nodes in this group
                    if (d.nodes.length > 0) {
                        let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
                        
                        // Find the bounding box for all nodes in the group
                        d.nodes.forEach(n => {
                            minX = Math.min(minX, n.x - 30);
                            minY = Math.min(minY, n.y - 30);
                            maxX = Math.max(maxX, n.x + 30);
                            maxY = Math.max(maxY, n.y + 40); // Extra space for labels
                        });
                        
                        // Add padding
                        const padding = 20;
                        minX -= padding;
                        minY -= padding;
                        maxX += padding;
                        maxY += padding;
                        
                        // Save group dimensions
                        d.x = minX;
                        d.y = minY;
                        d.width = maxX - minX;
                        d.height = maxY - minY;
                        d.centerX = minX + d.width / 2;
                        d.centerY = minY + d.height / 2;
                        
                        // Set position and size of the group container
                        d3.select(this)
                            .attr("x", minX)
                            .attr("y", minY)
                            .attr("width", d.width)
                            .attr("height", d.height);
                        
                        // Update group label position (top-left of group)
                        groupLabels.filter(g => g.id === d.id)
                            .attr("x", minX + 10)
                            .attr("y", minY + 20);
                    }
                });
                
                // Update group links between flows
                groupLink.attr("d", d => {
                    const sourceGroup = groups[d.source];
                    const targetGroup = groups[d.target];
                    
                    if (!sourceGroup || !targetGroup) return "";
                    
                    // Find intersection points with group boundaries
                    // This ensures links connect to the group's border rather than its center
                    
                    // Calculate centers of groups
                    const sx = sourceGroup.centerX;
                    const sy = sourceGroup.centerY;
                    const tx = targetGroup.centerX;
                    const ty = targetGroup.centerY;
                    
                    // Calculate angle between centers - used to find intersection points
                    const angle = Math.atan2(ty - sy, tx - sx);
                    
                    // Calculate intersection points with source group borders
                    // We cast a ray from center in the direction of the target
                    let sourceX, sourceY;
                    const cosA = Math.cos(angle);
                    const sinA = Math.sin(angle);
                    
                    // Check intersection with horizontal borders (top and bottom)
                    const ts_top = (sourceGroup.y - sy) / sinA;
                    const ts_bottom = (sourceGroup.y + sourceGroup.height - sy) / sinA;
                    
                    // Check intersection with vertical borders (left and right)
                    const ts_left = (sourceGroup.x - sx) / cosA;
                    const ts_right = (sourceGroup.x + sourceGroup.width - sx) / cosA;
                    
                    // Use the closest positive intersection (first hit with the boundary)
                    let t_source = Infinity;
                    if (ts_top > 0) t_source = Math.min(t_source, ts_top);
                    if (ts_bottom > 0) t_source = Math.min(t_source, ts_bottom);
                    if (ts_left > 0) t_source = Math.min(t_source, ts_left);
                    if (ts_right > 0) t_source = Math.min(t_source, ts_right);
                    
                    // Target group: Find intersection in the opposite direction
                    // We cast a ray from target center toward the source
                    let targetX, targetY;
                    const oppositeAngle = angle + Math.PI;
                    const cosOpp = Math.cos(oppositeAngle);
                    const sinOpp = Math.sin(oppositeAngle);
                    
                    // Check intersections for target group
                    const tt_top = (targetGroup.y - ty) / sinOpp;
                    const tt_bottom = (targetGroup.y + targetGroup.height - ty) / sinOpp;
                    const tt_left = (targetGroup.x - tx) / cosOpp;
                    const tt_right = (targetGroup.x + targetGroup.width - tx) / cosOpp;
                    
                    // Use the closest positive intersection
                    let t_target = Infinity;
                    if (tt_top > 0) t_target = Math.min(t_target, tt_top);
                    if (tt_bottom > 0) t_target = Math.min(t_target, tt_bottom);
                    if (tt_left > 0) t_target = Math.min(t_target, tt_left);
                    if (tt_right > 0) t_target = Math.min(t_target, tt_right);
                    
                    // Calculate actual border points using parametric equation:
                    // point = center + t * direction
                    if (t_source !== Infinity) {
                        sourceX = sx + cosA * t_source;
                        sourceY = sy + sinA * t_source;
                    } else {
                        sourceX = sx;
                        sourceY = sy;
                    }
                    
                    if (t_target !== Infinity) {
                        targetX = tx + cosOpp * t_target;
                        targetY = ty + sinOpp * t_target;
                    } else {
                        targetX = tx;
                        targetY = ty;
                    }
                    
                    // Create a straight line between the border points
                    return `M${sourceX},${sourceY} L${targetX},${targetY}`;
                });
                
                // Update group link labels
                groupLinkLabel.attr("x", d => {
                    const sourceGroup = groups[d.source];
                    const targetGroup = groups[d.target];
                    if (!sourceGroup || !targetGroup) return 0;
                    return (sourceGroup.centerX + targetGroup.centerX) / 2;
                })
                .attr("y", d => {
                    const sourceGroup = groups[d.source];
                    const targetGroup = groups[d.target];
                    if (!sourceGroup || !targetGroup) return 0;
                    return (sourceGroup.centerY + targetGroup.centerY) / 2 - 10;
                });
            });
            
            // Drag functions
            function dragstarted(event, d) {
                if (!event.active) simulation.alphaTarget(0.3).restart();
                d.fx = d.x;
                d.fy = d.y;
            }
            
            function dragged(event, d) {
                d.fx = event.x;
                d.fy = event.y;
            }
            
            function dragended(event, d) {
                if (!event.active) simulation.alphaTarget(0);
                d.fx = null;
                d.fy = null;
            }
        });
    </script>
</body>
</html>
"""

    # Replace the placeholders with the actual values
    html_content = html_content.replace("FILENAME_PLACEHOLDER", filename)
    html_content = html_content.replace("TITLE_PLACEHOLDER", html_title)

    # Write HTML to file
    html_path = os.path.join(output_dir, f"{filename}.html")
    with open(html_path, "w") as f:
        f.write(html_content)

    print(f"Visualization created at {html_path}")
    return html_path


def find_free_port():
    """Find a free port on localhost."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("", 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def start_http_server(directory, port=None):
    """Start an HTTP server in the given directory.

    Args:
        directory: Directory to serve files from
        port: Port to use (finds a free port if None)

    Returns:
        tuple: (server_thread, port)
    """
    if port is None:
        port = find_free_port()

    # Get the absolute path of the directory
    directory = str(Path(directory).absolute())

    # Change to the directory to serve files
    os.chdir(directory)

    # Create HTTP server
    handler = http.server.SimpleHTTPRequestHandler
    httpd = socketserver.TCPServer(("", port), handler)

    # Start server in a separate thread
    server_thread = threading.Thread(target=httpd.serve_forever)
    server_thread.daemon = (
        True  # This makes the thread exit when the main program exits
    )
    server_thread.start()

    print(f"Server started at http://localhost:{port}")
    return server_thread, port


def serve_and_open_visualization(html_path, auto_open=True):
    """Serve the HTML file and open it in a browser.

    Args:
        html_path: Path to the HTML file
        auto_open: Whether to automatically open the browser

    Returns:
        tuple: (server_thread, url)
    """
    # Get the directory and filename
    directory = os.path.dirname(os.path.abspath(html_path))
    filename = os.path.basename(html_path)

    # Start the server
    server_thread, port = start_http_server(directory)

    # Build the URL
    url = f"http://localhost:{port}/{filename}"

    # Open the URL in a browser
    if auto_open:
        print(f"Opening {url} in your browser...")
        webbrowser.open(url)
    else:
        print(f"Visualization available at {url}")

    return server_thread, url


def visualize_flow(
    flow: Flow,
    flow_name: str,
    serve: bool = True,
    auto_open: bool = True,
    output_dir: str = "./viz",
    html_title: Optional[str] = None,
) -> Union[str, Tuple[str, Any, str]]:
    """Helper function to visualize a flow with both mermaid and D3.js

    Args:
        flow: Flow object to visualize
        flow_name: Name of the flow (used for filename and display)
        serve: Whether to start a server for the visualization
        auto_open: Whether to automatically open in browser
        output_dir: Directory to save visualization files
        html_title: Custom title for the HTML page (defaults to flow_name if None)

    Returns:
        str or tuple: Path to HTML file, or (path, server_thread, url) if serve=True
    """
    print(f"\n--- {flow_name} Mermaid Diagram ---")
    print(build_mermaid(start=flow))

    print(f"\n--- {flow_name} D3.js Visualization ---")
    json_data = flow_to_json(flow)

    # Create the visualization
    output_filename = f"{flow_name.lower().replace(' ', '_')}"

    # Use flow_name as the HTML title if not specified
    if html_title is None:
        html_title = f"PocketFlow: {flow_name}"

    html_path = create_d3_visualization(
        json_data,
        output_dir=output_dir,
        filename=output_filename,
        html_title=html_title,
    )

    # Serve and open if requested
    if serve:
        server_thread, url = serve_and_open_visualization(html_path, auto_open)
        return html_path, server_thread, url

    return html_path


def load_flow_from_module(module_path: str, flow_variable: str) -> Flow:
    """Dynamically load a flow from a module.

    Args:
        module_path: Path to the module (e.g., 'my_package.my_module')
        flow_variable: Name of the flow variable in the module

    Returns:
        Flow: The loaded flow object
    """
    try:
        module = importlib.import_module(module_path)
        return getattr(module, flow_variable)
    except (ImportError, AttributeError) as e:
        print(f"Error loading flow: {e}")
        sys.exit(1)


# Example usage
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Visualize a PocketFlow flow")
    parser.add_argument(
        "--module", default="async_loop_flow", help="Module containing the flow"
    )
    parser.add_argument(
        "--flow", default="order_pipeline", help="Flow variable name in the module"
    )
    parser.add_argument(
        "--name", default="Flow Visualization", help="Name for the visualization"
    )
    parser.add_argument(
        "--output-dir", default="./viz", help="Directory to save visualization files"
    )
    parser.add_argument("--no-serve", action="store_true", help="Don't start a server")
    parser.add_argument(
        "--no-open", action="store_true", help="Don't open browser automatically"
    )
    parser.add_argument("--title", help="Custom HTML title")

    args = parser.parse_args()

    # Load flow from the specified module
    flow_obj = load_flow_from_module(args.module, args.flow)

    # Visualize the flow
    visualize_flow(
        flow=flow_obj,
        flow_name=args.name,
        serve=not args.no_serve,
        auto_open=not args.no_open,
        output_dir=args.output_dir,
        html_title=args.title,
    )

    # Keep server running if serving
    if not args.no_serve:
        try:
            print("\nServer is running. Press Ctrl+C to stop...")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nShutting down...")
</file>

<file path="cookbook/pocketflow-voice-chat/docs/design.md">
# Design Doc: PocketFlow Voice Chat

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

-   **Goal**: Enable users to interact with an LLM via voice in a continuous conversation, receiving spoken responses.
-   **User Story 1**: As a user, I want to speak my query into a microphone so that the application can understand what I'm asking.
-   **User Story 2**: As a user, I want the application to send my spoken query to an LLM for processing.
-   **User Story 3**: As a user, I want to hear the LLM's response spoken back to me.
-   **User Story 4**: As a user, after hearing the response, I want the application to be ready for my next spoken query without restarting.
-   **Core Functionalities**:
    1.  Capture audio input.
    2.  Convert speech to text (STT).
    3.  Process text with an LLM (maintaining conversation history).
    4.  Convert LLM text response to speech (TTS).
    5.  Play back synthesized audio.
    6.  Loop back to capture new audio input for a continuous conversation.

## Flow Design

> Notes for AI:
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

-   **Workflow**: A sequential workflow with a loop is most appropriate. Each step (audio capture, STT, LLM query, TTS, audio playback) directly follows the previous, and after playback, the flow returns to the audio capture stage.

### Flow high-level Design:

The application will operate in a loop to allow for continuous conversation:
1.  **`CaptureAudioNode`**: Records audio from the user\'s microphone when triggered.
2.  **`SpeechToTextNode`**: Converts the recorded audio into text.
3.  **`QueryLLMNode`**: Sends the transcribed text (with history) to an LLM and gets a text response.
4.  **`TextToSpeechNode`**: Converts the LLM\'s text response into in-memory audio data and then plays it. After completion, the flow transitions back to the `CaptureAudioNode`.

```mermaid
flowchart TD
    CaptureAudio[Capture Audio] --> SpeechToText[Speech to Text]
    SpeechToText --> QueryLLM[Query LLM]
    QueryLLM --> TextToSpeech[Text to Speech & Play]
    TextToSpeech -- "Next Turn" --> CaptureAudio
```

## Utility Functions

> Notes for AI:
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1.  **`record_audio()`** (`utils/audio_utils.py`)
    -   *Input*: (Optional) `sample_rate` (int, Hz, e.g., `DEFAULT_SAMPLE_RATE`), `channels` (int, e.g., `DEFAULT_CHANNELS`), `chunk_size_ms` (int, e.g., `DEFAULT_CHUNK_SIZE_MS`), `silence_threshold_rms` (float, e.g., `DEFAULT_SILENCE_THRESHOLD_RMS`), `min_silence_duration_ms` (int, e.g., `DEFAULT_MIN_SILENCE_DURATION_MS`), `max_recording_duration_s` (int, e.g., `DEFAULT_MAX_RECORDING_DURATION_S`), `pre_roll_chunks_count` (int, e.g., `DEFAULT_PRE_ROLL_CHUNKS`).
    -   *Output*: A tuple `(audio_data, sample_rate)` where `audio_data` is a NumPy array of float32 audio samples, and `sample_rate` is the recording sample rate (int). Returns `(None, sample_rate)` if no speech is detected or recording fails.
    -   *Description*: Records audio from the microphone using silence-based Voice Activity Detection (VAD). Buffers `pre_roll_chunks_count` of audio and starts full recording when sound is detected above `silence_threshold_rms`. Stops after `min_silence_duration_ms` of sound below the threshold or if `max_recording_duration_s` is reached.
    -   *Necessity*: Used by `CaptureAudioNode` to get user\'s voice input.

2.  **`speech_to_text_api(audio_data, sample_rate)`** (`utils/speech_to_text.py`)
    -   *Input*: `audio_data` (bytes), `sample_rate` (int, though the API might infer this from the audio format).
    -   *Output*: `transcribed_text` (str).
    -   *Necessity*: Used by `SpeechToTextNode` to convert in-memory audio data to text.
    -   *Example Model*: OpenAI `gpt-4o-transcribe`.

3.  **`call_llm(messages)`** (`utils/call_llm.py`)
    -   *Input*: `messages` (list of dicts, e.g., `[{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]`). This should be the complete conversation history including the latest user query.
    -   *Output*: `llm_response_text` (str)
    -   *Necessity*: Used by `QueryLLMNode` to get an intelligent response.
    -   *Example Model*: OpenAI `gpt-4o`.

4.  **`text_to_speech_api(text_to_synthesize)`** (`utils/text_to_speech.py`)
    -   *Input*: `text_to_synthesize` (str).
    -   *Output*: A tuple `(audio_data, sample_rate)` where `audio_data` is in-memory audio as bytes (e.g., MP3 format from OpenAI) and `sample_rate` is the audio sample rate (int, e.g., 24000 Hz for OpenAI `gpt-4o-mini-tts`).
    -   *Necessity*: Used by `TextToSpeechNode` to convert LLM text to speakable in-memory audio data.
    -   *Example Model*: OpenAI `gpt-4o-mini-tts`.

5.  **`play_audio_data(audio_data, sample_rate)`** (`utils/audio_utils.py`)
    -   *Input*: `audio_data` (NumPy array of float32 audio samples), `sample_rate` (int).
    -   *Output*: None
    -   *Necessity*: Used by `TextToSpeechNode` (in its `post` method) to play the in-memory synthesized speech.

## Node Design

### Shared Memory

> Notes for AI: Try to minimize data redundancy

The shared memory structure is organized as follows:

```python
shared = {
    "user_audio_data": None,      # In-memory audio data (NumPy array) from user
    "user_audio_sample_rate": None, # int: Sample rate of the user audio
    "chat_history": [],            # list: Conversation history [{"role": "user/assistant", "content": "..."}]
    "continue_conversation": True # boolean: Flag to control the main conversation loop
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1.  **`CaptureAudioNode`**
    -   *Purpose*: Record audio input from the user using VAD.
    -   *Type*: Regular
    -   *Steps*:
        -   *prep*: Check `shared["continue_conversation"]`. (Potentially load VAD parameters from `shared["config"]` if dynamic).
        -   *exec*: Call `utils.audio_utils.record_audio()` (passing VAD parameters if configured). This returns a NumPy array and sample rate.
        -   *post*: `audio_numpy_array, sample_rate = exec_res`. Write `audio_numpy_array` to `shared["user_audio_data"]` and `sample_rate` to `shared["user_audio_sample_rate"]`. Returns `"default"`.

2.  **`SpeechToTextNode`**
    -   *Purpose*: Convert the recorded in-memory audio to text.
    -   *Type*: Regular
    -   *Steps*:
        -   *prep*: Read `shared["user_audio_data"]` (NumPy array) and `shared["user_audio_sample_rate"]`. Return `(user_audio_data_numpy, user_audio_sample_rate)`.
        -   *exec*: `audio_numpy_array, sample_rate = prep_res`. **Convert `audio_numpy_array` to audio `bytes` (e.g., in WAV format using `scipy.io.wavfile.write` to an `io.BytesIO` object).** Call `utils.speech_to_text.speech_to_text_api(audio_bytes, sample_rate)`.
        -   *post*:
            -   Let `transcribed_text = exec_res`.
            -   Append `{"role": "user", "content": transcribed_text}` to `shared["chat_history"]`.
            -   Clear `shared["user_audio_data"]` and `shared["user_audio_sample_rate"]` as they are no longer needed.
            -   Returns `"default"` (assuming STT is successful as per simplification).

3.  **`QueryLLMNode`**
    -   *Purpose*: Get a response from the LLM based on the user's query and conversation history.
    -   *Type*: Regular
    -   *Steps*:
        -   *prep*: Read `shared["chat_history"]`. Return `chat_history`.
        -   *exec*: `history = prep_res`. Call `utils.call_llm.call_llm(messages=history)`.
        -   *post*:
            -   Let `llm_response = exec_res`.
            -   Append `{"role": "assistant", "content": llm_response}` to `shared["chat_history"]`.
            -   Returns `"default"` (assuming LLM call is successful).

4.  **`TextToSpeechNode`**
    -   *Purpose*: Convert the LLM's text response into speech and play it.
    -   *Type*: Regular
    -   *Steps*:
        -   *prep*: Read `shared["chat_history"]`. Identify the last message, which should be the LLM's response. Return its content.
        -   *exec*: `text_to_synthesize = prep_res`. Call `utils.text_to_speech.text_to_speech_api(text_to_synthesize)`. This returns `(llm_audio_bytes, llm_sample_rate)`.
        -   *post*: `llm_audio_bytes, llm_sample_rate = exec_res`.
            -   **Convert `llm_audio_bytes` (e.g., MP3 bytes from TTS API) to a NumPy array of audio samples (e.g., using a library like `pydub` or `soundfile` to decode).**
            -   Call `utils.audio_utils.play_audio_data(llm_audio_numpy_array, llm_sample_rate)`.
            -   (Optional) Log completion.
            -   If `shared["continue_conversation"]` is `True`, return `"next_turn"` to loop back.
            -   Otherwise, return `"end_conversation"`.
</file>

<file path="cookbook/pocketflow-voice-chat/utils/audio_utils.py">
import sounddevice as sd
import numpy as np

DEFAULT_SAMPLE_RATE = 44100
DEFAULT_CHANNELS = 1
DEFAULT_CHUNK_SIZE_MS = 50  # Process audio in 50ms chunks for VAD
DEFAULT_SILENCE_THRESHOLD_RMS = 0.01 # RMS value, needs tuning
DEFAULT_MIN_SILENCE_DURATION_MS = 1000 # 1 second of silence to stop
DEFAULT_MAX_RECORDING_DURATION_S = 15 # Safety cap for recording
DEFAULT_PRE_ROLL_CHUNKS = 3 # Number of chunks to keep before speech starts

def record_audio(sample_rate = DEFAULT_SAMPLE_RATE,
                 channels = DEFAULT_CHANNELS,
                 chunk_size_ms = DEFAULT_CHUNK_SIZE_MS,
                 silence_threshold_rms = DEFAULT_SILENCE_THRESHOLD_RMS,
                 min_silence_duration_ms = DEFAULT_MIN_SILENCE_DURATION_MS,
                 max_recording_duration_s = DEFAULT_MAX_RECORDING_DURATION_S,
                 pre_roll_chunks_count = DEFAULT_PRE_ROLL_CHUNKS):
    """
    Records audio from the microphone with silence-based VAD.
    Returns in-memory audio data (NumPy array of float32) and sample rate.
    Returns (None, sample_rate) if recording fails or max duration is met without speech.
    """
    chunk_size_frames = int(sample_rate * chunk_size_ms / 1000)
    min_silence_chunks = int(min_silence_duration_ms / chunk_size_ms)
    max_chunks = int(max_recording_duration_s * 1000 / chunk_size_ms)

    print(f"Listening... (max {max_recording_duration_s}s). Speak when ready.")
    print(f"(Silence threshold RMS: {silence_threshold_rms}, Min silence duration: {min_silence_duration_ms}ms)")

    recorded_frames = []
    pre_roll_frames = []
    is_recording = False
    silence_counter = 0
    chunks_recorded = 0

    with sd.InputStream(samplerate=sample_rate, channels=channels, dtype='float32') as stream:

        for i in range(max_chunks):
            audio_chunk, overflowed = stream.read(chunk_size_frames)
            if overflowed:
                print("Warning: Audio buffer overflowed!")
            
            rms = np.sqrt(np.mean(audio_chunk**2))

            if is_recording:
                recorded_frames.append(audio_chunk)
                chunks_recorded += 1
                if rms < silence_threshold_rms:
                    silence_counter += 1
                    if silence_counter >= min_silence_chunks:
                        print("Silence detected, stopping recording.")
                        break
                else:
                    silence_counter = 0 # Reset silence counter on sound
            else:
                pre_roll_frames.append(audio_chunk)
                if len(pre_roll_frames) > pre_roll_chunks_count:
                    pre_roll_frames.pop(0)
                
                if rms > silence_threshold_rms:
                    print("Speech detected, starting recording.")
                    is_recording = True
                    for frame_to_add in pre_roll_frames:
                        recorded_frames.append(frame_to_add)
                    chunks_recorded = len(recorded_frames)
                    pre_roll_frames.clear()
            
            if i == max_chunks - 1 and not is_recording:
                print("No speech detected within the maximum recording duration.")
                return None, sample_rate

        if not recorded_frames and is_recording:
            print("Recording started but captured no frames before stopping. This might be due to immediate silence.")

    if not recorded_frames:
        print("No audio was recorded.")
        return None, sample_rate

    audio_data = np.concatenate(recorded_frames)
    print(f"Recording finished. Total duration: {len(audio_data)/sample_rate:.2f}s")
    return audio_data, sample_rate

def play_audio_data(audio_data, sample_rate):
    """Plays in-memory audio data (NumPy array)."""
    try:
        print(f"Playing in-memory audio data (Sample rate: {sample_rate} Hz, Duration: {len(audio_data)/sample_rate:.2f}s)")
        sd.play(audio_data, sample_rate)
        sd.wait()
        print("Playback from memory finished.")
    except Exception as e:
        print(f"Error playing in-memory audio: {e}")


if __name__ == "__main__":
    print("--- Testing audio_utils.py ---")

    # Test 1: record_audio() and play_audio_data() (in-memory)
    print("\n--- Test: Record and Play In-Memory Audio ---")
    print("Please speak into the microphone. Recording will start on sound and stop on silence.")
    recorded_audio, rec_sr = record_audio(
        sample_rate=DEFAULT_SAMPLE_RATE,
        silence_threshold_rms=0.02, 
        min_silence_duration_ms=1500,
        max_recording_duration_s=10
    )

    if recorded_audio is not None and rec_sr is not None:
        print(f"Recorded audio data shape: {recorded_audio.shape}, Sample rate: {rec_sr} Hz")
        play_audio_data(recorded_audio, rec_sr)
    else:
        print("No audio recorded or recording failed.")

    print("\n--- audio_utils.py tests finished. ---")
</file>

<file path="cookbook/pocketflow-voice-chat/utils/call_llm.py">
from openai import OpenAI
import os

def call_llm(messages):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content

if __name__ == "__main__":
    # Test the LLM call
    messages = [{"role": "user", "content": "In a few words, what's the meaning of life?"}]
    response = call_llm(messages)
    print(f"Prompt: {messages[0]['content']}")
    print(f"Response: {response}")
</file>

<file path="cookbook/pocketflow-voice-chat/utils/speech_to_text.py">
import os
from openai import OpenAI
import io

def speech_to_text_api(audio_data: bytes, sample_rate: int):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    # The API expects a file-like object. We can use io.BytesIO for in-memory bytes.
    # We also need to give it a name, as if it were a file upload.
    audio_file = io.BytesIO(audio_data)
    audio_file.name = "audio.wav"  # Corrected to WAV format

    transcript = client.audio.transcriptions.create(
        model="gpt-4o-transcribe",
        file=audio_file
        # language="en" # Optional: specify language ISO-639-1 code
        # prompt="PocketFlow, LLM" # Optional: provide a prompt to guide the model
    )
    return transcript.text

if __name__ == "__main__":
    print("Testing Speech-to-Text API...")
    # The OpenAI client will raise an error if API key is not found or invalid.
    # No explicit check here to keep it minimal.
    test_audio_path = "tts_output.mp3"
    if os.path.exists(test_audio_path):
        print(f"Found {test_audio_path}, using it for STT test.")
        with open(test_audio_path, "rb") as f:
            audio_bytes_for_stt = f.read()
        
        # Sample rate for tts_output.mp3 from our TTS script is 24000
        # but Whisper should ideally infer or handle common formats well.
        stt_sample_rate = 24000 

        transcribed_text = speech_to_text_api(audio_bytes_for_stt, stt_sample_rate)

        if transcribed_text:
            print(f"Transcribed text: {transcribed_text}")
        else:
            print("Failed to transcribe audio (API returned empty data).")
    else:
        print(f"Test audio file '{test_audio_path}' not found.")
        print("Please run the text_to_speech.py test first to generate it, or place your own audio file")
        print(" (e.g., named 'test_audio.mp3') in the same directory as this script and modify the path.")
        print("Make sure it's a common audio format like MP3, WAV, M4A etc.")
</file>

<file path="cookbook/pocketflow-voice-chat/utils/text_to_speech.py">
import os
from openai import OpenAI

def text_to_speech_api(text_to_synthesize: str):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    response = client.audio.speech.create(
        model="gpt-4o-mini-tts",
        voice="alloy", # Other voices: echo, fable, onyx, nova, shimmer
        input=text_to_synthesize,
        response_format="mp3" # Other formats: opus, aac, flac. MP3 is widely supported.
                              # OpenAI default sample rate for tts-1 is 24kHz.
    )
    # The response.content is already bytes (the audio data)
    # Alternatively, for streaming and saving to file: response.stream_to_file("output.mp3")
    audio_data_bytes = response.content
    sample_rate = 24000 # OpenAI TTS model tts-1 outputs 24kHz
    return audio_data_bytes, sample_rate

if __name__ == "__main__":
    print("Testing Text-to-Speech API...")
    # The OpenAI client will raise an error if API key is not found or invalid.
    # No explicit check here to keep it minimal.
    text = "Hello from PocketFlow! This is a test of the text-to-speech functionality."
    audio_bytes, rate = text_to_speech_api(text)
    if audio_bytes and rate:
        print(f"Successfully converted text to speech. Audio data length: {len(audio_bytes)} bytes, Sample rate: {rate} Hz.")
        with open('tts_output.mp3', 'wb') as f:
            f.write(audio_bytes)
        print("Saved TTS output to tts_output.mp3")
    else: 
        print("Failed to convert text to speech (API returned empty data).")
</file>

<file path="cookbook/pocketflow-voice-chat/flow.py">
from pocketflow import Flow
from nodes import CaptureAudioNode, SpeechToTextNode, QueryLLMNode, TextToSpeechNode

def create_voice_chat_flow() -> Flow:
    """Creates and returns the voice chat flow."""
    # Create nodes
    capture_audio = CaptureAudioNode()
    speech_to_text = SpeechToTextNode()
    query_llm = QueryLLMNode()
    text_to_speech = TextToSpeechNode()

    # Define transitions
    capture_audio >> speech_to_text
    speech_to_text >> query_llm
    query_llm >> text_to_speech

    # Loop back for next turn or end
    text_to_speech - "next_turn" >> capture_audio
    # "end_conversation" action from any node will terminate the flow naturally
    # if no transition is defined for it from the current node.
    # Alternatively, one could explicitly transition to an EndNode if desired.

    # Create flow starting with the capture audio node
    voice_chat_flow = Flow(start=capture_audio)
    return voice_chat_flow
</file>

<file path="cookbook/pocketflow-voice-chat/main.py">
from flow import create_voice_chat_flow

def main():
    """Runs the PocketFlow Voice Chat application."""
    print("Starting PocketFlow Voice Chat...")
    print("Speak your query after 'Listening for your query...' appears.")
    print("The conversation will continue until an error occurs or the loop is intentionally stopped.")
    print("To attempt to stop, you might need to cause an error (e.g., silence during capture if not handled by VAD to end gracefully) or modify shared[\"continue_conversation\"] if a mechanism is added.")

    shared = {
        "user_audio_data": None,
        "user_audio_sample_rate": None,
        "chat_history": [],
        "continue_conversation": True # Flag to control the main conversation loop
    }

    # Create the flow
    voice_chat_flow = create_voice_chat_flow()

    # Run the flow
    # The flow will loop based on the "next_turn" action from TextToSpeechNode
    # and the continue_conversation flag checked within nodes or if an error action is returned.
    voice_chat_flow.run(shared)

if __name__ == "__main__":
    main()
</file>

<file path="cookbook/pocketflow-voice-chat/nodes.py">
import numpy as np
import scipy.io.wavfile
import io
import soundfile # For converting MP3 bytes to NumPy array

from pocketflow import Node
from utils.audio_utils import record_audio, play_audio_data
from utils.speech_to_text import speech_to_text_api
from utils.call_llm import call_llm
from utils.text_to_speech import text_to_speech_api

class CaptureAudioNode(Node):
    """Records audio input from the user using VAD."""
    def exec(self, _): # prep_res is not used as per design
        print("\nListening for your query...")
        audio_data, sample_rate = record_audio()
        if audio_data is None:
            return None, None
        return audio_data, sample_rate

    def post(self, shared, prep_res, exec_res):
        audio_numpy_array, sample_rate = exec_res
        if audio_numpy_array is None:
            shared["user_audio_data"] = None
            shared["user_audio_sample_rate"] = None
            print("CaptureAudioNode: Failed to capture audio.")
            return "end_conversation" 

        shared["user_audio_data"] = audio_numpy_array
        shared["user_audio_sample_rate"] = sample_rate
        print(f"Audio captured ({len(audio_numpy_array)/sample_rate:.2f}s), proceeding to STT.")

class SpeechToTextNode(Node):
    """Converts the recorded in-memory audio to text."""
    def prep(self, shared):
        user_audio_data = shared.get("user_audio_data")
        user_audio_sample_rate = shared.get("user_audio_sample_rate")
        if user_audio_data is None or user_audio_sample_rate is None:
            print("SpeechToTextNode: No audio data to process.")
            return None # Signal to skip exec
        return user_audio_data, user_audio_sample_rate

    def exec(self, prep_res):
        if prep_res is None:
            return None # Skip if no audio data

        audio_numpy_array, sample_rate = prep_res
        
        # Convert NumPy array to WAV bytes for the API
        byte_io = io.BytesIO()
        scipy.io.wavfile.write(byte_io, sample_rate, audio_numpy_array)
        wav_bytes = byte_io.getvalue()
        
        print("Converting speech to text...")
        transcribed_text = speech_to_text_api(audio_data=wav_bytes, sample_rate=sample_rate)
        return transcribed_text

    def post(self, shared, prep_res, exec_res):
        if exec_res is None:
            print("SpeechToTextNode: STT API returned no text.")
            return "end_conversation" 

        transcribed_text = exec_res
        print(f"User: {transcribed_text}")
        
        if "chat_history" not in shared:
            shared["chat_history"] = []
        shared["chat_history"].append({"role": "user", "content": transcribed_text})
        
        shared["user_audio_data"] = None
        shared["user_audio_sample_rate"] = None
        return "default"

class QueryLLMNode(Node):
    """Gets a response from the LLM."""
    def prep(self, shared):
        chat_history = shared.get("chat_history", [])
        
        if not chat_history:
            print("QueryLLMNode: Chat history is empty. Skipping LLM call.")
            return None 
        
        return chat_history

    def exec(self, prep_res):
        if prep_res is None: 
            return None 

        chat_history = prep_res
        print("Sending query to LLM...")
        llm_response_text = call_llm(messages=chat_history)
        return llm_response_text

    def post(self, shared, prep_res, exec_res):
        if exec_res is None:
            print("QueryLLMNode: LLM API returned no response.")
            return "end_conversation" 

        llm_response_text = exec_res
        print(f"LLM: {llm_response_text}")
        
        shared["chat_history"].append({"role": "assistant", "content": llm_response_text})
        return "default"

class TextToSpeechNode(Node):
    """Converts the LLM's text response into speech and plays it."""
    def prep(self, shared):
        chat_history = shared.get("chat_history", [])
        if not chat_history:
            print("TextToSpeechNode: Chat history is empty. No LLM response to synthesize.")
            return None
        
        last_message = chat_history[-1]
        if last_message.get("role") == "assistant" and last_message.get("content"):
            return last_message.get("content")
        else:
            print("TextToSpeechNode: Last message not from assistant or no content. Skipping TTS.")
            return None

    def exec(self, prep_res):
        if prep_res is None:
            return None, None
            
        llm_text_response = prep_res
        print("Converting LLM response to speech...")
        llm_audio_bytes, llm_sample_rate = text_to_speech_api(llm_text_response)
        return llm_audio_bytes, llm_sample_rate

    def post(self, shared, prep_res, exec_res):
        if exec_res is None or exec_res[0] is None:
            print("TextToSpeechNode: TTS failed or was skipped.")
            return "next_turn" 

        llm_audio_bytes, llm_sample_rate = exec_res
        
        print("Playing LLM response...")
        try:
            audio_segment, sr_from_file = soundfile.read(io.BytesIO(llm_audio_bytes))
            play_audio_data(audio_segment, sr_from_file)
        except Exception as e:
            print(f"Error playing TTS audio: {e}")
            return "next_turn" 

        if shared.get("continue_conversation", True):
            return "next_turn"
        else:
            print("Conversation ended by user flag.")
            return "end_conversation"
</file>

<file path="cookbook/pocketflow-voice-chat/README.md">
# PocketFlow Voice Chat

This project demonstrates a voice-based interactive chat application built with PocketFlow. Users can speak their queries, and the system will respond with spoken answers from an LLM, maintaining conversation history.

- Check out the [Substack Post Tutorial](https://pocketflow.substack.com/p/build-your-own-voice-chatbot-from) for more!


## Features

-   **Voice Activity Detection (VAD)**: Automatically detects when the user starts and stops speaking.
-   **Speech-to-Text (STT)**: Converts spoken audio into text using OpenAI.
-   **LLM Interaction**: Processes the transcribed text with an LLM (e.g., GPT-4o), maintaining conversation history.
-   **Text-to-Speech (TTS)**: Converts the LLM's text response back into audible speech using OpenAI.
-   **Continuous Conversation**: Loops back to listen for the next user query after responding, allowing for an ongoing dialogue.

## How to Run

1.  **Set your OpenAI API key**:
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```
    Ensure this environment variable is set, as the utility scripts for STT, LLM, and TTS rely on it.
    You can test individual utility functions (e.g., `python utils/call_llm.py`, `python utils/text_to_speech.py`) to help verify your API key and setup.

2.  **Install dependencies**:
    Make sure you have Python installed. Then, install the required libraries using pip:
    ```bash
    pip install -r requirements.txt
    ```
    This will install libraries such as `openai`, `pocketflow`, `sounddevice`, `numpy`, `scipy`, and `soundfile`.

    **Note for Linux users**: `sounddevice` may require PortAudio. If you encounter issues, you might need to install it first:
    ```bash
    sudo apt-get update && sudo apt-get install -y portaudio19-dev
    ```

3.  **Run the application**:
    ```bash
    python main.py
    ```
    Follow the console prompts. The application will start listening when you see "Listening for your query...".

## How It Works

The application uses a PocketFlow workflow to manage the conversation steps:

```mermaid
flowchart TD
    CaptureAudio[Capture Audio] --> SpeechToText[Speech to Text]
    SpeechToText --> QueryLLM[Query LLM]
    QueryLLM --> TextToSpeech[Text to Speech & Play]
    TextToSpeech -- "Next Turn" --> CaptureAudio
```

Here's what each node in the flow does:

1.  **`CaptureAudioNode`**: Records audio from the user's microphone. It uses Voice Activity Detection (VAD) to start recording when speech is detected and stop when silence is detected.
2.  **`SpeechToTextNode`**: Takes the recorded audio data, converts it to a suitable format, and sends it to OpenAI's STT API (gpt-4o-transcribe) to get the transcribed text.
3.  **`QueryLLMNode`**: Takes the transcribed text from the user, along with the existing conversation history, and sends it to an LLM (OpenAI's GPT-4o model) to generate an intelligent response.
4.  **`TextToSpeechNode`**: Receives the text response from the LLM, converts it into audio using OpenAI's TTS API (gpt-4o-mini-tts), and plays the audio back to the user. If the conversation is set to continue, it transitions back to the `CaptureAudioNode`.

## Example Interaction

When you run `main.py`:

1.  The console will display:
    ```
    Starting PocketFlow Voice Chat...
    Speak your query after 'Listening for your query...' appears.
    ...
    ```
2.  When you see `Listening for your query...`, speak clearly into your microphone.
3.  After you stop speaking, the console will show updates:
    ```
    Audio captured (X.XXs), proceeding to STT.
    Converting speech to text...
    User: [Your transcribed query will appear here]
    Sending query to LLM...
    LLM: [The LLM's response text will appear here]
    Converting LLM response to speech...
    Playing LLM response...
    ```
4.  You will hear the LLM's response spoken aloud.
5.  The application will then loop back, and you'll see `Listening for your query...` again, ready for your next input.

The conversation continues in this manner. To stop the application, you typically need to interrupt it (e.g., Ctrl+C in the terminal), as it's designed to loop continuously.
</file>

<file path="cookbook/pocketflow-voice-chat/requirements.txt">
openai
pocketflow
numpy
sounddevice
scipy
soundfile
</file>

<file path="cookbook/pocketflow-workflow/utils/call_llm.py">
import os
from openai import OpenAI

def call_llm(prompt):    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

# Example usage
if __name__ == "__main__":
    print(call_llm("Tell me a short joke"))
</file>

<file path="cookbook/pocketflow-workflow/flow.py">
from pocketflow import Flow
from nodes import GenerateOutline, WriteSimpleContent, ApplyStyle

def create_article_flow():
    """
    Create and configure the article writing workflow
    """
    # Create node instances
    outline_node = GenerateOutline()
    write_node = WriteSimpleContent()
    style_node = ApplyStyle()
    
    # Connect nodes in sequence
    outline_node >> write_node >> style_node
    
    # Create flow starting with outline node
    article_flow = Flow(start=outline_node)
    
    return article_flow
</file>

<file path="cookbook/pocketflow-workflow/main.py">
from flow import create_article_flow

def run_flow(topic="AI Safety"):
    """
    Run the article writing workflow with a specific topic
    
    Args:
        topic (str): The topic for the article
    """
    # Initialize shared data with the topic
    shared = {"topic": topic}
    
    # Print starting message
    print(f"\n=== Starting Article Workflow on Topic: {topic} ===\n")
    
    # Run the flow
    flow = create_article_flow()
    flow.run(shared)
    
    # Output summary
    print("\n=== Workflow Completed ===\n")
    print(f"Topic: {shared['topic']}")
    print(f"Outline Length: {len(shared['outline'])} characters")
    print(f"Draft Length: {len(shared['draft'])} characters")
    print(f"Final Article Length: {len(shared['final_article'])} characters")
    
    return shared

if __name__ == "__main__":
    import sys
    
    # Get topic from command line if provided
    topic = "AI Safety"  # Default topic
    if len(sys.argv) > 1:
        topic = " ".join(sys.argv[1:])
    
    run_flow(topic)
</file>

<file path="cookbook/pocketflow-workflow/nodes.py">
import re
from pocketflow import Node, BatchNode
from utils.call_llm import call_llm
import yaml

class GenerateOutline(Node):
    def prep(self, shared):
        return shared["topic"]
    
    def exec(self, topic):
        prompt = f"""
Create a simple outline for an article about {topic}.
Include at most 3 main sections (no subsections).

Output the sections in YAML format as shown below:

```yaml
sections:
    - |
        First section 
    - |
        Second section
    - |
        Third section
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        structured_result = yaml.safe_load(yaml_str)
        return structured_result
    
    def post(self, shared, prep_res, exec_res):
        # Store the structured data
        shared["outline_yaml"] = exec_res
        
        # Extract sections
        sections = exec_res["sections"]
        shared["sections"] = sections
        
        # Format for display
        formatted_outline = "\n".join([f"{i+1}. {section}" for i, section in enumerate(sections)])
        shared["outline"] = formatted_outline
        
        # Display the results
        print("\n===== OUTLINE (YAML) =====\n")
        print(yaml.dump(exec_res, default_flow_style=False))
        print("\n===== PARSED OUTLINE =====\n")
        print(formatted_outline)
        print("\n=========================\n")
        
        return "default"

class WriteSimpleContent(BatchNode):
    def prep(self, shared):
        # Get the list of sections to process and store for progress tracking
        self.sections = shared.get("sections", [])
        return self.sections
    
    def exec(self, section):
        prompt = f"""
Write a short paragraph (MAXIMUM 100 WORDS) about this section:

{section}

Requirements:
- Explain the idea in simple, easy-to-understand terms
- Use everyday language, avoiding jargon
- Keep it very concise (no more than 100 words)
- Include one brief example or analogy
"""
        content = call_llm(prompt)
        
        # Show progress for this section
        current_section_index = self.sections.index(section) if section in self.sections else 0
        total_sections = len(self.sections)
        print(f"âœ“ Completed section {current_section_index + 1}/{total_sections}: {section}")
        
        return section, content
    
    def post(self, shared, prep_res, exec_res_list):
        # exec_res_list contains [(section, content), (section, content), ...]
        section_contents = {}
        all_sections_content = []
        
        for section, content in exec_res_list:
            section_contents[section] = content
            all_sections_content.append(f"## {section}\n\n{content}\n")
        
        draft = "\n".join(all_sections_content)
        
        # Store the section contents and draft
        shared["section_contents"] = section_contents
        shared["draft"] = draft
        
        print("\n===== SECTION CONTENTS =====\n")
        for section, content in section_contents.items():
            print(f"--- {section} ---")
            print(content)
            print()
        print("===========================\n")
        
        return "default"

class ApplyStyle(Node):
    def prep(self, shared):
        """
        Get the draft from shared data
        """
        return shared["draft"]
    
    def exec(self, draft):
        """
        Apply a specific style to the article
        """
        prompt = f"""
        Rewrite the following draft in a conversational, engaging style:
        
        {draft}
        
        Make it:
        - Conversational and warm in tone
        - Include rhetorical questions that engage the reader
        - Add analogies and metaphors where appropriate
        - Include a strong opening and conclusion
        """
        return call_llm(prompt)
    
    def post(self, shared, prep_res, exec_res):
        """
        Store the final article in shared data
        """
        shared["final_article"] = exec_res
        print("\n===== FINAL ARTICLE =====\n")
        print(exec_res)
        print("\n========================\n")
        return "default"
</file>

<file path="cookbook/pocketflow-workflow/README.md">
# Article Writing Workflow

A PocketFlow example that demonstrates an article writing workflow using a sequence of LLM calls.

## Features

- Generate a simple outline with up to 3 main sections using YAML structured output
- Write concise (100 words max) content for each section in simple terms
- Apply a conversational, engaging style to the final article

## Getting Started

1. Install the required dependencies:

```bash
pip install -r requirements.txt
```

2. Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=your_api_key_here
```

3. Run the application with a default topic ("AI Safety"):

```bash
python main.py
```

4. Or specify your own topic:

```bash
python main.py Climate Change
```

## How It Works

The workflow consists of three sequential nodes:

```mermaid
graph LR
    Outline[Generate Outline] --> Write[Write Content]
    Write --> Style[Apply Style]
```

Here's what each node does:

1. **Generate Outline**: Creates a simple outline with up to 3 main sections using YAML structured output
2. **Write Simple Content**: Writes a concise 100-word explanation for each section
3. **Apply Style**: Rewrites the combined content in a conversational, engaging style

## Files

- [`main.py`](./main.py): Main entry point for running the article workflow
- [`flow.py`](./flow.py): Defines the flow that connects the nodes
- [`nodes.py`](./nodes.py): Contains the node classes for each step in the workflow
- [`utils/call_llm.py`](./utils/call_llm.py): LLM utility function
- [`requirements.txt`](./requirements.txt): Lists the required dependencies

## Example Output

```
=== Starting Article Workflow on Topic: AI Safety ===


===== OUTLINE (YAML) =====

sections:
- Introduction to AI Safety
- Key Challenges in AI Safety
- Strategies for Ensuring AI Safety


===== PARSED OUTLINE =====

1. Introduction to AI Safety
2. Key Challenges in AI Safety
3. Strategies for Ensuring AI Safety

=========================


===== SECTION CONTENTS =====

--- Introduction to AI Safety ---
AI Safety is about making sure that artificial intelligence (AI) systems are helpful and not harmful. Imagine teaching a robot to help with chores. AI Safety is like setting ground rules for the robot so it doesn't accidentally cause trouble, like mistaking a pet for a toy. By ensuring AI systems understand their tasks and limitations, we can trust them to act safely. It's about creating guidelines and checks to ensure AI assists us without unintended consequences.

--- Key Challenges in AI Safety ---
AI safety is about ensuring that artificial intelligence systems operate in ways that are beneficial and not harmful. One key challenge is making sure AI makes decisions that align with human values. Imagine teaching a robot to fetch coffee, but it ends up knocking things over because it doesn't understand the mess it creates. Similarly, if AI systems don't fully grasp human intentions, they might act in unexpected ways. The task is to make AI smart enough to achieve goals without causing problems, much like training a puppy to follow rules without chewing on your shoes.

--- Strategies for Ensuring AI Safety ---
Ensuring AI safety is about making sure artificial intelligence behaves as expected and doesnâ€™t cause harm. Imagine AI as a new driver on the road; we need rules and safeguards to prevent accidents. By testing AI systems under different conditions, setting clear rules for their behavior, and keeping human oversight, we can manage risks. For instance, just as cars have brakes to ensure safety, AI systems need to have fail-safes. This helps in building trust and avoiding unexpected issues, keeping both humans and AI on the right track.

===========================


===== FINAL ARTICLE =====

# Welcome to the World of AI Safety

Have you ever wondered what it would be like to have your very own robot helping you around the house? Sounds like a dream, right? But letâ€™s hit pause for a moment. What if this robot mistook your fluffy cat for a toy? Thatâ€™s exactly where AI Safety comes in. Think of AI Safety as setting some friendly ground rules for your household helper, ensuring that it knows the difference between doing chores and causing a bit of chaos. Itâ€™s all about making sure our AI allies play by the rules, making life easier without those pesky accidental hiccups.

# Navigating the Maze of AI Challenges

Picture this: you've asked your trusty robot to grab you a cup of coffee. But instead, it sends mugs flying and spills coffee because it doesnâ€™t quite get the concept of a mess. Frustrating, isnâ€™t it? One of the biggest hurdles in AI Safety is aligning AI decisions with our human values and intentions. Itâ€™s like training a puppy not to gnaw on your favorite pair of shoes. Our job is to teach AI how to reach its goals without stepping on our toes, all while being as reliable and lovable as a well-trained pup.

# Steering AI Toward Safe Horizons

Now, how do we keep our AI friends on the straight and narrow? Imagine AI as a new driver learning to navigate the roads of life. Just like we teach new drivers the rules of the road and equip cars with brakes for safety, we provide AI with guidelines and fail-safes to prevent any unintended mishaps. Testing AI systems in various scenarios and keeping a watchful human eye on them ensures they donâ€™t veer off track. Itâ€™s all about building trust and creating a partnership where both humans and AI are cruising smoothly together.

# Wrapping It Up

At the end of the day, AI Safety is about creating a harmonious relationship between humans and machines, where we trust our metal companions to support us without the fear of unexpected surprises. By setting boundaries and ensuring understanding, weâ€™re not just building smarter machinesâ€”weâ€™re crafting a future where AI and humanity can thrive together. So, next time youâ€™re imagining that helpful robot assistant, rest easy knowing that AI Safety is making sure it's ready to lend a hand without dropping the ballâ€”or your coffee mug!

========================


=== Workflow Completed ===

Topic: AI Safety
Outline Length: 96 characters
Draft Length: 1690 characters
Final Article Length: 2266 characters
```
</file>

<file path="cookbook/pocketflow-workflow/requirements.txt">
pocketflow>=0.0.1
openai>=1.0.0
pyyaml>=6.0
</file>

<file path="cookbook/pocketflow_demo.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pocketflow\n",
    "! pip install faiss-cpu\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial, sans-serif; font-size: 36px; font-weight: bold; color: #333; margin: 0; padding: 0;\">\n",
    "Cookbook: Pocket Flow + Cursor AI\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; color: #333; margin: 4px 0; padding: 0;\">\n",
    "1. Utility Function\n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333;\">\n",
    "    Utility Functions are the <b>helper functions</b> like <i>calling an LLM, generating embeddings, or using external APIs</i>.  Pocket Flow is deliberately kept minimal and does <b>NOT</b> provide any of these. \n",
    "</p>\n",
    "\n",
    "<p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333;\">\n",
    "But donâ€™t worry: you can simply ask Cursor AI to create them for you. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Help me implement (1) `call_llm` function that takes a prompt and returns the response from the OpenAI gpt-4o model. (2) `get_embedding` function that takes a text and returns the embedding from the OpenAI text-embedding-ada-002 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def call_llm(prompt):\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_embedding(text):\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Example usage:\n",
    "response = call_llm(\"What's the meaning of life?\")\n",
    "print(response)\n",
    "embedding = get_embedding(\"What's the meaning of life?\")\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; color: #333; margin: 4px 0; padding: 0;\">\n",
    "2. Node\n",
    "</p>\n",
    "\n",
    "  <!-- Description of a Node -->\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333;\">\n",
    "    A <strong>Node</strong> is your smallest unit of work with 3 steps \n",
    "    <code>prep-&gt;exec-&gt;post</code>:\n",
    "  </p>\n",
    "\n",
    "<!-- Ordered list of steps with spacing between lines inside each list item -->\n",
    "<ol style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin: 20px 0; padding-left: 20px;\">\n",
    "\n",
    "<li style=\"margin-bottom: 16px;\">\n",
    "    <p style=\"margin: 0 0 8px 0;\">\n",
    "    <code>prep(shared)</code>\n",
    "    </p>\n",
    "    <p style=\"margin: 0 0 8px 0;\">\n",
    "    - Reads and preprocess data from the <strong>shared store</strong>.\n",
    "    </p>\n",
    "    <p style=\"margin: 0;\">\n",
    "    - E.g., load a file, query a database, or turn data into a string.\n",
    "    </p>\n",
    "</li>\n",
    "\n",
    "<li style=\"margin-bottom: 16px;\">\n",
    "    <p style=\"margin: 0 0 8px 0;\">\n",
    "    <code>exec(prep_res)</code>\n",
    "    </p>\n",
    "    <p style=\"margin: 0 0 8px 0;\">\n",
    "    - Executes the core logic\n",
    "    </p>\n",
    "    <p style=\"margin: 0 0 8px 0;\">\n",
    "    - E.g., call an LLM, invoke remote APIs, or embed texts.\n",
    "    </p>\n",
    "</li>\n",
    "\n",
    "<li style=\"margin-bottom: 16px;\">\n",
    "    <p style=\"margin: 0 0 8px 0;\">\n",
    "    <code>post(shared, prep_res, exec_res)</code>\n",
    "    </p>\n",
    "    <p style=\"margin: 0;\">\n",
    "    - Writes data back to the <strong>shared store</strong>.\n",
    "    </p>\n",
    "</li>\n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"data:image/png;base64,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\"\n",
    "  alt=\"My Image\"\n",
    "  width=\"300\"\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Help me implement a single summarization node that reads data from the shared store, calls an LLM to summarize the text into 50 words, and writes the summary back to the shared store. Then, test it with a shared store that have pre-loaded data from `./data/PaulGrahamEssaysLarge/before.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: This essay highlights the counterintuitive nature of startups, emphasizing that instincts often lead to mistakes. Key advice includes trusting instincts about people, not needing deep startup knowledge, and focusing on creating products users want. Startups are all-consuming, best pursued after college, and require openness to learning and serendipity.\n"
     ]
    }
   ],
   "source": [
    "from pocketflow import Node\n",
    "\n",
    "class SummarizeNode(Node):\n",
    "    def prep(self, shared):\n",
    "        # Read data from shared store\n",
    "        return shared[\"data\"][\"before.txt\"]\n",
    "        \n",
    "    def exec(self, text):\n",
    "        # Call LLM to summarize\n",
    "        prompt = f\"Summarize this text in 50 words:\\n\\n{text}\"\n",
    "        return call_llm(prompt)\n",
    "    \n",
    "    def post(self, shared, prep_res, exec_res):\n",
    "        # Store the summary back\n",
    "        shared[\"summary\"] = exec_res\n",
    "        # No specific next action needed\n",
    "        return \"default\"\n",
    "\n",
    "# Create test data\n",
    "shared = {\n",
    "    \"data\": {},\n",
    "    \"summary\": None\n",
    "}\n",
    "\n",
    "# Load the file\n",
    "with open(\"./data/PaulGrahamEssaysLarge/before.txt\", \"r\") as f:\n",
    "    shared[\"data\"][\"before.txt\"] = f.read()\n",
    "\n",
    "# Create and run the node\n",
    "summarize_node = SummarizeNode()\n",
    "summarize_node.run(shared)\n",
    "\n",
    "# Print the result\n",
    "print(\"Summary:\", shared[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <!-- Title -->\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; color: #333; \">\n",
    "    3. Batch\n",
    "  </p>\n",
    "\n",
    "  <!-- Description of Batch processing -->\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin-bottom: 16px;\">\n",
    "    <strong>Batch</strong> helps repeat the same work multiple items. \n",
    "    Instead of calling <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">exec()</code> once, a Batch Node calls \n",
    "    <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">exec()</code> \n",
    "    for each item in a list from <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">prep()</code>. \n",
    "  </p>\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin-bottom: 16px;\">\n",
    "    Think of it as \"item-by-item\" processing:\n",
    "  </p>\n",
    "\n",
    "  <!-- Bullet points -->\n",
    "  <ul style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; list-style-type: disc; padding-left: 20px;\">\n",
    "    <li style=\"margin-bottom: 16px;\">\n",
    "      <code>prep(shared)</code>: Return a list of items.\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 16px;\">\n",
    "      <code>exec(item)</code>: Called once per item.\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 16px;\">\n",
    "      <code>post(shared, item_list, results_list)</code>: Combines all results.\n",
    "    </li>\n",
    "  </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"data:image/png;base64,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\"\n",
    "  alt=\"My Image\"\n",
    "  width=\"150\"\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Help me implement a batch summarization node that reads the list of data from the shared store, calls an LLM to summarize the text into 50 words, and writes the summary back to the shared store. Then, test it with a shared store that have pre-loaded all text files from `./data/PaulGrahamEssaysLarge/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "\n",
      "aord.txt:\n",
      "The text discusses the critical concern of whether startups are \"default alive\" or \"default dead,\" meaning whether they can reach profitability with existing resources. Many founders are unaware of this status. Addressing this concern early is vital since assumptions about easy fundraising can be misleading. Over-hiring is a common pitfall, emphasizing growth over prudent scaling.\n",
      "\n",
      "apple.txt:\n",
      "Apple's App Store approval process is harming its reputation with developers, damaging their goodwill and causing app delays. The approval system, akin to outdated software publishing, obstructs modern iterative app development. This misalignment with programmers' needs risks alienating talented potential employees and developers essential for Apple's platform success.\n",
      "\n",
      "avg.txt:\n",
      "In 1995, Paul Graham and Robert Morris founded Viaweb, a startup enabling users to create online stores. Using Lisp for its innovative capabilities, they gained a competitive edge due to Lisp's rapid development potential. Viaweb's success highlighted Lispâ€™s power, challenging conventional language choices and showcasing unconventional advantages in business.\n",
      "\n",
      "before.txt:\n",
      "The text advises potential startup founders to understand the counterintuitive nature of startups, emphasizing trust in instincts about people, focusing on solving user problems, and avoiding the illusion of gaming the system. It suggests gaining broad knowledge, exploring diverse interests, and delaying startup efforts until post-college to maximize potential and personal growth.\n",
      "\n",
      "addiction.txt:\n",
      "The text discusses the accelerating process of technological progress, leading to more addictive forms of various substances and experiences. It warns that this trend will continue, making it harder to distinguish between beneficial and harmful advancements. Society must adapt by developing new customs to manage increasing addiction, while individuals need to find personal strategies to avoid negative impacts.\n"
     ]
    }
   ],
   "source": [
    "from pocketflow import BatchNode\n",
    "import os\n",
    "\n",
    "class BatchSummarizeNode(BatchNode):\n",
    "    def prep(self, shared):\n",
    "        # Return list of (filename, content) tuples from shared store\n",
    "        return [(fn, content) for fn, content in shared[\"data\"].items()]\n",
    "        \n",
    "    def exec(self, item):\n",
    "        # Unpack the filename and content\n",
    "        filename, text = item\n",
    "        # Call LLM to summarize\n",
    "        prompt = f\"Summarize this text in 50 words:\\n\\n{text}\"\n",
    "        summary = call_llm(prompt)\n",
    "        return filename, summary\n",
    "    \n",
    "    def post(self, shared, prep_res, exec_res_list):\n",
    "        # Store all summaries in a dict by filename\n",
    "        shared[\"summaries\"] = {\n",
    "            filename: summary \n",
    "            for filename, summary in exec_res_list\n",
    "        }\n",
    "        return \"default\"\n",
    "\n",
    "# Create test data structure\n",
    "shared = {\n",
    "    \"data\": {},\n",
    "    \"summaries\": {}\n",
    "}\n",
    "\n",
    "# Load all files from the directory\n",
    "path = \"./data/PaulGrahamEssaysLarge\"\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), \"r\") as f:\n",
    "        shared[\"data\"][filename] = f.read()\n",
    "\n",
    "# Create and run the batch node\n",
    "batch_summarize = BatchSummarizeNode()\n",
    "batch_summarize.run(shared)\n",
    "\n",
    "# Print results\n",
    "print(\"Summaries:\")\n",
    "for filename, summary in shared[\"summaries\"].items():\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <!-- Title -->\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; color: #333; \">\n",
    "    4. Flow\n",
    "  </p>\n",
    "\n",
    "  <!-- Brief description of Flow -->\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin: 4px 0;\">\n",
    "    <strong>Flow</strong> connects your Nodes to a graph.\n",
    "  </p>\n",
    "\n",
    "  <!-- Unordered list of key points -->\n",
    "  <ul style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; list-style-type: disc; margin: 10px 0; padding-left: 20px;\">\n",
    "    <li style=\"margin-bottom: 8px;\">\n",
    "      <strong>Chaining</strong> \n",
    "      (<code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">node_1 &gt;&gt; node_2</code>): Break down complex problems into simple chained steps.\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 8px;\">\n",
    "      <strong>Directed Branching</strong> \n",
    "      (<code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">node_1 - \"action\" -&gt;&gt; node_2</code>): \n",
    "      Agentic decisionsâ€”where a Nodeâ€™s \n",
    "      <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">post()</code> return the action string.\n",
    "    </li>\n",
    "    <li style=\"margin-bottom: 8px;\">\n",
    "      <strong>Set a Start Point</strong>: Create flow by specifying \n",
    "      <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">Flow(start=node_a)</code>. \n",
    "      Then call \n",
    "      <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">flow.run(shared)</code>.\n",
    "    </li>\n",
    "  </ul>\n",
    "\n",
    "  <!-- Closing note -->\n",
    "  <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin: 0; padding: 0;\">\n",
    "    Thatâ€™s it! You can nest Flows, branch your actions, or keep it simple with a straight chain of Nodes.\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"data:image/png;base64,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\"\n",
    "  alt=\"My Image\"\n",
    "  width=\"350\"\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Help me implement a RAG chatbot that, given a userâ€™s input question, finds the most relevant file based on embeddings and then answers the user's question. Test it with a shared store that has preloaded all text files from `./data/PaulGrahamEssaysLarge/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"data:image/png;base64,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\"\n",
    "  alt=\"My Image\"\n",
    "  width=\"500\"\n",
    "/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: how to find startup idea\n",
      "A: To find a startup idea, the context advises not to make a conscious effort to think of startup ideas, as this often results in bad and plausible-sounding ideas that can waste time. Instead, it suggests turning your mind into the type that generates startup ideas unconsciously. This can be achieved by:\n",
      "\n",
      "1. Learning extensively about things that matter.\n",
      "2. Working on problems that genuinely interest you.\n",
      "3. Collaborating with people you like and respect.\n",
      "\n",
      "By engaging in these activities, you'll naturally start to encounter ideas that have the potential to become startups, often without initially realizing it. The essay emphasizes that many successful startups, like Apple, Yahoo, Google, and Facebook, began as side projects rather than direct pursuits to start a company.\n",
      "\n",
      "Source: before.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh2408/.venv/lib/python3.9/site-packages/pocketflow/__init__.py:43: UserWarning: Flow ends: 'end' not found in ['answer', 'end']\n",
      "  if not nxt and curr.successors: warnings.warn(f\"Flow ends: '{action}' not found in {list(curr.successors)}\")\n"
     ]
    }
   ],
   "source": [
    "from pocketflow import Node, Flow\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class PrepareEmbeddings(Node):\n",
    "    def prep(self, shared):\n",
    "        # Get list of (filename, content) pairs\n",
    "        return list(shared[\"data\"].items())\n",
    "        \n",
    "    def exec(self, items):\n",
    "        # Create embeddings for each document\n",
    "        embeddings = []\n",
    "        filenames = []\n",
    "        for filename, content in items:\n",
    "            embedding = get_embedding(content)\n",
    "            embeddings.append(embedding)\n",
    "            filenames.append(filename)\n",
    "            \n",
    "        # Create FAISS index\n",
    "        dim = len(embeddings[0])\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(np.array(embeddings).astype('float32'))\n",
    "        \n",
    "        return index, filenames\n",
    "    \n",
    "    def post(self, shared, prep_res, exec_res):\n",
    "        # Store index and filenames in shared store\n",
    "        index, filenames = exec_res\n",
    "        shared[\"search_index\"] = index\n",
    "        shared[\"filenames\"] = filenames\n",
    "        return \"default\"\n",
    "\n",
    "class FindRelevantDocument(Node):\n",
    "    def prep(self, shared):\n",
    "        # Get user question\n",
    "        question = input(\"Enter your question (or press Enter to quit): \")\n",
    "        if not question:\n",
    "            return None\n",
    "        return question\n",
    "        \n",
    "    def exec(self, question):\n",
    "        if question is None:\n",
    "            return None\n",
    "            \n",
    "        # Get question embedding and search\n",
    "        query_embedding = get_embedding(question)\n",
    "        \n",
    "        # Search for most similar document\n",
    "        D, I = shared[\"search_index\"].search(\n",
    "            np.array([query_embedding]).astype('float32'),\n",
    "            k=1\n",
    "        )\n",
    "        most_relevant_idx = I[0][0]\n",
    "        most_relevant_file = shared[\"filenames\"][most_relevant_idx]\n",
    "        \n",
    "        return question, most_relevant_file\n",
    "        \n",
    "    def post(self, shared, prep_res, exec_res):\n",
    "        if exec_res is None:\n",
    "            return \"end\"\n",
    "            \n",
    "        question, filename = exec_res\n",
    "        shared[\"current_question\"] = question\n",
    "        shared[\"relevant_file\"] = filename\n",
    "        shared[\"context\"] = shared[\"data\"][filename]\n",
    "        return \"answer\"\n",
    "    \n",
    "class AnswerQuestion(Node):\n",
    "    def prep(self, shared):\n",
    "        return (\n",
    "            shared[\"current_question\"],\n",
    "            shared[\"context\"]\n",
    "        )\n",
    "        \n",
    "    def exec(self, inputs):\n",
    "        question, context = inputs\n",
    "        prompt = f\"\"\"\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question based on the context above. If the context doesn't contain relevant information, say so.\n",
    "Answer:\"\"\"\n",
    "        return call_llm(prompt)\n",
    "    \n",
    "    def post(self, shared, prep_res, exec_res):\n",
    "        print(f\"\\nQ: {shared['current_question']}\")\n",
    "        print(f\"A: {exec_res}\")\n",
    "        print(f\"\\nSource: {shared['relevant_file']}\")\n",
    "        return \"continue\"  # Loop back for more questions\n",
    "\n",
    "# Create test data\n",
    "shared = {\"data\": {}}\n",
    "\n",
    "# Load all files\n",
    "path = \"./data/PaulGrahamEssaysLarge\"\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), \"r\") as f:\n",
    "        shared[\"data\"][filename] = f.read()\n",
    "\n",
    "# Create nodes and flow\n",
    "prep_embeddings = PrepareEmbeddings()\n",
    "find_relevant = FindRelevantDocument()\n",
    "answer = AnswerQuestion()\n",
    "\n",
    "# Connect nodes\n",
    "prep_embeddings >> find_relevant\n",
    "find_relevant - \"answer\" >> answer\n",
    "find_relevant - \"end\" >> None\n",
    "answer - \"continue\" >> find_relevant\n",
    "\n",
    "# Create and run flow\n",
    "rag_flow = Flow(start=prep_embeddings)\n",
    "rag_flow.run(shared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="cookbook/README.md">
# Pocket Flow Cookbook


<div align="center">
  
|  Name  | Difficulty    |  Description  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <br> *Dummy*   | A basic chat bot with conversation history |
| [Structured Output](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <br> *Dummy* | Extracting structured data from resumes by prompting |
| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <br> *Dummy*   | A writing workflow that outlines, writes content, and applies styling |
| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <br> *Dummy*   | A research agent that can search the web and answer questions |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <br> *Dummy*   | A simple Retrieval-augmented Generation process |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜†â˜†â˜† <br> *Dummy* | A resume qualification processor using map-reduce pattern for batch evaluation |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <br> *Dummy*   | A real-time LLM streaming demo with user interrupt capability |
| [Chat Guardrail](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <br> *Dummy*  | A travel advisor chatbot that only processes travel-related queries |
| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <br> *Beginner* | A Taboo word game for asynchronous communication between two agents |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <br> *Beginner* | Research agent is getting unreliable... Let's build a supervision process|
| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) | â˜…â˜†â˜† <br> *Beginner*   | A parallel execution demo that shows 3x speedup |
| [Parallel Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <br> *Beginner*   | A parallel image processing demo showing 8x speedup with multiple filters |
| [Thinking](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) | â˜…â˜†â˜† <br> *Beginner*   | Solve complex reasoning problems through Chain-of-Thought |
| [Memory](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) | â˜…â˜†â˜† <br> *Beginner* | A chat bot with short-term and long-term memory |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) | â˜…â˜†â˜† <br> *Beginner* |  Agent using Model Context Protocol for numerical operations |
| [Tracing](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-tracing) | â˜…â˜†â˜† <br> *Beginner* |  Trace and visualize the execution of your flow |

</div>

ðŸ‘€ Want to see other tutorials? [Create an issue!](https://github.com/The-Pocket/PocketFlow/issues/new)
</file>

<file path="docs/_includes/footer_custom.html">
<!-- PocketFlow Chatbot - Start -->
<script>
(function() {
    var script = document.createElement("script");
    script.src = "https://askthispage.com/embed/chatbot.js";
    script.onload = function() {
        initializeChatbot({
            extra_urls: ["https://github.com/The-Pocket/PocketFlow/blob/main/.cursorrules"],
            prefixes: ["https://github.com/The-Pocket","https://the-pocket.github.io/"],
            chatbotName: 'PocketFlow Website Chatbot',
            wsUrl: 'wss://askthispage.com/api/ws/chat',
            instruction: '',
            isOpen: false
        });
    };
    document.head.appendChild(script);
})();
</script>
<!-- PocketFlow Chatbot - End -->
</file>

<file path="docs/core_abstraction/async.md">
---
layout: default
title: "(Advanced) Async"
parent: "Core Abstraction"
nav_order: 5
---

# (Advanced) Async

**Async** Nodes implement `prep_async()`, `exec_async()`, `exec_fallback_async()`, and/or `post_async()`. This is useful for:

1. **prep_async()**: For *fetching/reading data (files, APIs, DB)* in an I/O-friendly way.
2. **exec_async()**: Typically used for async LLM calls.
3. **post_async()**: For *awaiting user feedback*, *coordinating across multi-agents* or any additional async steps after `exec_async()`.

**Note**: `AsyncNode` must be wrapped in `AsyncFlow`. `AsyncFlow` can also include regular (sync) nodes.

### Example

```python
class SummarizeThenVerify(AsyncNode):
    async def prep_async(self, shared):
        # Example: read a file asynchronously
        doc_text = await read_file_async(shared["doc_path"])
        return doc_text

    async def exec_async(self, prep_res):
        # Example: async LLM call
        summary = await call_llm_async(f"Summarize: {prep_res}")
        return summary

    async def post_async(self, shared, prep_res, exec_res):
        # Example: wait for user feedback
        decision = await gather_user_feedback(exec_res)
        if decision == "approve":
            shared["summary"] = exec_res
            return "approve"
        return "deny"

summarize_node = SummarizeThenVerify()
final_node = Finalize()

# Define transitions
summarize_node - "approve" >> final_node
summarize_node - "deny"    >> summarize_node  # retry

flow = AsyncFlow(start=summarize_node)

async def main():
    shared = {"doc_path": "document.txt"}
    await flow.run_async(shared)
    print("Final Summary:", shared.get("summary"))

asyncio.run(main())
```
</file>

<file path="docs/core_abstraction/batch.md">
---
layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4
---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:
- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.


### Example: Summarize a Large File

```python
class MapSummaries(BatchNode):
    def prep(self, shared):
        # Suppose we have a big file; chunk it
        content = shared["data"]
        chunk_size = 10000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        return chunks

    def exec(self, chunk):
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res_list):
        combined = "\n".join(exec_res_list)
        shared["summary"] = combined
        return "default"

map_summaries = MapSummaries()
flow = Flow(start=map_summaries)
flow.run(shared)
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Key Differences from BatchNode

**Important**: Unlike BatchNode, which processes items and modifies the shared store:

1. BatchFlow returns **parameters to pass to the child Flow**, not data to process
2. These parameters are accessed in child nodes via `self.params`, not from the shared store
3. Each child Flow runs independently with a different set of parameters
4. Child nodes can be regular Nodes, not BatchNodes (the batching happens at the Flow level)

### Example: Summarize Many Files

```python
class SummarizeAllFiles(BatchFlow):
    def prep(self, shared):
        # IMPORTANT: Return a list of param dictionaries (not data for processing)
        filenames = list(shared["data"].keys())  # e.g., ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

# Child node that accesses filename from params, not shared store
class LoadFile(Node):
    def prep(self, shared):
        # Access filename from params (not from shared)
        filename = self.params["filename"]  # Important! Use self.params, not shared
        return filename
        
    def exec(self, filename):
        with open(filename, 'r') as f:
            return f.read()
            
    def post(self, shared, prep_res, exec_res):
        # Store file content in shared
        shared["current_file_content"] = exec_res
        return "default"

# Summarize node that works on the currently loaded file
class Summarize(Node):
    def prep(self, shared):
        return shared["current_file_content"]
        
    def exec(self, content):
        prompt = f"Summarize this file in 50 words: {content}"
        return call_llm(prompt)
        
    def post(self, shared, prep_res, exec_res):
        # Store summary in shared, indexed by current filename
        filename = self.params["filename"]  # Again, using params
        if "summaries" not in shared:
            shared["summaries"] = {}
        shared["summaries"][filename] = exec_res
        return "default"

# Create a per-file flow
load_file = LoadFile()
summarize = Summarize()
load_file >> summarize
summarize_file = Flow(start=load_file)

# Wrap in a BatchFlow to process all files
summarize_all_files = SummarizeAllFiles(start=summarize_file)
summarize_all_files.run(shared)
```

### Under the Hood
1. `prep(shared)` in the BatchFlow returns a list of param dictsâ€”e.g., `[{"filename": "file1.txt"}, {"filename": "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlow's own `params` (if any): `{**batch_flow.params, **dict_from_prep}`
   - It calls `flow.run(shared)` using the merged parameters
   - **IMPORTANT**: These parameters are passed to the child Flow's nodes via `self.params`, NOT via the shared store
3. This means the sub-Flow is run **repeatedly**, once for every param dict, with each node in the flow accessing the parameters via `self.params`.

---

## 3. Nested or Multi-Level Batches

You can nest a **BatchFlow** in another **BatchFlow**. For instance:
- **Outer** batch: returns a list of directory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parentâ€™s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

```python

class FileBatchFlow(BatchFlow):
    def prep(self, shared):
        # Access directory from params (set by parent)
        directory = self.params["directory"]
        # e.g., files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(BatchFlow):
    def prep(self, shared):
        directories = [ "/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# The actual processing node
class ProcessFile(Node):
    def prep(self, shared):
        # Access both directory and filename from params
        directory = self.params["directory"]  # From outer batch
        filename = self.params["filename"]    # From inner batch
        full_path = os.path.join(directory, filename)
        return full_path
        
    def exec(self, full_path):
        # Process the file...
        return f"Processed {full_path}"
        
    def post(self, shared, prep_res, exec_res):
        # Store results, perhaps indexed by path
        if "results" not in shared:
            shared["results"] = {}
        shared["results"][prep_res] = exec_res
        return "default"

# Set up the nested batch structure
process_node = ProcessFile()
inner_flow = FileBatchFlow(start=process_node)
outer_flow = DirectoryBatchFlow(start=inner_flow)

# Run it
outer_flow.run(shared)
```
</file>

<file path="docs/core_abstraction/communication.md">
---
layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3
---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)** 

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).  
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
     
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate *Data Schema* from *Compute Logic*!  This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](./batch.md).
     {: .best-practice }

2. **Params (only for [Batch](./batch.md))** 
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:
```python
shared = {"data": {}, "summary": {}, "config": {...}, ...}
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```python
class LoadData(Node):
    def post(self, shared, prep_res, exec_res):
        # We write data to shared store
        shared["data"] = "Some text content"
        return None

class Summarize(Node):
    def prep(self, shared):
        # We read data from shared store
        return shared["data"]

    def exec(self, prep_res):
        # Call LLM to summarize
        prompt = f"Summarize: {prep_res}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res):
        # We write summary to shared store
        shared["summary"] = exec_res
        return "default"

load_data = LoadData()
summarize = Summarize()
load_data >> summarize
flow = Flow(start=load_data)

shared = {}
flow.run(shared)
```

Here:
- `LoadData` writes to `shared["data"]`.
- `Summarize` reads from `shared["data"]`, summarizes, and writes to `shared["summary"]`.

---

## 2. Params

**Params** let you store *per-Node* or *per-Flow* config that doesn't need to live in the shared store. They are:
- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `set_params()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow. 
> 
> If you need to set child node params, see [Batch](./batch.md).
{: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```python
# 1) Create a Node that uses params
class SummarizeFile(Node):
    def prep(self, shared):
        # Access the node's param
        filename = self.params["filename"]
        return shared["data"].get(filename, "")

    def exec(self, prep_res):
        prompt = f"Summarize: {prep_res}"
        return call_llm(prompt)

    def post(self, shared, prep_res, exec_res):
        filename = self.params["filename"]
        shared["summary"][filename] = exec_res
        return "default"

# 2) Set params
node = SummarizeFile()

# 3) Set Node params directly (for testing)
node.set_params({"filename": "doc1.txt"})
node.run(shared)

# 4) Create Flow
flow = Flow(start=node)

# 5) Set Flow params (overwrites node params)
flow.set_params({"filename": "doc2.txt"})
flow.run(shared)  # The node summarizes doc2, not doc1
```
</file>

<file path="docs/core_abstraction/flow.md">
---
layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2
---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `node_a >> node_b`
  This means if `node_a.post()` returns `"default"`, go to `node_b`. 
  (Equivalent to `node_a - "default" >> node_b`)

2. **Named action transition**: `node_a - "action_name" >> node_b`
  This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

It's possible to create loops, branching, or multi-step flows.

## 2. Creating a Flow

A **Flow** begins with a **start** node. You call `Flow(start=some_node)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```python
node_a >> node_b
flow = Flow(start=node_a)
flow.run(shared)
```

- When you run the flow, it executes `node_a`.  
- Suppose `node_a.post()` returns `"default"`.  
- The flow then sees `"default"` Action is linked to `node_b` and runs `node_b`.  
- `node_b.post()` returns `"default"` but we didn't define `node_b >> something_else`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision 
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```python
# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

flow = Flow(start=review)
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action. 
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
> 
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
{: .warning }

## 3. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.  
2. Combine multiple smaller Flows into a larger Flow for reuse.  
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `None` for `exec_res` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```python
# Create a sub-flow
node_a >> node_b
subflow = Flow(start=node_a)

# Connect it to another node
subflow >> node_c

# Create the parent flow
parent_flow = Flow(start=subflow)
```

When `parent_flow.run()` executes:
1. It starts `subflow`
2. `subflow` runs through its nodes (`node_a->node_b`)
3. After `subflow` completes, execution continues to `node_c`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```python
# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow

# Create the master flow
order_pipeline = Flow(start=payment_flow)

# Run the entire pipeline
order_pipeline.run(shared_data)
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```
</file>

<file path="docs/core_abstraction/index.md">
---
layout: default
title: "Core Abstraction"
nav_order: 2
has_children: true
---
</file>

<file path="docs/core_abstraction/node.md">
---
layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1
---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`
   - **Read and preprocess data** from `shared` store. 
   - Examples: *query DB, read files, or serialize data into a string*.
   - Return `prep_res`, which is used by `exec()` and `post()`.

2. `exec(prep_res)`
   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: *(mostly) LLM calls, remote APIs, tool use*.
   - âš ï¸ This shall be only for compute and **NOT** access `shared`.
   - âš ï¸ If retries enabled, ensure idempotent implementation.
   - âš ï¸ Defer exception handling to the Node's built-in retry mechanism.
   - Return `exec_res`, which is passed to `post()`.

3. `post(shared, prep_res, exec_res)`
   - **Postprocess and write data** back to `shared`.
   - Examples: *update DB, change states, log results*.
   - **Decide the next action** by returning a *string* (`action = "default"` if *None*).

> **Why 3 steps?** To enforce the principle of *separation of concerns*. The data storage and data processing are operated separately.
>
> All steps are *optional*. E.g., you can only implement `prep` and `post` if you just need to process data.
{: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting). 
`wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```python 
my_node = SummarizeFile(max_retries=3, wait=10)
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `max_retries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `self.cur_retry`.

```python 
class RetryNode(Node):
    def exec(self, prep_res):
        print(f"Retry {self.cur_retry} times")
        raise Exception("Failed")
```

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```python 
def exec_fallback(self, prep_res, exc):
    raise exc
```

By default, it just re-raises exception. But you can return a fallback result instead, which becomes the `exec_res` passed to `post()`.

### Example: Summarize file

```python 
class SummarizeFile(Node):
    def prep(self, shared):
        return shared["data"]

    def exec(self, prep_res):
        if not prep_res:
            return "Empty file content"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    def exec_fallback(self, prep_res, exc):
        # Provide a simple fallback instead of crashing
        return "There was an error processing your request."

    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # Return "default" by not returning

summarize_node = SummarizeFile(max_retries=3)

# node.run() calls prep->exec->post
# If exec() fails, it retries up to 3 times before calling exec_fallback()
action_result = summarize_node.run(shared)

print("Action returned:", action_result)  # "default"
print("Summary stored:", shared["summary"])
```
</file>

<file path="docs/core_abstraction/parallel.md">
---
layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6
---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple **Async** Nodes and Flows  **concurrently**â€”for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute. 

> Because of Pythonâ€™s GIL, parallel nodes and flows canâ€™t truly parallelize CPU-bound tasks (e.g., heavy numerical computations). However, they excel at overlapping I/O-bound workâ€”like LLM calls, database queries, API requests, or file I/O.
{: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
> 
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism (e.g., semaphores or sleep intervals).
> 
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
{: .best-practice }

## AsyncParallelBatchNode

Like **AsyncBatchNode**, but run `exec_async()` in **parallel**:

```python
class ParallelSummaries(AsyncParallelBatchNode):
    async def prep_async(self, shared):
        # e.g., multiple texts
        return shared["texts"]

    async def exec_async(self, text):
        prompt = f"Summarize: {text}"
        return await call_llm_async(prompt)

    async def post_async(self, shared, prep_res, exec_res_list):
        shared["summary"] = "\n\n".join(exec_res_list)
        return "default"

node = ParallelSummaries()
flow = AsyncFlow(start=node)
```

## AsyncParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using different parameters:

```python
class SummarizeMultipleFiles(AsyncParallelBatchFlow):
    async def prep_async(self, shared):
        return [{"filename": f} for f in shared["files"]]

sub_flow = AsyncFlow(start=LoadAndSummarizeFile())
parallel_flow = SummarizeMultipleFiles(start=sub_flow)
await parallel_flow.run_async(shared)
```
</file>

<file path="docs/design_pattern/agent.md">
---
layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.  
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodesâ€”and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide actionâ€”for example:

```python
f"""
### CONTEXT
Task: {task_description}
Previous Actions: {previous_actions}
Current State: {current_state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Conclude based on the results
  Parameters:
    - result (str): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
```"""
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide *relevant, minimal context.* For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide *a well-structured and unambiguous* set of actionsâ€”avoiding overlap like separate `read_databases` or  `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:
1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

```python
class DecideAction(Node):
    def prep(self, shared):
        context = shared.get("context", "No previous search")
        query = shared["query"]
        return query, context
        
    def exec(self, inputs):
        query, context = inputs
        prompt = f"""
Given input: {query}
Previous search results: {context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
```yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
```"""
        resp = call_llm(prompt)
        yaml_str = resp.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        assert isinstance(result, dict)
        assert "action" in result
        assert "reason" in result
        assert result["action"] in ["search", "answer"]
        if result["action"] == "search":
            assert "search_term" in result
        
        return result

    def post(self, shared, prep_res, exec_res):
        if exec_res["action"] == "search":
            shared["search_term"] = exec_res["search_term"]
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        return shared["search_term"]
        
    def exec(self, search_term):
        return search_web(search_term)
    
    def post(self, shared, prep_res, exec_res):
        prev_searches = shared.get("context", [])
        shared["context"] = prev_searches + [
            {"term": shared["search_term"], "result": exec_res}
        ]
        return "decide"
        
class DirectAnswer(Node):
    def prep(self, shared):
        return shared["query"], shared.get("context", "")
        
    def exec(self, inputs):
        query, context = inputs
        return call_llm(f"Context: {context}\nAnswer: {query}")

    def post(self, shared, prep_res, exec_res):
       print(f"Answer: {exec_res}")
       shared["answer"] = exec_res

# Connect nodes
decide = DecideAction()
search = SearchWeb()
answer = DirectAnswer()

decide - "search" >> search
decide - "answer" >> answer
search - "decide" >> decide  # Loop back

flow = Flow(start=decide)
flow.run({"query": "Who won the Nobel Prize in Physics 2024?"})
```
</file>

<file path="docs/design_pattern/index.md">
---
layout: default
title: "Design Pattern"
nav_order: 3
has_children: true
---
</file>

<file path="docs/design_pattern/mapreduce.md">
---
layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:
- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts. 

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```python
class SummarizeAllFiles(BatchNode):
    def prep(self, shared):
        files_dict = shared["files"]  # e.g. 10 files
        return list(files_dict.items())  # [("file1.txt", "aaa..."), ("file2.txt", "bbb..."), ...]

    def exec(self, one_file):
        filename, file_content = one_file
        summary_text = call_llm(f"Summarize the following file:\n{file_content}")
        return (filename, summary_text)

    def post(self, shared, prep_res, exec_res_list):
        shared["file_summaries"] = dict(exec_res_list)

class CombineSummaries(Node):
    def prep(self, shared):
        return shared["file_summaries"]

    def exec(self, file_summaries):
        # format as: "File1: summary\nFile2: summary...\n"
        text_list = []
        for fname, summ in file_summaries.items():
            text_list.append(f"{fname} summary:\n{summ}\n")
        big_text = "\n---\n".join(text_list)

        return call_llm(f"Combine these file summaries into one final summary:\n{big_text}")

    def post(self, shared, prep_res, final_summary):
        shared["all_files_summary"] = final_summary

batch_node = SummarizeAllFiles()
combine_node = CombineSummaries()
batch_node >> combine_node

flow = Flow(start=batch_node)

shared = {
    "files": {
        "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
        "file2.txt": "Some other interesting text ...",
        # ...
    }
}
flow.run(shared)
print("Individual Summaries:", shared["file_summaries"])
print("\nFinal Summary:\n", shared["all_files_summary"])
```

> **Performance Tip**: The example above works sequentially. You can speed up the map phase by running it in parallel. See [(Advanced) Parallel](../core_abstraction/parallel.md) for more details.
{: .note }
</file>

<file path="docs/design_pattern/multi_agent.md">
---
layout: default
title: "(Advanced) Multi-Agents"
parent: "Design Pattern"
nav_order: 6
---

# (Advanced) Multi-Agents

Multiple [Agents](./flow.md) can work together by handling subtasks and communicating the progress. 
Communication between agents is typically implemented using message queues in shared storage.

> Most of time, you don't need Multi-Agents. Start with a simple solution first.
{: .best-practice }

### Example Agent Communication: Message Queue

Here's a simple example showing how to implement agent communication using `asyncio.Queue`. 
The agent listens for messages, processes them, and continues listening:

```python
class AgentNode(AsyncNode):
    async def prep_async(self, _):
        message_queue = self.params["messages"]
        message = await message_queue.get()
        print(f"Agent received: {message}")
        return message

# Create node and flow
agent = AgentNode()
agent >> agent  # connect to self
flow = AsyncFlow(start=agent)

# Create heartbeat sender
async def send_system_messages(message_queue):
    counter = 0
    messages = [
        "System status: all systems operational",
        "Memory usage: normal",
        "Network connectivity: stable",
        "Processing load: optimal"
    ]
    
    while True:
        message = f"{messages[counter % len(messages)]} | timestamp_{counter}"
        await message_queue.put(message)
        counter += 1
        await asyncio.sleep(1)

async def main():
    message_queue = asyncio.Queue()
    shared = {}
    flow.set_params({"messages": message_queue})
    
    # Run both coroutines
    await asyncio.gather(
        flow.run_async(shared),
        send_system_messages(message_queue)
    )
    
asyncio.run(main())
```

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
```

### Interactive Multi-Agent Example: Taboo Game

Here's a more complex example where two agents play the word-guessing game Taboo. 
One agent provides hints while avoiding forbidden words, and another agent tries to guess the target word:

```python
class AsyncHinter(AsyncNode):
    async def prep_async(self, shared):
        guess = await shared["hinter_queue"].get()
        if guess == "GAME_OVER":
            return None
        return shared["target_word"], shared["forbidden_words"], shared.get("past_guesses", [])

    async def exec_async(self, inputs):
        if inputs is None:
            return None
        target, forbidden, past_guesses = inputs
        prompt = f"Generate hint for '{target}'\nForbidden words: {forbidden}"
        if past_guesses:
            prompt += f"\nPrevious wrong guesses: {past_guesses}\nMake hint more specific."
        prompt += "\nUse at most 5 words."
        
        hint = call_llm(prompt)
        print(f"\nHinter: Here's your hint - {hint}")
        return hint

    async def post_async(self, shared, prep_res, exec_res):
        if exec_res is None:
            return "end"
        await shared["guesser_queue"].put(exec_res)
        return "continue"

class AsyncGuesser(AsyncNode):
    async def prep_async(self, shared):
        hint = await shared["guesser_queue"].get()
        return hint, shared.get("past_guesses", [])

    async def exec_async(self, inputs):
        hint, past_guesses = inputs
        prompt = f"Given hint: {hint}, past wrong guesses: {past_guesses}, make a new guess. Directly reply a single word:"
        guess = call_llm(prompt)
        print(f"Guesser: I guess it's - {guess}")
        return guess

    async def post_async(self, shared, prep_res, exec_res):
        if exec_res.lower() == shared["target_word"].lower():
            print("Game Over - Correct guess!")
            await shared["hinter_queue"].put("GAME_OVER")
            return "end"
            
        if "past_guesses" not in shared:
            shared["past_guesses"] = []
        shared["past_guesses"].append(exec_res)
        
        await shared["hinter_queue"].put(exec_res)
        return "continue"

async def main():
    # Set up game
    shared = {
        "target_word": "nostalgia",
        "forbidden_words": ["memory", "past", "remember", "feeling", "longing"],
        "hinter_queue": asyncio.Queue(),
        "guesser_queue": asyncio.Queue()
    }
    
    print("Game starting!")
    print(f"Target word: {shared['target_word']}")
    print(f"Forbidden words: {shared['forbidden_words']}")

    # Initialize by sending empty guess to hinter
    await shared["hinter_queue"].put("")

    # Create nodes and flows
    hinter = AsyncHinter()
    guesser = AsyncGuesser()

    # Set up flows
    hinter_flow = AsyncFlow(start=hinter)
    guesser_flow = AsyncFlow(start=guesser)

    # Connect nodes to themselves
    hinter - "continue" >> hinter
    guesser - "continue" >> guesser

    # Run both agents concurrently
    await asyncio.gather(
        hinter_flow.run_async(shared),
        guesser_flow.run_async(shared)
    )

asyncio.run(main())
```

The Output:

```
Game starting!
Target word: nostalgia
Forbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']

Hinter: Here's your hint - Thinking of childhood summer days
Guesser: I guess it's - popsicle

Hinter: Here's your hint - When childhood cartoons make you emotional
Guesser: I guess it's - nostalgic

Hinter: Here's your hint - When old songs move you
Guesser: I guess it's - memories

Hinter: Here's your hint - That warm emotion about childhood
Guesser: I guess it's - nostalgia
Game Over - Correct guess!
```
</file>

<file path="docs/design_pattern/rag.md">
---
layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---
## Stage 1: Offline Indexing

We create three Nodes:
1. `ChunkDocs` â€“ [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` â€“ [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` â€“ stores embeddings into a [vector database](../utility_function/vector.md).

```python
class ChunkDocs(BatchNode):
    def prep(self, shared):
        # A list of file paths in shared["files"]. We process each file.
        return shared["files"]

    def exec(self, filepath):
        # read file content. In real usage, do error handling.
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        # chunk by 100 chars each
        chunks = []
        size = 100
        for i in range(0, len(text), size):
            chunks.append(text[i : i + size])
        return chunks
    
    def post(self, shared, prep_res, exec_res_list):
        # exec_res_list is a list of chunk-lists, one per file.
        # flatten them all into a single list of chunks.
        all_chunks = []
        for chunk_list in exec_res_list:
            all_chunks.extend(chunk_list)
        shared["all_chunks"] = all_chunks

class EmbedDocs(BatchNode):
    def prep(self, shared):
        return shared["all_chunks"]

    def exec(self, chunk):
        return get_embedding(chunk)

    def post(self, shared, prep_res, exec_res_list):
        # Store the list of embeddings.
        shared["all_embeds"] = exec_res_list
        print(f"Total embeddings: {len(exec_res_list)}")

class StoreIndex(Node):
    def prep(self, shared):
        # We'll read all embeds from shared.
        return shared["all_embeds"]

    def exec(self, all_embeds):
        # Create a vector index (faiss or other DB in real usage).
        index = create_index(all_embeds)
        return index

    def post(self, shared, prep_res, index):
        shared["index"] = index

# Wire them in sequence
chunk_node = ChunkDocs()
embed_node = EmbedDocs()
store_node = StoreIndex()

chunk_node >> embed_node >> store_node

OfflineFlow = Flow(start=chunk_node)
```

Usage example:

```python
shared = {
    "files": ["doc1.txt", "doc2.txt"],  # any text files
}
OfflineFlow.run(shared)
```

---
## Stage 2: Online Query & Answer

We have 3 nodes:
1. `EmbedQuery` â€“ embeds the userâ€™s question.
2. `RetrieveDocs` â€“ retrieves top chunk from the index.
3. `GenerateAnswer` â€“ calls the LLM with the question + chunk to produce the final answer.

```python
class EmbedQuery(Node):
    def prep(self, shared):
        return shared["question"]

    def exec(self, question):
        return get_embedding(question)

    def post(self, shared, prep_res, q_emb):
        shared["q_emb"] = q_emb

class RetrieveDocs(Node):
    def prep(self, shared):
        # We'll need the query embedding, plus the offline index/chunks
        return shared["q_emb"], shared["index"], shared["all_chunks"]

    def exec(self, inputs):
        q_emb, index, chunks = inputs
        I, D = search_index(index, q_emb, top_k=1)
        best_id = I[0][0]
        relevant_chunk = chunks[best_id]
        return relevant_chunk

    def post(self, shared, prep_res, relevant_chunk):
        shared["retrieved_chunk"] = relevant_chunk
        print("Retrieved chunk:", relevant_chunk[:60], "...")

class GenerateAnswer(Node):
    def prep(self, shared):
        return shared["question"], shared["retrieved_chunk"]

    def exec(self, inputs):
        question, chunk = inputs
        prompt = f"Question: {question}\nContext: {chunk}\nAnswer:"
        return call_llm(prompt)

    def post(self, shared, prep_res, answer):
        shared["answer"] = answer
        print("Answer:", answer)

embed_qnode = EmbedQuery()
retrieve_node = RetrieveDocs()
generate_node = GenerateAnswer()

embed_qnode >> retrieve_node >> generate_node
OnlineFlow = Flow(start=embed_qnode)
```

Usage example:

```python
# Suppose we already ran OfflineFlow and have:
# shared["all_chunks"], shared["index"], etc.
shared["question"] = "Why do people like cats?"

OnlineFlow.run(shared)
# final answer in shared["answer"]
```
</file>

<file path="docs/design_pattern/structure.md">
---
layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:
- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information 

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:
1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

```python
class SummarizeNode(Node):
    def exec(self, prep_res):
        # Suppose `prep_res` is the text to summarize.
        prompt = f"""
Please summarize the following text as YAML, with exactly 3 bullet points

{prep_res}

Now, output:
```yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()

        import yaml
        structured_result = yaml.safe_load(yaml_str)

        assert "summary" in structured_result
        assert isinstance(structured_result["summary"], list)

        return structured_result
```

> Besides using `assert` statements, another popular way to validate schemas is [Pydantic](https://github.com/pydantic/pydantic)
{: .note }

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**  

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**  

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotesâ€”just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.
</file>

<file path="docs/design_pattern/workflow.md">
---
layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be *too complex for one LLM call*.
> - You don't want to make each task **too granular**, because then *the LLM call doesn't have enough context* and results are *not consistent across nodes*.
> 
> You usually need multiple *iterations* to find the *sweet spot*. If the task has too many *edge cases*, consider using [Agents](./agent.md).
{: .best-practice }

### Example: Article Writing

```python
class GenerateOutline(Node):
    def prep(self, shared): return shared["topic"]
    def exec(self, topic): return call_llm(f"Create a detailed outline for an article about {topic}")
    def post(self, shared, prep_res, exec_res): shared["outline"] = exec_res

class WriteSection(Node):
    def prep(self, shared): return shared["outline"]
    def exec(self, outline): return call_llm(f"Write content based on this outline: {outline}")
    def post(self, shared, prep_res, exec_res): shared["draft"] = exec_res

class ReviewAndRefine(Node):
    def prep(self, shared): return shared["draft"]
    def exec(self, draft): return call_llm(f"Review and improve this draft: {draft}")
    def post(self, shared, prep_res, exec_res): shared["final_article"] = exec_res

# Connect nodes
outline = GenerateOutline()
write = WriteSection()
review = ReviewAndRefine()

outline >> write >> review

# Create and run flow
writing_flow = Flow(start=outline)
shared = {"topic": "AI Safety"}
writing_flow.run(shared)
```

For *dynamic cases*, consider using [Agents](./agent.md).
</file>

<file path="docs/utility_function/chunking.md">
---
layout: default
title: "Text Chunking"
parent: "Utility Function"
nav_order: 4
---

# Text Chunking

We recommend some implementations of commonly used text chunking approaches.


> Text Chunking is more a micro optimization, compared to the Flow Design.
> 
> It's recommended to start with the Naive Chunking and optimize later.
{: .best-practice }

---

## Example Python Code Samples

### 1. Naive (Fixed-Size) Chunking
Splits text by a fixed number of words, ignoring sentence or semantic boundaries.

```python
def fixed_size_chunk(text, chunk_size=100):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i : i + chunk_size])
    return chunks
```

However, sentences are often cut awkwardly, losing coherence.

### 2. Sentence-Based Chunking

```python
import nltk

def sentence_based_chunk(text, max_sentences=2):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    for i in range(0, len(sentences), max_sentences):
        chunks.append(" ".join(sentences[i : i + max_sentences]))
    return chunks
```

However, might not handle very long sentences or paragraphs well.

### 3. Other Chunking

- **Paragraph-Based**: Split text by paragraphs (e.g., newlines). Large paragraphs can create big chunks.
- **Semantic**: Use embeddings or topic modeling to chunk by semantic boundaries.
- **Agentic**: Use an LLM to decide chunk boundaries based on context or meaning.
</file>

<file path="docs/utility_function/embedding.md">
---
layout: default
title: "Embedding"
parent: "Utility Function"
nav_order: 5
---

# Embedding

Below you will find an overview table of various text embedding APIs, along with example Python code.

>  Embedding is more a micro optimization, compared to the Flow Design.
> 
> It's recommended to start with the most convenient one and optimize later.
{: .best-practice }


| **API** | **Free Tier** | **Pricing Model** | **Docs** |
| --- | --- | --- | --- |
| **OpenAI** | ~$5 credit | ~$0.0001/1K tokens | [OpenAI Embeddings](https://platform.openai.com/docs/api-reference/embeddings) |
| **Azure OpenAI** | $200 credit | Same as OpenAI (~$0.0001/1K tokens) | [Azure OpenAI Embeddings](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?tabs=portal) |
| **Google Vertex AI** | $300 credit | ~$0.025 / million chars | [Vertex AI Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) |
| **AWS Bedrock** | No free tier, but AWS credits may apply | ~$0.00002/1K tokens (Titan V2) | [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/) |
| **Cohere** | Limited free tier | ~$0.0001/1K tokens | [Cohere Embeddings](https://docs.cohere.com/docs/cohere-embed) |
| **Hugging Face** | ~$0.10 free compute monthly | Pay per second of compute | [HF Inference API](https://huggingface.co/docs/api-inference) |
| **Jina** | 1M tokens free | Pay per token after | [Jina Embeddings](https://jina.ai/embeddings/) |

## Example Python Code

### 1. OpenAI
```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")
response = client.embeddings.create(
    model="text-embedding-ada-002",
    input=text
)
    
# Extract the embedding vector from the response
embedding = response.data[0].embedding
embedding = np.array(embedding, dtype=np.float32)
print(embedding)
```

### 2. Azure OpenAI
```python
import openai

openai.api_type = "azure"
openai.api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com"
openai.api_version = "2023-03-15-preview"
openai.api_key = "YOUR_AZURE_API_KEY"

resp = openai.Embedding.create(engine="ada-embedding", input="Hello world")
vec = resp["data"][0]["embedding"]
print(vec)
```

### 3. Google Vertex AI
```python
from vertexai.preview.language_models import TextEmbeddingModel
import vertexai

vertexai.init(project="YOUR_GCP_PROJECT_ID", location="us-central1")
model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")

emb = model.get_embeddings(["Hello world"])
print(emb[0])
```

### 4. AWS Bedrock
```python
import boto3, json

client = boto3.client("bedrock-runtime", region_name="us-east-1")
body = {"inputText": "Hello world"}
resp = client.invoke_model(modelId="amazon.titan-embed-text-v2:0", contentType="application/json", body=json.dumps(body))
resp_body = json.loads(resp["body"].read())
vec = resp_body["embedding"]
print(vec)
```

### 5. Cohere
```python
import cohere

co = cohere.Client("YOUR_API_KEY")
resp = co.embed(texts=["Hello world"])
vec = resp.embeddings[0]
print(vec)
```

### 6. Hugging Face
```python
import requests

API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
HEADERS = {"Authorization": "Bearer YOUR_HF_TOKEN"}

res = requests.post(API_URL, headers=HEADERS, json={"inputs": "Hello world"})
vec = res.json()[0]
print(vec)
```

### 7. Jina
```python
import requests

url = "https://api.jina.ai/v2/embed"
headers = {"Authorization": "Bearer YOUR_JINA_TOKEN"}
payload = {"data": ["Hello world"], "model": "jina-embeddings-v3"}
res = requests.post(url, headers=headers, json=payload)
vec = res.json()["data"][0]["embedding"]
print(vec)
```
</file>

<file path="docs/utility_function/index.md">
---
layout: default
title: "Utility Function"
nav_order: 4
has_children: true
---
</file>

<file path="docs/utility_function/llm.md">
---
layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1
---

# LLM Wrappers

Check out libraries like [litellm](https://github.com/BerriAI/litellm). 
Here, we provide some minimal example implementations:

1. OpenAI
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_API_KEY_HERE")
        r = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content

    # Example usage
    call_llm("How are you?")
    ```
    > Store the API key in an environment variable like OPENAI_API_KEY for security.
    {: .best-practice }

2. Claude (Anthropic)
    ```python
    def call_llm(prompt):
        from anthropic import Anthropic
        client = Anthropic(api_key="YOUR_API_KEY_HERE")
        r = client.messages.create(
            model="claude-sonnet-4-0",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return r.content[0].text
    ```

3. Google (Generative AI Studio / PaLM API)
    ```python
    def call_llm(prompt):
    from google import genai
    client = genai.Client(api_key='GEMINI_API_KEY')
        response = client.models.generate_content(
        model='gemini-2.5-pro',
        contents=prompt
    )
    return response.text
    ```

4. Azure (Azure OpenAI)
    ```python
    def call_llm(prompt):
        from openai import AzureOpenAI
        client = AzureOpenAI(
            azure_endpoint="https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
            api_key="YOUR_API_KEY_HERE",
            api_version="2023-05-15"
        )
        r = client.chat.completions.create(
            model="<YOUR_DEPLOYMENT_NAME>",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```

5. Ollama (Local LLM)
    ```python
    def call_llm(prompt):
        from ollama import chat
        response = chat(
            model="llama2",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.message.content
    ```
    
6. DeepSeek
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_DEEPSEEK_API_KEY", base_url="https://api.deepseek.com")
        r = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```


## Improvements
Feel free to enhance your `call_llm` function as needed. Here are examples:

- Handle chat history:

```python
def call_llm(messages):
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return r.choices[0].message.content
```

- Add in-memory caching 

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def call_llm(prompt):
    # Your implementation here
    pass
```

> âš ï¸ Caching conflicts with Node retries, as retries yield the same result.
>
> To address this, you could use cached results only if not retried.
{: .warning }


```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_call(prompt):
    pass

def call_llm(prompt, use_cache):
    if use_cache:
        return cached_call(prompt)
    # Call the underlying function directly
    return cached_call.__wrapped__(prompt)

class SummarizeNode(Node):
    def exec(self, text):
        return call_llm(f"Summarize: {text}", self.cur_retry==0)
```

- Enable logging:

```python
def call_llm(prompt):
    import logging
    logging.info(f"Prompt: {prompt}")
    response = ... # Your implementation here
    logging.info(f"Response: {response}")
    return response
```
</file>

<file path="docs/utility_function/text_to_speech.md">
---
layout: default
title: "Text-to-Speech"
parent: "Utility Function"
nav_order: 7
---

# Text-to-Speech

| **Service**          | **Free Tier**         | **Pricing Model**                                            | **Docs**                                                            |
|----------------------|-----------------------|--------------------------------------------------------------|---------------------------------------------------------------------|
| **Amazon Polly**     | 5M std + 1M neural   | ~$4 /M (std), ~$16 /M (neural) after free tier               | [Polly Docs](https://aws.amazon.com/polly/)                         |
| **Google Cloud TTS** | 4M std + 1M WaveNet  | ~$4 /M (std), ~$16 /M (WaveNet) pay-as-you-go                | [Cloud TTS Docs](https://cloud.google.com/text-to-speech)           |
| **Azure TTS**        | 500K neural ongoing  | ~$15 /M (neural), discount at higher volumes                 | [Azure TTS Docs](https://azure.microsoft.com/products/cognitive-services/text-to-speech/) |
| **IBM Watson TTS**   | 10K chars Lite plan  | ~$0.02 /1K (i.e. ~$20 /M). Enterprise options available       | [IBM Watson Docs](https://www.ibm.com/cloud/watson-text-to-speech)   |
| **ElevenLabs**       | 10K chars monthly    | From ~$5/mo (30K chars) up to $330/mo (2M chars). Enterprise  | [ElevenLabs Docs](https://elevenlabs.io)                            |

## Example Python Code

### Amazon Polly
```python
import boto3

polly = boto3.client("polly", region_name="us-east-1",
                     aws_access_key_id="YOUR_AWS_ACCESS_KEY_ID",
                     aws_secret_access_key="YOUR_AWS_SECRET_ACCESS_KEY")

resp = polly.synthesize_speech(
    Text="Hello from Polly!",
    OutputFormat="mp3",
    VoiceId="Joanna"
)

with open("polly.mp3", "wb") as f:
    f.write(resp["AudioStream"].read())
```

### Google Cloud TTS
```python
from google.cloud import texttospeech

client = texttospeech.TextToSpeechClient()
input_text = texttospeech.SynthesisInput(text="Hello from Google Cloud TTS!")
voice = texttospeech.VoiceSelectionParams(language_code="en-US")
audio_cfg = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

resp = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_cfg)

with open("gcloud_tts.mp3", "wb") as f:
    f.write(resp.audio_content)
```

### Azure TTS
```python
import azure.cognitiveservices.speech as speechsdk

speech_config = speechsdk.SpeechConfig(
    subscription="AZURE_KEY", region="AZURE_REGION")
audio_cfg = speechsdk.audio.AudioConfig(filename="azure_tts.wav")

synthesizer = speechsdk.SpeechSynthesizer(
    speech_config=speech_config,
    audio_config=audio_cfg
)

synthesizer.speak_text_async("Hello from Azure TTS!").get()
```

### IBM Watson TTS
```python
from ibm_watson import TextToSpeechV1
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator

auth = IAMAuthenticator("IBM_API_KEY")
service = TextToSpeechV1(authenticator=auth)
service.set_service_url("IBM_SERVICE_URL")

resp = service.synthesize(
    "Hello from IBM Watson!",
    voice="en-US_AllisonV3Voice",
    accept="audio/mp3"
).get_result()

with open("ibm_tts.mp3", "wb") as f:
    f.write(resp.content)
```

### ElevenLabs
```python
import requests

api_key = "ELEVENLABS_KEY"
voice_id = "ELEVENLABS_VOICE"
url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
headers = {"xi-api-key": api_key, "Content-Type": "application/json"}

json_data = {
    "text": "Hello from ElevenLabs!",
    "voice_settings": {"stability": 0.75, "similarity_boost": 0.75}
}

resp = requests.post(url, headers=headers, json=json_data)

with open("elevenlabs.mp3", "wb") as f:
    f.write(resp.content)
```
</file>

<file path="docs/utility_function/vector.md">
---
layout: default
title: "Vector Databases"
parent: "Utility Function"
nav_order: 6
---

# Vector Databases


Below is a  table of the popular vector search solutions:

| **Tool** | **Free Tier** | **Pricing Model** | **Docs** |
| --- | --- | --- | --- |
| **FAISS** | N/A, self-host | Open-source | [Faiss.ai](https://faiss.ai) |
| **Pinecone** | 2GB free | From $25/mo | [pinecone.io](https://pinecone.io) |
| **Qdrant** | 1GB free cloud | Pay-as-you-go | [qdrant.tech](https://qdrant.tech) |
| **Weaviate** | 14-day sandbox | From $25/mo | [weaviate.io](https://weaviate.io) |
| **Milvus** | 5GB free cloud | PAYG or $99/mo dedicated | [milvus.io](https://milvus.io) |
| **Chroma** | N/A, self-host | Free (Apache 2.0) | [trychroma.com](https://trychroma.com) |
| **Redis** | 30MB free | From $5/mo | [redis.io](https://redis.io) |

---
## Example Python Code

Below are basic usage snippets for each tool.

### FAISS
```python
import faiss
import numpy as np

# Dimensionality of embeddings
d = 128

# Create a flat L2 index
index = faiss.IndexFlatL2(d)

# Random vectors
data = np.random.random((1000, d)).astype('float32')
index.add(data)

# Query
query = np.random.random((1, d)).astype('float32')
D, I = index.search(query, k=5)

print("Distances:", D)
print("Neighbors:", I)
```

### Pinecone
```python
import pinecone

pinecone.init(api_key="YOUR_API_KEY", environment="YOUR_ENV")

index_name = "my-index"

# Create the index if it doesn't exist
if index_name not in pinecone.list_indexes():
    pinecone.create_index(name=index_name, dimension=128)

# Connect
index = pinecone.Index(index_name)

# Upsert
vectors = [
    ("id1", [0.1]*128),
    ("id2", [0.2]*128)
]
index.upsert(vectors)

# Query
response = index.query([[0.15]*128], top_k=3)
print(response)
```

### Qdrant
```python
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct

client = qdrant_client.QdrantClient(
    url="https://YOUR-QDRANT-CLOUD-ENDPOINT",
    api_key="YOUR_API_KEY"
)

collection = "my_collection"
client.recreate_collection(
    collection_name=collection,
    vectors_config=VectorParams(size=128, distance=Distance.COSINE)
)

points = [
    PointStruct(id=1, vector=[0.1]*128, payload={"type": "doc1"}),
    PointStruct(id=2, vector=[0.2]*128, payload={"type": "doc2"}),
]

client.upsert(collection_name=collection, points=points)

results = client.search(
    collection_name=collection,
    query_vector=[0.15]*128,
    limit=2
)
print(results)
```

### Weaviate
```python
import weaviate

client = weaviate.Client("https://YOUR-WEAVIATE-CLOUD-ENDPOINT")

schema = {
    "classes": [
        {
            "class": "Article",
            "vectorizer": "none"
        }
    ]
}
client.schema.create(schema)

obj = {
    "title": "Hello World",
    "content": "Weaviate vector search"
}
client.data_object.create(obj, "Article", vector=[0.1]*128)

resp = (
    client.query
    .get("Article", ["title", "content"])
    .with_near_vector({"vector": [0.15]*128})
    .with_limit(3)
    .do()
)
print(resp)
```

### Milvus
```python
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection
import numpy as np

connections.connect(alias="default", host="localhost", port="19530")

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
]
schema = CollectionSchema(fields)
collection = Collection("MyCollection", schema)

emb = np.random.rand(10, 128).astype('float32')
ids = list(range(10))
collection.insert([ids, emb])

index_params = {
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128},
    "metric_type": "L2"
}
collection.create_index("embedding", index_params)
collection.load()

query_emb = np.random.rand(1, 128).astype('float32')
results = collection.search(query_emb, "embedding", param={"nprobe": 10}, limit=3)
print(results)
```

### Chroma
```python
import chromadb
from chromadb.config import Settings

client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_data"
))

coll = client.create_collection("my_collection")

vectors = [[0.1, 0.2, 0.3], [0.2, 0.2, 0.2]]
metas = [{"doc": "text1"}, {"doc": "text2"}]
ids = ["id1", "id2"]
coll.add(embeddings=vectors, metadatas=metas, ids=ids)

res = coll.query(query_embeddings=[[0.15, 0.25, 0.3]], n_results=2)
print(res)
```

### Redis
```python
import redis
import struct

r = redis.Redis(host="localhost", port=6379)

# Create index
r.execute_command(
    "FT.CREATE", "my_idx", "ON", "HASH",
    "SCHEMA", "embedding", "VECTOR", "FLAT", "6",
    "TYPE", "FLOAT32", "DIM", "128",
    "DISTANCE_METRIC", "L2"
)

# Insert
vec = struct.pack('128f', *[0.1]*128)
r.hset("doc1", mapping={"embedding": vec})

# Search
qvec = struct.pack('128f', *[0.15]*128)
q = "*=>[KNN 3 @embedding $BLOB AS dist]"
res = r.ft("my_idx").search(q, query_params={"BLOB": qvec})
print(res.docs)
```
</file>

<file path="docs/utility_function/viz.md">
---
layout: default
title: "Viz and Debug"
parent: "Utility Function"
nav_order: 2
---

# Visualization and Debugging

Similar to LLM wrappers, we **don't** provide built-in visualization and debugging. Here, we recommend some *minimal* (and incomplete) implementations These examples can serve as a starting point for your own tooling.

## 1. Visualization with Mermaid

This code recursively traverses the nested graph, assigns unique IDs to each node, and treats Flow nodes as subgraphs to generate Mermaid syntax for a hierarchical visualization.

{% raw %}
```python
def build_mermaid(start):
    ids, visited, lines = {}, set(), ["graph LR"]
    ctr = 1
    def get_id(n):
        nonlocal ctr
        return ids[n] if n in ids else (ids.setdefault(n, f"N{ctr}"), (ctr := ctr + 1))[0]
    def link(a, b):
        lines.append(f"    {a} --> {b}")
    def walk(node, parent=None):
        if node in visited:
            return parent and link(parent, get_id(node))
        visited.add(node)
        if isinstance(node, Flow):
            node.start_node and parent and link(parent, get_id(node.start_node))
            lines.append(f"\n    subgraph sub_flow_{get_id(node)}[{type(node).__name__}]")
            node.start_node and walk(node.start_node)
            for nxt in node.successors.values():
                node.start_node and walk(nxt, get_id(node.start_node)) or (parent and link(parent, get_id(nxt))) or walk(nxt)
            lines.append("    end\n")
        else:
            lines.append(f"    {(nid := get_id(node))}['{type(node).__name__}']")
            parent and link(parent, nid)
            [walk(nxt, nid) for nxt in node.successors.values()]
    walk(start)
    return "\n".join(lines)
```
{% endraw %}


For example, suppose we have a complex Flow for data science:

```python
class DataPrepBatchNode(BatchNode):
    def prep(self,shared): return []
class ValidateDataNode(Node): pass
class FeatureExtractionNode(Node): pass
class TrainModelNode(Node): pass
class EvaluateModelNode(Node): pass
class ModelFlow(Flow): pass
class DataScienceFlow(Flow):pass

feature_node = FeatureExtractionNode()
train_node = TrainModelNode()
evaluate_node = EvaluateModelNode()
feature_node >> train_node >> evaluate_node
model_flow = ModelFlow(start=feature_node)
data_prep_node = DataPrepBatchNode()
validate_node = ValidateDataNode()
data_prep_node >> validate_node >> model_flow
data_science_flow = DataScienceFlow(start=data_prep_node)
result = build_mermaid(start=data_science_flow)
```

The code generates a Mermaid diagram:

```mermaid
graph LR
    subgraph sub_flow_N1[DataScienceFlow]
    N2['DataPrepBatchNode']
    N3['ValidateDataNode']
    N2 --> N3
    N3 --> N4

    subgraph sub_flow_N5[ModelFlow]
    N4['FeatureExtractionNode']
    N6['TrainModelNode']
    N4 --> N6
    N7['EvaluateModelNode']
    N6 --> N7
    end

    end
```

For visualization based on d3.js, check out [the cookbook](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-visualization).

## 2. Call Stack Debugging

It would be useful to print the Node call stacks for debugging. This can be achieved by inspecting the runtime call stack:

```python
import inspect

def get_node_call_stack():
    stack = inspect.stack()
    node_names = []
    seen_ids = set()
    for frame_info in stack[1:]:
        local_vars = frame_info.frame.f_locals
        if 'self' in local_vars:
            caller_self = local_vars['self']
            if isinstance(caller_self, BaseNode) and id(caller_self) not in seen_ids:
                seen_ids.add(id(caller_self))
                node_names.append(type(caller_self).__name__)
    return node_names
```

For example, suppose we have a complex Flow for data science:

```python
class DataPrepBatchNode(BatchNode): 
    def prep(self, shared): return []
class ValidateDataNode(Node): pass
class FeatureExtractionNode(Node): pass
class TrainModelNode(Node): pass
class EvaluateModelNode(Node): 
    def prep(self, shared):
        stack = get_node_call_stack()
        print("Call stack:", stack)
class ModelFlow(Flow): pass
class DataScienceFlow(Flow):pass

feature_node = FeatureExtractionNode()
train_node = TrainModelNode()
evaluate_node = EvaluateModelNode()
feature_node >> train_node >> evaluate_node
model_flow = ModelFlow(start=feature_node)
data_prep_node = DataPrepBatchNode()
validate_node = ValidateDataNode()
data_prep_node >> validate_node >> model_flow
data_science_flow = DataScienceFlow(start=data_prep_node)
data_science_flow.run({})
```

The output would be: `Call stack: ['EvaluateModelNode', 'ModelFlow', 'DataScienceFlow']`

For a more complete implementation, check out [the cookbook](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-tracing).
</file>

<file path="docs/utility_function/websearch.md">
---
layout: default
title: "Web Search"
parent: "Utility Function"
nav_order: 3
---
# Web Search

We recommend some implementations of commonly used web search tools.

| **API**                         | **Free Tier**                                | **Pricing Model**                                              | **Docs**                                                  |
|---------------------------------|-----------------------------------------------|-----------------------------------------------------------------|------------------------------------------------------------------------|
| **Google Custom Search JSON API** | 100 queries/day free       | $5 per 1000 queries.           | [Link](https://developers.google.com/custom-search/v1/overview)        |
| **Bing Web Search API**         | 1,000 queries/month               | $15â€“$25 per 1,000 queries. | [Link](https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/) |
| **DuckDuckGo Instant Answer**   | Completely free (Instant Answers only, **no URLs**) | No paid plans; usage unlimited, but data is limited             | [Link](https://duckduckgo.com/api)                                     |
| **Brave Search API**         | 2,000 queries/month free | $3 per 1k queries for Base, $5 per 1k for Pro | [Link](https://brave.com/search/api/)                                  |
| **SerpApi**              | 100 searches/month free            | Start at $75/month for 5,000 searches| [Link](https://serpapi.com/)                                             |
| **RapidAPI**           | Many  options    | Many  options             | [Link](https://rapidapi.com/search?term=search&sortBy=ByRelevance)      |

## Example Python Code

### 1. Google Custom Search JSON API
```python
import requests

API_KEY = "YOUR_API_KEY"
CX_ID = "YOUR_CX_ID"
query = "example"

url = "https://www.googleapis.com/customsearch/v1"
params = {
    "key": API_KEY,
    "cx": CX_ID,
    "q": query
}

response = requests.get(url, params=params)
results = response.json()
print(results)
```

### 2. Bing Web Search API
```python
import requests

SUBSCRIPTION_KEY = "YOUR_BING_API_KEY"
query = "example"

url = "https://api.bing.microsoft.com/v7.0/search"
headers = {"Ocp-Apim-Subscription-Key": SUBSCRIPTION_KEY}
params = {"q": query}

response = requests.get(url, headers=headers, params=params)
results = response.json()
print(results)
```

### 3. DuckDuckGo Instant Answer
```python
import requests

query = "example"
url = "https://api.duckduckgo.com/"
params = {
    "q": query,
    "format": "json"
}

response = requests.get(url, params=params)
results = response.json()
print(results)
```

### 4. Brave Search API
```python
import requests

SUBSCRIPTION_TOKEN = "YOUR_BRAVE_API_TOKEN"
query = "example"

url = "https://api.search.brave.com/res/v1/web/search"
headers = {
    "X-Subscription-Token": SUBSCRIPTION_TOKEN
}
params = {
    "q": query
}

response = requests.get(url, headers=headers, params=params)
results = response.json()
print(results)
```

### 5. SerpApi
```python
import requests

API_KEY = "YOUR_SERPAPI_KEY"
query = "example"

url = "https://serpapi.com/search"
params = {
    "engine": "google",
    "q": query,
    "api_key": API_KEY
}

response = requests.get(url, params=params)
results = response.json()
print(results)
```
</file>

<file path="docs/_config.yml">
# Basic site settings
title: Pocket Flow
tagline: A 100-line LLM framework
description: Pocket Flow â€“ Minimalist LLM Framework in 100 Lines, Enabling LLMs to Program Themselves

# Theme settings
remote_theme: just-the-docs/just-the-docs
search_enabled: true

# SEO & sitemap
plugins:
  - jekyll-seo-tag
  - jekyll-sitemap

jekyll-seo-tag:
  social:
    name: "Pocket Flow"
    twitter: "ZacharyHuang12"
    github: "the-pocket/PocketFlow"

# Navigation
nav_sort: case_sensitive

# Aux links (shown in upper right)
aux_links:
  "View on GitHub":
    - "//github.com/the-pocket/PocketFlow"
    
# Color scheme
color_scheme: light

# Author settings
author:
    name: Zachary Huang
    url: https://www.columbia.edu/~zh2408/
    twitter: ZacharyHuang12

# Mermaid settings
mermaid:
  version: "9.1.3"
  config: |
    directionLR

# Callouts settings
callouts:
  warning:
    title: Warning
    color: red
  note:
    title: Note
    color: blue
  best-practice:
    title: Best Practice
    color: green
  
# Custom navigation
nav:
  - Home: index.md
  - GitHub: "https://github.com/the-pocket/PocketFlow"
  - Discord: "https://discord.gg/hUHHE9Sa6T"
</file>

<file path="docs/guide.md">
---
layout: default
title: "Agentic Coding"
---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agent involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
{: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps                  | Human      | AI        | Comment                                                                 |
|:-----------------------|:----------:|:---------:|:------------------------------------------------------------------------|
| 1. Requirements | â˜…â˜…â˜… High  | â˜…â˜†â˜† Low   | Humans understand the requirements and context.                    |
| 2. Flow          | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium |  Humans specify the high-level design, and the AI fills in the details. |
| 3. Utilities   | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Data          | â˜…â˜†â˜† Low    | â˜…â˜…â˜… High   | AI designs the data schema, and humans verify.                            |
| 5. Node          | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  | The AI helps design the node based on the flow.          |
| 6. Implementation      | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI implements the flow based on the design. |
| 7. Optimization        | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans evaluate the results, and the AI helps optimize. |
| 8. Reliability         | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI writes test cases and addresses corner cases.     |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit. 
    - Understand AI systems' strengths and limitations:
      - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
      - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
      - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
    - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
    - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.
    - Identify applicable design patterns (e.g., [Map Reduce](./design_pattern/mapreduce.md), [Agent](./design_pattern/agent.md), [RAG](./design_pattern/rag.md)).
      - For each node in the flow, start with a high-level one-line description of what it does.
      - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
      - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
      - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
    - Outline the flow and draw it in a mermaid diagram. For example:
      ```mermaid
      flowchart LR
          start[Start] --> batch[Batch]
          batch --> check[Check]
          check -->|OK| process
          check -->|Error| fix[Fix]
          fix --> check
          
          subgraph process[Process]
            step1[Step 1] --> step2[Step 2]
          end
          
          process --> endNode[End]
      ```
    - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
      {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.
    - Think of your AI system as the brain. It needs a bodyâ€”these *external utility functions*â€”to interact with the real world:
        <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

        - Reading inputs (e.g., retrieving Slack messages, reading emails)
        - Writing outputs (e.g., generating reports, sending emails)
        - Using external tools (e.g., calling LLMs, searching the web)
        - **NOTE**: *LLM-based tasks* (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are *core functions* internal in the AI system.
    - For each utility function, implement it and write a simple test.
    - Document their input/output, as well as why they are necessary. For example:
      - `name`: `get_embedding` (`utils/get_embedding.py`)
      - `input`: `str`
      - `output`: a vector of 3072 floats
      - `necessity`: Used by the second node to embed text
    - Example utility implementation:
      ```python
      # utils/call_llm.py
      from openai import OpenAI

      def call_llm(prompt):    
          client = OpenAI(api_key="YOUR_API_KEY_HERE")
          r = client.chat.completions.create(
              model="gpt-4o",
              messages=[{"role": "user", "content": prompt}]
          )
          return r.choices[0].message.content
          
      if __name__ == "__main__":
          prompt = "What is the meaning of life?"
          print(call_llm(prompt))
      ```
    - > **Sometimes, design Utilities before Flow:**  For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
      {: .best-practice }
    - > **Avoid Exception Handling in Utilities**: If a utility function is called from a Node's `exec()` method, avoid using `try...except` blocks within the utility. Let the Node's built-in retry mechanism handle failures.
      {: .warning }

4. **Data Design**: Design the shared store that nodes will use to communicate.
   - One core design principle for PocketFlow is to use a well-designed [shared store](./core_abstraction/communication.md)â€”a data contract that all nodes agree upon to retrieve and store data.
      - For simple systems, use an in-memory dictionary.
      - For more complex systems or when persistence is required, use a database.
      - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
      - Example shared store design:
        ```python
        shared = {
            "user": {
                "id": "user123",
                "context": {                # Another nested dict
                    "weather": {"temp": 72, "condition": "sunny"},
                    "location": "San Francisco"
                }
            },
            "results": {}                   # Empty dict to store outputs
        }
        ```

5. **Node Design**: Plan how each node will read and write data, and use utility functions.
   - For each [Node](./core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Regular (or Batch, or Async)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function. **Avoid exception handling here**; let the Node's retry mechanism manage failures.
     - `post`: Write "embedding" to the shared store

6. **Implementation**: Implement the initial nodes and flows based on the design.
   - ðŸŽ‰ If you've reached this step, humans have finished the design. Now *Agentic Coding* begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Leverage the built-in [Node](./core_abstraction/node.md) retry and fallback mechanisms to handle failures gracefully. This helps you quickly identify weak points in the system.
   - Add logging throughout the code to facilitate debugging.

7. **Optimization**:
   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:
     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3â€“6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     {: .best-practice }

8. **Reliability**  
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `max_retries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my_project/
â”œâ”€â”€ main.py
â”œâ”€â”€ nodes.py
â”œâ”€â”€ flow.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ call_llm.py
â”‚   â””â”€â”€ search_web.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs/
    â””â”€â”€ design.md
```

- **`requirements.txt`**: Lists the Python dependencies for the project.
  ```
  PyYAML
  pocketflow
  ```

- **`docs/design.md`**: Contains project documentation for each step above. This should be *high-level* and *no-code*.
  ~~~
  # Design Doc: Your Project Name

  > Please DON'T remove notes for AI

  ## Requirements

  > Notes for AI: Keep it simple and clear.
  > If the requirements are abstract, write concrete user stories


  ## Flow Design

  > Notes for AI:
  > 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
  > 2. Present a concise, high-level description of the workflow.

  ### Applicable Design Pattern:

  1. Map the file summary into chunks, then reduce these chunks into a final summary.
  2. Agentic file finder
    - *Context*: The entire summary of the file
    - *Action*: Find the file

  ### Flow high-level Design:

  1. **First Node**: This node is for ...
  2. **Second Node**: This node is for ...
  3. **Third Node**: This node is for ...

  ```mermaid
  flowchart TD
      firstNode[First Node] --> secondNode[Second Node]
      secondNode --> thirdNode[Third Node]
  ```
  ## Utility Functions

  > Notes for AI:
  > 1. Understand the utility function definition thoroughly by reviewing the doc.
  > 2. Include only the necessary utility functions, based on nodes in the flow.

  1. **Call LLM** (`utils/call_llm.py`)
    - *Input*: prompt (str)
    - *Output*: response (str)
    - Generally used by most nodes for LLM tasks

  2. **Embedding** (`utils/get_embedding.py`)
    - *Input*: str
    - *Output*: a vector of 3072 floats
    - Used by the second node to embed text

  ## Node Design

  ### Shared Store

  > Notes for AI: Try to minimize data redundancy

  The shared store structure is organized as follows:

  ```python
  shared = {
      "key": "value"
  }
  ```

  ### Node Steps

  > Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

  1. First Node
    - *Purpose*: Provide a short explanation of the nodeâ€™s function
    - *Type*: Decide between Regular, Batch, or Async
    - *Steps*:
      - *prep*: Read "key" from the shared store
      - *exec*: Call the utility function
      - *post*: Write "key" to the shared store

  2. Second Node
    ...
  ~~~


- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one Python file to each API call, for example `call_llm.py` or `search_web.py`.
  - Each file should also include a `main()` function to try that API call
  ```python
  from google import genai
  import os

  def call_llm(prompt: str) -> str:
      client = genai.Client(
          api_key=os.getenv("GEMINI_API_KEY", ""),
      )
      model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
      response = client.models.generate_content(model=model, contents=[prompt])
      return response.text

  if __name__ == "__main__":
      test_prompt = "Hello, how are you?"

      # First call - should hit the API
      print("Making call...")
      response1 = call_llm(test_prompt, use_cache=False)
      print(f"Response: {response1}")
  ```

- **`nodes.py`**: Contains all the node definitions.
  ```python
  # nodes.py
  from pocketflow import Node
  from utils.call_llm import call_llm

  class GetQuestionNode(Node):
      def exec(self, _):
          # Get question directly from user input
          user_question = input("Enter your question: ")
          return user_question
      
      def post(self, shared, prep_res, exec_res):
          # Store the user's question
          shared["question"] = exec_res
          return "default"  # Go to the next node

  class AnswerNode(Node):
      def prep(self, shared):
          # Read question from shared
          return shared["question"]
      
      def exec(self, question):
          # Call LLM to get the answer
          return call_llm(question)
      
      def post(self, shared, prep_res, exec_res):
          # Store the answer in shared
          shared["answer"] = exec_res
  ```
- **`flow.py`**: Implements functions that create flows by importing node definitions and connecting them.
  ```python
  # flow.py
  from pocketflow import Flow
  from nodes import GetQuestionNode, AnswerNode

  def create_qa_flow():
      """Create and return a question-answering flow."""
      # Create nodes
      get_question_node = GetQuestionNode()
      answer_node = AnswerNode()
      
      # Connect nodes in sequence
      get_question_node >> answer_node
      
      # Create flow starting with input node
      return Flow(start=get_question_node)
  ```
- **`main.py`**: Serves as the project's entry point.
  ```python
  # main.py
  from flow import create_qa_flow

  # Example main function
  # Please replace this with your own main function
  def main():
      shared = {
          "question": None,  # Will be populated by GetQuestionNode from user input
          "answer": None     # Will be populated by AnswerNode
      }

      # Create the flow and run it
      qa_flow = create_qa_flow()
      qa_flow.run(shared)
      print(f"Question: {shared['question']}")
      print(f"Answer: {shared['answer']}")

  if __name__ == "__main__":
      main()
  ```
</file>

<file path="docs/index.md">
---
layout: default
title: "Home"
nav_order: 1
---

# Pocket Flow

A [100-line](https://github.com/the-pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalist LLM framework for *Agents, Task Decomposition, RAG, etc*.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworksâ€”([Multi-](./design_pattern/multi_agent.html))[Agents](./design_pattern/agent.html), [Workflow](./design_pattern/workflow.html), [RAG](./design_pattern/rag.html), and more.  
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="400"/>
</div>


## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](./core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](./core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](./core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](./core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [Async](./core_abstraction/async.md) nodes/flows allow waiting for asynchronous tasks.
- [(Advanced) Parallel](./core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" alt="Pocket Flow â€“ Core Abstraction" width="700"/>
</div>

## Design Pattern

From there, itâ€™s easy to implement popular design patterns:

- [Agent](./design_pattern/agent.md) autonomously makes decisions.
- [Workflow](./design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](./design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](./design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](./design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](./design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" alt="Pocket Flow â€“ Design Pattern" width="700"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer *examples*â€”please *implement your own*:

- [LLM Wrapper](./utility_function/llm.md)
- [Viz and Debug](./utility_function/viz.md)
- [Web Search](./utility_function/websearch.md)
- [Chunking](./utility_function/chunking.md)
- [Embedding](./utility_function/embedding.md)
- [Vector Databases](./utility_function/vector.md)
- [Text-to-Speech](./utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a *bad practice* for vendor-specific APIs in a general framework:
- *API Volatility*: Frequent changes lead to heavy maintenance for hardcoded APIs.
- *Flexibility*: You may want to switch vendors, use fine-tuned models, or run them locally.
- *Optimizations*: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps? 

Check out [Agentic Coding Guidance](./guide.md), the fastest way to develop LLM projects with Pocket Flow!
</file>

<file path="pocketflow/__init__.py">
import asyncio, warnings, copy, time

class BaseNode:
    def __init__(self): self.params,self.successors={},{}
    def set_params(self,params): self.params=params
    def next(self,node,action="default"):
        if action in self.successors: warnings.warn(f"Overwriting successor for action '{action}'")
        self.successors[action]=node; return node
    def prep(self,shared): pass
    def exec(self,prep_res): pass
    def post(self,shared,prep_res,exec_res): pass
    def _exec(self,prep_res): return self.exec(prep_res)
    def _run(self,shared): p=self.prep(shared); e=self._exec(p); return self.post(shared,p,e)
    def run(self,shared): 
        if self.successors: warnings.warn("Node won't run successors. Use Flow.")  
        return self._run(shared)
    def __rshift__(self,other): return self.next(other)
    def __sub__(self,action):
        if isinstance(action,str): return _ConditionalTransition(self,action)
        raise TypeError("Action must be a string")

class _ConditionalTransition:
    def __init__(self,src,action): self.src,self.action=src,action
    def __rshift__(self,tgt): return self.src.next(tgt,self.action)

class Node(BaseNode):
    def __init__(self,max_retries=1,wait=0): super().__init__(); self.max_retries,self.wait=max_retries,wait
    def exec_fallback(self,prep_res,exc): raise exc
    def _exec(self,prep_res):
        for self.cur_retry in range(self.max_retries):
            try: return self.exec(prep_res)
            except Exception as e:
                if self.cur_retry==self.max_retries-1: return self.exec_fallback(prep_res,e)
                if self.wait>0: time.sleep(self.wait)

class BatchNode(Node):
    def _exec(self,items): return [super(BatchNode,self)._exec(i) for i in (items or [])]

class Flow(BaseNode):
    def __init__(self,start=None): super().__init__(); self.start_node=start
    def start(self,start): self.start_node=start; return start
    def get_next_node(self,curr,action):
        nxt=curr.successors.get(action or "default")
        if not nxt and curr.successors: warnings.warn(f"Flow ends: '{action}' not found in {list(curr.successors)}")
        return nxt
    def _orch(self,shared,params=None):
        curr,p,last_action =copy.copy(self.start_node),(params or {**self.params}),None
        while curr: curr.set_params(p); last_action=curr._run(shared); curr=copy.copy(self.get_next_node(curr,last_action))
        return last_action
    def _run(self,shared): p=self.prep(shared); o=self._orch(shared); return self.post(shared,p,o)
    def post(self,shared,prep_res,exec_res): return exec_res

class BatchFlow(Flow):
    def _run(self,shared):
        pr=self.prep(shared) or []
        for bp in pr: self._orch(shared,{**self.params,**bp})
        return self.post(shared,pr,None)

class AsyncNode(Node):
    async def prep_async(self,shared): pass
    async def exec_async(self,prep_res): pass
    async def exec_fallback_async(self,prep_res,exc): raise exc
    async def post_async(self,shared,prep_res,exec_res): pass
    async def _exec(self,prep_res): 
        for self.cur_retry in range(self.max_retries):
            try: return await self.exec_async(prep_res)
            except Exception as e:
                if self.cur_retry==self.max_retries-1: return await self.exec_fallback_async(prep_res,e)
                if self.wait>0: await asyncio.sleep(self.wait)
    async def run_async(self,shared): 
        if self.successors: warnings.warn("Node won't run successors. Use AsyncFlow.")  
        return await self._run_async(shared)
    async def _run_async(self,shared): p=await self.prep_async(shared); e=await self._exec(p); return await self.post_async(shared,p,e)
    def _run(self,shared): raise RuntimeError("Use run_async.")

class AsyncBatchNode(AsyncNode,BatchNode):
    async def _exec(self,items): return [await super(AsyncBatchNode,self)._exec(i) for i in items]

class AsyncParallelBatchNode(AsyncNode,BatchNode):
    async def _exec(self,items): return await asyncio.gather(*(super(AsyncParallelBatchNode,self)._exec(i) for i in items))

class AsyncFlow(Flow,AsyncNode):
    async def _orch_async(self,shared,params=None):
        curr,p,last_action =copy.copy(self.start_node),(params or {**self.params}),None
        while curr: curr.set_params(p); last_action=await curr._run_async(shared) if isinstance(curr,AsyncNode) else curr._run(shared); curr=copy.copy(self.get_next_node(curr,last_action))
        return last_action
    async def _run_async(self,shared): p=await self.prep_async(shared); o=await self._orch_async(shared); return await self.post_async(shared,p,o)
    async def post_async(self,shared,prep_res,exec_res): return exec_res

class AsyncBatchFlow(AsyncFlow,BatchFlow):
    async def _run_async(self,shared):
        pr=await self.prep_async(shared) or []
        for bp in pr: await self._orch_async(shared,{**self.params,**bp})
        return await self.post_async(shared,pr,None)

class AsyncParallelBatchFlow(AsyncFlow,BatchFlow):
    async def _run_async(self,shared): 
        pr=await self.prep_async(shared) or []
        await asyncio.gather(*(self._orch_async(shared,{**self.params,**bp}) for bp in pr))
        return await self.post_async(shared,pr,None)
</file>

<file path="pocketflow/__init__.pyi">
import asyncio
from typing import Any, Dict, List, Optional, Union, TypeVar, Generic

# Type variables for better type relationships
_PrepResult = TypeVar('_PrepResult')
_ExecResult = TypeVar('_ExecResult')
_PostResult = TypeVar('_PostResult')

# More specific parameter types
ParamValue = Union[str, int, float, bool, None, List[Any], Dict[str, Any]]
SharedData = Dict[str, Any]
Params = Dict[str, ParamValue]

class BaseNode(Generic[_PrepResult, _ExecResult, _PostResult]):
    params: Params
    successors: Dict[str, BaseNode[Any, Any, Any]]
    
    def __init__(self) -> None: ...
    def set_params(self, params: Params) -> None: ...
    def next(self, node: BaseNode[Any, Any, Any], action: str = "default") -> BaseNode[Any, Any, Any]: ...
    def prep(self, shared: SharedData) -> _PrepResult: ...
    def exec(self, prep_res: _PrepResult) -> _ExecResult: ...
    def post(self, shared: SharedData, prep_res: _PrepResult, exec_res: _ExecResult) -> _PostResult: ...
    def _exec(self, prep_res: _PrepResult) -> _ExecResult: ...
    def _run(self, shared: SharedData) -> _PostResult: ...
    def run(self, shared: SharedData) -> _PostResult: ...
    def __rshift__(self, other: BaseNode[Any, Any, Any]) -> BaseNode[Any, Any, Any]: ...
    def __sub__(self, action: str) -> _ConditionalTransition: ...

class _ConditionalTransition:
    src: BaseNode[Any, Any, Any]
    action: str
    
    def __init__(self, src: BaseNode[Any, Any, Any], action: str) -> None: ...
    def __rshift__(self, tgt: BaseNode[Any, Any, Any]) -> BaseNode[Any, Any, Any]: ...

class Node(BaseNode[_PrepResult, _ExecResult, _PostResult]):
    max_retries: int
    wait: Union[int, float]
    cur_retry: int
    
    def __init__(self, max_retries: int = 1, wait: Union[int, float] = 0) -> None: ...
    def exec_fallback(self, prep_res: _PrepResult, exc: Exception) -> _ExecResult: ...
    def _exec(self, prep_res: _PrepResult) -> _ExecResult: ...

class BatchNode(Node[Optional[List[_PrepResult]], List[_ExecResult], _PostResult]):
    def _exec(self, items: Optional[List[_PrepResult]]) -> List[_ExecResult]: ...

class Flow(BaseNode[_PrepResult, Any, _PostResult]):
    start_node: Optional[BaseNode[Any, Any, Any]]
    
    def __init__(self, start: Optional[BaseNode[Any, Any, Any]] = None) -> None: ...
    def start(self, start: BaseNode[Any, Any, Any]) -> BaseNode[Any, Any, Any]: ...
    def get_next_node(
        self, curr: BaseNode[Any, Any, Any], action: Optional[str]
    ) -> Optional[BaseNode[Any, Any, Any]]: ...
    def _orch(
        self, shared: SharedData, params: Optional[Params] = None
    ) -> Any: ...
    def _run(self, shared: SharedData) -> _PostResult: ...
    def post(self, shared: SharedData, prep_res: _PrepResult, exec_res: Any) -> _PostResult: ...

class BatchFlow(Flow[Optional[List[Params]], Any, _PostResult]):
    def _run(self, shared: SharedData) -> _PostResult: ...

class AsyncNode(Node[_PrepResult, _ExecResult, _PostResult]):
    async def prep_async(self, shared: SharedData) -> _PrepResult: ...
    async def exec_async(self, prep_res: _PrepResult) -> _ExecResult: ...
    async def exec_fallback_async(self, prep_res: _PrepResult, exc: Exception) -> _ExecResult: ...
    async def post_async(
        self, shared: SharedData, prep_res: _PrepResult, exec_res: _ExecResult
    ) -> _PostResult: ...
    async def _exec(self, prep_res: _PrepResult) -> _ExecResult: ...
    async def run_async(self, shared: SharedData) -> _PostResult: ...
    async def _run_async(self, shared: SharedData) -> _PostResult: ...
    def _run(self, shared: SharedData) -> _PostResult: ...

class AsyncBatchNode(AsyncNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult], BatchNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult]):
    async def _exec(self, items: Optional[List[_PrepResult]]) -> List[_ExecResult]: ...

class AsyncParallelBatchNode(AsyncNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult], BatchNode[Optional[List[_PrepResult]], List[_ExecResult], _PostResult]):
    async def _exec(self, items: Optional[List[_PrepResult]]) -> List[_ExecResult]: ...

class AsyncFlow(Flow[_PrepResult, Any, _PostResult], AsyncNode[_PrepResult, Any, _PostResult]):
    async def _orch_async(
        self, shared: SharedData, params: Optional[Params] = None
    ) -> Any: ...
    async def _run_async(self, shared: SharedData) -> _PostResult: ...
    async def post_async(
        self, shared: SharedData, prep_res: _PrepResult, exec_res: Any
    ) -> _PostResult: ...

class AsyncBatchFlow(AsyncFlow[Optional[List[Params]], Any, _PostResult], BatchFlow[Optional[List[Params]], Any, _PostResult]):
    async def _run_async(self, shared: SharedData) -> _PostResult: ...

class AsyncParallelBatchFlow(AsyncFlow[Optional[List[Params]], Any, _PostResult], BatchFlow[Optional[List[Params]], Any, _PostResult]):
    async def _run_async(self, shared: SharedData) -> _PostResult: ...
</file>

<file path="tests/test_async_batch_flow.py">
import unittest
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import AsyncNode, AsyncBatchFlow

class AsyncDataProcessNode(AsyncNode):
    async def prep_async(self, shared_storage):
        key = self.params.get('key')
        data = shared_storage['input_data'][key]
        if 'results' not in shared_storage:
            shared_storage['results'] = {}
        shared_storage['results'][key] = data
        return data

    async def post_async(self, shared_storage, prep_result, proc_result):
        await asyncio.sleep(0.01)  # Simulate async work
        key = self.params.get('key')
        shared_storage['results'][key] = prep_result * 2  # Double the value
        return "processed"

class AsyncErrorNode(AsyncNode):
    async def post_async(self, shared_storage, prep_result, proc_result):
        key = self.params.get('key')
        if key == 'error_key':
            raise ValueError(f"Async error processing key: {key}")
        return "processed"

class TestAsyncBatchFlow(unittest.TestCase):
    def setUp(self):
        self.process_node = AsyncDataProcessNode()

    def test_basic_async_batch_processing(self):
        """Test basic async batch processing with multiple keys"""
        class SimpleTestAsyncBatchFlow(AsyncBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {
                'a': 1,
                'b': 2,
                'c': 3
            }
        }

        flow = SimpleTestAsyncBatchFlow(start=self.process_node)
        asyncio.run(flow.run_async(shared_storage))

        expected_results = {
            'a': 2,  # 1 * 2
            'b': 4,  # 2 * 2
            'c': 6   # 3 * 2
        }
        self.assertEqual(shared_storage['results'], expected_results)

    def test_empty_async_batch(self):
        """Test async batch processing with empty input"""
        class EmptyTestAsyncBatchFlow(AsyncBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {}
        }

        flow = EmptyTestAsyncBatchFlow(start=self.process_node)
        asyncio.run(flow.run_async(shared_storage))

        self.assertEqual(shared_storage.get('results', {}), {})

    def test_async_error_handling(self):
        """Test error handling during async batch processing"""
        class ErrorTestAsyncBatchFlow(AsyncBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {
                'normal_key': 1,
                'error_key': 2,
                'another_key': 3
            }
        }

        flow = ErrorTestAsyncBatchFlow(start=AsyncErrorNode())
        
        with self.assertRaises(ValueError):
            asyncio.run(flow.run_async(shared_storage))

    def test_nested_async_flow(self):
        """Test async batch processing with nested flows"""
        class AsyncInnerNode(AsyncNode):
            async def post_async(self, shared_storage, prep_result, proc_result):
                key = self.params.get('key')
                if 'intermediate_results' not in shared_storage:
                    shared_storage['intermediate_results'] = {}
                shared_storage['intermediate_results'][key] = shared_storage['input_data'][key] + 1
                await asyncio.sleep(0.01)
                return "next"

        class AsyncOuterNode(AsyncNode):
            async def post_async(self, shared_storage, prep_result, proc_result):
                key = self.params.get('key')
                if 'results' not in shared_storage:
                    shared_storage['results'] = {}
                shared_storage['results'][key] = shared_storage['intermediate_results'][key] * 2
                await asyncio.sleep(0.01)
                return "done"

        class NestedAsyncBatchFlow(AsyncBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        # Create inner flow
        inner_node = AsyncInnerNode()
        outer_node = AsyncOuterNode()
        inner_node - "next" >> outer_node

        shared_storage = {
            'input_data': {
                'x': 1,
                'y': 2
            }
        }

        flow = NestedAsyncBatchFlow(start=inner_node)
        asyncio.run(flow.run_async(shared_storage))

        expected_results = {
            'x': 4,  # (1 + 1) * 2
            'y': 6   # (2 + 1) * 2
        }
        self.assertEqual(shared_storage['results'], expected_results)

    def test_custom_async_parameters(self):
        """Test async batch processing with additional custom parameters"""
        class CustomParamAsyncNode(AsyncNode):
            async def post_async(self, shared_storage, prep_result, proc_result):
                key = self.params.get('key')
                multiplier = self.params.get('multiplier', 1)
                await asyncio.sleep(0.01)
                if 'results' not in shared_storage:
                    shared_storage['results'] = {}
                shared_storage['results'][key] = shared_storage['input_data'][key] * multiplier
                return "done"

        class CustomParamAsyncBatchFlow(AsyncBatchFlow):
            async def prep_async(self, shared_storage):
                return [{
                    'key': k,
                    'multiplier': i + 1
                } for i, k in enumerate(shared_storage['input_data'].keys())]

        shared_storage = {
            'input_data': {
                'a': 1,
                'b': 2,
                'c': 3
            }
        }

        flow = CustomParamAsyncBatchFlow(start=CustomParamAsyncNode())
        asyncio.run(flow.run_async(shared_storage))

        expected_results = {
            'a': 1 * 1,  # first item, multiplier = 1
            'b': 2 * 2,  # second item, multiplier = 2
            'c': 3 * 3   # third item, multiplier = 3
        }
        self.assertEqual(shared_storage['results'], expected_results)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_async_batch_node.py">
import unittest
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import AsyncNode, AsyncBatchNode, AsyncFlow

class AsyncArrayChunkNode(AsyncBatchNode):
    def __init__(self, chunk_size=10):
        super().__init__()
        self.chunk_size = chunk_size
    
    async def prep_async(self, shared_storage):
        # Get array from shared storage and split into chunks
        array = shared_storage.get('input_array', [])
        chunks = []
        for start in range(0, len(array), self.chunk_size):
            end = min(start + self.chunk_size, len(array))
            chunks.append(array[start:end])
        return chunks
    
    async def exec_async(self, chunk):
        # Simulate async processing of each chunk
        await asyncio.sleep(0.01)
        return sum(chunk)
        
    async def post_async(self, shared_storage, prep_result, proc_result):
        # Store chunk results in shared storage
        shared_storage['chunk_results'] = proc_result
        return "processed"

class AsyncSumReduceNode(AsyncNode):
    async def prep_async(self, shared_storage):
        # Get chunk results from shared storage
        chunk_results = shared_storage.get('chunk_results', [])
        await asyncio.sleep(0.01)  # Simulate async processing
        total = sum(chunk_results)
        shared_storage['total'] = total
        return "reduced"

class TestAsyncBatchNode(unittest.TestCase):
    def test_array_chunking(self):
        """
        Test that the array is correctly split into chunks and processed asynchronously
        """
        shared_storage = {
            'input_array': list(range(25))  # [0,1,2,...,24]
        }
        
        chunk_node = AsyncArrayChunkNode(chunk_size=10)
        asyncio.run(chunk_node.run_async(shared_storage))
        
        results = shared_storage['chunk_results']
        self.assertEqual(results, [45, 145, 110])  # Sum of chunks [0-9], [10-19], [20-24]
        
    # def test_async_map_reduce_sum(self):
    #     """
    #     Test a complete async map-reduce pipeline that sums a large array:
    #     1. Map: Split array into chunks and sum each chunk asynchronously
    #     2. Reduce: Sum all the chunk sums asynchronously
    #     """
    #     array = list(range(100))
    #     expected_sum = sum(array)  # 4950
        
    #     shared_storage = {
    #         'input_array': array
    #     }
        
    #     # Create nodes
    #     chunk_node = AsyncArrayChunkNode(chunk_size=10)
    #     reduce_node = AsyncSumReduceNode()
        
    #     # Connect nodes
    #     chunk_node - "processed" >> reduce_node
        
    #     # Create and run pipeline
    #     pipeline = AsyncFlow(start=chunk_node)
    #     asyncio.run(pipeline.run_async(shared_storage))
        
    #     self.assertEqual(shared_storage['total'], expected_sum)
        
    # def test_uneven_chunks(self):
    #     """
    #     Test that the async map-reduce works correctly with array lengths
    #     that don't divide evenly by chunk_size
    #     """
    #     array = list(range(25))
    #     expected_sum = sum(array)  # 300
        
    #     shared_storage = {
    #         'input_array': array
    #     }
        
    #     chunk_node = AsyncArrayChunkNode(chunk_size=10)
    #     reduce_node = AsyncSumReduceNode()
        
    #     chunk_node - "processed" >> reduce_node
    #     pipeline = AsyncFlow(start=chunk_node)
    #     asyncio.run(pipeline.run_async(shared_storage))
        
    #     self.assertEqual(shared_storage['total'], expected_sum)

    # def test_custom_chunk_size(self):
    #     """
    #     Test that the async map-reduce works with different chunk sizes
    #     """
    #     array = list(range(100))
    #     expected_sum = sum(array)
        
    #     shared_storage = {
    #         'input_array': array
    #     }
        
    #     # Use chunk_size=15 instead of default 10
    #     chunk_node = AsyncArrayChunkNode(chunk_size=15)
    #     reduce_node = AsyncSumReduceNode()
        
    #     chunk_node - "processed" >> reduce_node
    #     pipeline = AsyncFlow(start=chunk_node)
    #     asyncio.run(pipeline.run_async(shared_storage))
        
    #     self.assertEqual(shared_storage['total'], expected_sum)
        
    # def test_single_element_chunks(self):
    #     """
    #     Test extreme case where chunk_size=1
    #     """
    #     array = list(range(5))
    #     expected_sum = sum(array)
        
    #     shared_storage = {
    #         'input_array': array
    #     }
        
    #     chunk_node = AsyncArrayChunkNode(chunk_size=1)
    #     reduce_node = AsyncSumReduceNode()
        
    #     chunk_node - "processed" >> reduce_node
    #     pipeline = AsyncFlow(start=chunk_node)
    #     asyncio.run(pipeline.run_async(shared_storage))
        
    #     self.assertEqual(shared_storage['total'], expected_sum)

    # def test_empty_array(self):
    #     """
    #     Test edge case of empty input array
    #     """
    #     shared_storage = {
    #         'input_array': []
    #     }
        
    #     chunk_node = AsyncArrayChunkNode(chunk_size=10)
    #     reduce_node = AsyncSumReduceNode()
        
    #     chunk_node - "processed" >> reduce_node
    #     pipeline = AsyncFlow(start=chunk_node)
    #     asyncio.run(pipeline.run_async(shared_storage))
        
    #     self.assertEqual(shared_storage['total'], 0)

    # def test_error_handling(self):
    #     """
    #     Test error handling in async batch processing
    #     """
    #     class ErrorAsyncBatchNode(AsyncBatchNode):
    #         async def exec_async(self, item):
    #             if item == 2:
    #                 raise ValueError("Error processing item 2")
    #             return item

    #     shared_storage = {
    #         'input_array': [1, 2, 3]
    #     }
        
    #     error_node = ErrorAsyncBatchNode()
    #     with self.assertRaises(ValueError):
    #         asyncio.run(error_node.run_async(shared_storage))

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_async_flow.py">
import unittest
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import Node, AsyncNode, AsyncFlow

class AsyncNumberNode(AsyncNode):
    """
    Simple async node that sets 'current' to a given number.
    Demonstrates overriding .process() (sync) and using
    post_async() for the async portion.
    """
    def __init__(self, number):
        super().__init__()
        self.number = number

    async def prep_async(self, shared_storage):
        # Synchronous work is allowed inside an AsyncNode,
        # but final 'condition' is determined by post_async().
        shared_storage['current'] = self.number
        return "set_number"

    async def post_async(self, shared_storage, prep_result, proc_result):
        # Possibly do asynchronous tasks here
        await asyncio.sleep(0.01)
        # Return a condition for the flow
        return "number_set"

class AsyncIncrementNode(AsyncNode):
    """
    Demonstrates incrementing the 'current' value asynchronously.
    """
    async def prep_async(self, shared_storage):
        shared_storage['current'] = shared_storage.get('current', 0) + 1
        return "incremented"

    async def post_async(self, shared_storage, prep_result, proc_result):
        await asyncio.sleep(0.01)  # simulate async I/O
        return "done"

class AsyncSignalNode(AsyncNode):
    """ An async node that returns a specific signal string from post_async. """
    def __init__(self, signal="default_async_signal"):
        super().__init__()
        self.signal = signal

    # No prep needed usually if just signaling
    async def prep_async(self, shared_storage):
        await asyncio.sleep(0.01) # Simulate async work

    async def post_async(self, shared_storage, prep_result, exec_result):
        # Store the signal in shared storage for verification
        shared_storage['last_async_signal_emitted'] = self.signal
        await asyncio.sleep(0.01) # Simulate async work
        print(self.signal)
        return self.signal # Return the specific action string

class AsyncPathNode(AsyncNode):
    """ An async node to indicate which path was taken in the outer flow. """
    def __init__(self, path_id):
        super().__init__()
        self.path_id = path_id

    async def prep_async(self, shared_storage):
        await asyncio.sleep(0.01) # Simulate async work
        shared_storage['async_path_taken'] = self.path_id

    # post_async implicitly returns None (for default transition out if needed)
    async def post_async(self, shared_storage, prep_result, exec_result):
         await asyncio.sleep(0.01)
         # Return None by default

class TestAsyncNode(unittest.TestCase):
    """
    Test the AsyncNode (and descendants) in isolation (not in a flow).
    """
    def test_async_number_node_direct_call(self):
        """
        Even though AsyncNumberNode is designed for an async flow,
        we can still test it directly by calling run_async().
        """
        async def run_node():
            node = AsyncNumberNode(42)
            shared_storage = {}
            condition = await node.run_async(shared_storage)
            return shared_storage, condition

        shared_storage, condition = asyncio.run(run_node())
        self.assertEqual(shared_storage['current'], 42)
        self.assertEqual(condition, "number_set")

    def test_async_increment_node_direct_call(self):
        async def run_node():
            node = AsyncIncrementNode()
            shared_storage = {'current': 10}
            condition = await node.run_async(shared_storage)
            return shared_storage, condition

        shared_storage, condition = asyncio.run(run_node())
        self.assertEqual(shared_storage['current'], 11)
        self.assertEqual(condition, "done")


class TestAsyncFlow(unittest.TestCase):
    """
    Test how AsyncFlow orchestrates multiple async nodes.
    """
    def test_simple_async_flow(self):
        """
        Flow:
          1) AsyncNumberNode(5) -> sets 'current' to 5
          2) AsyncIncrementNode() -> increments 'current' to 6
        """

        # Create our nodes
        start = AsyncNumberNode(5)
        inc_node = AsyncIncrementNode()

        # Chain them: start >> inc_node
        start - "number_set" >> inc_node

        # Create an AsyncFlow with start
        flow = AsyncFlow(start)

        # We'll run the flow synchronously (which under the hood is asyncio.run())
        shared_storage = {}
        asyncio.run(flow.run_async(shared_storage))

        self.assertEqual(shared_storage['current'], 6)

    def test_async_flow_branching(self):
        """
        Demonstrate a branching scenario where we return different
        conditions. For example, you could have an async node that
        returns "go_left" or "go_right" in post_async, but here
        we'll keep it simpler for demonstration.
        """

        class BranchingAsyncNode(AsyncNode):
            def exec(self, data):
                value = shared_storage.get("value", 0)
                shared_storage["value"] = value
                # We'll decide branch based on whether 'value' is positive
                return None

            async def post_async(self, shared_storage, prep_result, proc_result):
                await asyncio.sleep(0.01)
                if shared_storage["value"] >= 0:
                    return "positive_branch"
                else:
                    return "negative_branch"

        class PositiveNode(Node):
            def exec(self, data):
                shared_storage["path"] = "positive"
                return None

        class NegativeNode(Node):
            def exec(self, data):
                shared_storage["path"] = "negative"
                return None

        shared_storage = {"value": 10}

        start = BranchingAsyncNode()
        positive_node = PositiveNode()
        negative_node = NegativeNode()

        # Condition-based chaining
        start - "positive_branch" >> positive_node
        start - "negative_branch" >> negative_node

        flow = AsyncFlow(start)
        asyncio.run(flow.run_async(shared_storage))

        self.assertEqual(shared_storage["path"], "positive", 
                         "Should have taken the positive branch")

    def test_async_composition_with_action_propagation(self):
        """
        Test AsyncFlow branches based on action from nested AsyncFlow's last node.
        """
        async def run_test():
            shared_storage = {}

            # 1. Define an inner async flow ending with AsyncSignalNode
            # Use existing AsyncNumberNode which should return None from post_async implicitly
            inner_start_node = AsyncNumberNode(200)
            inner_end_node = AsyncSignalNode("async_inner_done") # post_async -> "async_inner_done"
            inner_start_node - "number_set" >> inner_end_node
            # Inner flow will execute start->end, Flow exec returns "async_inner_done"
            inner_flow = AsyncFlow(start=inner_start_node)

            # 2. Define target async nodes for the outer flow branches
            path_a_node = AsyncPathNode("AsyncA") # post_async -> None
            path_b_node = AsyncPathNode("AsyncB") # post_async -> None

            # 3. Define the outer async flow starting with the inner async flow
            outer_flow = AsyncFlow(start=inner_flow)

            # 4. Define branches FROM the inner_flow object based on its returned action
            inner_flow - "async_inner_done" >> path_b_node  # This path should be taken
            inner_flow - "other_action" >> path_a_node      # This path should NOT be taken

            # 5. Run the outer async flow and capture the last action
            # Execution: inner_start -> inner_end -> path_b
            last_action_outer = await outer_flow.run_async(shared_storage)

            # 6. Return results for assertion
            return shared_storage, last_action_outer

        # Run the async test function
        shared_storage, last_action_outer = asyncio.run(run_test())

        # 7. Assert the results
        # Check state after inner flow execution
        self.assertEqual(shared_storage.get('current'), 200) # From AsyncNumberNode
        self.assertEqual(shared_storage.get('last_async_signal_emitted'), "async_inner_done")
        # Check that the correct outer path was taken
        self.assertEqual(shared_storage.get('async_path_taken'), "AsyncB")
        # Check the action returned by the outer flow. The last node executed was
        # path_b_node, which returns None from its post_async method.
        self.assertIsNone(last_action_outer)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_async_parallel_batch_flow.py">
import unittest
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import AsyncNode, AsyncParallelBatchNode, AsyncParallelBatchFlow

class AsyncParallelNumberProcessor(AsyncParallelBatchNode):
    def __init__(self, delay=0.1):
        super().__init__()
        self.delay = delay
    
    async def prep_async(self, shared_storage):
        batch = shared_storage['batches'][self.params['batch_id']]
        return batch
    
    async def exec_async(self, number):
        await asyncio.sleep(self.delay)  # Simulate async processing
        return number * 2
        
    async def post_async(self, shared_storage, prep_result, exec_result):
        if 'processed_numbers' not in shared_storage:
            shared_storage['processed_numbers'] = {}
        shared_storage['processed_numbers'][self.params['batch_id']] = exec_result
        return "processed"

class AsyncAggregatorNode(AsyncNode):
    async def prep_async(self, shared_storage):
        # Combine all batch results in order
        all_results = []
        processed = shared_storage.get('processed_numbers', {})
        for i in range(len(processed)):
            all_results.extend(processed[i])
        return all_results
    
    async def exec_async(self, prep_result):
        await asyncio.sleep(0.01)
        return sum(prep_result)
    
    async def post_async(self, shared_storage, prep_result, exec_result):
        shared_storage['total'] = exec_result
        return "aggregated"

class TestAsyncParallelBatchFlow(unittest.TestCase):
    def setUp(self):
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    def tearDown(self):
        self.loop.close()

    def test_parallel_batch_flow(self):
        """
        Test basic parallel batch processing flow with batch IDs
        """
        class TestParallelBatchFlow(AsyncParallelBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'batch_id': i} for i in range(len(shared_storage['batches']))]

        shared_storage = {
            'batches': [
                [1, 2, 3],  # batch_id: 0
                [4, 5, 6],  # batch_id: 1
                [7, 8, 9]   # batch_id: 2
            ]
        }

        processor = AsyncParallelNumberProcessor(delay=0.1)
        aggregator = AsyncAggregatorNode()
        
        processor - "processed" >> aggregator
        flow = TestParallelBatchFlow(start=processor)
        
        start_time = self.loop.time()
        self.loop.run_until_complete(flow.run_async(shared_storage))
        execution_time = self.loop.time() - start_time

        # Verify each batch was processed correctly
        expected_batch_results = {
            0: [2, 4, 6],    # [1,2,3] * 2
            1: [8, 10, 12],  # [4,5,6] * 2
            2: [14, 16, 18]  # [7,8,9] * 2
        }
        self.assertEqual(shared_storage['processed_numbers'], expected_batch_results)
        
        # Verify total
        expected_total = sum(num * 2 for batch in shared_storage['batches'] for num in batch)
        self.assertEqual(shared_storage['total'], expected_total)
        
        # Verify parallel execution
        self.assertLess(execution_time, 0.2)

    def test_error_handling(self):
        """
        Test error handling in parallel batch flow
        """
        class ErrorProcessor(AsyncParallelNumberProcessor):
            async def exec_async(self, item):
                if item == 2:
                    raise ValueError(f"Error processing item {item}")
                return item

        class ErrorBatchFlow(AsyncParallelBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'batch_id': i} for i in range(len(shared_storage['batches']))]

        shared_storage = {
            'batches': [
                [1, 2, 3],  # Contains error-triggering value
                [4, 5, 6]
            ]
        }

        processor = ErrorProcessor()
        flow = ErrorBatchFlow(start=processor)
        
        with self.assertRaises(ValueError):
            self.loop.run_until_complete(flow.run_async(shared_storage))

    def test_multiple_batch_sizes(self):
        """
        Test parallel batch flow with varying batch sizes
        """
        class VaryingBatchFlow(AsyncParallelBatchFlow):
            async def prep_async(self, shared_storage):
                return [{'batch_id': i} for i in range(len(shared_storage['batches']))]

        shared_storage = {
            'batches': [
                [1],           # batch_id: 0
                [2, 3, 4],    # batch_id: 1
                [5, 6],       # batch_id: 2
                [7, 8, 9, 10] # batch_id: 3
            ]
        }

        processor = AsyncParallelNumberProcessor(delay=0.05)
        aggregator = AsyncAggregatorNode()
        
        processor - "processed" >> aggregator
        flow = VaryingBatchFlow(start=processor)
        
        self.loop.run_until_complete(flow.run_async(shared_storage))
        
        # Verify each batch was processed correctly
        expected_batch_results = {
            0: [2],                 # [1] * 2
            1: [4, 6, 8],          # [2,3,4] * 2
            2: [10, 12],           # [5,6] * 2
            3: [14, 16, 18, 20]    # [7,8,9,10] * 2
        }
        self.assertEqual(shared_storage['processed_numbers'], expected_batch_results)
        
        # Verify total
        expected_total = sum(num * 2 for batch in shared_storage['batches'] for num in batch)
        self.assertEqual(shared_storage['total'], expected_total)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_async_parallel_batch_node.py">
import unittest
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import AsyncParallelBatchNode, AsyncParallelBatchFlow

class AsyncParallelNumberProcessor(AsyncParallelBatchNode):
    def __init__(self, delay=0.1):
        super().__init__()
        self.delay = delay
    
    async def prep_async(self, shared_storage):
        numbers = shared_storage.get('input_numbers', [])
        return numbers
    
    async def exec_async(self, number):
        await asyncio.sleep(self.delay)  # Simulate async processing
        return number * 2
        
    async def post_async(self, shared_storage, prep_result, exec_result):
        shared_storage['processed_numbers'] = exec_result
        return "processed"

class TestAsyncParallelBatchNode(unittest.TestCase):
    def setUp(self):
        # Reset the event loop for each test
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    def tearDown(self):
        self.loop.close()
    
    def test_parallel_processing(self):
        """
        Test that numbers are processed in parallel by measuring execution time
        """
        shared_storage = {
            'input_numbers': list(range(5))
        }
        
        processor = AsyncParallelNumberProcessor(delay=0.1)
        
        # Run the processor
        start_time = asyncio.get_event_loop().time()
        self.loop.run_until_complete(processor.run_async(shared_storage))
        end_time = asyncio.get_event_loop().time()
        
        # Check results
        expected = [0, 2, 4, 6, 8]  # Each number doubled
        self.assertEqual(shared_storage['processed_numbers'], expected)
        
        # Since processing is parallel, total time should be approximately
        # equal to the delay of a single operation, not delay * number_of_items
        execution_time = end_time - start_time
        self.assertLess(execution_time, 0.2)  # Should be around 0.1s plus minimal overhead
    
    def test_empty_input(self):
        """
        Test processing of empty input
        """
        shared_storage = {
            'input_numbers': []
        }
        
        processor = AsyncParallelNumberProcessor()
        self.loop.run_until_complete(processor.run_async(shared_storage))
        
        self.assertEqual(shared_storage['processed_numbers'], [])
    
    def test_single_item(self):
        """
        Test processing of a single item
        """
        shared_storage = {
            'input_numbers': [42]
        }
        
        processor = AsyncParallelNumberProcessor()
        self.loop.run_until_complete(processor.run_async(shared_storage))
        
        self.assertEqual(shared_storage['processed_numbers'], [84])
    
    def test_large_batch(self):
        """
        Test processing of a large batch of numbers
        """
        input_size = 100
        shared_storage = {
            'input_numbers': list(range(input_size))
        }
        
        processor = AsyncParallelNumberProcessor(delay=0.01)
        self.loop.run_until_complete(processor.run_async(shared_storage))
        
        expected = [x * 2 for x in range(input_size)]
        self.assertEqual(shared_storage['processed_numbers'], expected)
    
    def test_error_handling(self):
        """
        Test error handling during parallel processing
        """
        class ErrorProcessor(AsyncParallelNumberProcessor):
            async def exec_async(self, item):
                if item == 2:
                    raise ValueError(f"Error processing item {item}")
                return item
        
        shared_storage = {
            'input_numbers': [1, 2, 3]
        }
        
        processor = ErrorProcessor()
        with self.assertRaises(ValueError):
            self.loop.run_until_complete(processor.run_async(shared_storage))
    
    def test_concurrent_execution(self):
        """
        Test that tasks are actually running concurrently by tracking execution order
        """
        execution_order = []
        
        class OrderTrackingProcessor(AsyncParallelNumberProcessor):
            async def exec_async(self, item):
                delay = 0.1 if item % 2 == 0 else 0.05
                await asyncio.sleep(delay)
                execution_order.append(item)
                return item
        
        shared_storage = {
            'input_numbers': list(range(4))  # [0, 1, 2, 3]
        }
        
        processor = OrderTrackingProcessor()
        self.loop.run_until_complete(processor.run_async(shared_storage))
        
        # Odd numbers should finish before even numbers due to shorter delay
        self.assertLess(execution_order.index(1), execution_order.index(0))
        self.assertLess(execution_order.index(3), execution_order.index(2))

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_batch_flow.py">
import unittest
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import Node, BatchFlow, Flow

class DataProcessNode(Node):
    def prep(self, shared_storage):
        key = self.params.get('key')
        data = shared_storage['input_data'][key]
        if 'results' not in shared_storage:
            shared_storage['results'] = {}
        shared_storage['results'][key] = data * 2

class ErrorProcessNode(Node):
    def prep(self, shared_storage):
        key = self.params.get('key')
        if key == 'error_key':
            raise ValueError(f"Error processing key: {key}")
        if 'results' not in shared_storage:
            shared_storage['results'] = {}
        shared_storage['results'][key] = True

class TestBatchFlow(unittest.TestCase):
    def setUp(self):
        self.process_node = DataProcessNode()
        
    def test_basic_batch_processing(self):
        """Test basic batch processing with multiple keys"""
        class SimpleTestBatchFlow(BatchFlow):
            def prep(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {
                'a': 1,
                'b': 2,
                'c': 3
            }
        }

        flow = SimpleTestBatchFlow(start=self.process_node)
        flow.run(shared_storage)

        expected_results = {
            'a': 2,
            'b': 4,
            'c': 6
        }
        self.assertEqual(shared_storage['results'], expected_results)

    def test_empty_input(self):
        """Test batch processing with empty input dictionary"""
        class EmptyTestBatchFlow(BatchFlow):
            def prep(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {}
        }

        flow = EmptyTestBatchFlow(start=self.process_node)
        flow.run(shared_storage)

        self.assertEqual(shared_storage.get('results', {}), {})

    def test_single_item(self):
        """Test batch processing with single item"""
        class SingleItemBatchFlow(BatchFlow):
            def prep(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {
                'single': 5
            }
        }

        flow = SingleItemBatchFlow(start=self.process_node)
        flow.run(shared_storage)

        expected_results = {
            'single': 10
        }
        self.assertEqual(shared_storage['results'], expected_results)

    def test_error_handling(self):
        """Test error handling during batch processing"""
        class ErrorTestBatchFlow(BatchFlow):
            def prep(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        shared_storage = {
            'input_data': {
                'normal_key': 1,
                'error_key': 2,
                'another_key': 3
            }
        }

        flow = ErrorTestBatchFlow(start=ErrorProcessNode())
        
        with self.assertRaises(ValueError):
            flow.run(shared_storage)

    def test_nested_flow(self):
        """Test batch processing with nested flows"""
        class InnerNode(Node):
            def exec(self, prep_result):
                key = self.params.get('key')
                if 'intermediate_results' not in shared_storage:
                    shared_storage['intermediate_results'] = {}
                shared_storage['intermediate_results'][key] = shared_storage['input_data'][key] + 1

        class OuterNode(Node):
            def exec(self, prep_result):
                key = self.params.get('key')
                if 'results' not in shared_storage:
                    shared_storage['results'] = {}
                shared_storage['results'][key] = shared_storage['intermediate_results'][key] * 2

        class NestedBatchFlow(BatchFlow):
            def prep(self, shared_storage):
                return [{'key': k} for k in shared_storage['input_data'].keys()]

        # Create inner flow
        inner_node = InnerNode()
        outer_node = OuterNode()
        inner_node >> outer_node

        shared_storage = {
            'input_data': {
                'x': 1,
                'y': 2
            }
        }

        flow = NestedBatchFlow(start=inner_node)
        flow.run(shared_storage)

        expected_results = {
            'x': 4,  # (1 + 1) * 2
            'y': 6   # (2 + 1) * 2
        }
        self.assertEqual(shared_storage['results'], expected_results)

    def test_custom_parameters(self):
        """Test batch processing with additional custom parameters"""
        class CustomParamNode(Node):
            def exec(self, prep_result):
                key = self.params.get('key')
                multiplier = self.params.get('multiplier', 1)
                if 'results' not in shared_storage:
                    shared_storage['results'] = {}
                shared_storage['results'][key] = shared_storage['input_data'][key] * multiplier

        class CustomParamBatchFlow(BatchFlow):
            def prep(self, shared_storage):
                return [{
                    'key': k,
                    'multiplier': i + 1
                } for i, k in enumerate(shared_storage['input_data'].keys())]

        shared_storage = {
            'input_data': {
                'a': 1,
                'b': 2,
                'c': 3
            }
        }

        flow = CustomParamBatchFlow(start=CustomParamNode())
        flow.run(shared_storage)

        expected_results = {
            'a': 1 * 1,  # first item, multiplier = 1
            'b': 2 * 2,  # second item, multiplier = 2
            'c': 3 * 3   # third item, multiplier = 3
        }
        self.assertEqual(shared_storage['results'], expected_results)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_batch_node.py">
import unittest
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import Node, BatchNode, Flow

class ArrayChunkNode(BatchNode):
    def __init__(self, chunk_size=10):
        super().__init__()
        self.chunk_size = chunk_size
    
    def prep(self, shared_storage):
        # Get array from shared storage and split into chunks
        array = shared_storage.get('input_array', [])
        chunks = []
        for start in range(0, len(array), self.chunk_size):
            end = min(start + self.chunk_size, len(array))
            chunks.append(array[start: end])
        return chunks
    
    def exec(self, chunk):
        # Process the chunk and return its sum
        chunk_sum = sum(chunk)
        return chunk_sum
        
    def post(self, shared_storage, prep_result, proc_result):
        # Store chunk results in shared storage
        shared_storage['chunk_results'] = proc_result
        return "default"

class SumReduceNode(Node):
    def prep(self, shared_storage):
        # Get chunk results from shared storage and sum them
        chunk_results = shared_storage.get('chunk_results', [])
        total = sum(chunk_results)
        shared_storage['total'] = total

class TestBatchNode(unittest.TestCase):
    def test_array_chunking(self):
        """
        Test that the array is correctly split into chunks
        """
        shared_storage = {
            'input_array': list(range(25))  # [0,1,2,...,24]
        }
        
        chunk_node = ArrayChunkNode(chunk_size=10)
        chunk_node.run(shared_storage)
        results = shared_storage['chunk_results']
        self.assertEqual(results, [45, 145, 110])
        
    def test_map_reduce_sum(self):
        """
        Test a complete map-reduce pipeline that sums a large array:
        1. Map: Split array into chunks and sum each chunk
        2. Reduce: Sum all the chunk sums
        """
        # Create test array: [0,1,2,...,99]
        array = list(range(100))
        expected_sum = sum(array)  # 4950
        
        shared_storage = {
            'input_array': array
        }
        
        # Create nodes
        chunk_node = ArrayChunkNode(chunk_size=10)
        reduce_node = SumReduceNode()
        
        # Connect nodes
        chunk_node >> reduce_node
        
        # Create and run pipeline
        pipeline = Flow(start=chunk_node)
        pipeline.run(shared_storage)
        
        self.assertEqual(shared_storage['total'], expected_sum)
        
    def test_uneven_chunks(self):
        """
        Test that the map-reduce works correctly with array lengths
        that don't divide evenly by chunk_size
        """
        array = list(range(25))
        expected_sum = sum(array)  # 300
        
        shared_storage = {
            'input_array': array
        }
        
        chunk_node = ArrayChunkNode(chunk_size=10)
        reduce_node = SumReduceNode()
        
        chunk_node >> reduce_node
        pipeline = Flow(start=chunk_node)
        pipeline.run(shared_storage)
        
        self.assertEqual(shared_storage['total'], expected_sum)

    def test_custom_chunk_size(self):
        """
        Test that the map-reduce works with different chunk sizes
        """
        array = list(range(100))
        expected_sum = sum(array)
        
        shared_storage = {
            'input_array': array
        }
        
        # Use chunk_size=15 instead of default 10
        chunk_node = ArrayChunkNode(chunk_size=15)
        reduce_node = SumReduceNode()
        
        chunk_node >> reduce_node
        pipeline = Flow(start=chunk_node)
        pipeline.run(shared_storage)
        
        self.assertEqual(shared_storage['total'], expected_sum)
        
    def test_single_element_chunks(self):
        """
        Test extreme case where chunk_size=1
        """
        array = list(range(5))
        expected_sum = sum(array)
        
        shared_storage = {
            'input_array': array
        }
        
        chunk_node = ArrayChunkNode(chunk_size=1)
        reduce_node = SumReduceNode()
        
        chunk_node >> reduce_node
        pipeline = Flow(start=chunk_node)
        pipeline.run(shared_storage)
        
        self.assertEqual(shared_storage['total'], expected_sum)

    def test_empty_array(self):
        """
        Test edge case of empty input array
        """
        shared_storage = {
            'input_array': []
        }
        
        chunk_node = ArrayChunkNode(chunk_size=10)
        reduce_node = SumReduceNode()
        
        chunk_node >> reduce_node
        pipeline = Flow(start=chunk_node)
        pipeline.run(shared_storage)
        
        self.assertEqual(shared_storage['total'], 0)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_fall_back.py">
import unittest
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import Node, AsyncNode, Flow, AsyncFlow

class FallbackNode(Node):
    def __init__(self, should_fail=True, max_retries=1):
        super().__init__(max_retries=max_retries)
        self.should_fail = should_fail
        self.attempt_count = 0
    
    def prep(self, shared_storage):
        if 'results' not in shared_storage:
            shared_storage['results'] = []
        return None
    
    def exec(self, prep_result):
        self.attempt_count += 1
        if self.should_fail:
            raise ValueError("Intentional failure")
        return "success"
    
    def exec_fallback(self, prep_result, exc):
        return "fallback"
    
    def post(self, shared_storage, prep_result, exec_result):
        shared_storage['results'].append({
            'attempts': self.attempt_count,
            'result': exec_result
        })

class AsyncFallbackNode(AsyncNode):
    def __init__(self, should_fail=True, max_retries=1):
        super().__init__(max_retries=max_retries)
        self.should_fail = should_fail
        self.attempt_count = 0
    
    async def prep_async(self, shared_storage):
        if 'results' not in shared_storage:
            shared_storage['results'] = []
        return None
    
    async def exec_async(self, prep_result):
        self.attempt_count += 1
        if self.should_fail:
            raise ValueError("Intentional async failure")
        return "success"
    
    async def exec_fallback_async(self, prep_result, exc):
        await asyncio.sleep(0.01)  # Simulate async work
        return "async_fallback"
    
    async def post_async(self, shared_storage, prep_result, exec_result):
        shared_storage['results'].append({
            'attempts': self.attempt_count,
            'result': exec_result
        })

class TestExecFallback(unittest.TestCase):
    def test_successful_execution(self):
        """Test that exec_fallback is not called when execution succeeds"""
        shared_storage = {}
        node = FallbackNode(should_fail=False)
        result = node.run(shared_storage)
        
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['attempts'], 1)
        self.assertEqual(shared_storage['results'][0]['result'], "success")

    def test_fallback_after_failure(self):
        """Test that exec_fallback is called after all retries are exhausted"""
        shared_storage = {}
        node = FallbackNode(should_fail=True, max_retries=2)
        result = node.run(shared_storage)
        
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['attempts'], 2)
        self.assertEqual(shared_storage['results'][0]['result'], "fallback")

    def test_fallback_in_flow(self):
        """Test that fallback works within a Flow"""
        class ResultNode(Node):
            def prep(self, shared_storage):
                return shared_storage.get('results', [])
                
            def exec(self, prep_result):
                return prep_result
                
            def post(self, shared_storage, prep_result, exec_result):
                shared_storage['final_result'] = exec_result
                return None
        
        shared_storage = {}
        fallback_node = FallbackNode(should_fail=True)
        result_node = ResultNode()
        fallback_node >> result_node
        
        flow = Flow(start=fallback_node)
        flow.run(shared_storage)
        
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['result'], "fallback")
        self.assertEqual(shared_storage['final_result'], [{'attempts': 1, 'result': 'fallback'}] )

    def test_no_fallback_implementation(self):
        """Test that default fallback behavior raises the exception"""
        class NoFallbackNode(Node):
            def prep(self, shared_storage):
                if 'results' not in shared_storage:
                    shared_storage['results'] = []
                return None
            
            def exec(self, prep_result):
                raise ValueError("Test error")
            
            def post(self, shared_storage, prep_result, exec_result):
                shared_storage['results'].append({'result': exec_result})
                return exec_result
        
        shared_storage = {}
        node = NoFallbackNode()
        with self.assertRaises(ValueError):
            node.run(shared_storage)

    def test_retry_before_fallback(self):
        """Test that retries are attempted before calling fallback"""
        shared_storage = {}
        node = FallbackNode(should_fail=True, max_retries=3)
        node.run(shared_storage)
        
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['attempts'], 3)
        self.assertEqual(shared_storage['results'][0]['result'], "fallback")

class TestAsyncExecFallback(unittest.TestCase):
    def setUp(self):
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    def tearDown(self):
        self.loop.close()

    def test_async_successful_execution(self):
        """Test that async exec_fallback is not called when execution succeeds"""
        async def run_test():
            shared_storage = {}
            node = AsyncFallbackNode(should_fail=False)
            await node.run_async(shared_storage)
            return shared_storage
        
        shared_storage = self.loop.run_until_complete(run_test())
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['attempts'], 1)
        self.assertEqual(shared_storage['results'][0]['result'], "success")

    def test_async_fallback_after_failure(self):
        """Test that async exec_fallback is called after all retries are exhausted"""
        async def run_test():
            shared_storage = {}
            node = AsyncFallbackNode(should_fail=True, max_retries=2)
            await node.run_async(shared_storage)
            return shared_storage
        
        shared_storage = self.loop.run_until_complete(run_test())
        
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['attempts'], 2)
        self.assertEqual(shared_storage['results'][0]['result'], "async_fallback")

    def test_async_fallback_in_flow(self):
        """Test that async fallback works within an AsyncFlow"""
        class AsyncResultNode(AsyncNode):
            async def prep_async(self, shared_storage):
                return shared_storage['results'][-1]['result']  # Get last result
                
            async def exec_async(self, prep_result):
                return prep_result
                
            async def post_async(self, shared_storage, prep_result, exec_result):
                shared_storage['final_result'] = exec_result
                return "done"
        
        async def run_test():
            shared_storage = {}
            fallback_node = AsyncFallbackNode(should_fail=True)
            result_node = AsyncResultNode()
            fallback_node >> result_node
            
            flow = AsyncFlow(start=fallback_node)
            await flow.run_async(shared_storage)
            return shared_storage
        
        shared_storage = self.loop.run_until_complete(run_test())
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['result'], "async_fallback")
        self.assertEqual(shared_storage['final_result'], "async_fallback")

    def test_async_no_fallback_implementation(self):
        """Test that default async fallback behavior raises the exception"""
        class NoFallbackAsyncNode(AsyncNode):
            async def prep_async(self, shared_storage):
                if 'results' not in shared_storage:
                    shared_storage['results'] = []
                return None
            
            async def exec_async(self, prep_result):
                raise ValueError("Test async error")
            
            async def post_async(self, shared_storage, prep_result, exec_result):
                shared_storage['results'].append({'result': exec_result})
                return exec_result
        
        async def run_test():
            shared_storage = {}
            node = NoFallbackAsyncNode()
            await node.run_async(shared_storage)
        
        with self.assertRaises(ValueError):
            self.loop.run_until_complete(run_test())

    def test_async_retry_before_fallback(self):
        """Test that retries are attempted before calling async fallback"""
        async def run_test():
            shared_storage = {}
            node = AsyncFallbackNode(should_fail=True, max_retries=3)
            result = await node.run_async(shared_storage)
            return result, shared_storage
        
        result, shared_storage = self.loop.run_until_complete(run_test())
        self.assertEqual(len(shared_storage['results']), 1)
        self.assertEqual(shared_storage['results'][0]['attempts'], 3)
        self.assertEqual(shared_storage['results'][0]['result'], "async_fallback")

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_flow_basic.py">
# tests/test_flow_basic.py
import unittest
import sys
from pathlib import Path
import warnings

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import Node, Flow

# --- Node Definitions ---
# Nodes intended for default transitions (>>) should NOT return a specific
# action string from post. Let it return None by default.
# Nodes intended for conditional transitions (-) MUST return the action string.

class NumberNode(Node):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def prep(self, shared_storage):
        shared_storage['current'] = self.number
    # post implicitly returns None - used for default transition

class AddNode(Node):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def prep(self, shared_storage):
        shared_storage['current'] += self.number
    # post implicitly returns None - used for default transition

class MultiplyNode(Node):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def prep(self, shared_storage):
        shared_storage['current'] *= self.number
    # post implicitly returns None - used for default transition

class CheckPositiveNode(Node):
   # This node IS designed for conditional branching
   def prep(self, shared_storage):
       pass
   def post(self, shared_storage, prep_result, proc_result):
        # MUST return the specific action string for branching
        if shared_storage['current'] >= 0:
            return 'positive'
        else:
            return 'negative'

class NoOpNode(Node):
    # Just a placeholder node
    pass # post implicitly returns None

class EndSignalNode(Node):
    # A node specifically to return a value when it's the end
    def __init__(self, signal="finished"):
        super().__init__()
        self.signal = signal
    def post(self, shared_storage, prep_result, exec_result):
        return self.signal # Return a specific signal

# --- Test Class ---
class TestFlowBasic(unittest.TestCase):

    def test_start_method_initialization(self):
        """Test initializing flow with start() after creation."""
        shared_storage = {}
        n1 = NumberNode(5)
        pipeline = Flow()
        pipeline.start(n1)
        last_action = pipeline.run(shared_storage)
        self.assertEqual(shared_storage['current'], 5)
        # NumberNode.post returns None (default)
        self.assertIsNone(last_action)

    def test_start_method_chaining(self):
        """Test fluent chaining using start().next()..."""
        shared_storage = {}
        pipeline = Flow()
        # Chain: NumberNode -> AddNode -> MultiplyNode
        # All use default transitions (post returns None)
        pipeline.start(NumberNode(5)).next(AddNode(3)).next(MultiplyNode(2))
        last_action = pipeline.run(shared_storage)
        self.assertEqual(shared_storage['current'], 16)
        # Last node (MultiplyNode) post returns None
        self.assertIsNone(last_action)

    def test_sequence_with_rshift(self):
        """Test a simple linear pipeline using >>"""
        shared_storage = {}
        n1 = NumberNode(5)
        n2 = AddNode(3)
        n3 = MultiplyNode(2)

        pipeline = Flow()
        # All default transitions (post returns None)
        pipeline.start(n1) >> n2 >> n3

        last_action = pipeline.run(shared_storage)
        self.assertEqual(shared_storage['current'], 16)
        # Last node (n3: MultiplyNode) post returns None
        self.assertIsNone(last_action)

    def test_branching_positive(self):
        """Test positive branch: CheckPositiveNode returns 'positive'"""
        shared_storage = {}
        start_node = NumberNode(5)    # post -> None
        check_node = CheckPositiveNode() # post -> 'positive' or 'negative'
        add_if_positive = AddNode(10) # post -> None
        add_if_negative = AddNode(-20) # post -> None (won't run)

        pipeline = Flow()
        # start -> check (default); check branches on 'positive'/'negative'
        pipeline.start(start_node) >> check_node
        check_node - "positive" >> add_if_positive
        check_node - "negative" >> add_if_negative

        # Execution: start_node -> check_node -> add_if_positive
        last_action = pipeline.run(shared_storage)
        self.assertEqual(shared_storage['current'], 15) # 5 + 10
        # Last node executed was add_if_positive, its post returns None
        self.assertIsNone(last_action)

    def test_branching_negative(self):
        """Test negative branch: CheckPositiveNode returns 'negative'"""
        shared_storage = {}
        start_node = NumberNode(-5)   # post -> None
        check_node = CheckPositiveNode() # post -> 'positive' or 'negative'
        add_if_positive = AddNode(10) # post -> None (won't run)
        add_if_negative = AddNode(-20) # post -> None

        pipeline = Flow()
        pipeline.start(start_node) >> check_node
        check_node - "positive" >> add_if_positive
        check_node - "negative" >> add_if_negative

        # Execution: start_node -> check_node -> add_if_negative
        last_action = pipeline.run(shared_storage)
        self.assertEqual(shared_storage['current'], -25) # -5 + -20
        # Last node executed was add_if_negative, its post returns None
        self.assertIsNone(last_action)

    def test_cycle_until_negative_ends_with_signal(self):
        """Test cycle, ending on a node that returns a signal"""
        shared_storage = {}
        n1 = NumberNode(10)           # post -> None
        check = CheckPositiveNode()   # post -> 'positive' or 'negative'
        subtract3 = AddNode(-3)       # post -> None
        end_node = EndSignalNode("cycle_done") # post -> "cycle_done"

        pipeline = Flow()
        pipeline.start(n1) >> check
        # Branching from CheckPositiveNode
        check - 'positive' >> subtract3
        check - 'negative' >> end_node # End on negative branch
        # After subtracting, go back to check (default transition)
        subtract3 >> check

        # Execution: n1->check->sub3->check->sub3->check->sub3->check->sub3->check->end_node
        last_action = pipeline.run(shared_storage)
        self.assertEqual(shared_storage['current'], -2) # 10 -> 7 -> 4 -> 1 -> -2
        # Last node executed was end_node, its post returns "cycle_done"
        self.assertEqual(last_action, "cycle_done")

    def test_flow_ends_warning_default_missing(self):
        """Test warning when default transition is needed but not found"""
        shared_storage = {}
        # Node that returns a specific action from post
        class ActionNode(Node):
            def post(self, *args): return "specific_action"
        start_node = ActionNode()
        next_node = NoOpNode()

        pipeline = Flow()
        pipeline.start(start_node)
        # Define successor only for the specific action
        start_node - "specific_action" >> next_node

        # Make start_node return None instead, triggering default search
        start_node.post = lambda *args: None

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            # Run flow. start_node runs, post returns None.
            # Flow looks for "default", but only "specific_action" exists.
            last_action = pipeline.run(shared_storage)

            self.assertEqual(len(w), 1)
            self.assertTrue(issubclass(w[-1].category, UserWarning))
            # Warning message should indicate "default" wasn't found
            self.assertIn("Flow ends: 'None' not found in ['specific_action']", str(w[-1].message))
        # Last action is from start_node's post
        self.assertIsNone(last_action)

    def test_flow_ends_warning_specific_missing(self):
        """Test warning when specific action is returned but not found"""
        shared_storage = {}
        # Node that returns a specific action from post
        class ActionNode(Node):
            def post(self, *args): return "specific_action"
        start_node = ActionNode()
        next_node = NoOpNode()

        pipeline = Flow()
        pipeline.start(start_node)
        # Define successor only for "default"
        start_node >> next_node # same as start_node.next(next_node, "default")

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            # Run flow. start_node runs, post returns "specific_action".
            # Flow looks for "specific_action", but only "default" exists.
            last_action = pipeline.run(shared_storage)

            self.assertEqual(len(w), 1)
            self.assertTrue(issubclass(w[-1].category, UserWarning))
            # Warning message should indicate "specific_action" wasn't found
            self.assertIn("Flow ends: 'specific_action' not found in ['default']", str(w[-1].message))
        # Last action is from start_node's post
        self.assertEqual(last_action, "specific_action")


if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_flow_composition.py">
# tests/test_flow_composition.py
import unittest
import asyncio # Keep import, might be needed if other tests use it indirectly
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from pocketflow import Node, Flow

# --- Existing Nodes ---
class NumberNode(Node):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def prep(self, shared_storage):
        shared_storage['current'] = self.number
    # post implicitly returns None

class AddNode(Node):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def prep(self, shared_storage):
        shared_storage['current'] += self.number
    # post implicitly returns None

class MultiplyNode(Node):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def prep(self, shared_storage):
        shared_storage['current'] *= self.number
    # post implicitly returns None

# --- New Nodes for Action Propagation Test ---
class SignalNode(Node):
    """A node that returns a specific signal string from its post method."""
    def __init__(self, signal="default_signal"):
        super().__init__()
        self.signal = signal
    # No prep needed usually if just signaling
    def post(self, shared_storage, prep_result, exec_result):
        # Store the signal in shared storage for verification
        shared_storage['last_signal_emitted'] = self.signal
        return self.signal # Return the specific action string

class PathNode(Node):
    """A node to indicate which path was taken in the outer flow."""
    def __init__(self, path_id):
        super().__init__()
        self.path_id = path_id
    def prep(self, shared_storage):
        shared_storage['path_taken'] = self.path_id
    # post implicitly returns None

# --- Test Class ---
class TestFlowComposition(unittest.TestCase):

    # --- Existing Tests (Unchanged) ---
    def test_flow_as_node(self):
        """
        1) Create a Flow (f1) starting with NumberNode(5), then AddNode(10), then MultiplyNode(2).
        2) Create a second Flow (f2) whose start is f1.
        3) Create a wrapper Flow (f3) that contains f2 to ensure proper execution.
        Expected final result in shared_storage['current']: (5 + 10) * 2 = 30.
        """
        shared_storage = {}
        f1 = Flow(start=NumberNode(5))
        f1 >> AddNode(10) >> MultiplyNode(2)
        f2 = Flow(start=f1)
        f3 = Flow(start=f2)
        f3.run(shared_storage)
        self.assertEqual(shared_storage['current'], 30)

    def test_nested_flow(self):
        """
        Demonstrates nested flows with proper wrapping:
        inner_flow: NumberNode(5) -> AddNode(3)
        middle_flow: starts with inner_flow -> MultiplyNode(4)
        wrapper_flow: contains middle_flow to ensure proper execution
        Expected final result: (5 + 3) * 4 = 32.
        """
        shared_storage = {}
        inner_flow = Flow(start=NumberNode(5))
        inner_flow >> AddNode(3)
        middle_flow = Flow(start=inner_flow)
        middle_flow >> MultiplyNode(4)
        wrapper_flow = Flow(start=middle_flow)
        wrapper_flow.run(shared_storage)
        self.assertEqual(shared_storage['current'], 32)

    def test_flow_chaining_flows(self):
        """
        Demonstrates chaining two flows with proper wrapping:
        flow1: NumberNode(10) -> AddNode(10) # final = 20
        flow2: MultiplyNode(2) # final = 40
        wrapper_flow: contains both flow1 and flow2 to ensure proper execution
        Expected final result: (10 + 10) * 2 = 40.
        """
        shared_storage = {}
        numbernode = NumberNode(10)
        numbernode >> AddNode(10)
        flow1 = Flow(start=numbernode)
        flow2 = Flow(start=MultiplyNode(2))
        flow1 >> flow2 # Default transition based on flow1 returning None
        wrapper_flow = Flow(start=flow1)
        wrapper_flow.run(shared_storage)
        self.assertEqual(shared_storage['current'], 40)

    def test_composition_with_action_propagation(self):
        """
        Test that an outer flow can branch based on the action returned
        by the last node's post() within an inner flow.
        """
        shared_storage = {}

        # 1. Define an inner flow that ends with a node returning a specific action
        inner_start_node = NumberNode(100)       # current = 100, post -> None
        inner_end_node = SignalNode("inner_done") # post -> "inner_done"
        inner_start_node >> inner_end_node
        # Inner flow will execute start->end, and the Flow's execution will return "inner_done"
        inner_flow = Flow(start=inner_start_node)

        # 2. Define target nodes for the outer flow branches
        path_a_node = PathNode("A") # post -> None
        path_b_node = PathNode("B") # post -> None

        # 3. Define the outer flow starting with the inner flow
        outer_flow = Flow()
        outer_flow.start(inner_flow) # Use the start() method

        # 4. Define branches FROM the inner_flow object based on its returned action
        inner_flow - "inner_done" >> path_b_node  # This path should be taken
        inner_flow - "other_action" >> path_a_node # This path should NOT be taken

        # 5. Run the outer flow and capture the last action
        # Execution: inner_start -> inner_end -> path_b
        last_action_outer = outer_flow.run(shared_storage)

        # 6. Assert the results
        # Check state after inner flow execution
        self.assertEqual(shared_storage.get('current'), 100)
        self.assertEqual(shared_storage.get('last_signal_emitted'), "inner_done")
        # Check that the correct outer path was taken
        self.assertEqual(shared_storage.get('path_taken'), "B")
        # Check the action returned by the outer flow. The last node executed was
        # path_b_node, which returns None from its post method.
        self.assertIsNone(last_action_outer)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="utils/update_pocketflow_mdc.py">
#!/usr/bin/env python3
"""
Script to generate MDC files from the PocketFlow docs folder, creating one MDC file per MD file.

Usage:
    python update_pocketflow_mdc.py [--docs-dir PATH] [--rules-dir PATH]
"""

import os
import re
import shutil
from pathlib import Path
import sys
import html.parser

class HTMLTagStripper(html.parser.HTMLParser):
    """HTML Parser subclass to strip HTML tags from content"""
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs = True
        self.text = []
    
    def handle_data(self, data):
        self.text.append(data)
    
    def get_text(self):
        return ''.join(self.text)

def strip_html_tags(html_content):
    """Remove HTML tags from content"""
    stripper = HTMLTagStripper()
    stripper.feed(html_content)
    return stripper.get_text()

def extract_frontmatter(file_path):
    """Extract title, parent, and nav_order from markdown frontmatter"""
    frontmatter = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # Extract frontmatter between --- markers
            fm_match = re.search(r'^---\s*(.+?)\s*---', content, re.DOTALL)
            if fm_match:
                frontmatter_text = fm_match.group(1)
                
                # Extract fields
                title_match = re.search(r'title:\s*"?([^"\n]+)"?', frontmatter_text)
                parent_match = re.search(r'parent:\s*"?([^"\n]+)"?', frontmatter_text)
                nav_order_match = re.search(r'nav_order:\s*(\d+)', frontmatter_text)
                
                if title_match:
                    frontmatter['title'] = title_match.group(1)
                if parent_match:
                    frontmatter['parent'] = parent_match.group(1)
                if nav_order_match:
                    frontmatter['nav_order'] = int(nav_order_match.group(1))
    except Exception as e:
        print(f"Error reading frontmatter from {file_path}: {e}")
    
    return frontmatter

def extract_first_heading(file_path):
    """Extract the first heading from markdown content"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # Remove frontmatter
            content = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
            
            # Find first heading
            heading_match = re.search(r'#\s+(.+)', content)
            if heading_match:
                return heading_match.group(1).strip()
    except Exception as e:
        print(f"Error extracting heading from {file_path}: {e}")
    
    # Fallback to filename if no heading found
    return Path(file_path).stem.replace('_', ' ').title()

def get_mdc_description(md_file, frontmatter, heading):
    """Generate a description for the MDC file based on file metadata"""
    section = ""
    subsection = ""
    
    # Determine section from path
    path_parts = Path(md_file).parts
    if 'core_abstraction' in path_parts:
        section = "Core Abstraction"
    elif 'design_pattern' in path_parts:
        section = "Design Pattern"
    elif 'utility_function' in path_parts:
        section = "Utility Function"
    
    # Use frontmatter title or heading as subsection
    if 'title' in frontmatter:
        subsection = frontmatter['title']
    else:
        subsection = heading
    
    # For the combined guide and index
    if Path(md_file).name == "guide.md":
        return "Guidelines for using PocketFlow, Agentic Coding"
    
    # For index.md at root level, use a different format
    if Path(md_file).name == "index.md" and section == "":
        return "Guidelines for using PocketFlow, a minimalist LLM framework"
    
    # For other files, create a more specific description
    if section:
        return f"Guidelines for using PocketFlow, {section}, {subsection}"
    else:
        return f"Guidelines for using PocketFlow, {subsection}"

def process_markdown_content(content, remove_local_refs=False):
    """Process markdown content to make it suitable for MDC file"""
    # Remove frontmatter
    content = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
    
    # Replace HTML div tags and their content
    content = re.sub(r'<div.*?>.*?</div>', '', content, flags=re.DOTALL)
    
    if remove_local_refs:
        # Replace markdown links to local documentation with just the text in brackets
        # This prevents automatically including all docs when the file is loaded
        # Keep the brackets around the text for better discoverability
        content = re.sub(r'\[([^\]]+)\]\(\./[^)]+\)', r'[\1]', content)
    else:
        # Adjust relative links to maintain references within the docs structure
        content = re.sub(r'\]\(\./([^)]+)\)', r'](mdc:./\1)', content)
        
        # Ensure links to md files work correctly
        content = re.sub(r'\]\(mdc:\./(.+?)\.md\)', r'](mdc:./\1.md)', content)
        content = re.sub(r'\]\(mdc:\./(.+?)\.html\)', r'](mdc:./\1.md)', content)
    
    # Strip remaining HTML tags
    content = strip_html_tags(content)
    
    return content

def get_documentation_first_policy():
    """Return the DOCUMENTATION FIRST POLICY text to be included in the guide"""
    return """# DOCUMENTATION FIRST POLICY

**CRITICAL INSTRUCTION**: When implementing a Pocket Flow app:

1. **ALWAYS REQUEST MDC FILES FIRST** - Before writing any code, request and review all relevant MDC documentation files. This doc provides an explaination of the documents.
2. **UNDERSTAND THE FRAMEWORK** - Gain comprehensive understanding of the Pocket Flow framework from documentation
3. **AVOID ASSUMPTION-DRIVEN DEVELOPMENT** - Do not base your implementation on assumptions or guesswork. Even if the human didn't explicitly mention pocket flow in their request, if the code you are editing is using pocket flow, you should request relevant docs to help you understand best practice as well before editing.

**VERIFICATION**: Begin each implementation with a brief summary of the documentation you've reviewed to inform your approach.

"""

def generate_mdc_header(md_file, description, always_apply=False):
    """Generate MDC file header with appropriate frontmatter"""
    # Determine if we should include globs
    # For index.md and guide.md, we include **/*.py to provide high-level context for Python files
    # For other files, leave it empty to be less intrusive
    globs = "**/*.py" if always_apply else ""
    
    return f"""---
description: {description}
globs: {globs}
alwaysApply: {"true" if always_apply else "false"}
---
"""

def has_substantive_content(content):
    """Check if the processed content has substantive content beyond the frontmatter"""
    # Remove frontmatter
    content_without_frontmatter = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
    
    # Remove whitespace and common HTML/markdown formatting
    cleaned_content = re.sub(r'\s+', '', content_without_frontmatter)
    cleaned_content = re.sub(r'{:.*?}', '', cleaned_content)
    
    # If there's almost nothing left after cleaning, consider it empty
    return len(cleaned_content) > 20  # Arbitrary threshold, adjust as needed

def create_combined_guide(docs_dir, rules_dir):
    """Create a combined guide that includes both the guide and index content"""
    docs_path = Path(docs_dir)
    rules_path = Path(rules_dir)
    
    guide_file = docs_path / "guide.md"
    index_file = docs_path / "index.md"
    
    if not guide_file.exists() or not index_file.exists():
        print("Warning: guide.md or index.md not found, skipping combined guide creation")
        return False
    
    # Get guide content and index content
    with open(guide_file, 'r', encoding='utf-8') as f:
        guide_content = f.read()
    
    with open(index_file, 'r', encoding='utf-8') as f:
        index_content = f.read()
    
    # Process the content
    processed_guide = process_markdown_content(guide_content, remove_local_refs=True)
    processed_index = process_markdown_content(index_content, remove_local_refs=True)
    
    # Get the documentation first policy
    doc_first_policy = get_documentation_first_policy()
    
    # Combine the content with the documentation first policy at the beginning
    combined_content = doc_first_policy + processed_guide + "\n\n" + processed_index
    
    # Generate the MDC header
    description = "Guidelines for using PocketFlow, Agentic Coding"
    mdc_header = generate_mdc_header(guide_file, description, always_apply=True)
    
    # Combine header and processed content
    mdc_content = mdc_header + combined_content
    
    # Create the output path with the new filename
    output_path = rules_path / "guide_for_pocketflow.mdc"
    
    # Write the MDC file
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(mdc_content)
    
    print(f"Created combined guide MDC file: {output_path}")
    return True

def convert_md_to_mdc(md_file, output_dir, docs_dir, special_treatment=False):
    """Convert a markdown file to MDC format and save to the output directory"""
    try:
        print(f"Processing: {md_file}")
        
        # Skip guide.md and index.md as they'll be handled separately
        file_name = Path(md_file).name
        if file_name in ["guide.md", "index.md"]:
            print(f"Skipping {file_name} for individual processing - it will be included in the combined guide")
            return True
        
        # Skip empty index.md files in subfolders
        parent_dir = Path(md_file).parent.name
        
        # Check if this is an index.md in a subfolder (not the main index.md)
        if (file_name == "index.md" and parent_dir != "docs" and 
            parent_dir in ["core_abstraction", "design_pattern", "utility_function"]):
            
            # Read the content
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Skip if it doesn't have substantive content
            if not has_substantive_content(content):
                print(f"Skipping empty subfolder index: {md_file}")
                return True
        
        # Extract metadata from file
        frontmatter = extract_frontmatter(md_file)
        heading = extract_first_heading(md_file)
        description = get_mdc_description(md_file, frontmatter, heading)
        
        # Read the content
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Process the content
        processed_content = process_markdown_content(content, remove_local_refs=special_treatment)
        
        # Generate the MDC header
        mdc_header = generate_mdc_header(md_file, description, always_apply=special_treatment)
        
        # Combine header and processed content
        mdc_content = mdc_header + processed_content
        
        # Perform a final check to ensure the processed content is substantive
        if not has_substantive_content(processed_content):
            print(f"Skipping file with no substantive content after processing: {md_file}")
            return True
        
        # Get the path relative to the docs directory
        rel_path = os.path.relpath(md_file, start=Path(docs_dir))
        
        # Extract just the filename and directory structure without the 'docs/' prefix
        path_parts = Path(rel_path).parts
        if len(path_parts) > 1 and path_parts[0] == 'docs':
            # Remove the 'docs/' prefix from the path
            rel_path = os.path.join(*path_parts[1:])
        
        # Create the output path
        output_path = Path(output_dir) / rel_path
        
        # Create output directory if it doesn't exist
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Change extension from .md to .mdc
        output_path = output_path.with_suffix('.mdc')
        
        # Write the MDC file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(mdc_content)
        
        print(f"Created MDC file: {output_path}")
        return True
    
    except Exception as e:
        print(f"Error converting {md_file} to MDC: {e}")
        return False

def generate_mdc_files(docs_dir, rules_dir):
    """Generate MDC files from all markdown files in the docs directory"""
    docs_path = Path(docs_dir)
    rules_path = Path(rules_dir)
    
    # Make sure the docs directory exists
    if not docs_path.exists() or not docs_path.is_dir():
        raise ValueError(f"Directory not found: {docs_dir}")
    
    print(f"Generating MDC files from docs in: {docs_dir}")
    print(f"Output will be written to: {rules_dir}")
    
    # Create the rules directory if it doesn't exist
    rules_path.mkdir(parents=True, exist_ok=True)
    
    # Create the combined guide file first (includes both guide.md and index.md)
    create_combined_guide(docs_dir, rules_dir)
    
    # Process all other markdown files
    success_count = 0
    failure_count = 0
    
    # Find all markdown files
    md_files = list(docs_path.glob("**/*.md"))
    
    # Skip the main index.md and guide.md files as we've already processed them in create_combined_guide
    md_files = [f for f in md_files if f.name != "index.md" and f.name != "guide.md"]
    
    # Process each markdown file
    for md_file in md_files:
        if convert_md_to_mdc(md_file, rules_path, docs_dir):
            success_count += 1
        else:
            failure_count += 1
    
    print(f"\nProcessed {len(md_files) + 1} markdown files:")  # +1 for the combined guide
    print(f"  - Successfully converted: {success_count + 1}")  # +1 for the combined guide
    print(f"  - Failed conversions: {failure_count}")
    
    return success_count > 0 and failure_count == 0

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate MDC files from PocketFlow docs")
    
    # Get script directory
    script_dir = Path(__file__).parent.absolute()
    
    # Default to PocketFlow/docs directory relative to script location
    default_docs_dir = (script_dir.parent / "docs").as_posix()
    
    # Default rules directory - changed to .cursor/rules
    default_rules_dir = (script_dir.parent / ".cursor" / "rules").as_posix()
    
    parser.add_argument("--docs-dir", 
                        default=default_docs_dir, 
                        help="Path to PocketFlow docs directory")
    parser.add_argument("--rules-dir", 
                        default=default_rules_dir, 
                        help="Output directory for MDC files")
    
    args = parser.parse_args()
    
    try:
        success = generate_mdc_files(args.docs_dir, args.rules_dir)
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)
</file>

<file path=".cursorrules">
---
layout: default
title: "Agentic Coding"
---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agent involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
{: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps                  | Human      | AI        | Comment                                                                 |
|:-----------------------|:----------:|:---------:|:------------------------------------------------------------------------|
| 1. Requirements | â˜…â˜…â˜… High  | â˜…â˜†â˜† Low   | Humans understand the requirements and context.                    |
| 2. Flow          | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium |  Humans specify the high-level design, and the AI fills in the details. |
| 3. Utilities   | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Data          | â˜…â˜†â˜† Low    | â˜…â˜…â˜… High   | AI designs the data schema, and humans verify.                            |
| 5. Node          | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  | The AI helps design the node based on the flow.          |
| 6. Implementation      | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI implements the flow based on the design. |
| 7. Optimization        | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans evaluate the results, and the AI helps optimize. |
| 8. Reliability         | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI writes test cases and addresses corner cases.     |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit. 
    - Understand AI systems' strengths and limitations:
      - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
      - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
      - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
    - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
    - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.
    - Identify applicable design patterns (e.g., [Map Reduce](./design_pattern/mapreduce.md), [Agent](./design_pattern/agent.md), [RAG](./design_pattern/rag.md)).
      - For each node in the flow, start with a high-level one-line description of what it does.
      - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
      - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
      - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
    - Outline the flow and draw it in a mermaid diagram. For example:
      ```mermaid
      flowchart LR
          start[Start] --> batch[Batch]
          batch --> check[Check]
          check -->|OK| process
          check -->|Error| fix[Fix]
          fix --> check
          
          subgraph process[Process]
            step1[Step 1] --> step2[Step 2]
          end
          
          process --> endNode[End]
      ```
    - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
      {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.
    - Think of your AI system as the brain. It needs a bodyâ€”these *external utility functions*â€”to interact with the real world:
        <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

        - Reading inputs (e.g., retrieving Slack messages, reading emails)
        - Writing outputs (e.g., generating reports, sending emails)
        - Using external tools (e.g., calling LLMs, searching the web)
        - **NOTE**: *LLM-based tasks* (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are *core functions* internal in the AI system.
    - For each utility function, implement it and write a simple test.
    - Document their input/output, as well as why they are necessary. For example:
      - `name`: `get_embedding` (`utils/get_embedding.py`)
      - `input`: `str`
      - `output`: a vector of 3072 floats
      - `necessity`: Used by the second node to embed text
    - Example utility implementation:
      ```python
      # utils/call_llm.py
      from openai import OpenAI

      def call_llm(prompt):    
          client = OpenAI(api_key="YOUR_API_KEY_HERE")
          r = client.chat.completions.create(
              model="gpt-4o",
              messages=[{"role": "user", "content": prompt}]
          )
          return r.choices[0].message.content
          
      if __name__ == "__main__":
          prompt = "What is the meaning of life?"
          print(call_llm(prompt))
      ```
    - > **Sometimes, design Utilities before Flow:**  For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
      {: .best-practice }
    - > **Avoid Exception Handling in Utilities**: If a utility function is called from a Node's `exec()` method, avoid using `try...except` blocks within the utility. Let the Node's built-in retry mechanism handle failures.
      {: .warning }

4. **Data Design**: Design the shared store that nodes will use to communicate.
   - One core design principle for PocketFlow is to use a well-designed [shared store](./core_abstraction/communication.md)â€”a data contract that all nodes agree upon to retrieve and store data.
      - For simple systems, use an in-memory dictionary.
      - For more complex systems or when persistence is required, use a database.
      - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
      - Example shared store design:
        ```python
        shared = {
            "user": {
                "id": "user123",
                "context": {                # Another nested dict
                    "weather": {"temp": 72, "condition": "sunny"},
                    "location": "San Francisco"
                }
            },
            "results": {}                   # Empty dict to store outputs
        }
        ```

5. **Node Design**: Plan how each node will read and write data, and use utility functions.
   - For each [Node](./core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Regular (or Batch, or Async)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function. **Avoid exception handling here**; let the Node's retry mechanism manage failures.
     - `post`: Write "embedding" to the shared store

6. **Implementation**: Implement the initial nodes and flows based on the design.
   - ðŸŽ‰ If you've reached this step, humans have finished the design. Now *Agentic Coding* begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Leverage the built-in [Node](./core_abstraction/node.md) retry and fallback mechanisms to handle failures gracefully. This helps you quickly identify weak points in the system.
   - Add logging throughout the code to facilitate debugging.

7. **Optimization**:
   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:
     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3â€“6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     {: .best-practice }

8. **Reliability**  
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `max_retries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my_project/
â”œâ”€â”€ main.py
â”œâ”€â”€ nodes.py
â”œâ”€â”€ flow.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ call_llm.py
â”‚   â””â”€â”€ search_web.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs/
    â””â”€â”€ design.md
```

- **`requirements.txt`**: Lists the Python dependencies for the project.
  ```
  PyYAML
  pocketflow
  ```

- **`docs/design.md`**: Contains project documentation for each step above. This should be *high-level* and *no-code*.
  ~~~
  # Design Doc: Your Project Name

  > Please DON'T remove notes for AI

  ## Requirements

  > Notes for AI: Keep it simple and clear.
  > If the requirements are abstract, write concrete user stories


  ## Flow Design

  > Notes for AI:
  > 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
  > 2. Present a concise, high-level description of the workflow.

  ### Applicable Design Pattern:

  1. Map the file summary into chunks, then reduce these chunks into a final summary.
  2. Agentic file finder
    - *Context*: The entire summary of the file
    - *Action*: Find the file

  ### Flow high-level Design:

  1. **First Node**: This node is for ...
  2. **Second Node**: This node is for ...
  3. **Third Node**: This node is for ...

  ```mermaid
  flowchart TD
      firstNode[First Node] --> secondNode[Second Node]
      secondNode --> thirdNode[Third Node]
  ```
  ## Utility Functions

  > Notes for AI:
  > 1. Understand the utility function definition thoroughly by reviewing the doc.
  > 2. Include only the necessary utility functions, based on nodes in the flow.

  1. **Call LLM** (`utils/call_llm.py`)
    - *Input*: prompt (str)
    - *Output*: response (str)
    - Generally used by most nodes for LLM tasks

  2. **Embedding** (`utils/get_embedding.py`)
    - *Input*: str
    - *Output*: a vector of 3072 floats
    - Used by the second node to embed text

  ## Node Design

  ### Shared Store

  > Notes for AI: Try to minimize data redundancy

  The shared store structure is organized as follows:

  ```python
  shared = {
      "key": "value"
  }
  ```

  ### Node Steps

  > Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

  1. First Node
    - *Purpose*: Provide a short explanation of the nodeâ€™s function
    - *Type*: Decide between Regular, Batch, or Async
    - *Steps*:
      - *prep*: Read "key" from the shared store
      - *exec*: Call the utility function
      - *post*: Write "key" to the shared store

  2. Second Node
    ...
  ~~~


- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one Python file to each API call, for example `call_llm.py` or `search_web.py`.
  - Each file should also include a `main()` function to try that API call
  ```python
  from google import genai
  import os

  def call_llm(prompt: str) -> str:
      client = genai.Client(
          api_key=os.getenv("GEMINI_API_KEY", ""),
      )
      model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
      response = client.models.generate_content(model=model, contents=[prompt])
      return response.text

  if __name__ == "__main__":
      test_prompt = "Hello, how are you?"

      # First call - should hit the API
      print("Making call...")
      response1 = call_llm(test_prompt, use_cache=False)
      print(f"Response: {response1}")
  ```

- **`nodes.py`**: Contains all the node definitions.
  ```python
  # nodes.py
  from pocketflow import Node
  from utils.call_llm import call_llm

  class GetQuestionNode(Node):
      def exec(self, _):
          # Get question directly from user input
          user_question = input("Enter your question: ")
          return user_question
      
      def post(self, shared, prep_res, exec_res):
          # Store the user's question
          shared["question"] = exec_res
          return "default"  # Go to the next node

  class AnswerNode(Node):
      def prep(self, shared):
          # Read question from shared
          return shared["question"]
      
      def exec(self, question):
          # Call LLM to get the answer
          return call_llm(question)
      
      def post(self, shared, prep_res, exec_res):
          # Store the answer in shared
          shared["answer"] = exec_res
  ```
- **`flow.py`**: Implements functions that create flows by importing node definitions and connecting them.
  ```python
  # flow.py
  from pocketflow import Flow
  from nodes import GetQuestionNode, AnswerNode

  def create_qa_flow():
      """Create and return a question-answering flow."""
      # Create nodes
      get_question_node = GetQuestionNode()
      answer_node = AnswerNode()
      
      # Connect nodes in sequence
      get_question_node >> answer_node
      
      # Create flow starting with input node
      return Flow(start=get_question_node)
  ```
- **`main.py`**: Serves as the project's entry point.
  ```python
  # main.py
  from flow import create_qa_flow

  # Example main function
  # Please replace this with your own main function
  def main():
      shared = {
          "question": None,  # Will be populated by GetQuestionNode from user input
          "answer": None     # Will be populated by AnswerNode
      }

      # Create the flow and run it
      qa_flow = create_qa_flow()
      qa_flow.run(shared)
      print(f"Question: {shared['question']}")
      print(f"Answer: {shared['answer']}")

  if __name__ == "__main__":
      main()
  ```

================================================
File: docs/index.md
================================================
---
layout: default
title: "Home"
nav_order: 1
---

# Pocket Flow

A [100-line](https://github.com/the-pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalist LLM framework for *Agents, Task Decomposition, RAG, etc*.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworksâ€”([Multi-](./design_pattern/multi_agent.html))[Agents](./design_pattern/agent.html), [Workflow](./design_pattern/workflow.html), [RAG](./design_pattern/rag.html), and more.  
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](./core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](./core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](./core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](./core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [Async](./core_abstraction/async.md) nodes/flows allow waiting for asynchronous tasks.
- [(Advanced) Parallel](./core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="500"/>
</div>

## Design Pattern

From there, itâ€™s easy to implement popular design patterns:

- [Agent](./design_pattern/agent.md) autonomously makes decisions.
- [Workflow](./design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](./design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](./design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](./design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](./design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="500"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer *examples*â€”please *implement your own*:

- [LLM Wrapper](./utility_function/llm.md)
- [Viz and Debug](./utility_function/viz.md)
- [Web Search](./utility_function/websearch.md)
- [Chunking](./utility_function/chunking.md)
- [Embedding](./utility_function/embedding.md)
- [Vector Databases](./utility_function/vector.md)
- [Text-to-Speech](./utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a *bad practice* for vendor-specific APIs in a general framework:
- *API Volatility*: Frequent changes lead to heavy maintenance for hardcoded APIs.
- *Flexibility*: You may want to switch vendors, use fine-tuned models, or run them locally.
- *Optimizations*: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps? 

Check out [Agentic Coding Guidance](./guide.md), the fastest way to develop LLM projects with Pocket Flow!

================================================
File: docs/core_abstraction/async.md
================================================
---
layout: default
title: "(Advanced) Async"
parent: "Core Abstraction"
nav_order: 5
---

# (Advanced) Async

**Async** Nodes implement `prep_async()`, `exec_async()`, `exec_fallback_async()`, and/or `post_async()`. This is useful for:

1. **prep_async()**: For *fetching/reading data (files, APIs, DB)* in an I/O-friendly way.
2. **exec_async()**: Typically used for async LLM calls.
3. **post_async()**: For *awaiting user feedback*, *coordinating across multi-agents* or any additional async steps after `exec_async()`.

**Note**: `AsyncNode` must be wrapped in `AsyncFlow`. `AsyncFlow` can also include regular (sync) nodes.

### Example

```python
class SummarizeThenVerify(AsyncNode):
    async def prep_async(self, shared):
        # Example: read a file asynchronously
        doc_text = await read_file_async(shared["doc_path"])
        return doc_text

    async def exec_async(self, prep_res):
        # Example: async LLM call
        summary = await call_llm_async(f"Summarize: {prep_res}")
        return summary

    async def post_async(self, shared, prep_res, exec_res):
        # Example: wait for user feedback
        decision = await gather_user_feedback(exec_res)
        if decision == "approve":
            shared["summary"] = exec_res
            return "approve"
        return "deny"

summarize_node = SummarizeThenVerify()
final_node = Finalize()

# Define transitions
summarize_node - "approve" >> final_node
summarize_node - "deny"    >> summarize_node  # retry

flow = AsyncFlow(start=summarize_node)

async def main():
    shared = {"doc_path": "document.txt"}
    await flow.run_async(shared)
    print("Final Summary:", shared.get("summary"))

asyncio.run(main())
```

================================================
File: docs/core_abstraction/batch.md
================================================
---
layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4
---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:
- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.


### Example: Summarize a Large File

```python
class MapSummaries(BatchNode):
    def prep(self, shared):
        # Suppose we have a big file; chunk it
        content = shared["data"]
        chunk_size = 10000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        return chunks

    def exec(self, chunk):
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res_list):
        combined = "\n".join(exec_res_list)
        shared["summary"] = combined
        return "default"

map_summaries = MapSummaries()
flow = Flow(start=map_summaries)
flow.run(shared)
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Example: Summarize Many Files

```python
class SummarizeAllFiles(BatchFlow):
    def prep(self, shared):
        # Return a list of param dicts (one per file)
        filenames = list(shared["data"].keys())  # e.g., ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

# Suppose we have a per-file Flow (e.g., load_file >> summarize >> reduce):
summarize_file = SummarizeFile(start=load_file)

# Wrap that flow into a BatchFlow:
summarize_all_files = SummarizeAllFiles(start=summarize_file)
summarize_all_files.run(shared)
```

### Under the Hood
1. `prep(shared)` returns a list of param dictsâ€”e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlowâ€™s own `params`.
   - It calls `flow.run(shared)` using the merged result.
3. This means the sub-Flow is run **repeatedly**, once for every param dict.

---

## 3. Nested or Multi-Level Batches

You can nest a **BatchFlow** in another **BatchFlow**. For instance:
- **Outer** batch: returns a list of diretory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parentâ€™s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

```python

class FileBatchFlow(BatchFlow):
    def prep(self, shared):
        directory = self.params["directory"]
        # e.g., files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(BatchFlow):
    def prep(self, shared):
        directories = [ "/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# MapSummaries have params like {"directory": "/path/to/dirA", "filename": "file1.txt"}
inner_flow = FileBatchFlow(start=MapSummaries())
outer_flow = DirectoryBatchFlow(start=inner_flow)
```

================================================
File: docs/core_abstraction/communication.md
================================================
---
layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3
---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)** 

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).  
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
     
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate *Data Schema* from *Compute Logic*!  This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](./batch.md).
     {: .best-practice }

2. **Params (only for [Batch](./batch.md))** 
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:
```python
shared = {"data": {}, "summary": {}, "config": {...}, ...}
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```python
class LoadData(Node):
    def post(self, shared, prep_res, exec_res):
        # We write data to shared store
        shared["data"] = "Some text content"
        return None

class Summarize(Node):
    def prep(self, shared):
        # We read data from shared store
        return shared["data"]

    def exec(self, prep_res):
        # Call LLM to summarize
        prompt = f"Summarize: {prep_res}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res):
        # We write summary to shared store
        shared["summary"] = exec_res
        return "default"

load_data = LoadData()
summarize = Summarize()
load_data >> summarize
flow = Flow(start=load_data)

shared = {}
flow.run(shared)
```

Here:
- `LoadData` writes to `shared["data"]`.
- `Summarize` reads from `shared["data"]`, summarizes, and writes to `shared["summary"]`.

---

## 2. Params

**Params** let you store *per-Node* or *per-Flow* config that doesn't need to live in the shared store. They are:
- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `set_params()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow. 
> 
> If you need to set child node params, see [Batch](./batch.md).
{: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```python
# 1) Create a Node that uses params
class SummarizeFile(Node):
    def prep(self, shared):
        # Access the node's param
        filename = self.params["filename"]
        return shared["data"].get(filename, "")

    def exec(self, prep_res):
        prompt = f"Summarize: {prep_res}"
        return call_llm(prompt)

    def post(self, shared, prep_res, exec_res):
        filename = self.params["filename"]
        shared["summary"][filename] = exec_res
        return "default"

# 2) Set params
node = SummarizeFile()

# 3) Set Node params directly (for testing)
node.set_params({"filename": "doc1.txt"})
node.run(shared)

# 4) Create Flow
flow = Flow(start=node)

# 5) Set Flow params (overwrites node params)
flow.set_params({"filename": "doc2.txt"})
flow.run(shared)  # The node summarizes doc2, not doc1
```

================================================
File: docs/core_abstraction/flow.md
================================================
---
layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2
---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `node_a >> node_b`
  This means if `node_a.post()` returns `"default"`, go to `node_b`. 
  (Equivalent to `node_a - "default" >> node_b`)

2. **Named action transition**: `node_a - "action_name" >> node_b`
  This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

It's possible to create loops, branching, or multi-step flows.

## 2. Creating a Flow

A **Flow** begins with a **start** node. You call `Flow(start=some_node)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```python
node_a >> node_b
flow = Flow(start=node_a)
flow.run(shared)
```

- When you run the flow, it executes `node_a`.  
- Suppose `node_a.post()` returns `"default"`.  
- The flow then sees `"default"` Action is linked to `node_b` and runs `node_b`.  
- `node_b.post()` returns `"default"` but we didn't define `node_b >> something_else`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision 
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```python
# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

flow = Flow(start=review)
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action. 
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
> 
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
{: .warning }

## 3. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.  
2. Combine multiple smaller Flows into a larger Flow for reuse.  
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `None` for `exec_res` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```python
# Create a sub-flow
node_a >> node_b
subflow = Flow(start=node_a)

# Connect it to another node
subflow >> node_c

# Create the parent flow
parent_flow = Flow(start=subflow)
```

When `parent_flow.run()` executes:
1. It starts `subflow`
2. `subflow` runs through its nodes (`node_a->node_b`)
3. After `subflow` completes, execution continues to `node_c`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```python
# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow

# Create the master flow
order_pipeline = Flow(start=payment_flow)

# Run the entire pipeline
order_pipeline.run(shared_data)
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

================================================
File: docs/core_abstraction/node.md
================================================
---
layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1
---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`
   - **Read and preprocess data** from `shared` store. 
   - Examples: *query DB, read files, or serialize data into a string*.
   - Return `prep_res`, which is used by `exec()` and `post()`.

2. `exec(prep_res)`
   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: *(mostly) LLM calls, remote APIs, tool use*.
   - âš ï¸ This shall be only for compute and **NOT** access `shared`.
   - âš ï¸ If retries enabled, ensure idempotent implementation.
   - âš ï¸ Defer exception handling to the Node's built-in retry mechanism.
   - Return `exec_res`, which is passed to `post()`.

3. `post(shared, prep_res, exec_res)`
   - **Postprocess and write data** back to `shared`.
   - Examples: *update DB, change states, log results*.
   - **Decide the next action** by returning a *string* (`action = "default"` if *None*).

> **Why 3 steps?** To enforce the principle of *separation of concerns*. The data storage and data processing are operated separately.
>
> All steps are *optional*. E.g., you can only implement `prep` and `post` if you just need to process data.
{: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting). 
`wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```python 
my_node = SummarizeFile(max_retries=3, wait=10)
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `max_retries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `self.cur_retry`.

```python 
class RetryNode(Node):
    def exec(self, prep_res):
        print(f"Retry {self.cur_retry} times")
        raise Exception("Failed")
```

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```python 
def exec_fallback(self, prep_res, exc):
    raise exc
```

By default, it just re-raises exception. But you can return a fallback result instead, which becomes the `exec_res` passed to `post()`.

### Example: Summarize file

```python 
class SummarizeFile(Node):
    def prep(self, shared):
        return shared["data"]

    def exec(self, prep_res):
        if not prep_res:
            return "Empty file content"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    def exec_fallback(self, prep_res, exc):
        # Provide a simple fallback instead of crashing
        return "There was an error processing your request."

    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # Return "default" by not returning

summarize_node = SummarizeFile(max_retries=3)

# node.run() calls prep->exec->post
# If exec() fails, it retries up to 3 times before calling exec_fallback()
action_result = summarize_node.run(shared)

print("Action returned:", action_result)  # "default"
print("Summary stored:", shared["summary"])
```

================================================
File: docs/core_abstraction/parallel.md
================================================
---
layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6
---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple **Async** Nodes and Flows  **concurrently**â€”for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute. 

> Because of Pythonâ€™s GIL, parallel nodes and flows canâ€™t truly parallelize CPU-bound tasks (e.g., heavy numerical computations). However, they excel at overlapping I/O-bound workâ€”like LLM calls, database queries, API requests, or file I/O.
{: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
> 
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism (e.g., semaphores or sleep intervals).
> 
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
{: .best-practice }

## AsyncParallelBatchNode

Like **AsyncBatchNode**, but run `exec_async()` in **parallel**:

```python
class ParallelSummaries(AsyncParallelBatchNode):
    async def prep_async(self, shared):
        # e.g., multiple texts
        return shared["texts"]

    async def exec_async(self, text):
        prompt = f"Summarize: {text}"
        return await call_llm_async(prompt)

    async def post_async(self, shared, prep_res, exec_res_list):
        shared["summary"] = "\n\n".join(exec_res_list)
        return "default"

node = ParallelSummaries()
flow = AsyncFlow(start=node)
```

## AsyncParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using different parameters:

```python
class SummarizeMultipleFiles(AsyncParallelBatchFlow):
    async def prep_async(self, shared):
        return [{"filename": f} for f in shared["files"]]

sub_flow = AsyncFlow(start=LoadAndSummarizeFile())
parallel_flow = SummarizeMultipleFiles(start=sub_flow)
await parallel_flow.run_async(shared)
```

================================================
File: docs/design_pattern/agent.md
================================================
---
layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.  
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodesâ€”and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide actionâ€”for example:

```python
f"""
### CONTEXT
Task: {task_description}
Previous Actions: {previous_actions}
Current State: {current_state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Conclude based on the results
  Parameters:
    - result (str): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
```"""
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide *relevant, minimal context.* For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide *a well-structured and unambiguous* set of actionsâ€”avoiding overlap like separate `read_databases` or  `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:
1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

```python
class DecideAction(Node):
    def prep(self, shared):
        context = shared.get("context", "No previous search")
        query = shared["query"]
        return query, context
        
    def exec(self, inputs):
        query, context = inputs
        prompt = f"""
Given input: {query}
Previous search results: {context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
```yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
```"""
        resp = call_llm(prompt)
        yaml_str = resp.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        assert isinstance(result, dict)
        assert "action" in result
        assert "reason" in result
        assert result["action"] in ["search", "answer"]
        if result["action"] == "search":
            assert "search_term" in result
        
        return result

    def post(self, shared, prep_res, exec_res):
        if exec_res["action"] == "search":
            shared["search_term"] = exec_res["search_term"]
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        return shared["search_term"]
        
    def exec(self, search_term):
        return search_web(search_term)
    
    def post(self, shared, prep_res, exec_res):
        prev_searches = shared.get("context", [])
        shared["context"] = prev_searches + [
            {"term": shared["search_term"], "result": exec_res}
        ]
        return "decide"
        
class DirectAnswer(Node):
    def prep(self, shared):
        return shared["query"], shared.get("context", "")
        
    def exec(self, inputs):
        query, context = inputs
        return call_llm(f"Context: {context}\nAnswer: {query}")

    def post(self, shared, prep_res, exec_res):
       print(f"Answer: {exec_res}")
       shared["answer"] = exec_res

# Connect nodes
decide = DecideAction()
search = SearchWeb()
answer = DirectAnswer()

decide - "search" >> search
decide - "answer" >> answer
search - "decide" >> decide  # Loop back

flow = Flow(start=decide)
flow.run({"query": "Who won the Nobel Prize in Physics 2024?"})
```

================================================
File: docs/design_pattern/mapreduce.md
================================================
---
layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:
- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts. 

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```python
class SummarizeAllFiles(BatchNode):
    def prep(self, shared):
        files_dict = shared["files"]  # e.g. 10 files
        return list(files_dict.items())  # [("file1.txt", "aaa..."), ("file2.txt", "bbb..."), ...]

    def exec(self, one_file):
        filename, file_content = one_file
        summary_text = call_llm(f"Summarize the following file:\n{file_content}")
        return (filename, summary_text)

    def post(self, shared, prep_res, exec_res_list):
        shared["file_summaries"] = dict(exec_res_list)

class CombineSummaries(Node):
    def prep(self, shared):
        return shared["file_summaries"]

    def exec(self, file_summaries):
        # format as: "File1: summary\nFile2: summary...\n"
        text_list = []
        for fname, summ in file_summaries.items():
            text_list.append(f"{fname} summary:\n{summ}\n")
        big_text = "\n---\n".join(text_list)

        return call_llm(f"Combine these file summaries into one final summary:\n{big_text}")

    def post(self, shared, prep_res, final_summary):
        shared["all_files_summary"] = final_summary

batch_node = SummarizeAllFiles()
combine_node = CombineSummaries()
batch_node >> combine_node

flow = Flow(start=batch_node)

shared = {
    "files": {
        "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
        "file2.txt": "Some other interesting text ...",
        # ...
    }
}
flow.run(shared)
print("Individual Summaries:", shared["file_summaries"])
print("\nFinal Summary:\n", shared["all_files_summary"])
```

================================================
File: docs/design_pattern/rag.md
================================================
---
layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---
## Stage 1: Offline Indexing

We create three Nodes:
1. `ChunkDocs` â€“ [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` â€“ [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` â€“ stores embeddings into a [vector database](../utility_function/vector.md).

```python
class ChunkDocs(BatchNode):
    def prep(self, shared):
        # A list of file paths in shared["files"]. We process each file.
        return shared["files"]

    def exec(self, filepath):
        # read file content. In real usage, do error handling.
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        # chunk by 100 chars each
        chunks = []
        size = 100
        for i in range(0, len(text), size):
            chunks.append(text[i : i + size])
        return chunks
    
    def post(self, shared, prep_res, exec_res_list):
        # exec_res_list is a list of chunk-lists, one per file.
        # flatten them all into a single list of chunks.
        all_chunks = []
        for chunk_list in exec_res_list:
            all_chunks.extend(chunk_list)
        shared["all_chunks"] = all_chunks

class EmbedDocs(BatchNode):
    def prep(self, shared):
        return shared["all_chunks"]

    def exec(self, chunk):
        return get_embedding(chunk)

    def post(self, shared, prep_res, exec_res_list):
        # Store the list of embeddings.
        shared["all_embeds"] = exec_res_list
        print(f"Total embeddings: {len(exec_res_list)}")

class StoreIndex(Node):
    def prep(self, shared):
        # We'll read all embeds from shared.
        return shared["all_embeds"]

    def exec(self, all_embeds):
        # Create a vector index (faiss or other DB in real usage).
        index = create_index(all_embeds)
        return index

    def post(self, shared, prep_res, index):
        shared["index"] = index

# Wire them in sequence
chunk_node = ChunkDocs()
embed_node = EmbedDocs()
store_node = StoreIndex()

chunk_node >> embed_node >> store_node

OfflineFlow = Flow(start=chunk_node)
```

Usage example:

```python
shared = {
    "files": ["doc1.txt", "doc2.txt"],  # any text files
}
OfflineFlow.run(shared)
```

---
## Stage 2: Online Query & Answer

We have 3 nodes:
1. `EmbedQuery` â€“ embeds the userâ€™s question.
2. `RetrieveDocs` â€“ retrieves top chunk from the index.
3. `GenerateAnswer` â€“ calls the LLM with the question + chunk to produce the final answer.

```python
class EmbedQuery(Node):
    def prep(self, shared):
        return shared["question"]

    def exec(self, question):
        return get_embedding(question)

    def post(self, shared, prep_res, q_emb):
        shared["q_emb"] = q_emb

class RetrieveDocs(Node):
    def prep(self, shared):
        # We'll need the query embedding, plus the offline index/chunks
        return shared["q_emb"], shared["index"], shared["all_chunks"]

    def exec(self, inputs):
        q_emb, index, chunks = inputs
        I, D = search_index(index, q_emb, top_k=1)
        best_id = I[0][0]
        relevant_chunk = chunks[best_id]
        return relevant_chunk

    def post(self, shared, prep_res, relevant_chunk):
        shared["retrieved_chunk"] = relevant_chunk
        print("Retrieved chunk:", relevant_chunk[:60], "...")

class GenerateAnswer(Node):
    def prep(self, shared):
        return shared["question"], shared["retrieved_chunk"]

    def exec(self, inputs):
        question, chunk = inputs
        prompt = f"Question: {question}\nContext: {chunk}\nAnswer:"
        return call_llm(prompt)

    def post(self, shared, prep_res, answer):
        shared["answer"] = answer
        print("Answer:", answer)

embed_qnode = EmbedQuery()
retrieve_node = RetrieveDocs()
generate_node = GenerateAnswer()

embed_qnode >> retrieve_node >> generate_node
OnlineFlow = Flow(start=embed_qnode)
```

Usage example:

```python
# Suppose we already ran OfflineFlow and have:
# shared["all_chunks"], shared["index"], etc.
shared["question"] = "Why do people like cats?"

OnlineFlow.run(shared)
# final answer in shared["answer"]
```

================================================
File: docs/design_pattern/structure.md
================================================
---
layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:
- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information 

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:
1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

```python
class SummarizeNode(Node):
    def exec(self, prep_res):
        # Suppose `prep_res` is the text to summarize.
        prompt = f"""
Please summarize the following text as YAML, with exactly 3 bullet points

{prep_res}

Now, output:
```yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()

        import yaml
        structured_result = yaml.safe_load(yaml_str)

        assert "summary" in structured_result
        assert isinstance(structured_result["summary"], list)

        return structured_result
```

> Besides using `assert` statements, another popular way to validate schemas is [Pydantic](https://github.com/pydantic/pydantic)
{: .note }

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**  

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**  

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotesâ€”just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.

================================================
File: docs/design_pattern/workflow.md
================================================
---
layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be *too complex for one LLM call*.
> - You don't want to make each task **too granular**, because then *the LLM call doesn't have enough context* and results are *not consistent across nodes*.
> 
> You usually need multiple *iterations* to find the *sweet spot*. If the task has too many *edge cases*, consider using [Agents](./agent.md).
{: .best-practice }

### Example: Article Writing

```python
class GenerateOutline(Node):
    def prep(self, shared): return shared["topic"]
    def exec(self, topic): return call_llm(f"Create a detailed outline for an article about {topic}")
    def post(self, shared, prep_res, exec_res): shared["outline"] = exec_res

class WriteSection(Node):
    def prep(self, shared): return shared["outline"]
    def exec(self, outline): return call_llm(f"Write content based on this outline: {outline}")
    def post(self, shared, prep_res, exec_res): shared["draft"] = exec_res

class ReviewAndRefine(Node):
    def prep(self, shared): return shared["draft"]
    def exec(self, draft): return call_llm(f"Review and improve this draft: {draft}")
    def post(self, shared, prep_res, exec_res): shared["final_article"] = exec_res

# Connect nodes
outline = GenerateOutline()
write = WriteSection()
review = ReviewAndRefine()

outline >> write >> review

# Create and run flow
writing_flow = Flow(start=outline)
shared = {"topic": "AI Safety"}
writing_flow.run(shared)
```

For *dynamic cases*, consider using [Agents](./agent.md).

================================================
File: docs/utility_function/llm.md
================================================
---
layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1
---

# LLM Wrappers

Check out libraries like [litellm](https://github.com/BerriAI/litellm). 
Here, we provide some minimal example implementations:

1. OpenAI
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_API_KEY_HERE")
        r = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content

    # Example usage
    call_llm("How are you?")
    ```
    > Store the API key in an environment variable like OPENAI_API_KEY for security.
    {: .best-practice }

2. Claude (Anthropic)
    ```python
    def call_llm(prompt):
        from anthropic import Anthropic
        client = Anthropic(api_key="YOUR_API_KEY_HERE")
        r = client.messages.create(
            model="claude-sonnet-4-0",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return r.content[0].text
    ```

3. Google (Generative AI Studio / PaLM API)
    ```python
    def call_llm(prompt):
    from google import genai
    client = genai.Client(api_key='GEMINI_API_KEY')
        response = client.models.generate_content(
        model='gemini-2.5-pro',
        contents=prompt
    )
    return response.text
    ```

4. Azure (Azure OpenAI)
    ```python
    def call_llm(prompt):
        from openai import AzureOpenAI
        client = AzureOpenAI(
            azure_endpoint="https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
            api_key="YOUR_API_KEY_HERE",
            api_version="2023-05-15"
        )
        r = client.chat.completions.create(
            model="<YOUR_DEPLOYMENT_NAME>",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```

5. Ollama (Local LLM)
    ```python
    def call_llm(prompt):
        from ollama import chat
        response = chat(
            model="llama2",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.message.content
    ```

## Improvements
Feel free to enhance your `call_llm` function as needed. Here are examples:

- Handle chat history:

```python
def call_llm(messages):
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return r.choices[0].message.content
```

- Add in-memory caching 

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def call_llm(prompt):
    # Your implementation here
    pass
```

> âš ï¸ Caching conflicts with Node retries, as retries yield the same result.
>
> To address this, you could use cached results only if not retried.
{: .warning }


```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_call(prompt):
    pass

def call_llm(prompt, use_cache):
    if use_cache:
        return cached_call(prompt)
    # Call the underlying function directly
    return cached_call.__wrapped__(prompt)

class SummarizeNode(Node):
    def exec(self, text):
        return call_llm(f"Summarize: {text}", self.cur_retry==0)
```

- Enable logging:

```python
def call_llm(prompt):
    import logging
    logging.info(f"Prompt: {prompt}")
    response = ... # Your implementation here
    logging.info(f"Response: {response}")
    return response
```
</file>

<file path=".gitignore">
# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db


# IDE specific files
.idea/
.vscode/
*.swp
*.swo
*~

# Node
node_modules/
npm-debug.log
yarn-debug.log
yarn-error.log
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
venv/
ENV/

# Logs and databases
*.log
*.sql
*.sqlite

# Build output
dist/
build/
out/

# Coverage reports
coverage/
.coverage
.coverage.*
htmlcov/

# Misc
*.bak
*.tmp
*.temp


test.ipynb
.pytest_cache/
cookbook/pocketflow-multi-agent/.python-version


# local
uv.lock
.python-version
pyproject.toml
usage.md
cookbook/pocketflow-minimal-example/viz/flow_visualization.html
cookbook/pocketflow-minimal-example/viz/flow_visualization.json
.claude/
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Zachary Huang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="README.md">
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/title.png" alt="Pocket Flow â€“ 100-line minimalist LLM framework" width="600"/>
</div>

<!-- For translation, replace English with [English](https://github.com/The-Pocket/PocketFlow/blob/main/README.md), and remove the link for the target language. -->

English | [ä¸­æ–‡](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_CHINESE.md) | [EspaÃ±ol](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_SPANISH.md) | [æ—¥æœ¬èªž](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_JAPANESE.md) | [Deutsch](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_GERMAN.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_RUSSIAN.md) | [PortuguÃªs](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_PORTUGUESE.md) | [FranÃ§ais](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_FRENCH.md) | [í•œêµ­ì–´](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-batch/translations/README_KOREAN.md)

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

Pocket Flow is a [100-line](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalist LLM framework

- **Lightweight**: Just 100 lines. Zero bloat, zero dependencies, zero vendor lock-in.
  
- **Expressive**: Everything you loveâ€”([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), and more.

- **[Agentic Coding](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Let AI Agents (e.g., Cursor AI) build Agentsâ€”10x productivity boost!

Get started with Pocket Flow:
- To install, ```pip install pocketflow```or just copy the [source code](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) (only 100 lines).
- To learn more, check out the [video tutorial](https://youtu.be/0Zr3NwcvpA0) and [documentation](https://the-pocket.github.io/PocketFlow/)
- ðŸŽ‰ Join our [Discord](https://discord.gg/hUHHE9Sa6T) to connect with other developers building with Pocket Flow!
- ðŸŽ‰ Pocket Flow now has [Typescript](https://github.com/The-Pocket/PocketFlow-Typescript), [Java](https://github.com/The-Pocket/PocketFlow-Java), [C++](https://github.com/The-Pocket/PocketFlow-CPP), [Go](https://github.com/The-Pocket/PocketFlow-Go), [Rust](https://github.com/The-Pocket/PocketFlow-Rust) and [PHP](https://github.com/The-Pocket/PocketFlow-PHP) versions!

## Why Pocket Flow?

Current LLM frameworks are bloated... You only need 100 lines for LLM Framework!

<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/meme.jpg" width="400"/>


  |                | **Abstraction**          | **App-Specific Wrappers**                                      | **Vendor-Specific Wrappers**                                    | **Lines**       | **Size**    |
|----------------|:-----------------------------: |:-----------------------------------------------------------:|:------------------------------------------------------------:|:---------------:|:----------------------------:|
| LangChain  | Agent, Chain               | Many <br><sup><sub>(e.g., QA, Summarization)</sub></sup>              | Many <br><sup><sub>(e.g., OpenAI, Pinecone, etc.)</sub></sup>                   | 405K          | +166MB                     |
| CrewAI     | Agent, Chain            | Many <br><sup><sub>(e.g., FileReadTool, SerperDevTool)</sub></sup>         | Many <br><sup><sub>(e.g., OpenAI, Anthropic, Pinecone, etc.)</sub></sup>        | 18K           | +173MB                     |
| SmolAgent   | Agent                      | Some <br><sup><sub>(e.g., CodeAgent, VisitWebTool)</sub></sup>         | Some <br><sup><sub>(e.g., DuckDuckGo, Hugging Face, etc.)</sub></sup>           | 8K            | +198MB                     |
| LangGraph   | Agent, Graph           | Some <br><sup><sub>(e.g., Semantic Search)</sub></sup>                     | Some <br><sup><sub>(e.g., PostgresStore, SqliteSaver, etc.) </sub></sup>        | 37K           | +51MB                      |
| AutoGen    | Agent                | Some <br><sup><sub>(e.g., Tool Agent, Chat Agent)</sub></sup>              | Many <sup><sub>[Optional]<br> (e.g., OpenAI, Pinecone, etc.)</sub></sup>        | 7K <br><sup><sub>(core-only)</sub></sup>    | +26MB <br><sup><sub>(core-only)</sub></sup>          |
| **PocketFlow** | **Graph**                    | **None**                                                 | **None**                                                  | **100**       | **+56KB**                  |

</div>

## How does Pocket Flow work?

The [100 lines](https://github.com/The-Pocket/PocketFlow/blob/main/pocketflow/__init__.py) capture the core abstraction of LLM frameworks: Graph!
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/abstraction.png" width="900"/>
</div>
<br>

From there, it's easy to implement popular design patterns like ([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), etc.
<br>
<div align="center">
  <img src="https://github.com/The-Pocket/.github/raw/main/assets/design.png" width="900"/>
</div>
<br>
âœ¨ Below are basic tutorials:

<div align="center">
  
|  Name  | Difficulty    |  Description  |  
| :-------------:  | :-------------: | :--------------------- |  
| [Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat) | â˜†â˜†â˜† <sup>*Dummy*</sup>  | A basic chat bot with conversation history |
| [Structured Output](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-structured-output) | â˜†â˜†â˜† <sup>*Dummy*</sup> | Extracting structured data from resumes by prompting |
| [Workflow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-workflow) | â˜†â˜†â˜† <sup>*Dummy*</sup> | A writing workflow that outlines, writes content, and applies styling |
| [Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-agent) | â˜†â˜†â˜† <sup>*Dummy*</sup>  | A research agent that can search the web and answer questions |
| [RAG](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-rag) | â˜†â˜†â˜† <sup>*Dummy*</sup> | A simple Retrieval-augmented Generation process |
| [Batch](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-batch) | â˜†â˜†â˜† <sup>*Dummy*</sup> | A batch processor that translates markdown into multiple languages |
| [Streaming](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-llm-streaming) | â˜†â˜†â˜† <sup>*Dummy*</sup> | A real-time LLM streaming demo with user interrupt capability |
| [Chat Guardrail](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-guardrail) | â˜†â˜†â˜† <sup>*Dummy*</sup> | A travel advisor chatbot that only processes travel-related queries |
| [Majority Vote](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-majority-vote) | â˜†â˜†â˜† <sup>*Dummy*</sup> | Improve reasoning accuracy by aggregating multiple solution attempts |
| [Map-Reduce](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-map-reduce) | â˜†â˜†â˜† <sup>*Dummy*</sup>  | Batch resume qualification using map-reduce pattern |
| [CLI HITL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-cli-hitl) | â˜†â˜†â˜† <sup>*Dummy*</sup>  | A command-line joke generator with human-in-the-loop feedback |
| [Multi-Agent](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-multi-agent) | â˜…â˜†â˜† <sup>*Beginner*</sup> | A Taboo word game for async communication between 2 agents |
| [Supervisor](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-supervisor) | â˜…â˜†â˜† <sup>*Beginner*</sup> | Research agent is getting unreliable... Let's build a supervision process|
| [Parallel](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch) |  â˜…â˜†â˜† <sup>*Beginner*</sup> | A parallel execution demo that shows 3x speedup |
| [Parallel Flow](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-parallel-batch-flow) | â˜…â˜†â˜† <sup>*Beginner*</sup> | A parallel image processing showing 8x speedup |
| [Thinking](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-thinking) |  â˜…â˜†â˜† <sup>*Beginner*</sup> | Solve complex reasoning problems through Chain-of-Thought |
| [Memory](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-chat-memory) |  â˜…â˜†â˜† <sup>*Beginner*</sup> | A chat bot with short-term and long-term memory |
| [Text2SQL](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-text2sql) |  â˜…â˜†â˜† <sup>*Beginner*</sup>  | Convert natural language to SQL queries with an auto-debug loop |
| [Code Generator](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-code-generator) | â˜…â˜†â˜† <sup>*Beginner*</sup> | Generate test cases, implement solutions, and iteratively improve code |
| [MCP](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-mcp) |  â˜…â˜†â˜† <sup>*Beginner*</sup> |  Agent using Model Context Protocol for numerical operations |
| [A2A](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-a2a) |  â˜…â˜†â˜† <sup>*Beginner*</sup> | Agent wrapped with A2A protocol for inter-agent communication |
| [Streamlit FSM](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-streamlit-fsm) | â˜…â˜†â˜† <sup>*Beginner*</sup> | Streamlit app with finite state machine for HITL image generation |
| [FastAPI WebSocket](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-fastapi-websocket) | â˜…â˜†â˜† <sup>*Beginner*</sup> | Real-time chat interface with streaming LLM responses via WebSocket |
| [FastAPI Background](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-fastapi-background) | â˜…â˜†â˜† <sup>*Beginner*</sup> | FastAPI app with background jobs and real-time progress via SSE |
| [Voice Chat](https://github.com/The-Pocket/PocketFlow/tree/main/cookbook/pocketflow-voice-chat) | â˜…â˜†â˜† <sup>*Beginner*</sup> | An interactive voice chat application with VAD, STT, LLM, and TTS. |

</div>

ðŸ‘€ Want to see other tutorials for dummies? [Create an issue!](https://github.com/The-Pocket/PocketFlow/issues/new)

## How to Use Pocket Flow?

ðŸš€ Through **Agentic Coding**â€”the fastest LLM App development paradigm-where *humans design* and *agents code*!

<br>
<div align="center">
  <a href="https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to" target="_blank">
    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F423a39af-49e8-483b-bc5a-88cc764350c6_1050x588.png" width="700" alt="IMAGE ALT TEXT" style="cursor: pointer;">
  </a>
</div>
<br>

âœ¨ Below are examples of more complex LLM Apps:

<div align="center">
  
|  App Name     |  Difficulty    | Topics  | Human Design | Agent Code |
| :-------------:  | :-------------: | :---------------------: |  :---: |  :---: |
| [Website Chatbot](https://github.com/The-Pocket/PocketFlow-Tutorial-Website-Chatbot) <br> <sup><sub>Turn your website into a 24/7 customer support genius</sup></sub> | â˜…â˜…â˜† <br> *Medium* | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) <br> [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) | [Design Doc](https://github.com/The-Pocket/PocketFlow-Tutorial-Website-Chatbot/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/PocketFlow-Tutorial-Website-Chatbot/blob/main/flow.py)
| [Danganronpa Simulator](https://github.com/The-Pocket/PocketFlow-Tutorial-Danganronpa-Simulator) <br> <sup><sub>Forget the Turing test. Danganronpa, the ultimate AI experiment!</sup></sub> | â˜…â˜…â˜… <br> *Advanced*   | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) <br> [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Design Doc](https://github.com/The-Pocket/PocketFlow-Tutorial-Danganronpa-Simulator/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/PocketFlow-Tutorial-Danganronpa-Simulator/blob/main/flow.py)
| [Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge) <br> <sup><sub>Life's too short to stare at others' code in confusion</sup></sub> |  â˜…â˜…â˜† <br> *Medium* | [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html) | [Design Doc](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge/blob/main/flow.py)
| [Build Cursor with Cursor](https://github.com/The-Pocket/Tutorial-Cursor) <br> <sup><sub>We'll reach the singularity soon ...</sup></sub> | â˜…â˜…â˜… <br> *Advanced*   | [Agent](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html) | [Design Doc](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Cursor/blob/main/flow.py)
| [Ask AI Paul Graham](https://github.com/The-Pocket/Tutorial-YC-Partner) <br> <sup><sub>Ask AI Paul Graham, in case you don't get in</sup></sub> | â˜…â˜…â˜† <br> *Medium*  | [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html) <br> [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [TTS](https://the-pocket.github.io/PocketFlow/utility_function/text_to_speech.html) | [Design Doc](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-AI-Paul-Graham/blob/main/flow.py)
| [Youtube Summarizer](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple)  <br> <sup><sub> Explain YouTube Videos to you like you're 5 </sup></sub> | â˜…â˜†â˜† <br> *Beginner*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) |  [Design Doc](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple/blob/main/flow.py)
| [Cold Opener Generator](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization)  <br> <sup><sub> Instant icebreakers that turn cold leads hot </sup></sub> | â˜…â˜†â˜† <br> *Beginner*   | [Map Reduce](https://the-pocket.github.io/PocketFlow/design_pattern/mapreduce.html) <br> [Web Search](https://the-pocket.github.io/PocketFlow/utility_function/websearch.html) |  [Design Doc](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/docs/design.md) | [Flow Code](https://github.com/The-Pocket/Tutorial-Cold-Email-Personalization/blob/master/flow.py)


</div>

- Want to learn **Agentic Coding**?

  - Check out [my YouTube](https://www.youtube.com/@ZacharyLLM?sub_confirmation=1) for video tutorial on how some apps above are made!

  - Want to build your own LLM App? Read this [post](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)! Start with [this template](https://github.com/The-Pocket/PocketFlow-Template-Python)!
</file>

<file path="setup.py">
from setuptools import setup, find_packages

setup(
    name="pocketflow",
    version="0.0.3",
    packages=find_packages(),
    author="Zachary Huang",
    author_email="zh2408@columbia.edu",
    description="Pocket Flow: 100-line LLM framework. Let Agents build Agents!",
    url="https://github.com/The-Pocket/PocketFlow",
)
</file>

</files>
</file>

<file path=".docs/zig-0.15.1-io-interface-context7-docs.txt">
================
CODE SNIPPETS
================
TITLE: Zig: Add io.Writer.writeStruct
DESCRIPTION: Adds a `writeStruct` method to the `io.Writer` interface. This allows for convenient writing of struct data to output streams.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
add io.Writer.writeStruct
```

--------------------------------

TITLE: Configure Zig IO Mode to Evented
DESCRIPTION: This code snippet demonstrates how to configure the IO mode for a Zig application to be evented. Setting `pub const io_mode = .evented;` in the root source file enables asynchronous I/O operations where functions like `std.os.read` will suspend on EAGAIN until the file descriptor is available for reading. This integration currently works on Linux.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
pub const io_mode = .evented;
```

--------------------------------

TITLE: Zig system interface selection
DESCRIPTION: This Zig code snippet defines the `system` constant, which selects the appropriate system interface based on the target operating system and whether libc is linked. If libc is linked, it uses `std.c`. Otherwise, it dispatches to OS-specific interfaces like `darwin`, `freebsd`, `linux`, `wasi`, `windows`, or `zen`. For unsupported OSes, it defaults to an empty struct.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
/// When linking libc, this is the C API. Otherwise, it is the OS-specific system interface. pub const system = if (builtin.link_libc) std.c else switch (builtin.os) {
    .macosx, .ios, .watchos, .tvos => darwin,
    .freebsd => freebsd,
    .linux => linux,
    .netbsd => netbsd,
    .dragonfly => dragonfly,
    .wasi => wasi,
    .windows => windows,
    .zen => zen,
    else => struct {},
};
```

--------------------------------

TITLE: Check if Zig IO Mode is Async
DESCRIPTION: This snippet shows how to check if the current IO mode in Zig is set to asynchronous. The `std.io.is_async` boolean is equivalent to checking if `std.io.mode` is equal to `.evented`. This flag influences how `std.io.InStream` handles read operations, potentially using async function pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.io.is_async
```

LANGUAGE: Zig
CODE:
```
std.io.mode == .evented
```

--------------------------------

TITLE: Zig: Linux fallocate() to io_uring
DESCRIPTION: Adds the `fallocate()` system call to the Linux `io_uring` interface. This allows for pre-allocation of disk space for files asynchronously.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
linux: add fallocate() to io_uring
```

--------------------------------

TITLE: BoundedArray Add Writer interface
DESCRIPTION: Adds `Writer` interface compatibility to `BoundedArray`. This allows `BoundedArray` instances to be used with functions and libraries that expect a `Writer`, facilitating easier data output.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
BoundedArray: Add Writer interface
```

--------------------------------

TITLE: Zig Allocator Interface Update
DESCRIPTION: Illustrates the breaking changes to the `mem.Allocator` interface in Zig 0.9. It shows how to update function parameter types and struct field types from `*Allocator` to `Allocator`, and how to access the allocator instance from a pointer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
*Allocator
```

LANGUAGE: zig
CODE:
```
Allocator
```

LANGUAGE: zig
CODE:
```
&gpa.allocator
```

LANGUAGE: zig
CODE:
```
gpa.allocator()
```

--------------------------------

TITLE: Zig Allocator Interface Resize
DESCRIPTION: Demonstrates how the Zig Allocator interface now allows implementations to refuse shrinking, improving efficiency for ArrayList by avoiding unnecessary data copying during resize operations. It shows a fallback mechanism for allocating new buffers and copying data when in-place resizing fails.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
const old_memory = self.allocatedSlice();
if (allocator.resize(old_memory, new_capacity)) {
    self.capacity = new_capacity;
} else {
    const new_memory = try allocator.alignedAlloc(T, alignment, new_capacity);
    @memcpy(new_memory[0..self.items.len], self.items);
    allocator.free(old_memory);
    self.items.ptr = new_memory.ptr;
    self.capacity = new_memory.len;
}
```

--------------------------------

TITLE: Fix Reader.readUntilDelimiterOrEofAlloc() API
DESCRIPTION: This update fixes the API for Reader.readUntilDelimiterOrEofAlloc(), ensuring correct behavior when reading until a delimiter or the end of the file in Zig's io module.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Zig Standard Library: io_uring Enhancements for Linux
DESCRIPTION: Details improvements to the `io_uring` interface in Zig's standard library for Linux. This includes fixes for version checks in tests, optimizations for `IO_Uring.copy_cqe`, and the addition of `recvmsg` and `sendmsg` operations. It also covers automatic buffer selection, new flags and opcodes, and support for optional arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example usage of io_uring (conceptual)
const io_uring = std.io.uring;

pi@async fn example() !void {
    var ring = try io_uring.Ring.init(.{});
    defer ring.deinit();

    // Example: Using recvmsg
    var msg: msghdr = undefined;
    var cmsg: cmsghdr = undefined;
    msg.msg_control = &cmsg;
    // ... setup msg and associated buffers ...
    try ring.recvmsg(1, &msg, null);

    // Example: Using sendmsg
    try ring.sendmsg(2, &msg, null);

    // Example: Automatic buffer selection
    try ring.read(3, .{ .buf_group = .auto });
}

```

--------------------------------

TITLE: Adapt Stream to New API in Zig
DESCRIPTION: Demonstrates how to use `adaptToNewApi()` to create a new stream interface from an old one. This is useful when migrating from older stream implementations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.15.1/release-notes.html

LANGUAGE: zig
CODE:
```
fn foo(old_writer: anytype) !void {
    var adapter = old_writer.adaptToNewApi(&.{});
    const w: *std.Io.Writer = &adapter.new_interface;
    try w.print("{s}", .{"example");
    // ...
}
```

--------------------------------

TITLE: Zig: Target.Abi add gnuilp32
DESCRIPTION: Adds the `gnuilp32` ABI (Application Binary Interface) to the `Target.Abi` enumeration. This supports targeting systems that use this specific ABI.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Target.Abi: add gnuilp32
```

--------------------------------

TITLE: Improve Target Handling in Zig CLI
DESCRIPTION: Introduces breaking changes to how targets are handled in the Zig compiler. The command-line interface now uses `-target [name]` instead of `--target-*` arguments, aligning with clang's interface. It also renames `builtin.Environ` to `builtin.Abi` and `builtin.environ` to `builtin.abi`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: Zig
CODE:
```
-target [name]
```

LANGUAGE: Zig
CODE:
```
builtin.Environ
```

LANGUAGE: Zig
CODE:
```
builtin.Abi
```

LANGUAGE: Zig
CODE:
```
builtin.environ
```

LANGUAGE: Zig
CODE:
```
builtin.abi
```

--------------------------------

TITLE: Update Allocator Interface for Freed Memory
DESCRIPTION: The `std.mem.Allocator` interface has been updated to set memory to undefined when freed. This change, though planned to be reverted to allocator implementations, aims to improve memory safety by preventing accidental use of freed memory.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.mem.Allocator
```

--------------------------------

TITLE: Zig: `ArrayListUnmanaged` Writer Implementation
DESCRIPTION: Implements the `writer()` interface for `ArrayListUnmanaged`, enabling it to be used with I/O operations that expect a writer, similar to `ArrayList`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
`ArrayListUnmanaged`: implement writer().
```

--------------------------------

TITLE: Zig Hello World Program
DESCRIPTION: A basic 'Hello, world!' program in Zig. It imports the standard IO library and prints the message to standard output. The example also shows how to compile and run the Zig code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: zig
CODE:
```
const io = @import("std").io;

pub fn main() -> %void {
    %%io.stdout.printf("Hello, world!\n");
}
```

--------------------------------

TITLE: Zig Standard Library: std.io.readLine
DESCRIPTION: Announces the addition of `std.io.readLine` to the Zig standard library, providing a convenient way to read a line of input from an I/O stream.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const readLine = std.io.readLine;
```

--------------------------------

TITLE: Zig MachO Linker: Cross-Compilation to iOS
DESCRIPTION: Details the process and requirements for cross-compiling Zig code to iOS and iOS simulator targets using the MachO linker. It references an external repository for detailed instructions and examples.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
// To cross-compile for iOS, you need a sysroot provided by the user.
// Refer to https://github.com/kubkon/zig-ios-example for detailed setup and usage.

// Example command structure (conceptual):
// zig build-exe --target ios-aarch64 --sysroot /path/to/ios/sysroot main.zig

// The linker handles MachO generation for the target platform.
```

--------------------------------

TITLE: Configure Zig Async IO Stream Frame Size
DESCRIPTION: This code demonstrates how to configure the frame size for `std.io.InStream` when using asynchronous I/O in Zig. By declaring `pub const stack_size_std_io_InStream` in the root source file, developers can adjust the buffer size used for read calls, which is important for performance and avoiding runtime safety issues related to buffer allocation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
pub const stack_size_std_io_InStream = 1234;
```

--------------------------------

TITLE: Fix IP Parsing on macOS with libc
DESCRIPTION: Addresses an issue with parsing IP addresses on macOS by utilizing the libc `if_nametoindex` function. This ensures correct handling of network interface names when converting them to indices.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.1/release-notes.html

LANGUAGE: Zig
CODE:
```
use libc `if_nametoindex` for macOS when parsing IPs
```

--------------------------------

TITLE: Zig iOS and iPhone Simulator Support
DESCRIPTION: Zig 0.9.0 now targets iOS and iPhone Simulator platforms, leveraging improvements in the self-hosted linker and support for Objective-C compilation. A GitHub repository provides a complete example for using Zig with iOS.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Thanks to major improvements in the {#link|Self-Hosted Linker#} combined with support for compiling {#link|Objective-C|Objective-C and Objective-C++#}, along with miscellaneous improvements in the {#link|Self-Hosted Compiler#}, Zig now targets iOS and iPhone Simulator platforms. See [kubkon/zig-ios-example](https://github.com/kubkon/zig-ios-example) for a complete example.
```

--------------------------------

TITLE: Implement Zig Allocator Interface
DESCRIPTION: To implement a custom allocator in Zig, you need to define an `allocFn` and a `resizeFn` that adhere to the Allocator interface documented in `std/mem.zig`. Examples can be found in `std/heap.zig`, such as the `GeneralPurposeAllocator`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Define your allocator struct
const MyAllocator = struct {
    // Add any necessary fields for your allocator

    // Implement the allocFn
    pub fn allocFn(self: *MyAllocator, size: usize, align: usize) ![*]u8 {
        // Your allocation logic here
        // Example: return std.heap.page_allocator.allocFn(self, size, align);
        unimplemented;
    }

    // Implement the resizeFn
    pub fn resizeFn(self: *MyAllocator, ptr: [*]u8, old_size: usize, new_size: usize, align: usize) ![*]u8 {
        // Your resizing logic here
        // Example: return std.heap.page_allocator.resizeFn(self, ptr, old_size, new_size, align);
        unimplemented;
    }
};

// Example usage (assuming you have a way to instantiate MyAllocator)
// var my_allocator = MyAllocator{};
// var buffer = try my_allocator.alloc(1024, 8);

```

--------------------------------

TITLE: Zig std.io.readLine Enhancements
DESCRIPTION: The std.io.readLine function has been revamped with a new prototype and additional helper functions like std.io.readLineFrom, std.io.readLineSlice, and std.io.readLineSliceFrom. These improvements offer more flexibility in reading lines from input streams.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.io.readLine
```

LANGUAGE: zig
CODE:
```
std.io.readLineFrom
```

LANGUAGE: zig
CODE:
```
std.io.readLineSlice
```

LANGUAGE: zig
CODE:
```
std.io.readLineSliceFrom
```

--------------------------------

TITLE: Zig Hello World Program
DESCRIPTION: A basic 'Hello, world!' program in Zig. It imports the standard IO library and prints the message to standard output. The example also shows how to compile and run the Zig code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.0/index.html

LANGUAGE: zig
CODE:
```
const io = @import("std").io;

pub fn main() -> %void {
    %%io.stdout.printf("Hello, world!\n");
}
```

--------------------------------

TITLE: Zig WASI Example: Accessing Command Line Arguments
DESCRIPTION: Shows how to use the WebAssembly System Interface (WASI) in Zig to access and print command-line arguments. It utilizes Zig's standard library for process argument handling and memory allocation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    
    pub fn main() !void {
        var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
        const gpa = &general_purpose_allocator.allocator;
        const args = try std.process.argsAlloc(gpa);
        defer std.process.argsFree(gpa, args);
    
        for (args) |arg, i| {
            std.debug.print("{}: \{\n", .{ i, arg });
        }
    }
```

--------------------------------

TITLE: Implement Zig Allocator Interface
DESCRIPTION: To implement a custom allocator in Zig, you need to define an `allocFn` and a `resizeFn` that adhere to the Allocator interface documented in `std/mem.zig`. Examples can be found in `std/heap.zig`, such as the `GeneralPurposeAllocator`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Define your allocator struct
const MyAllocator = struct {
    // Add any necessary fields for your allocator

    // Implement the allocFn
    pub fn allocFn(self: *MyAllocator, size: usize, align: usize) ![*]u8 {
        // Your allocation logic here
        // Example: return std.heap.page_allocator.allocFn(self, size, align);
        unimplemented;
    }

    // Implement the resizeFn
    pub fn resizeFn(self: *MyAllocator, ptr: [*]u8, old_size: usize, new_size: usize, align: usize) ![*]u8 {
        // Your resizing logic here
        // Example: return std.heap.page_allocator.resizeFn(self, ptr, old_size, new_size, align);
        unimplemented;
    }
};

// Example usage (assuming you have a way to instantiate MyAllocator)
// var my_allocator = MyAllocator{};
// var buffer = try my_allocator.alloc(1024, 8);

```

--------------------------------

TITLE: Zig WASI Example: Accessing Command Line Arguments
DESCRIPTION: Shows how to use the WebAssembly System Interface (WASI) in Zig to access and print command-line arguments. It utilizes Zig's standard library for process argument handling and memory allocation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    
    pub fn main() !void {
        var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
        const gpa = &general_purpose_allocator.allocator;
        const args = try std.process.argsAlloc(gpa);
        defer std.process.argsFree(gpa, args);
    
        for (args) |arg, i| {
            std.debug.print("{}: \{\n", .{ i, arg });
        }
    }
```

--------------------------------

TITLE: Implement Zig Allocator Interface
DESCRIPTION: To implement a custom allocator in Zig, you need to define an `allocFn` and a `resizeFn` that adhere to the Allocator interface documented in `std/mem.zig`. Examples can be found in `std/heap.zig`, such as the `GeneralPurposeAllocator`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Define your allocator struct
const MyAllocator = struct {
    // Add any necessary fields for your allocator

    // Implement the allocFn
    pub fn allocFn(self: *MyAllocator, size: usize, align: usize) ![*]u8 {
        // Your allocation logic here
        // Example: return std.heap.page_allocator.allocFn(self, size, align);
        unimplemented;
    }

    // Implement the resizeFn
    pub fn resizeFn(self: *MyAllocator, ptr: [*]u8, old_size: usize, new_size: usize, align: usize) ![*]u8 {
        // Your resizing logic here
        // Example: return std.heap.page_allocator.resizeFn(self, ptr, old_size, new_size, align);
        unimplemented;
    }
};

// Example usage (assuming you have a way to instantiate MyAllocator)
// var my_allocator = MyAllocator{};
// var buffer = try my_allocator.alloc(1024, 8);

```

--------------------------------

TITLE: Remove deceptive std.io.readLine
DESCRIPTION: The `std.io.readLine` function has been removed. It was misleadingly named and intended for CLI text input, but lacked the necessary functionality. For reading line-delimited input, users should refer to the I/O Streams documentation. The `guess_number` example demonstrates using `std.fs.File.read` for CLI input.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.io.readLine
```

LANGUAGE: Zig
CODE:
```
std.fs.File.read
```

--------------------------------

TITLE: Implement Zig Allocator Interface
DESCRIPTION: To implement a custom allocator in Zig, you need to define an `allocFn` and a `resizeFn` that adhere to the Allocator interface documented in `std/mem.zig`. Examples can be found in `std/heap.zig`, such as the `GeneralPurposeAllocator`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Define your allocator struct
const MyAllocator = struct {
    // Add any necessary fields for your allocator

    // Implement the allocFn
    pub fn allocFn(self: *MyAllocator, size: usize, align: usize) ![*]u8 {
        // Your allocation logic here
        // Example: return std.heap.page_allocator.allocFn(self, size, align);
        unimplemented;
    }

    // Implement the resizeFn
    pub fn resizeFn(self: *MyAllocator, ptr: [*]u8, old_size: usize, new_size: usize, align: usize) ![*]u8 {
        // Your resizing logic here
        // Example: return std.heap.page_allocator.resizeFn(self, ptr, old_size, new_size, align);
        unimplemented;
    }
};

// Example usage (assuming you have a way to instantiate MyAllocator)
// var my_allocator = MyAllocator{};
// var buffer = try my_allocator.alloc(1024, 8);

```

--------------------------------

TITLE: Implement Zig Allocator Interface
DESCRIPTION: To implement a custom allocator in Zig, you need to define an `allocFn` and a `resizeFn` that adhere to the Allocator interface documented in `std/mem.zig`. Examples can be found in `std/heap.zig`, such as the `GeneralPurposeAllocator`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Define your allocator struct
const MyAllocator = struct {
    // Add any necessary fields for your allocator

    // Implement the allocFn
    pub fn allocFn(self: *MyAllocator, size: usize, align: usize) ![*]u8 {
        // Your allocation logic here
        // Example: return std.heap.page_allocator.allocFn(self, size, align);
        unimplemented;
    }

    // Implement the resizeFn
    pub fn resizeFn(self: *MyAllocator, ptr: [*]u8, old_size: usize, new_size: usize, align: usize) ![*]u8 {
        // Your resizing logic here
        // Example: return std.heap.page_allocator.resizeFn(self, ptr, old_size, new_size, align);
        unimplemented;
    }
};

// Example usage (assuming you have a way to instantiate MyAllocator)
// var my_allocator = MyAllocator{};
// var buffer = try my_allocator.alloc(1024, 8);

```

--------------------------------

TITLE: Zig: Linux io_uring sync with liburing
DESCRIPTION: Synchronizes the Zig `io_uring` library with the upstream `liburing` library. This ensures that Zig's asynchronous I/O capabilities are up-to-date with the latest features and fixes.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
linux: sync io_uring library with liburing
```

--------------------------------

TITLE: Allow Pointers to Anything in Extern/Exported Declarations (Zig)
DESCRIPTION: Enables the use of pointers to any type within `extern` and `exported` declarations. This provides greater flexibility when interfacing with C code or other external interfaces.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
// Example of allowing pointers to anything in extern declarations:
extern fn process_data(data: *const u8) void;

// Example of allowing pointers to anything in exported declarations:
export fn handle_input(input: [*]const i32) usize {
    // ... implementation ...
    return 0;
}

```

--------------------------------

TITLE: Zig io_uring Enhancements
DESCRIPTION: This snippet details improvements to Zig's io_uring integration, including new functions for event file registration, asynchronous operations, and poll updates. It also covers cancellation mechanisms and file registration updates.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
register_eventfd
register_eventfd_async
unregister_eventfd
read_fixed
write_fixed
statx
poll_update
cancel
io_uring_prep_cancel
register_files_update
link_timeout
```

--------------------------------

TITLE: Zig: Linux IO_Uring.timeout fix
DESCRIPTION: Corrects an issue with the `timeout` functionality in the Linux `IO_Uring` implementation. This ensures that timeouts for asynchronous I/O operations are handled correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
linux: fix IO_Uring.timeout
```

--------------------------------

TITLE: Zig C ABI Compatible Enum
DESCRIPTION: Explains and demonstrates the use of `extern enum` in Zig to create enums that are compatible with the C Application Binary Interface (ABI). This is crucial when interfacing Zig code with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
const Foo = extern enum { a, b, c };
export fn entry(foo: Foo) void { }

    $ zig build-obj test.zig
```

--------------------------------

TITLE: Zig Build System: Upgrade iovec Structure
DESCRIPTION: This shows an upgrade in the Zig build system, specifically how the `iovec` structure is used. It demonstrates the change from using `.iov_base` and `.iov_len` to `.base` and `.len` for specifying buffer information.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.13.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Old syntax (pre-upgrade):
// .{ .iov_base = message.ptr, .iov_len = message.len },

// New syntax (post-upgrade):
.{ .base = message.ptr, .len = message.len },

```

--------------------------------

TITLE: Introduce std.io.poll
DESCRIPTION: Adds the std.io.poll functionality, providing a mechanism for monitoring multiple file descriptors for I/O readiness.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
introduce std.io.poll
```

--------------------------------

TITLE: Zig std.io.InStream.readStruct Return Value
DESCRIPTION: The std.io.InStream(E).readStruct method now returns a value instead of taking a pointer. This change improves type safety and simplifies data deserialization.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.io.InStream(E).readStruct
```

--------------------------------

TITLE: Add readAllArrayListAligned to Reader
DESCRIPTION: Introduces `readAllArrayListAligned` to the `Reader` interface, allowing for arbitrary alignment. This fixes a compile error in `fs.File.readToEndAllocOptions` when specifying alignment.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: zig
CODE:
```
pub fn readAllArrayListAligned(self: Self, comptime T: type, allocator: Allocator, alignment: usize) !T {
    // ... implementation details ...
}
```

--------------------------------

TITLE: WASI: Fix os.isatty on type mismatch
DESCRIPTION: The `os.isatty` function for WASI has been fixed to correctly handle type mismatches. This ensures that checks for terminal interactivity are accurate and robust.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os.isatty
```

--------------------------------

TITLE: Cross-compile C to WASI with Zig
DESCRIPTION: Zig now ships with WASI libc and supports cross-compiling C code to the WebAssembly System Interface (WASI).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: c
CODE:
```
// Example C code to be compiled for WASI
int main() {
    return 0;
}
```

LANGUAGE: zig
CODE:
```
#zig build-exe src/main.c --target wasm32-wasi
```

--------------------------------

TITLE: Zig Operating System Abstractions
DESCRIPTION: Zig's OS abstractions have been reorganized. `std.os` provides a Zig-flavored POSIX interface with error translation and slice usage. Cross-platform abstractions are in category-specific namespaces like `std.fs.File.openRead`. Windows support includes `std.os.windows` with error translation for `GetLastError` and direct access to Windows APIs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os
O_RDONLY
open
std.os.windows
GetLastError
std.os.windows.kernel32.ExitProcess
```

--------------------------------

TITLE: Zig Pointers: Sentinel-Terminated Pointers
DESCRIPTION: Shows how to work with sentinel-terminated pointers in Zig, commonly used when interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: zig
CODE:
```
const c_str = "example\0";
const ptr_to_str: [*]const u8 = c_str;

// Iterate until null terminator
var i: usize = 0;
while (ptr_to_str[i] != 0) {
    // process character
    i += 1;
}

```

--------------------------------

TITLE: Zig: Implement fmtDuration using Formatter
DESCRIPTION: Implements the `fmtDuration` function using the `Formatter` interface, referencing issue #8137. This change standardizes duration formatting and potentially improves performance.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Implement fmtDuration using Formatter ([#8137](https://github.com/ziglang/zig/issues/8137))
```

--------------------------------

TITLE: Improve std.io.Reader and std.io.Writer
DESCRIPTION: Adds streamUntilDelimiter to std.io.Reader and support for non-power-of-two integer sizes to std.io.Writer. Also includes fixes for Reader.readIntoBoundedBytes and support for non-comptime streams in multi-writer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.io.reader.Reader: add `streamUntilDelimiter`
std.io.Writer: add support for non-power-of-two int sizes
Fix type mismatch for Reader.readIntoBoundedBytes
std.io.multi-writer: support non-comptime streams
```

--------------------------------

TITLE: Zig Async I/O Mode
DESCRIPTION: Configuration snippet for enabling evented I/O mode in Zig, which is compatible with the new async I/O features.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
pub const io_mode = .evented;
```

--------------------------------

TITLE: Zig Volatile Pointer for Memory-Mapped I/O
DESCRIPTION: Demonstrates the use of `volatile` pointers in Zig for handling memory-mapped I/O (MMIO). It shows how to declare a volatile pointer and asserts its type, ensuring loads and stores have side effects and maintain order.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const assert = @import("std").debug.assert;
    
test "volatile" {
        const mmio_ptr = @intToPtr(*volatile u8, 0x12345678);
        assert(@typeOf(mmio_ptr) == *volatile u8);
    }
```

--------------------------------

TITLE: Zig WebAssembly Support
DESCRIPTION: This section details Zig's support for WebAssembly, including freestanding compilation and the WebAssembly System Interface (WASI).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: Zig
CODE:
```
Freestanding
WASI
```

--------------------------------

TITLE: Zig std.io.SeekableStream and Debug Info
DESCRIPTION: Introduced std.io.SeekableStream, which is now used by Dwarf debug info instead of std.os.File directly. This change facilitates easier debug info parsing, especially for bare metal projects.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.io.SeekableStream
```

--------------------------------

TITLE: Zig @intToPtr Function
DESCRIPTION: Converts an integer value to a pointer of a specified destination type. This is a low-level operation useful for memory manipulation or interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: zig
CODE:
```
comptime DestType: type, int: usize
```

--------------------------------

TITLE: Zig Async I/O Mode Configuration
DESCRIPTION: This snippet shows how to configure the Zig build to use evented I/O, enabling the use of 'await' within the main function. This is a fundamental step for utilizing the experimental async I/O features.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
pub const io_mode = .evented;
```

--------------------------------

TITLE: Zig Integer Read/Write Function Rework
DESCRIPTION: Significant rework of integer read/write functions across io.InStream and mem modules. This includes renaming, parameter changes, introduction of new functions like readIntForeign and writeIntNative, and modifications to endianness handling.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
io.InStream().readIntNative
```

LANGUAGE: zig
CODE:
```
io.InStream().readIntLittle
```

LANGUAGE: zig
CODE:
```
io.InStream().readIntBig
```

LANGUAGE: zig
CODE:
```
io.InStream().readIntForeign
```

LANGUAGE: zig
CODE:
```
io.InStream().writeIntNative
```

LANGUAGE: zig
CODE:
```
io.InStream().writeIntForeign
```

LANGUAGE: zig
CODE:
```
io.InStream().writeIntLittle
```

LANGUAGE: zig
CODE:
```
io.InStream().writeIntBig
```

LANGUAGE: zig
CODE:
```
mem.readIntNative
```

LANGUAGE: zig
CODE:
```
mem.readIntForeign
```

LANGUAGE: zig
CODE:
```
mem.readIntBig
```

LANGUAGE: zig
CODE:
```
mem.readIntLittle
```

LANGUAGE: zig
CODE:
```
mem.readIntSliceNative
```

LANGUAGE: zig
CODE:
```
mem.readIntSliceForeign
```

LANGUAGE: zig
CODE:
```
mem.readIntSliceLittle
```

LANGUAGE: zig
CODE:
```
mem.readIntSliceBig
```

LANGUAGE: zig
CODE:
```
mem.readIntSlice
```

LANGUAGE: zig
CODE:
```
mem.writeIntNative
```

LANGUAGE: zig
CODE:
```
mem.writeIntForeign
```

LANGUAGE: zig
CODE:
```
mem.writeIntBig
```

LANGUAGE: zig
CODE:
```
mem.writeIntLittle
```

LANGUAGE: zig
CODE:
```
mem.writeIntSliceForeign
```

LANGUAGE: zig
CODE:
```
mem.writeIntSliceNative
```

LANGUAGE: zig
CODE:
```
mem.writeIntSliceBig
```

LANGUAGE: zig
CODE:
```
mem.writeIntSliceLittle
```

LANGUAGE: zig
CODE:
```
mem.writeIntSlice
```

LANGUAGE: zig
CODE:
```
mem.littleToNative
```

LANGUAGE: zig
CODE:
```
mem.bigToNative
```

LANGUAGE: zig
CODE:
```
mem.toNative
```

LANGUAGE: zig
CODE:
```
mem.nativeTo
```

LANGUAGE: zig
CODE:
```
mem.nativeToLittle
```

LANGUAGE: zig
CODE:
```
mem.nativeToBig
```

--------------------------------

TITLE: Zig: Packed Struct for Memory-Mapped I/O
DESCRIPTION: Defines a packed struct for memory-mapped I/O (MMIO) and demonstrates the correct way to write to a volatile pointer to this struct. It warns against direct field assignment to volatile pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
pub const GpioRegister = packed struct(u8) {
    GPIO0: bool,
    GPIO1: bool,
    GPIO2: bool,
    GPIO3: bool,
    reserved: u4 = 0,
};

const gpio: *volatile GpioRegister = @ptrFromInt(0x0123);

pub fn writeToGpio(new_states: GpioRegister) void {
    // Example of what not to do:
    // BAD! gpio.GPIO0 = true; BAD!

    // Instead, do this:
    gpio.* = new_states;
}
```

--------------------------------

TITLE: Zig 'extern struct' Declaration
DESCRIPTION: Demonstrates declaring C-compatible structs in Zig using the 'extern struct' keyword, useful for interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
extern struct CStruct {
    field1: c_int,
    field2: [*c]c_char,
}
```

--------------------------------

TITLE: Shell Commands for Zig CLI Allocation Example
DESCRIPTION: Provides the shell commands to compile and run the Zig code example for arena allocation in a command-line interface context.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: shell
CODE:
```
$ zig build-exe cli_allocation.zig
$ ./cli_allocation
ptr=i32@7fb79fc72010
```

--------------------------------

TITLE: Add io.counting_reader
DESCRIPTION: Adds io.counting_reader to the Zig standard library, providing a reader that counts the number of bytes read, useful for monitoring data streams.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: This Zig code snippet demonstrates how to read command-line arguments using the WASI interface. It allocates memory, retrieves arguments, and prints them with their index. The code requires the standard library and is compiled for the wasm32-wasi target.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator: std.heap.GeneralPurposeAllocator(.{}) = .init;
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}
", .{ i, arg });
    }
}
```

LANGUAGE: shell
CODE:
```
$ zig build-exe wasi_args.zig -target wasm32-wasi
```

LANGUAGE: shell
CODE:
```
$ wasmtime wasi_args.wasm 123 hello
0: wasi_args.wasm
1: 123
2: hello
```

--------------------------------

TITLE: Zig: `ppoll` Function Argument Casting
DESCRIPTION: Ensures correct type casting for the number of file descriptors (`fds`) passed to the `ppoll` function, converting it to `nfds_t` for compatibility with system interfaces.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
ppoll: cast number of fds to nfds_t.
```

--------------------------------

TITLE: Zig @intToPtr Function
DESCRIPTION: Converts an integer value to a pointer of a specified destination type. This is a low-level operation useful for memory manipulation or interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.0/index.html

LANGUAGE: zig
CODE:
```
comptime DestType: type, int: usize
```

--------------------------------

TITLE: Zig: fifo.LinearFifo expose reader/writer
DESCRIPTION: Exposes the reader and writer types for `fifo.LinearFifo`. This allows users to directly interact with the read and write ends of the FIFO queue.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
fifo.LinearFifo - Expose reader and writer type.
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: This Zig code snippet demonstrates how to read command-line arguments using the WASI interface. It allocates memory, retrieves arguments, and prints them with their index. The code requires the standard library and is compiled for the wasm32-wasi target.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator: std.heap.GeneralPurposeAllocator(.{}) = .init;
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}
", .{ i, arg });
    }
}
```

LANGUAGE: shell
CODE:
```
$ zig build-exe wasi_args.zig -target wasm32-wasi
```

LANGUAGE: shell
CODE:
```
$ wasmtime wasi_args.wasm 123 hello
0: wasi_args.wasm
1: 123
2: hello
```

--------------------------------

TITLE: Zig Mutex Lock/Unlock API Change
DESCRIPTION: This is a breaking change to Zig's `Thread.Mutex` API, simplifying the interface to `lock()` and `unlock()` methods for acquiring and releasing mutexes.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
`Thread.Mutex`: change API to `lock()` and `unlock()`.
```

--------------------------------

TITLE: Added Support for Darwin Framework Search Directories
DESCRIPTION: The ability to specify Darwin framework search directories has been added, allowing for more flexible linking of macOS and iOS frameworks.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Shell
CODE:
```
zig build -F /path/to/frameworks
```

--------------------------------

TITLE: Zig: Implement C va_arg with @cVaArg
DESCRIPTION: The @cVaArg function implements the C macro `va_arg` for handling variadic arguments in Zig when interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
@cVaArg(operand: *std.builtin.VaList, comptime T: type) T
```

--------------------------------

TITLE: Skip Zig Tests with Async Suspend Point
DESCRIPTION: Shows how tests with suspend points are skipped by default in blocking IO mode. The example uses an async function with a suspend point.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

test "async skip test" {
    var frame = async func();
    const result = await frame;
    try std.testing.expect(result == 1);
}

fn func() i32 {
    suspend {
        resume @frame();
    }
    return 1;
}
```

--------------------------------

TITLE: Zig: Write to File (Evented)
DESCRIPTION: A Zig program that writes to a file using evented I/O. This example utilizes the `io_mode = .evented` setting, demonstrating how Zig handles asynchronous file operations, often involving thread pools for blocking tasks on systems without native async file system support.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
pub const io_mode = .evented;
pub fn main() anyerror!void {
    const file = try std.fs.cwd().createFile("hello.txt", .{
    });
    defer file.close();
    try file.writeAll("hello\n");
}
```

--------------------------------

TITLE: Zig: Remove io.AutoIndentingStream
DESCRIPTION: Removes the `io.AutoIndentingStream` type. This suggests that its functionality may have been deprecated, merged, or replaced by other mechanisms.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
remove io.AutoIndentingStream
```

--------------------------------

TITLE: Zig Allocator Interface
DESCRIPTION: Demonstrates how to implement a custom allocator in Zig by providing `reallocFn` and `shrinkFn`. It references documentation comments in `std/mem.zig` and provides examples of existing allocators.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

const Allocator = std.mem.Allocator;

pub fn reallocFn(self: *const Allocator, old_memory: [*]u8, old_size: usize, new_size: usize, alignment: usize) ![*]u8 {
    // Implementation details for reallocation
    return undefined;
}

pub fn shrinkFn(self: *const Allocator, memory: [*]u8, size: usize, new_size: usize, alignment: usize) ![*]u8 {
    // Implementation details for shrinking memory
    return undefined;
}

// Example usage would involve creating a struct that holds these functions
// and passing it as an Allocator instance.
```

--------------------------------

TITLE: Zig: `io.FixedBufferStream` Seek Functionality
DESCRIPTION: Fixes a bug in the `seekTo` method of `io.FixedBufferStream`, ensuring that seeking operations within fixed-size buffers function correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
Fix bug in io.FixedBufferStream seekTo ([#9023](https://github.com/ziglang/zig/issues/9023)).
```

--------------------------------

TITLE: Zig Assembly Integration
DESCRIPTION: Zig allows for inline assembly, enabling direct interaction with the underlying hardware and C ABI. This is useful for performance-critical sections or when interfacing with existing assembly code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

fn add(a: i32, b: i32) i32 {
    var sum: i32;
    asm volatile (
        "add %0, %1, %2\n"
        : "=r"(sum) // Output operand: sum is stored in a general-purpose register
        : "r"(a), "r"(b) // Input operands: a and b are in general-purpose registers
    );
    return sum;
}

pi const result = add(5, 7);
std.debug.print("Assembly add result: {d}\\n", .{result});

```

--------------------------------

TITLE: Zig Assembly Integration
DESCRIPTION: Zig allows for inline assembly, enabling direct interaction with the underlying hardware and C ABI. This is useful for performance-critical sections or when interfacing with existing assembly code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

fn add(a: i32, b: i32) i32 {
    var sum: i32;
    asm volatile (
        "add %0, %1, %2\n"
        : "=r"(sum) // Output operand: sum is stored in a general-purpose register
        : "r"(a), "r"(b) // Input operands: a and b are in general-purpose registers
    );
    return sum;
}

pi const result = add(5, 7);
std.debug.print("Assembly add result: {d}\\n", .{result});

```

--------------------------------

TITLE: Zig Opaque Type Declaration
DESCRIPTION: Shows how to declare opaque types in Zig, which have an unknown but non-zero size and alignment. This is useful for type safety when interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Add Termios Types to c.linux and os
DESCRIPTION: Includes missing termios types in the `c.linux` and `os` modules. This provides necessary definitions for terminal I/O control, enabling better interaction with terminal devices.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Added missing termios types to `c.linux` and `os`.
```

--------------------------------

TITLE: Zig POSIX Termios: Type-Safe TTY Input Mode
DESCRIPTION: This example demonstrates how Zig's updated POSIX termios API provides type safety for manipulating terminal settings. It shows the transition from bitwise operations to direct boolean assignments for setting immediate input mode.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: zig
CODE:
```
const in = std.io.getStdIn(); // copy original settings and restore them once done const original_termios = try std.posix.tcgetattr(in.handle); defer std.posix.tcsetattr(in.handle, .FLUSH, original_termios) catch {}; // set immediate input mode var termios = original_termios; termios.lflag &= ~@as(std.posix.system.tcflag_t, std.posix.system.ICANON); // flush changes try std.posix.tcsetattr(in.handle, .FLUSH, termios);
```

LANGUAGE: zig
CODE:
```
// set immediate input mode var termios = original_termios; termios.lflag.ICANON = false;
```

--------------------------------

TITLE: Zig: Fix Implicit Casting to *c_void
DESCRIPTION: Corrects the implicit casting behavior when converting to a `*c_void` pointer, ensuring proper type compatibility with C interfaces.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
fix implicit casting to {#syntax#}\*c_void{#endsyntax#} ([#1588](https://github.com/ziglang/zig/issues/1588))
```

--------------------------------

TITLE: Add fnctl for File Control Operations
DESCRIPTION: The `std.os.fnctl` function has been added, providing a way to perform various file control operations. This function offers a POSIX-compliant interface for manipulating file descriptors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os.fnctl
```

--------------------------------

TITLE: Update wasi-libc to commit d03829489904d38c624f6de9983190f1e5e7c9c5
DESCRIPTION: This snippet indicates that the bundled wasi-libc copy has been updated to a specific commit hash. This is relevant for WebAssembly System Interface (WASI) targets.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: text
CODE:
```
This release bumps the bundled wasi-libc copy to commit `d03829489904d38c624f6de9983190f1e5e7c9c5`.
```

--------------------------------

TITLE: Zig Async I/O with Blocking Fallback
DESCRIPTION: Illustrates how Zig's asynchronous I/O code functions seamlessly within a blocking I/O environment. This example uses the same concurrent task structure as the evented I/O example, demonstrating the flexibility of Zig's async implementation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() anyerror!void {
    var a_frame = async doA();
    var b_frame = async doB();

    try await a_frame;
    try await b_frame;
}

fn doA() !void {
    const file = try std.fs.cwd().createFile("a.txt", .{}) ;
    defer file.close();
    try file.writeAll("A\n");
}

fn doB() !void {
    const file = try std.fs.cwd().createFile("b.txt", .{}) ;
    defer file.close();
    try file.writeAll("B\n");
}
```

--------------------------------

TITLE: Zig Sentinel-Terminated Pointer for C printf
DESCRIPTION: Demonstrates the use of sentinel-terminated pointers, specifically `[*:0]const u8`, for interfacing with C functions like `printf`. It highlights the requirement for a null terminator.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// This is also available as `std.c.printf`.
pub extern "c" fn printf(format: [*:0]const u8, ...) c_int;

pub fn main() anyerror!void {
    _ = printf("Hello, world!\n"); // OK

    const msg = "Hello, world!\n";
    const non_null_terminated_msg: [msg.len]u8 = msg.*;
    _ = printf(&non_null_terminated_msg);
}
```

--------------------------------

TITLE: Improved CLI Error Message for Missing Sub-Architecture
DESCRIPTION: The command-line interface (CLI) now provides a clearer error message when a sub-architecture is missing, aiding users in diagnosing build issues.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Shell
CODE:
```
zig build --target=x86_64-windows-gnu
```

--------------------------------

TITLE: io_uring sqe prep methods in Zig
DESCRIPTION: Zig 0.9.0 includes new sqe prep methods for `epoll_ctl`, `poll_add`, and `poll_remove` within the io_uring functionality.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
*   Addd sqe prep methods for `epoll_ctl`, `poll_add`, and `poll_remove`.
```

--------------------------------

TITLE: Add Zig std.io.PeekStream and SliceStream
DESCRIPTION: Introduces `std.io.PeekStream` for look-ahead reading and `std.io.SliceStream` (renamed from SliceStream) as a read-only stream wrapper for byte slices.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.io.PeekStream std.io.Slicestream SliceStream read-only stream wrapper slice bytes adapt algorithms InStreams in-memory data PeekStream stream wrapper put back bytes look-ahead parsers easier write
```

--------------------------------

TITLE: Zig @OpaqueType
DESCRIPTION: Creates a new type with an unknown but non-zero size and alignment, typically used for type safety when interfacing with C code that does not expose struct details.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
type
Creates a new type with an unknown (but non-zero) size and alignment.
This is typically used for type safety when interacting with C code that does not expose struct details. Example:
test.zig

    const Derp = @OpaqueType();
    const Wat = @OpaqueType();
    
    extern fn bar(d: *Derp) void;
    fn foo(w: *Wat) callconv(.C) void {
        bar(w);
    }
    
    test "call foo" {
        foo(undefined);
    }

    $ zig test test.zig
    ./docgen_tmp/test.zig:6:9: error: expected type '*Derp', found '*Wat'
        bar(w);
            ^
    ./docgen_tmp/test.zig:6:9: note: pointer type child 'Wat' cannot cast into pointer type child 'Derp'
        bar(w);
            ^
```

--------------------------------

TITLE: Zig Compiler Protocol Communication
DESCRIPTION: Details the binary protocol used for communication between the Zig build system and the Zig compiler. This protocol, enabled by the `--listen` argument, uses TCP or stdio for more detailed information exchange, improving build system integration.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.zig.Server
std.Build.Step.evalZigProcess
```

--------------------------------

TITLE: CLI Alias `-l` for `--library` Parameter
DESCRIPTION: The command-line interface (CLI) now accepts `-l` as an alias for the `--library` parameter, simplifying library specification, e.g., `-lc` instead of `--library c`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Shell
CODE:
```
zig build --library c
```

--------------------------------

TITLE: Zig Keyword: callconv
DESCRIPTION: The `callconv` keyword in Zig allows specifying the calling convention for function types. This is crucial for interoperability, especially when interfacing with C code or different ABIs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
fn add(a: i32, b: i32) i32;

const my_add_ptr: fn(i32, i32) callconv(.C) i32 = add;

// Example of using a different calling convention (hypothetical)
// fn custom_call(f: fn() callconv(.Std) void) void;

```

--------------------------------

TITLE: Implement Basic Linux Termios
DESCRIPTION: A basic implementation of the termios API for Linux has been added. This provides foundational support for controlling terminal I/O characteristics.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Linux termios implementation
```

--------------------------------

TITLE: Zig Keyword: callconv
DESCRIPTION: The `callconv` keyword in Zig allows specifying the calling convention for function types. This is crucial for interoperability, especially when interfacing with C code or different ABIs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
fn add(a: i32, b: i32) i32;

const my_add_ptr: fn(i32, i32) callconv(.C) i32 = add;

// Example of using a different calling convention (hypothetical)
// fn custom_call(f: fn() callconv(.Std) void) void;

```

--------------------------------

TITLE: Zig Allocator Interface Definition
DESCRIPTION: Defines the Allocator struct with reallocFn and shrinkFn signatures. reallocFn handles resizing or moving allocations, potentially returning OutOfMemory. shrinkFn is for deallocating memory and must succeed.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
pub const Allocator = struct { pub const Error = error{OutOfMemory};
/// Realloc is used to modify the size or alignment of an existing allocation, /// as well as to provide the allocator with an opportunity to move an allocation /// to a better location.
/// When the size/alignment is greater than the previous allocation, this function /// returns `error.OutOfMemory` when the requested new allocation could not be granted.
/// When the size/alignment is less than or equal to the previous allocation, /// this function returns `error.OutOfMemory` when the allocator decides the client /// would be better off keeping the extra alignment/size. Clients will call /// `shrinkFn` when they require the allocator to track a new alignment/size, /// and so this function should only return success when the allocator considers /// the reallocation desirable from the allocator's perspective.
/// As an example, `std.ArrayList` tracks a "capacity", and therefore can handle /// reallocation failure, even when `new_n` <= `old_mem.len`. A `FixedBufferAllocator` /// would always return `error.OutOfMemory` for `reallocFn` when the size/alignment /// is less than or equal to the old allocation, because it cannot reclaim the memory, /// and thus the `std.ArrayList` would be better off retaining its capacity.
/// When `reallocFn` returns,
/// `return_value[0..min(old_mem.len, new_byte_count)]` must be the same
/// as `old_mem` was when `reallocFn` is called. The bytes of
/// `return_value[old_mem.len..]` have undefined values.
/// The returned slice must have its pointer aligned at least to `new_alignment` bytes.
reallocFn: fn ( self: *Allocator,
/// Guaranteed to be the same as what was returned from most recent call to
/// `reallocFn` or `shrinkFn`.
/// If `old_mem.len == 0` then this is a new allocation and `new_byte_count`
/// is guaranteed to be >= 1.
old_mem: []u8,
/// If `old_mem.len == 0` then this is `undefined`, otherwise:
/// Guaranteed to be the same as what was returned from most recent call to
/// `reallocFn` or `shrinkFn`.
/// Guaranteed to be >= 1.
/// Guaranteed to be a power of 2.
old_alignment: u29,
/// If `new_byte_count` is 0 then this is a free and it is guaranteed that
/// `old_mem.len != 0`.
new_byte_count: usize,
/// Guaranteed to be >= 1.
/// Guaranteed to be a power of 2.
/// Returned slice's pointer must have this alignment.
new_alignment: u29,
) Error![]u8,
/// This function deallocates memory. It must succeed.
shrinkFn: fn ( self: *Allocator,
/// Guaranteed to be the same as what was returned from most recent call to
/// `reallocFn` or `shrinkFn`.
old_mem: []u8,
/// Guaranteed to be the same as what was returned from most recent call to
/// `reallocFn` or `shrinkFn`.
old_alignment: u29,
/// Guaranteed to be less than or equal to `old_mem.len`.
new_byte_count: usize,
/// If `new_byte_count == 0` then this is `undefined`, otherwise:
/// Guaranteed to be less than or equal to `old_alignment`.
new_alignment: u29,
) []u8,
};
```

--------------------------------

TITLE: Zig Keyword: callconv
DESCRIPTION: The `callconv` keyword in Zig allows specifying the calling convention for function types. This is crucial for interoperability, especially when interfacing with C code or different ABIs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
fn add(a: i32, b: i32) i32;

const my_add_ptr: fn(i32, i32) callconv(.C) i32 = add;

// Example of using a different calling convention (hypothetical)
// fn custom_call(f: fn() callconv(.Std) void) void;

```

--------------------------------

TITLE: Zig Builtin: @offsetOf
DESCRIPTION: Demonstrates the `@offsetOf` builtin function in Zig, which returns the byte offset of a field within a struct. This is essential for memory layout calculations and interfacing with C structures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

const Point = struct {
    x: f32,
    y: f32,
};

pub fn main() void {
    const offset_y = @offsetOf(Point, "y");
    std.debug.print("Byte offset of field 'y': {d}", .{offset_y});
}

```

--------------------------------

TITLE: Zig Builtin: @cUndef
DESCRIPTION: Demonstrates the `@cUndef` builtin function in Zig, which represents an undefined value in a C context. This is used when interfacing with C code that might have uninitialized variables.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

extern fn process_value(val: [*c]const u8) void;

pub fn main() void {
    // In a real scenario, this would be passed to a C function
    // that expects an undefined value.
    // For demonstration, we'll just show the concept.
    const undefined_val = @cUndef(u8);
    std.debug.print("Representing an undefined u8 value.");
    // process_value(@ptrCast([*c]const u8, &undefined_val)); // Example usage
}

```

--------------------------------

TITLE: Zig extern Keyword
DESCRIPTION: The `extern` keyword is used to declare functions or variables that will be resolved at link time, either statically or dynamically. It allows Zig code to interface with external libraries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
extern fn puts(s: [*c]const u8) c_int;
// ...
puts("Hello from Zig!");
```

--------------------------------

TITLE: Zig Compiler Command Line Option for Evented I/O Testing
DESCRIPTION: Enables evented I/O mode for running Zig tests. This is useful for testing asynchronous operations and I/O-bound scenarios within the Zig test runner.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
--test-evented-io
```

--------------------------------

TITLE: Zig Async Test Skip Example
DESCRIPTION: Illustrates skipping an asynchronous Zig test. The test runner skips the test when it encounters a suspend point in blocking IO mode, as shown in the output.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

test "async skip test" {
    var frame = async func();
    const result = await frame;
    try std.testing.expect(result == 1);
}

fn func() i32 {
    suspend {
        resume @frame();
    }
    return 1;
}
```

--------------------------------

TITLE: Added `-D` CLI Parameter for C Preprocessor Definitions
DESCRIPTION: The command-line interface (CLI) now includes the `-D` parameter for setting C preprocessor definitions, allowing users to define macros during compilation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Shell
CODE:
```
zig build -DENABLE_FEATURE=1
```

--------------------------------

TITLE: Zig CLI: Override Unwind Tables
DESCRIPTION: The Zig command-line interface (CLI) allows users to override default unwind table behavior by adding flags to control their generation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
zig build -Doptimize=release-fast -Dtarget=x86_64-windows-gnu -Dcpu=x86-64 -Denable-stack-probing=false -Dsingle_threaded=true -Ddebug_symbols=false -Dstrip_debug_symbols=true -Dstrip_symbols=true -Dstrip_unwind_tables=true
```

--------------------------------

TITLE: Zig WASI Example with Command Line Arguments
DESCRIPTION: An example of a Zig program compiled for WebAssembly System Interface (WASI). It demonstrates accessing command-line arguments using the standard library and printing them to standard error.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    // TODO a better default allocator that isn't as wasteful!
    const args = try std.process.argsAlloc(std.heap.page_allocator);
    defer std.process.argsFree(std.heap.page_allocator, args);

    for (args) |arg, i| {
        std.debug.warn("{}: {}\n", .{i, arg});
    }
}
```

--------------------------------

TITLE: Zig Tuples: Element Access and Length
DESCRIPTION: Demonstrates how tuples in Zig support element access using array-like indexing and provide a `.len` field for determining the number of elements. This example showcases modifying tuple elements and iterating over them.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuples support element access and .len field" {
    var x: i32 = 1234;
    var y: i32 = 4567;
    var tup = .{ x, y };

    tup[0] += 1; // works as long as the indexes are comptime-known
    tup[1] -= 1;

    expect(tup[0] == 1235);
    expect(tup[1] == 4566);

    // now we iterate over the fields
    var sum: i32 = 0;
    comptime var index = 0;
    inline while (index < tup.len) : (index += 1) {
        sum += tup[index];
    }
    expect(sum == 1235 + 4566);
}
```

--------------------------------

TITLE: Zig C-ABI Compatible Enum Export
DESCRIPTION: Illustrates how to declare an enum compatible with the C Application Binary Interface (ABI) using `extern enum`. This is necessary for exporting functions that use enums to C.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: zig
CODE:
```
const Foo = enum { A, B, C };
export fn entry(foo: Foo) void { }

    $ zig build-obj test.zig
    /home/andy/dev/zig/docgen_tmp/test.zig:2:22: error: parameter of type 'Foo' not allowed in function with calling convention 'ccc'
    export fn entry(foo: Foo) void { 
                         ^
    
    
```

LANGUAGE: zig
CODE:
```
const Foo = extern enum { A, B, C };
export fn entry(foo: Foo) void { }

    $ zig build-obj test.zig
```

--------------------------------

TITLE: Zig: Inline Switch Prongs for Comptime Analysis
DESCRIPTION: Demonstrates using inline switch prongs in Zig to analyze struct fields for optionality at compile time. It shows how `isFieldOptional` is unrolled for specific indices and tests its behavior with different field types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T)."struct".fields;
    return switch (field_index) {
        // This prong is analyzed twice with `idx` being a
        // comptime-known value each time.
        inline 0, 1 => |idx| @typeInfo(fields[idx].type) == .optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function:
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: Zig: Inline Switch Prongs for Comptime Analysis
DESCRIPTION: Demonstrates using inline switch prongs in Zig to analyze struct fields for optionality at compile time. It shows how `isFieldOptional` is unrolled for specific indices and tests its behavior with different field types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T)."struct".fields;
    return switch (field_index) {
        // This prong is analyzed twice with `idx` being a
        // comptime-known value each time.
        inline 0, 1 => |idx| @typeInfo(fields[idx].type) == .optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function:
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: Improve BufferedInputStream ReadByte Performance
DESCRIPTION: The `std.io.BufferedInStream.readByte` function has been optimized, resulting in a significant performance improvement of approximately 75%. This enhancement is part of ongoing efforts to boost the efficiency of core I/O operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.io.BufferedInStream.readByte
```

--------------------------------

TITLE: Zig Target: wasm64-wasi
DESCRIPTION: This entry indicates support for the wasm64 architecture with WASI (WebAssembly System Interface). The specific icons suggest it's a primary target with build and test support.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.15.1/release-notes.html

LANGUAGE: text
CODE:
```
wasm64-wasi
```

--------------------------------

TITLE: Zig @alignOf
DESCRIPTION: Returns the alignment requirement for a type according to the C ABI. This is useful for ensuring proper memory alignment, especially when interfacing with C code. The result is a compile-time constant.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
comptime T: type) comptime_int

This function returns the number of bytes that this type should be aligned to for the current target to match the C ABI. When the child type of a pointer has this alignment, the alignment can be omitted from the type.

    const expect = @import("std").debug.assert;
    comptime {
        assert(*u32 == *align(@alignOf(u32)) u32);
    }

The result is a target-specific compile time constant. It is guaranteed to be less than or equal to [@sizeOf(T)](#sizeOf).
```

--------------------------------

TITLE: Zig Build System: Global Lock for Inherited Stdio
DESCRIPTION: This explains a change in the Zig build system's `Step.Run` functionality. When `stdio` is set to "inherit", a global lock is now obtained to prevent concurrent execution of steps that share standard input/output, ensuring safer process management.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.13.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example of configuring a Run step to inherit stdio:
// var run_step = b.addRunArtifact(exe);
// run_step.stdio = .{
//     .inherit = true,
// };
// run_step.expectStdErr(); // Example of expecting output

// When stdio is set to inherit, the build system now:
// - Considers the step to have side-effects.
// - Always executes the step.
// - Obtains a global lock to prevent other steps from running concurrently.
// - Fails the step if the subprocess crashes or returns a non-zero exit code.

```

--------------------------------

TITLE: Zig Struct Declaration
DESCRIPTION: Defines two Zig structs: `Point` with default alignment and `Point2` as a `packed struct` for specific byte ordering, suitable for interfacing with external systems like OpenGL.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: Zig
CODE:
```
// Declare a struct.
// Zig gives no guarantees about the order of fields and whether or
// not there will be padding.
const Point = struct {
    x: f32,
    y: f32,
};

// Maybe we want to pass it to OpenGL so we want to be particular about
// how the bytes are arranged.
const Point2 = packed struct {
    x: f32,
    y: f32,
};


// Declare an instance of a struct.
const p = Point {

```

--------------------------------

TITLE: Zig @alignOf
DESCRIPTION: Returns the alignment requirement for a type according to the C ABI. This is useful for ensuring proper memory alignment, especially when interfacing with C code. The result is a compile-time constant.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: zig
CODE:
```
comptime T: type) comptime_int

This function returns the number of bytes that this type should be aligned to for the current target to match the C ABI. When the child type of a pointer has this alignment, the alignment can be omitted from the type.

    const expect = @import("std").debug.assert;
    comptime {
        assert(*u32 == *align(@alignOf(u32)) u32);
    }

The result is a target-specific compile time constant. It is guaranteed to be less than or equal to [@sizeOf(T)](#sizeOf).
```

--------------------------------

TITLE: Remove Verbose AST and Tokenize Flags in Zig CLI
DESCRIPTION: Removes the `--verbose-ast` and `--verbose-tokenize` flags from the Zig command-line interface. These flags were likely for debugging and are no longer supported.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
CLI: remove --verbose-ast and --verbose-tokenize ([#9034](https://github.com/ziglang/zig/issues/9034)).
```

--------------------------------

TITLE: Zig: Inline Switch Prongs for Comptime Field Analysis
DESCRIPTION: Demonstrates using Zig's `inline` keyword within a switch statement to analyze struct fields at compile time. The `isFieldOptional` function checks if a field is optional, with the prong being evaluated twice for comptime values. Includes a test case and an unrolled equivalent function.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T)."struct".fields;
    return switch (field_index) {
        // This prong is analyzed twice with `idx` being a
        // comptime-known value each time.
        inline 0, 1 => |idx| @typeInfo(fields[idx].type) == .optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function:
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: UEFI Support Enhancements in Zig
DESCRIPTION: This snippet highlights improvements made to UEFI support in Zig, including checks for UEFI in io.StreamSource, implementation of std.time.sleep for UEFI, and fixes for alignment errors in the UEFI FileInfo protocol. It also mentions a fix for a shift in the pool allocator related to UEFI.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
io.StreamSource
std.time.sleep
std.os.uefi.FileInfo
```

--------------------------------

TITLE: Zig Self-Hosted CLI Revision
DESCRIPTION: Revises the command-line interface (CLI) of the self-hosted Zig compiler. This may include changes to command structure, options, or output formatting for improved usability.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Shell
CODE:
```
# Example self-hosted compiler command (conceptual)
zig build --output zig-out/program
```

--------------------------------

TITLE: Zig: io.FindByteOutStream removed, use io.FindByteWriter
DESCRIPTION: The `io.FindByteOutStream` type is removed. Use `io.FindByteWriter` for finding bytes within a stream.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
io.FindByteOutStream
```

LANGUAGE: Zig
CODE:
```
io.FindByteWriter
```

--------------------------------

TITLE: Zig Target: wasm32-wasi
DESCRIPTION: This entry indicates support for the wasm32 architecture with WASI (WebAssembly System Interface) on Linux. The specific icons suggest it's a primary target with build and test support.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.15.1/release-notes.html

LANGUAGE: text
CODE:
```
wasm32-wasi
```

--------------------------------

TITLE: Zig Concurrency Updates
DESCRIPTION: Zig 0.11 includes fixes for condition variable broadcast in FutexImpl and corrects the alignment type for `std.Thread.Futex.PosixImpl.Address.from`. Additionally, `std.Thread.Id` has been made smaller where possible.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
// Concurrency fixes are applied to the relevant std.Thread and Futex implementations.
// Example of a potential fix related to FutexImpl:
// const FutexImpl = std.Thread.Futex.Impl;
// FutexImpl.broadcast(cond_var, count);
```

--------------------------------

TITLE: Zig: Add sendmsg
DESCRIPTION: Adds the `sendmsg` system call to the operating system interface. This function allows for more advanced control over sending data over sockets, including sending multiple data buffers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
add sendmsg
```

--------------------------------

TITLE: Zig HashMap getOrPut Update (0.8.0)
DESCRIPTION: Demonstrates the modification to Zig's HashMap.getOrPut function's return type, where the entry value is now accessed via a pointer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
// old
const result = try map.getOrPut(key);
if (!result.found_existing) {
    result.entry.value = new_value;
}

// new
const result = try map.getOrPut(key);
if (!result.found_existing) {
    result.value_ptr.* = new_value;
}

```

--------------------------------

TITLE: Zig Opaque Type Declaration and C Interoperability
DESCRIPTION: Shows how to declare opaque types in Zig for interfacing with C code where struct details are unknown. Opaque types have non-zero size and alignment and can contain declarations similar to structs. The example highlights a type mismatch error when passing an incorrect opaque pointer to an external function.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig: io.findByteOutStream removed, use io.findByteWriter
DESCRIPTION: The `io.findByteOutStream` function is removed. Use `io.findByteWriter` for finding bytes within a stream.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
io.findByteOutStream
```

LANGUAGE: Zig
CODE:
```
io.findByteWriter
```

--------------------------------

TITLE: Zig: Integer-Pointer Conversion
DESCRIPTION: Demonstrates the use of `@intToPtr` to convert an integer address into a pointer and `@ptrToInt` to convert a pointer back to an integer (usize). This is useful for low-level memory manipulation or when interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@ptrToInt and @intToPtr" {
    const ptr = @intToPtr(*i32, 0xdeadbee0);
    const addr = @ptrToInt(ptr);
    try expect(@TypeOf(addr) == usize);
    try expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Zig extern Keyword
DESCRIPTION: The `extern` keyword in Zig is used to declare functions or variables that will be resolved at link time, either statically or dynamically at runtime. It's essential for interfacing with external libraries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: Zig
CODE:
```
extern fn printf(format: [*c]const u8, ...) callconv(.C) i32;

fn printMessage() void {
    printf("Hello from Zig!\n", .{
        @tagName(u8),
    });
}

```

--------------------------------

TITLE: Add Int Writing and SkipBytes to Zig Streams
DESCRIPTION: Adds integer writing functions to `std.io.OutStream` and a `skipBytes` function to `std.io.InStream` for more versatile stream operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
int writing functions OutStream skipBytes function InStream
```

--------------------------------

TITLE: Zig: Integer-Pointer Conversion
DESCRIPTION: Demonstrates the use of `@intToPtr` to convert an integer address into a pointer and `@ptrToInt` to convert a pointer back to an integer (usize). This is useful for low-level memory manipulation or when interfacing with C code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@ptrToInt and @intToPtr" {
    const ptr = @intToPtr(*i32, 0xdeadbee0);
    const addr = @ptrToInt(ptr);
    try expect(@TypeOf(addr) == usize);
    try expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Use `zig cc` with stdin
DESCRIPTION: The `zig cc` command now supports reading input from stdin, enabling its use with tools like meson.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: bash
CODE:
```
# Pipe C code to zig cc for compilation
echo "int main() { return 0; }" | zig cc -o main.o -c -
```

--------------------------------

TITLE: Zig: External Declarations with extern
DESCRIPTION: The `extern` keyword in Zig declares functions or variables that will be resolved at link time, either statically or dynamically at runtime. This is used to interface with code written in other languages or libraries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
extern fn puts(s: [*c]const u8) c_int;

pub fn greet() void {
    puts("Hello from Zig!\n");
}
```

--------------------------------

TITLE: Zig Struct Declaration
DESCRIPTION: Defines two Zig structs: `Point` with default alignment and `Point2` as a `packed struct` for specific byte ordering, suitable for interfacing with external systems like OpenGL.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.0/index.html

LANGUAGE: Zig
CODE:
```
// Declare a struct.
// Zig gives no guarantees about the order of fields and whether or
// not there will be padding.
const Point = struct {
    x: f32,
    y: f32,
};

// Maybe we want to pass it to OpenGL so we want to be particular about
// how the bytes are arranged.
const Point2 = packed struct {
    x: f32,
    y: f32,
};


// Declare an instance of a struct.
const p = Point {

```

--------------------------------

TITLE: Zig: elf expose parsing decoupled from fs.File
DESCRIPTION: Decouples the ELF file parsing logic from the `fs.File` type. This allows ELF files to be parsed from memory buffers or other sources, not just file handles.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
elf: expose parsing decoupled from fs.File
```

--------------------------------

TITLE: Zig: std.os.Stat structs gain time methods
DESCRIPTION: The `std.os.Stat` structs now include methods to abstract platform differences for accessing file metadata like modification time (`mtime`), change time (`ctime`), and access time (`atime`).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os.Stat
```

--------------------------------

TITLE: Zig: External Declarations with extern
DESCRIPTION: The `extern` keyword in Zig declares functions or variables that will be resolved at link time, either statically or dynamically at runtime. This is used to interface with code written in other languages or libraries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
extern fn puts(s: [*c]const u8) c_int;

pub fn greet() void {
    puts("Hello from Zig!\n");
}
```

--------------------------------

TITLE: Opaque Types in Zig
DESCRIPTION: Explains the use of `opaque {}` to declare types with unknown size and alignment, primarily for type safety when interfacing with C code. It includes an example of declaring and using opaque types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig HTTP Server Initialization (Old vs New)
DESCRIPTION: Compares the old and new ways of initializing an HTTP server in Zig, highlighting the shift from depending on std.net to Io.Reader and Io.Writer, and the removal of arbitrary limitations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.15.1/release-notes.html

LANGUAGE: Zig
CODE:
```
var read_buffer: [8000]u8 = undefined;
var server = std.http.Server.init(connection, &read_buffer);
```

LANGUAGE: Zig
CODE:
```
var recv_buffer: [4000]u8 = undefined;
var send_buffer: [4000]u8 = undefined;
var conn_reader = connection.stream.reader(&recv_buffer);
var conn_writer = connection.stream.writer(&send_buffer);
var server = std.http.Server.init(conn_reader.interface(), &conn_writer.interface);
```

--------------------------------

TITLE: Zig CLI Help Output Improvement
DESCRIPTION: Enhances the help output of the Zig command-line interface (CLI). This makes it easier for users to understand and utilize the various compiler commands and options.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Shell
CODE:
```
zig --help
```

--------------------------------

TITLE: Opaque Types in Zig
DESCRIPTION: Explains the use of `opaque {}` to declare types with unknown size and alignment, primarily for type safety when interfacing with C code. It includes an example of declaring and using opaque types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Fix WASI os.isatty on type mismatch
DESCRIPTION: Corrects the behavior of os.isatty in the WASI implementation when a type mismatch occurs, referencing issue #13813.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.1/release-notes.html

LANGUAGE: Zig
CODE:
```
wasi: fixes os.isatty on type mismatch (#13813)
```

--------------------------------

TITLE: Zig BufferOutStream Import Fix
DESCRIPTION: Corrects an import error for `BufferOutStream`, clarifying that it is defined within `std.io.zig`. This ensures the type is correctly referenced and available for use.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const BufferOutStream = std.io.BufferOutStream;
```

--------------------------------

TITLE: Zig Standard Library: Miscellaneous Updates
DESCRIPTION: This section covers various other improvements in the Zig standard library. It includes adding missing termios types for c/linux.zig and os.zig, fixing readUntilDelimiter to only read if the buffer is not full, avoiding duplicate TLS startup symbols, and correcting rounding in parse_hex_float.zig. Progress reporting API has been made infallible, and fixes for suffix printing and data races in std.Progress.maybeRefresh() are included. Additionally, ELF and COFF machine types have been added to Target.CPU.Arch conversions, sigaction double panic is fixed, sem_open and sem_close are added, SIMD utility functions are available, and unicode.replacement_character is defined.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

// Example: Using sem_open
// const sem = try std.posix.sem_open("my_semaphore", std.posix.O.CREAT, 0o644, 1);
// defer std.posix.sem_close(sem);

// Example: Progress line handling
// var progress = std.Progress.init(std.io.getStdOut().writer());
// progress.maybeRefresh(); // Only prints if timer is ready or explicitly called

// Example: Unicode replacement character
const replacement = std.unicode.replacement_character;

```

--------------------------------

TITLE: Use 32-bit fchown on 32-bit Linux
DESCRIPTION: Ensures that the `fchown` syscall uses the 32-bit uid/gid variant when targeting 32-bit Linux platforms. This maintains compatibility with older or specific 32-bit system interfaces.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.1/release-notes.html

LANGUAGE: Zig
CODE:
```
fchown: use the 32-bit uid/gid variant of the syscall on 32-bit linux targets.
```

--------------------------------

TITLE: Zig: Using Primitives as Struct Field Names
DESCRIPTION: Shows how Zig allows primitive type names like `i32`, `true`, `false`, `undefined`, and `null` to be used as struct field names. The `@""` syntax is required for disambiguation when referencing these fields.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const Foo = struct {
    true: i32,
    false: i32,
    undefined: i32,
    null: i32,
};
```

--------------------------------

TITLE: Zig std.io.NullOutStream and CountingOutStream
DESCRIPTION: Added std.io.NullOutStream and std.io.CountingOutStream for efficient handling of output streams. NullOutStream discards all written data, while CountingOutStream counts the bytes written.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.io.NullOutStream
```

LANGUAGE: zig
CODE:
```
std.io.CountingOutStream
```

--------------------------------

TITLE: Zig: Address-of Operator with Result Location Semantics
DESCRIPTION: This Zig code illustrates the forwarding of result types through the address-of operator ('&'). It shows how anonymous initializations and casting builtins work correctly with the address-of operator in the presence of result types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const S = struct {
    x: u32,
};
const int: u64 = 123;
const val: *const S = &.{ .x = @intCast(int) };

comptime {
    _ = val;
}
```

--------------------------------

TITLE: Zig: External Declarations
DESCRIPTION: The `extern` keyword in Zig is used to declare functions or variables that will be resolved at link time, either statically or dynamically at runtime. This is crucial for interfacing with C libraries or other pre-compiled code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
extern fn puts(s: [*]const u8) c_int;

pub fn main() void {
    puts("Hello from Zig!");
}
```

--------------------------------

TITLE: Networking Enhancements in Zig
DESCRIPTION: Zig 0.10.0 introduces HTTP method and status definitions, improves error handling for network connections, and adds support for `tcdrain` on Linux. The `std.os` module now handles `error.UnreachableAddress` in `send()`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.http.Method
```

LANGUAGE: Zig
CODE:
```
std.http.Status
```

LANGUAGE: Zig
CODE:
```
std.net.getAddressList
```

LANGUAGE: Zig
CODE:
```
std.os.tcdrain
```

LANGUAGE: Zig
CODE:
```
std.os.send
```

--------------------------------

TITLE: Zig: Organize concurrency primitives and add RwLock
DESCRIPTION: Organizes the standard library's concurrency primitives and adds a `RwLock` (Read-Write Lock). This provides better structure and a new synchronization tool for concurrent programming.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Organize std lib concurrency primitives and add RwLock
```

--------------------------------

TITLE: Zig: External Declarations
DESCRIPTION: The `extern` keyword in Zig is used to declare functions or variables that will be resolved at link time, either statically or dynamically at runtime. This is crucial for interfacing with C libraries or other pre-compiled code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
extern fn puts(s: [*]const u8) c_int;

pub fn main() void {
    puts("Hello from Zig!");
}
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read and print command-line arguments when compiled for WASI. It utilizes `std.process.argsAlloc` for argument retrieval and `std.debug.print` for output.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig Deflate Compressor API Comparison
DESCRIPTION: Compares the old and new API for the deflate stream functionality in Zig. The new API offers improved performance and a more modern interface for compression and decompression tasks.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Old API:

{#syntax#}inflateStream(reader: anytype, window_slice: []u8){#endsyntax#}

New API:

{#syntax#}decompressor(allocator: mem.Allocator, reader: anytype, dictionary: ?[]const u8)
compressor(allocator: mem.Allocator, writer: anytype, options: CompressorOptions){#endsyntax#}
```

--------------------------------

TITLE: Zig Progress Protocol: Terminal Refresh Thread
DESCRIPTION: This describes the separate thread responsible for periodically refreshing the terminal display in the Zig Progress Protocol. It details the process of serializing data, handling child process communication via pipes, and updating the terminal.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.13.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Conceptual representation of the terminal refresh thread's logic:

// This thread periodically wakes up on a timer.
// It iterates over a preallocated parents array.
// It serializes data atomically.
// If a node is linked to a child process via a pipe:
//   - It reads data from the pipe.
//   - Merges this data into the serialized buffer.
// After serialization, it computes tree structures and writes to the terminal.
// Handles SIGWINCH for terminal resize events.

```

--------------------------------

TITLE: Zig Implicit Cast: *T and [*]T to ?*c_void
DESCRIPTION: Allows implicit casting from pointers to single elements (`*T`) or slices (`[*]T`) to an optional pointer to `c_void` (`?*c_void`). This is useful for interfacing with C APIs that expect generic void pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
var x: u32 = 10;
const void_ptr: ?*c_void = &x;

const arr: [5]u8 = .{ 1, 2, 3, 4, 5 };
const slice_void_ptr: ?*c_void = &arr;
```

--------------------------------

TITLE: Zig extern Keyword
DESCRIPTION: The `extern` keyword in Zig is used to declare functions or variables that will be resolved at link time, either statically or dynamically at runtime. It's essential for interfacing with external libraries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
extern fn printf(format: [*c]const u8, ...) callconv(.C) i32;

fn printMessage() void {
    printf("Hello from Zig!\n", .{
        @tagName(u8),
    });
}

```

--------------------------------

TITLE: Zig Formatting Structs via Reflection
DESCRIPTION: Updates `std.fmt.format` to handle non-pointer struct, union, and enum types, including support for printing structs using reflection.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.fmt.format
```

--------------------------------

TITLE: Zig Concurrent Async I/O Example
DESCRIPTION: Demonstrates concurrent execution of asynchronous tasks using Zig's evented I/O mode. It creates two files, 'a.txt' and 'b.txt', writing 'A\n' and 'B\n' respectively, showcasing parallel file operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub const io_mode = .evented;

pub fn main() anyerror!void {
    var a_frame = async doA();
    var b_frame = async doB();

    try await a_frame;
    try await b_frame;
}

fn doA() !void {
    const file = try std.fs.cwd().createFile("a.txt", .{}) ;
    defer file.close();
    try file.writeAll("A\n");
}

fn doB() !void {
    const file = try std.fs.cwd().createFile("b.txt", .{}) ;
    defer file.close();
    try file.writeAll("B\n");
}
```

--------------------------------

TITLE: Zig Implicit Cast: *T and [*]T to ?*c_void
DESCRIPTION: Enables implicit casting from pointers to single elements (`*T`) or slices (`[*]T`) to an optional pointer to `c_void` (`?*c_void`). This is useful for interfacing with C APIs that expect generic void pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
var x: u32 = 10;
const void_ptr: ?*c_void = &x;

const arr: [5]u8 = .{ 1, 2, 3, 4, 5 };
const slice_void_ptr: ?*c_void = &arr;
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read command line arguments within a WASI environment. It allocates memory for arguments and prints them with their index.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig Built-in Function: @atomicStore
DESCRIPTION: The `@atomicStore` builtin function performs an atomic store operation to a memory location. It guarantees that the write operation is indivisible and visible across threads.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Int(u32) = undefined;

    @atomicStore(&atomic_var, 456, .SeqCst);

    const value = std.atomic.load(&atomic_var, .SeqCst);
    std.debug.print("Stored atomic value: {d}\\n", .{
        value,
    });
}
```

--------------------------------

TITLE: Zig Target Triple Command-Line Examples
DESCRIPTION: Illustrates various ways to specify target architectures, CPU features, and OS versions using Zig's command-line interface. This includes setting baseline features, adding/removing specific features, and targeting different architectures like RISC-V and ARM.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Shell
CODE:
```
# Native architecture, OS, and ABI, but baseline CPU features:
-target native -mcpu=baseline
```

LANGUAGE: Shell
CODE:
```
# RISC-V 64-bit architecture, OS linux, default ABI, native CPU plus the rdpid feature, minus the sse3 feature:
-target riscv64-linux -mcpu=native+rdpid-sse3
```

LANGUAGE: Shell
CODE:
```
# Target the RPi Zero:
-target arm-linux-musleabi -mcpu=arm1176jzf_s
```

LANGUAGE: Shell
CODE:
```
# Minimum Windows version: XP
# Maximum Windows version: 10
-target x86_64-windows.xp...win10-msvc
```

LANGUAGE: Shell
CODE:
```
# Minimum Windows version: 7
# Maximum Windows version: latest
-target x86_64-windows.win7-msvc
```

LANGUAGE: Shell
CODE:
```
# Linux example with OS version range:
-target aarch64-linux.3.16...5.3.1-musl
```

LANGUAGE: Shell
CODE:
```
# Specifying glibc version:
-target mipsel-linux.4.10-gnu.2.1
```

LANGUAGE: Shell
CODE:
```
# Simplified target without sub-architecture (e.g., v7a):
-target arm-linux-gnu
```

LANGUAGE: Shell
CODE:
```
# Targeting a different sub-architecture (e.g., v6kz) using -mcpu:
-target arm-linux-gnu -mcpu=generic+v6kz
```

--------------------------------

TITLE: Zig: Improve std.elf.Elf open functions
DESCRIPTION: The `std.elf.Elf` open functions have been improved to directly return the `Elf` struct instead of requiring a pointer to be filled. This change is noted in issue #2998.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.elf.Elf
```

--------------------------------

TITLE: Zig Opaque Types for C Interoperability
DESCRIPTION: Illustrates the use of `opaque {}` to declare types with unknown size and alignment, commonly used for type safety when interfacing with C code. The example shows how opaque types can be used in function signatures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read command line arguments within a WASI environment. It allocates memory for arguments and prints them with their index.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig: Fix type error for u8 in writeIntSlice
DESCRIPTION: Corrects a type error that occurred when using `u8` with the `writeIntSlice` function. This ensures proper handling of unsigned 8-bit integers in slice writing operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
writeIntSlice
```

--------------------------------

TITLE: Zig Aligned Struct Fields
DESCRIPTION: Shows how to specify alignment for individual struct fields using the `align(N)` keyword. This provides fine-grained control over memory layout, which is essential for performance-critical code or interfacing with hardware.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expectEqual = std.testing.expectEqual;

test "aligned struct fields" {
    const S = struct {
        a: u32 align(2),
        b: u32 align(64),
    };
    var foo = S{ .a = 1, .b = 2 };

    try expectEqual(64, @alignOf(S));
    try expectEqual(*align(2) u32, @TypeOf(&foo.a));
    try expectEqual(*align(64) u32, @TypeOf(&foo.b));
}
```

--------------------------------

TITLE: Zig @OpaqueType Function
DESCRIPTION: Creates a new type with an unknown size and alignment. This is primarily used for type safety when interfacing with C code where struct details are not exposed, preventing accidental misuse of incompatible opaque types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: zig
CODE:
```
const Derp = @OpaqueType();
const Wat = @OpaqueType();

extern fn bar(d: &Derp);
export fn foo(w: &Wat) {
    bar(w);
}
```

--------------------------------

TITLE: Fix Windows x86_64 i128 ABI Issue (Zig)
DESCRIPTION: Resolves an Application Binary Interface (ABI) issue specific to the i128 integer type on the x86_64 Windows platform. This ensures correct function calling conventions and data passing.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
// No specific code snippet provided, but the fix addresses the Windows x86_64 i128 ABI issue.
```

--------------------------------

TITLE: Zig Peer Type Resolution: Array and Slice Compatibility
DESCRIPTION: Illustrates peer type resolution with arrays and slices, showing how they can be implicitly cast to a const slice. Includes tests for both runtime and compile-time evaluation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;
const mem = std.mem;

test "peer resolve arrays of different size to const slice" {
    assert(mem.eql(u8, boolToStr(true), "true"));
    assert(mem.eql(u8, boolToStr(false), "false"));
    comptime assert(mem.eql(u8, boolToStr(true), "true"));
    comptime assert(mem.eql(u8, boolToStr(false), "false"));
}
fn boolToStr(b: bool) []const u8 {
    return if (b) "true" else "false";
}
```

--------------------------------

TITLE: Zig std.os.read implementation
DESCRIPTION: This Zig code snippet demonstrates the implementation of the `std.os.read` function. It handles reading from file descriptors, with specific logic for Windows, WASI (when not linking libc), and a general system call fallback. It also includes error handling for various POSIX error codes and integrates with an event loop for non-blocking operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
/// Returns the number of bytes that were read, which can be less than /// buf.len. If 0 bytes were read, that means EOF. /// If the application has a global event loop enabled, EAGAIN is handled /// via the event loop. Otherwise EAGAIN results in error.WouldBlock. pub fn read(fd: fd_t, buf: []u8) ReadError!usize {
    if (builtin.os == .windows) {
        return windows.ReadFile(fd, buf);
    }
    if (builtin.os == .wasi and !builtin.link_libc) {
        const iovs = [_]iovec{iovec{ .iov_base = buf.ptr, .iov_len = buf.len, }};
        var nread: usize = undefined;
        switch (wasi.fd_read(fd, &iovs, iovs.len, &nread)) {
            0 => return nread,
            else => |err| return unexpectedErrno(err),
        }
    }
    while (true) {
        const rc = system.read(fd, buf.ptr, buf.len);
        switch (errno(rc)) {
            0 => return @intCast(usize, rc),
            EINTR => continue,
            EINVAL => unreachable,
            EFAULT => unreachable,
            EAGAIN => if (std.event.Loop.instance) |loop| {
                loop.waitUntilFdReadable(fd);
                continue;
            } else {
                return error.WouldBlock;
            },
            EBADF => unreachable, // Always a race condition.
            EIO => return error.InputOutput,
            EISDIR => return error.IsDir,
            ENOBUFS => return error.SystemResources,
            ENOMEM => return error.SystemResources,
            ECONNRESET => return error.ConnectionResetByPeer,
            else => |err| return unexpectedErrno(err),
        }
    }
    return index;
}
```

--------------------------------

TITLE: Zig: `Allocator.reallocBytes` Public Access
DESCRIPTION: Makes `Allocator.reallocBytes` public, which is beneficial for scenarios requiring custom alignment, such as interfacing with C code that uses custom allocation callbacks. This provides greater flexibility in memory management.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
Publicize Allocator.reallocBytes. This is useful when dealing with runtime-known alignments, eg. interfacing with C code that accepts custom allocation callbacks ([#9394](https://github.com/ziglang/zig/issues/9394)).
```

--------------------------------

TITLE: Zig ZON Serialization Functions
DESCRIPTION: Offers runtime capabilities for serializing Zig data into ZON format. Functions include serialize, serializeMaxDepth, serializeArbitraryDepth, and serializer. The serializer provides a fine-grained interface for piece-by-piece serialization.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.zon.stringify.serialize
std.zon.stringify.serializeMaxDepth
std.zon.stringify.serializeArbitraryDepth
std.zon.stringify.serializer
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read command line arguments within a WASI environment. It allocates memory for arguments and prints them with their index.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Fix Compiler-RT ABI for x86_64 Windows (Zig)
DESCRIPTION: Ensures correct Application Binary Interface (ABI) compatibility for the compiler runtime on the x86_64 Windows platform. This is crucial for interoperability with system libraries and other compiled code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
// No specific code snippet provided, but the fix relates to the compiler-rt ABI for x86_64 Windows.
```

--------------------------------

TITLE: Zig: Opaque type interaction with C
DESCRIPTION: Provides an example of using Zig's opaque types for type safety when interacting with C code. It defines opaque types and demonstrates a function call that requires casting.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig Opaque Types for C Interoperability
DESCRIPTION: Illustrates the use of `opaque {}` in Zig to declare types with unknown size and alignment, commonly used for type safety when interfacing with C code. It shows how opaque types can be used in function signatures and how type mismatches are handled.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig: Volatile Memory Access
DESCRIPTION: Explains and demonstrates the use of the `volatile` keyword in Zig for memory-mapped input/output (MMIO). It shows how to declare a volatile pointer and asserts its type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

test "volatile" {
    const mmio_ptr = @intToPtr(*volatile u8, 0x12345678);
    assert(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig @alignOf
DESCRIPTION: Returns the required byte alignment for a type according to the C ABI. This is useful for ensuring correct memory alignment, especially when interfacing with C code or when manually managing memory. The result is a compile-time constant.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
    @alignOf(comptime T: type) comptime_int
```

LANGUAGE: zig
CODE:
```
    const assert = @import("std").debug.assert;
    comptime {
        assert(*u32 == *align(@alignOf(u32)) u32);
    }
```

--------------------------------

TITLE: Zig Standard Library: Linux Syscall Signatures and Features
DESCRIPTION: This snippet covers various updates to the Zig standard library related to Linux system calls and functionalities. It includes corrections for `os.rusage` linking with the C library, the addition of `shm_open` and `shm_unlink` in `std.c`, and fixes for `preadv`/`pwritev` on 64-bit platforms. It also mentions the addition of POSIX file locking support and fixes for `ucontext_t` and `getdents64` error handling.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example: Using fchown with 32-bit uid/gid on 32-bit Linux
try std.os.fchown(fd, uid_32, gid_32);

// Example: Using shm_open and shm_unlink
const shm_fd = try std.c.shm_open(c"my_shm", std.os.O.RDWR | std.os.O.CREAT, 0644);
try std.c.shm_unlink(c"my_shm");

// Example: POSIX file locking with fcntl
var flock: std.os.flock_t = undefined;
// ... populate flock struct ...
try std.os.fcntl(fd, std.os.FcntlCmd.SETLK, &flock);

// Example: Handling EINVAL from getdents64
var dir_entries: [1024]u8 = undefined;
const bytes_read = try std.fs.cwd().read(&dir_entries);
// ... process dir_entries, handle potential EINVAL ...

```

--------------------------------

TITLE: Zig @OpaqueType Function
DESCRIPTION: Creates a new type with an unknown size and alignment. This is primarily used for type safety when interfacing with C code where struct details are not exposed, preventing accidental misuse of incompatible opaque types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = @OpaqueType();
const Wat = @OpaqueType();

extern fn bar(d: &Derp);
export fn foo(w: &Wat) {
    bar(w);
}
```

--------------------------------

TITLE: Fix asyncCall with Non-ABI-Aligned Arguments
DESCRIPTION: Addresses issues with `asyncCall` when passing non-ABI-aligned arguments. The code for calculating variable slot indices in the frame now matches structure layout calculations, preventing LLVM errors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: zig
CODE:
```
const result = await asyncCall(myAsyncFn, non_aligned_arg);
```

--------------------------------

TITLE: Zig Sentinel-Terminated Pointer for C printf
DESCRIPTION: Shows how to use a sentinel-terminated pointer in Zig to interface with C functions like `printf`. The example defines `printf` using a `[*:0]const u8` type for the format string, which expects a null terminator. It also demonstrates a compilation error when passing a non-null-terminated string.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
    
    // This is also available as `std.c.printf`.
    pub extern "c" fn printf(format: [*:0]const u8, ...) c_int;
    
    pub fn main() anyerror!void {
    _ = printf("Hello, world!\n"); // OK

    const msg = "Hello, world!\n";
    const non_null_terminated_msg: [msg.len]u8 = msg.*;
    _ = printf(&non_null_terminated_msg);
}
```

--------------------------------

TITLE: Zig @setAlignStack for Stack Alignment
DESCRIPTION: The @setAlignStack builtin ensures that a function's stack frame has at least the specified byte alignment. This can be important for performance-critical code or when interfacing with external libraries that have specific stack alignment requirements.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
@setAlignStack(comptime alignment: u29)
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read and print command line arguments when compiling for WASI. It utilizes `std.process.argsAlloc` to get the arguments and `std.debug.print` for output. The code requires a WASI-compatible runtime like `wasmtime`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig WebAssembly Support
DESCRIPTION: This section explains how Zig can be used to compile code for WebAssembly, covering both freestanding WebAssembly targets and those utilizing the WebAssembly System Interface (WASI). This is key for cross-platform development and web-based applications.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
Freestanding
WASI
```

--------------------------------

TITLE: Zig Hello World to Stdout
DESCRIPTION: Demonstrates the basic 'Hello, world!' program in Zig, printing output to standard output using the `std.io.getStdOut().outStream().print` function. It shows how to import the standard library and handle potential errors during output.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    const stdout = std.io.getStdOut().outStream();
    try stdout.print("Hello, {}!\n", .{"world});
}
```

--------------------------------

TITLE: Zig Async Test Skip Example
DESCRIPTION: Illustrates skipping an asynchronous Zig test. The test runner skips tests that involve suspend points in blocking IO mode. The example includes an async function and a test that awaits its result.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

test "async skip test" {
    var frame = async func();
    const result = await frame;
    try std.testing.expect(result == 1);
}

fn func() i32 {
    suspend {
        resume @frame();
    }
    return 1;
}
```

--------------------------------

TITLE: Zig atomic load operation
DESCRIPTION: Demonstrates the `@atomicLoad` built-in function for performing atomic read operations in Zig.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
var atomic_var: atomic(i32) = undefined;
const value = @atomicLoad(&atomic_var, .SeqCst);
```

--------------------------------

TITLE: Zig @setAlignStack Builtin
DESCRIPTION: The @setAlignStack builtin ensures that a function's stack alignment meets a specified minimum byte requirement. This can be important for performance-critical code or when interfacing with external libraries that have specific stack alignment expectations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
comptime {
    @setAlignStack(comptime alignment: u29) void
}
```

--------------------------------

TITLE: Zig @alignOf: Get Type Alignment
DESCRIPTION: The @alignOf function returns the number of bytes required for a type's alignment according to the C ABI for the current target. This is useful for ensuring correct memory alignment, especially when interfacing with C code. The result is a compile-time constant.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: zig
CODE:
```
comptime_int @alignOf(comptime T: type) comptime_int
```

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;
comptime {
    assert(*u32 == *align(@alignOf(u32)) u32);
}
```

--------------------------------

TITLE: Zig @setAlignStack Builtin
DESCRIPTION: The @setAlignStack builtin ensures that a function's stack alignment meets a specified minimum byte requirement. This can be important for performance-critical code or when interfacing with external libraries that have specific stack alignment expectations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
comptime {
    @setAlignStack(comptime alignment: u29) void
}
```

--------------------------------

TITLE: Zig Case Study: print function
DESCRIPTION: Demonstrates the usage of the 'print' function in Zig, a common operation for outputting formatted strings to the console. This example serves as a practical introduction to I/O operations in Zig.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() {
    std.debug.print("Hello, {}!\n", .{"world"});
}
```

--------------------------------

TITLE: Zig Synchronizing External Operations with fetchAdd
DESCRIPTION: Illustrates simulating fences for external operations in Zig using `fetchAdd(0, .seq_cst)`. This is a workaround when direct control over the ordering of external function calls is not possible.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: zig
CODE:
```
fetchAdd(0, .seq_cst)
```

--------------------------------

TITLE: Zig @setAlignStack Builtin
DESCRIPTION: The @setAlignStack builtin ensures that a function's stack alignment meets a specified minimum byte requirement. This can be important for performance-critical code or when interfacing with external libraries that have specific stack alignment expectations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
comptime {
    @setAlignStack(comptime alignment: u29) void
}
```

--------------------------------

TITLE: Zig Case Study: print function
DESCRIPTION: Demonstrates the usage of the 'print' function in Zig, a common operation for outputting formatted strings to the console. This example serves as a practical introduction to I/O operations in Zig.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() {
    std.debug.print("Hello, {}!\n", .{"world"});
}
```

--------------------------------

TITLE: WASI: Enable experimental WASI-threads support
DESCRIPTION: Zig now offers experimental support for WASI-threads, allowing multi-threaded applications when targeting WASI. This feature is based on phase 1 of the WASI specification and requires specific compiler flags and CPU features.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
-fno-single-threaded --shared-memory
```

--------------------------------

TITLE: Zig Crypto Keccak Permutation API
DESCRIPTION: Exposes the Keccak permutation with a public interface in `crypto.core.keccak`. `KeccakF` supports permutation sizes from 200 to 1600 bits and a configurable number of rounds, while `State` offers APIs for sponge constructions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.crypto.core.keccak.KeccakF
```

LANGUAGE: zig
CODE:
```
std.crypto.core.keccak.State
```

--------------------------------

TITLE: Build WebAssembly Executable with WASI (Zig)
DESCRIPTION: This Zig command compiles a Zig file into a WebAssembly executable that uses the WebAssembly System Interface (WASI). This allows the WASM module to interact with the host system, such as accessing command-line arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = &general_purpose_allocator.allocator;
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig Fully Anonymous Struct Type Inference
DESCRIPTION: Demonstrates type inference for anonymous structs where the type is not explicitly provided. The `dump` function accepts a variadic argument and asserts the types and values of its fields, showcasing flexible struct handling.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;

test "fully anonymous struct" {
    dump(.{
        .int = @as(u32, 1234),
        .float = @as(f64, 12.34),
        .b = true,
        .s = "hi",
    });
}

fn dump(args: var) void {
    assert(args.int == 1234);
    assert(args.float == 12.34);
    assert(args.b);
    assert(args.s[0] == 'h');
    assert(args.s[1] == 'i');
}
```

--------------------------------

TITLE: Zig: Linking Sections with linksection
DESCRIPTION: The `linksection` keyword in Zig allows developers to place code or data into specific sections of the final executable or library. This is often used for custom memory management or interfacing with specific linker scripts.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
@linkSection("my_data", .{
    .type = .data,
    .size = 1024,
}) align(16) var buffer: [1024]u8;

fn processBuffer() void {
    // Use the buffer located in the 'my_data' section
    buffer[0] = 1;
}
```

--------------------------------

TITLE: Improve ChildProcess Error Communication on Linux
DESCRIPTION: The `std.ChildProcess` on Linux now utilizes eventfds for communicating errors from child to parent processes, replacing the previous pipe-based mechanism. This change is expected to improve the robustness and efficiency of inter-process communication.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.ChildProcess
```

--------------------------------

TITLE: Zig Sentinel-Terminated Pointers for C printf
DESCRIPTION: Illustrates the use of sentinel-terminated pointers, specifically '[*:0]const u8', for interfacing with C functions like printf. The example shows how to correctly pass a null-terminated string to printf and highlights a common error when a non-terminated string is passed, leading to a compile-time error.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

// This is also available as `std.c.printf`.
pub extern "c" fn printf(format: [*:0]const u8, ...) c_int;

pub fn main() anyerror!void {
    _ = printf("Hello, world!\n"); // OK

    const msg = "Hello, world!\n";
    const non_null_terminated_msg: [msg.len]u8 = msg.*;
    _ = printf(&non_null_terminated_msg);
}
```

--------------------------------

TITLE: Zig stdlib Random Improvements
DESCRIPTION: Zig's standard library has seen improvements in random number generation, including the addition of `std.rand.RomuTrio` and changes to the random interface. The `random` module now includes a `weightedIndex` function for more flexible random selection.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
// Example of using std.rand.RomuTrio (conceptual)
const RomuTrio = std.rand.RomuTrio;
var rng = RomuTrio.init(seed_value);
const random_number = rng.next();

// Example of weightedIndex (conceptual)
const weights = [_]u32{ 10, 20, 70 };
const index = std.rand.weightedIndex(rng, weights);

```

--------------------------------

TITLE: WASI: Fix IterableDir.nextWasi for large directories
DESCRIPTION: A bug in `IterableDir.nextWasi` for handling large directories on WASI has been resolved. This ensures that directory iteration functions correctly even when dealing with a significant number of entries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.fs.Dir.iterate
```

--------------------------------

TITLE: Zig: Linking Sections with linksection
DESCRIPTION: The `linksection` keyword in Zig allows developers to place code or data into specific sections of the final executable or library. This is often used for custom memory management or interfacing with specific linker scripts.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
@linkSection("my_data", .{
    .type = .data,
    .size = 1024,
}) align(16) var buffer: [1024]u8;

fn processBuffer() void {
    // Use the buffer located in the 'my_data' section
    buffer[0] = 1;
}
```

--------------------------------

TITLE: Zig Keyword: callconv
DESCRIPTION: The `callconv` keyword in Zig is used within a function type declaration to specify the calling convention. This is crucial for interoperability, especially when interfacing with code written in different languages or using different ABI standards.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const MyFuncType = fn(a: i32) i32; // Default calling convention
const MyCConvFuncType = fn(a: i32) i32 callconv(.C) i32;

```

--------------------------------

TITLE: Zig: Create Opaque Type
DESCRIPTION: Creates a new type with an unknown but non-zero size and alignment. This is useful for type safety when interfacing with C code that does not expose struct details. The example demonstrates creating opaque types and a type mismatch error.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: zig
CODE:
```
    @OpaqueType() type
```

LANGUAGE: zig
CODE:
```
const Derp = @OpaqueType();
const Wat = @OpaqueType();

extern fn bar(d: *Derp) void;
export fn foo(w: *Wat) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig Standard Library: Process Management Enhancements
DESCRIPTION: Significant updates have been made to the process module. This includes changes to ArgIterator.next return types, buffer size fixes for process.argsAlloc, and added support for single quotes in ArgIteratorGeneral. Implementations now leverage posix_spawn as an alternative to fork-exec where possible, and ChildProcess.init no longer performs heap allocations. Error handling for empty PATH in ChildProcess and buffer overflows in std.os.execvpe have also been addressed.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

// Example: Using ArgIteratorGeneral with single quote support
var iter = std.process.argsAlloc.generalIterator(std.heap.page_allocator, "'quoted arg'");
while (iter.next()) |arg|
{
    std.debug.print("Argument: {s}\\n", .{arg});
}

// Example: Using posix_spawn (conceptual)
// const child = try std.ChildProcess.init(&.{ "./my_program" }, .{});
// try child.spawn(.{ .use_posix_spawn = true });

```

--------------------------------

TITLE: Zig Keyword: callconv
DESCRIPTION: The `callconv` keyword in Zig is used within a function type declaration to specify the calling convention. This is crucial for interoperability, especially when interfacing with code written in different languages or using different ABI standards.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
const MyFuncType = fn(a: i32) i32; // Default calling convention
const MyCConvFuncType = fn(a: i32) i32 callconv(.C) i32;

```

--------------------------------

TITLE: Zig ArrayList API Update
DESCRIPTION: The ArrayList API in Zig has been updated to make the 'items' field directly usable as the slice of valid objects. Capacity is now managed separately. This change simplifies usage but may break existing callsites. Deprecated functions like 'toSlice' and 'at' should be replaced with direct 'items' slice access.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const ArrayList = struct {
    items: []T,
    capacity: usize,
};

// Example of direct access:
var list: ArrayList(i32) = undefined;
const first_item = list.items[0];

// Deprecated usage (to be avoided):
// const first_item = list.toSlice()[0];
```

--------------------------------

TITLE: Zig Integer-Pointer Conversion
DESCRIPTION: Illustrates how to convert between integer memory addresses and Zig pointers using the built-in functions @ptrFromInt and @intFromPtr. This is useful for low-level memory manipulation or interfacing with C code. The example verifies the type and value of the converted address.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@intFromPtr and @ptrFromInt" {
    const ptr: *i32 = @ptrFromInt(0xdeadbee0);
    const addr = @intFromPtr(ptr);
    try expect(@TypeOf(addr) == usize);
    try expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Zig Fully Anonymous Struct Type Inference
DESCRIPTION: Demonstrates how Zig can infer the type of a struct literal when it's passed to a function expecting `anytype`, allowing for flexible and anonymous data structures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    const expect = std.testing.expect;
    
    test "fully anonymous struct" {
        dump(.{
            .int = @as(u32, 1234),
            .float = @as(f64, 12.34),
            .b = true,
            .s = "hi",
        });
    }
    
    fn dump(args: anytype) void {
        expect(args.int == 1234);
        expect(args.float == 12.34);
        expect(args.b);
        expect(args.s[0] == 'h');
        expect(args.s[1] == 'i');
    }

    $ zig test struct_anon.zig
    1/1 test "fully anonymous struct"... OK
    All 1 tests passed.
    
```

--------------------------------

TITLE: Zig Build System: Link System Library
DESCRIPTION: Demonstrates how to use the Zig Build System's `linkSystemLibrary` API, which hints Zig to search system default paths for libraries. This functionality can also be exposed to the command line interface.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: Zig
CODE:
```
zig build-exe --c-source hello.c --library c -target x86_64-linux-musl
```

--------------------------------

TITLE: Zig: Sentinel-Terminated Pointers Example
DESCRIPTION: Demonstrates passing strings to Zig and C functions using sentinel-terminated pointers. The `do_it_the_zig_way` function accepts a slice `[]const u8`, while `do_it_the_c_way` accepts a C-style null-terminated pointer `*const [N:0]u8`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    do_it_the_zig_way("world");
    do_it_the_c_way("world");
}

fn do_it_the_zig_way(arg: []const u8) void {
    std.debug.warn("hello {}\n", .{arg});
}

fn do_it_the_c_way(arg: *const [N:0]u8) void {
    _ = std.c.printf("hello %s\n", arg);
}
```

--------------------------------

TITLE: Zig: Add io.Reader.readUntilDelimiter
DESCRIPTION: Adds `io.Reader.readUntilDelimiter`, a method to read data from a reader until a specified delimiter is encountered.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
io.Reader.readUntilDelimiter
```

--------------------------------

TITLE: Add waitid Syscall on Linux
DESCRIPTION: Introduces the `waitid` syscall to the Linux OS bindings, providing a more flexible way to wait for process status changes.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Add waitid syscall on linux ([#9335](https://github.com/ziglang/zig/issues/9335)).
```

--------------------------------

TITLE: Zig Volatile Pointers
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig for memory-mapped input/output (MMIO). This example shows how to declare a pointer to a volatile type and verifies its type. It highlights that `volatile` ensures loads and stores happen as specified in the source code, preventing compiler optimizations that might reorder or omit them.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Build Commands for Different Modes
DESCRIPTION: Demonstrates how to compile Zig executables using different build modes via the Zig command-line interface. Each command corresponds to a specific set of optimizations and safety checks, affecting compilation speed, runtime performance, and binary size.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: bash
CODE:
```
$ zig build-exe example.zig
```

LANGUAGE: bash
CODE:
```
$ zig build-exe example.zig -O ReleaseFast
```

LANGUAGE: bash
CODE:
```
$ zig build-exe example.zig -O ReleaseSafe
```

LANGUAGE: bash
CODE:
```
$ zig build-exe example.zig -O ReleaseSmall
```

--------------------------------

TITLE: Zig C-compatible `main` Export
DESCRIPTION: Illustrates how to export a C-compatible `main` function when the Zig compilation links libc. This function signature matches the C `main` function, accepting argument count (`argc`) and argument values (`argv`), and returning an integer status code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
pub export fn main(argc: c_int, argv: [*]const [*:0]const u8) c_int {
    const args = argv[0..@intCast(argc)];
    std.debug.print("Hello! argv[0] is '{s}'\n", .{args[0]});
    return 0;
}

const std = @import("std");
```

--------------------------------

TITLE: Zig Volatile Pointers
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig for memory-mapped input/output (MMIO). This example shows how to declare a pointer to a volatile type and verifies its type. It highlights that `volatile` ensures loads and stores happen as specified in the source code, preventing compiler optimizations that might reorder or omit them.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Opaque Type Declaration and Usage
DESCRIPTION: Illustrates the use of `opaque {}` in Zig to declare types with unknown size and alignment, often used for type safety when interfacing with C code. It shows how opaque types can be declared and used in function signatures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig CC: Support for stdin input
DESCRIPTION: Describes the enhancement in `zig cc` that allows it to read source code directly from standard input, useful for piping code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: shell
CODE:
```
echo "int main() { return 0; }" | zig cc -o main -
```

--------------------------------

TITLE: Zig: Remove mem.spanZ in favor of mem.sliceTo
DESCRIPTION: The `mem.spanZ` function has been removed. Use `mem.sliceTo` instead for creating slices from null-terminated strings.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
mem.spanZ
```

LANGUAGE: Zig
CODE:
```
mem.sliceTo
```

--------------------------------

TITLE: C Header for Zig Exported Library
DESCRIPTION: A C header file (`mathtest.h`) that is automatically generated by Zig when exporting a library. This header declares the exported Zig functions, allowing C code to correctly interface with the compiled Zig library.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: C
CODE:
```
// This header is generated by zig from mathtest.zig
#include "mathtest.h"
#include <stdio.h>

int main(int argc, char **argv) {
    int32_t result = add(42, 1337);
    printf("%d\n", result);
    return 0;
}
```

--------------------------------

TITLE: Zig: Single-Threaded Build Option
DESCRIPTION: Enables the `-fsingle-threaded` compile option, which treats thread-local variables as container-level variables and optimizes async function overhead. It also affects runtime APIs like `std.Mutex`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example usage within a Zig file
// The effects are compiler-level, not directly shown in code snippets here.
```

--------------------------------

TITLE: Zig Async Test Skip Example
DESCRIPTION: Illustrates skipping an asynchronous Zig test that contains a suspend point. The test is skipped because the suspend point is encountered during execution in the default blocking IO mode. The output explicitly mentions 'async test' for the skip.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

test "async skip test" {
    var frame = async func();
    const result = await frame;
    try std.testing.expect(result == 1);
}

fn func() i32 {
    suspend {
        resume @frame();
    }
    return 1;
}
```

--------------------------------

TITLE: Zig Debug: Nosuspend for stderr.print
DESCRIPTION: Ensures that stderr.print calls are wrapped with nosuspend to fix compilation errors that occur in non-blocking I/O mode. This improves the robustness of debug output handling.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: Zig
CODE:
```
debug: Add nosuspend around stderr.print calls. Fixes compilation errors with non-blocking I/O mode.
```

--------------------------------

TITLE: Zig Opaque Type Declaration and Usage
DESCRIPTION: Illustrates the declaration and usage of opaque types in Zig, which have unknown size and alignment. Opaque types are useful for interfacing with C code where struct details are not exposed, ensuring type safety. The example shows an external function expecting an opaque pointer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.c) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to read command line arguments using Zig's standard library with WASI. It allocates memory for arguments and prints them with their index. Requires the `std` module.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Fix os.uefi Packed Struct Bitfields
DESCRIPTION: Corrects the handling of packed struct bitfields within the `os.uefi` module, ensuring accurate representation of UEFI structures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
os.uefi: fix packed struct bitfields.
```

--------------------------------

TITLE: Zig StringPoolContext for Hash Maps
DESCRIPTION: Illustrates a `StringPoolContext` struct that holds a `StringPool` and provides `hash` and `eql` functions to operate on `StringPool.ID` types, enabling specialized key handling in hash maps.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
pub const StringPoolContext = struct {
    pool: StringPool,
    pub fn hash(self: @This(), s: StringPool.ID) u64 {
        return self.pool.getStringHash(s);
    }
    pub fn eql(self: @This(), a: StringPool.ID, b: StringPool.ID) bool {
        return a == b;
    }
};
```

--------------------------------

TITLE: Zig Syscall Example for Printing
DESCRIPTION: This Zig code snippet demonstrates how to use inline assembly to perform a 'write' syscall, printing 'Hello, world!\n' to standard output one million times. It also includes a 'exit' syscall. The code is optimized for performance, showing a significant reduction in wall clock time and memory usage.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
pub export fn _start() noreturn {
    print(); // repeated 1,000,000 times
    exit();
}

fn print() void {
    asm volatile ("syscall"
        : 
        : [number] "{rax}" (1),
          [arg1] "{rdi}" (1),
          [arg2] "{rsi}" (@ptrToInt("Hello, world!\n")),
          [arg3] "{rdx}" (14)
        : "rcx", "r11", "memory"
    );
    return;
}

fn exit() noreturn {
    asm volatile ("syscall"
        : 
        : [number] "{rax}" (231),
          [arg1] "{rdi}" (0)
        : "rcx", "r11", "memory"
    );
    unreachable;
}
```

--------------------------------

TITLE: Zig Standard Library: macOS Syscall and Debugging Improvements
DESCRIPTION: This section details enhancements to Zig's standard library for macOS, including updated libc headers and the use of `if_nametoindex` for IP parsing. It covers the implementation of a segfault handler for both x86_64 and aarch64 architectures, fixes in macOS debug symbol lookup, and the addition of `mach_*` syscalls for process management. It also notes updates to Mach routines for page info and fixes for incorrect return types on some libc functions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example: Using if_nametoindex for IP parsing on macOS
const ip_str = "192.168.1.1";
const if_index = try std.c.if_nametoindex(ip_str);

// Example: Mach syscalls for process management (conceptual)
const pid = std.os.getpid();
var proc_info: std.os.mach.task_basic_info_data_t = undefined;
var count = @sizeOf(proc_info) / @sizeOf(c_uint);
try std.os.mach.task_info(std.os.mach.mach_task_self(), std.os.mach.TASK_BASIC_INFO, @ptrCast(&proc_info), &count);

// Example: Handling segfaults (conceptual)
std.debug.global_atexit.register(segfault_handler);

fn segfault_handler() void {
    // Handle segfault
}

```

--------------------------------

TITLE: Zig Opaque Type Declaration and Usage
DESCRIPTION: Illustrates the declaration and usage of opaque types in Zig, which have unknown size and alignment. This is useful for type safety when interfacing with C code. The example shows an attempt to cast between different opaque types, resulting in a compile-time error.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Zig: os.uefi.protocols.FileProtocol fixes
DESCRIPTION: Applies fixes to the `os.uefi.protocols.FileProtocol` implementation, specifically for `get_position` and `set_position`, referencing issue #7762. This ensures correct file pointer manipulation in UEFI.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
os.uefi.protocols.FileProtocol: fix and expose get_position, set_position ([#7762](https://github.com/ziglang/zig/issues/7762))
```

--------------------------------

TITLE: Zig C-compatible Entry Point 'export fn main'
DESCRIPTION: Defines an entry point function 'main' with a C-compatible signature, allowing it to be exported. This is useful when Zig applications link against libc. The function receives argc and argv and prints the first argument.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
pub export fn main(argc: c_int, argv: [*]const [*:0]const u8) c_int {
    const args = argv[0..@intCast(argc)];
    std.debug.print("Hello! argv[0] is '{s}'\n", .{args[0]});
    return 0;
}

const std = @import("std");
```

--------------------------------

TITLE: Enhance std.os Functionality
DESCRIPTION: Adds 0-length buffer checks to os.read and os.write to prevent undefined pointer errors. Fixes alignment for Sigaction.handler_fn, optimizes os.isCygwinPty, adds mincore syscall, and includes missing mmap errors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
Add 0-length buffer checks to os.read and os.write
std.os: fix alignment of Sigaction.handler_fn
os.isCygwinPty: Fix a bug, replace kernel32 call, and optimize
std.os: add mincore syscall
std.os: add missing mmap errors
```

--------------------------------

TITLE: Peer Type Resolution for Slices and Pointers in Zig
DESCRIPTION: Explains LemonBoy's implementation of peer type resolution between `?[]T` and `*[N]T` slices/pointers in Zig, referencing issue #4767.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
?[]T
```

LANGUAGE: zig
CODE:
```
*[N]T
```

--------------------------------

TITLE: Zig C-compatible Entry Point 'export fn main'
DESCRIPTION: Defines an entry point function 'main' with a C-compatible signature, allowing it to be exported. This is useful when Zig applications link against libc. The function receives argc and argv and prints the first argument.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: Zig
CODE:
```
pub export fn main(argc: c_int, argv: [*]const [*:0]const u8) c_int {
    const args = argv[0..@intCast(argc)];
    std.debug.print("Hello! argv[0] is '{s}'\n", .{args[0]});
    return 0;
}

const std = @import("std");
```

--------------------------------

TITLE: Zig: Function Reflection
DESCRIPTION: Shows how to use Zig's reflection capabilities to inspect function properties. This example uses `@TypeOf` to check the return type and `is_var_args` property of a function.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

test "fn reflection" {
    assert(@TypeOf(assert).ReturnType == void);
    assert(@TypeOf(assert).is_var_args == false);
}
```

--------------------------------

TITLE: Zig Compiler C++ Memory Allocation Overhaul
DESCRIPTION: Details of a significant overhaul to C++ memory allocation within the Zig compiler. This includes introducing a new `mem::Allocator` interface, implementing `heap::CAllocator` and `heap::ArenaAllocator`, and updating container types to accept explicit allocators.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: C++
CODE:
```
new `mem::Allocator` interface
```

LANGUAGE: C++
CODE:
```
new `heap::CAllocator` impl with global `heap::c_allocator`
```

LANGUAGE: C++
CODE:
```
new `heap::ArenaAllocator` impl
```

LANGUAGE: C++
CODE:
```
new `mem::List` takes explicit `Allocator&` parameter
```

LANGUAGE: C++
CODE:
```
new `mem::HashMap` takes explicit `Allocator&` parameter
```

LANGUAGE: C++
CODE:
```
add `Codegen.pass1_arena` and use for all `ZigValue` allocs
```

LANGUAGE: C++
CODE:
```
deinit `Codegen.pass1_arena` early in `zig_llvm_emit_output()`
```

--------------------------------

TITLE: Zig Fully Anonymous Struct Type Inference
DESCRIPTION: Demonstrates how Zig can infer the type of a struct literal when it's passed to a function expecting `anytype`, allowing for flexible and anonymous data structures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    const expect = std.testing.expect;
    
    test "fully anonymous struct" {
        dump(.{
            .int = @as(u32, 1234),
            .float = @as(f64, 12.34),
            .b = true,
            .s = "hi",
        });
    }
    
    fn dump(args: anytype) void {
        expect(args.int == 1234);
        expect(args.float == 12.34);
        expect(args.b);
        expect(args.s[0] == 'h');
        expect(args.s[1] == 'i');
    }

    $ zig test struct_anon.zig
    1/1 test "fully anonymous struct"... OK
    All 1 tests passed.
    
```

--------------------------------

TITLE: Zig Noreturn Type Compatibility
DESCRIPTION: Explains the `noreturn` type in Zig and its compatibility with other types in control flow structures like `if` and `switch`. It also shows an example of using `noreturn` with an external function like `ExitProcess`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: zig
CODE:
```
fn foo(condition: bool, b: u32) {
    const a = if (condition) b else return;
    bar(a);
}

extern fn bar(value: u32);
```

LANGUAGE: zig
CODE:
```
pub extern "kernel32" stdcallcc fn ExitProcess(exit_code: c_uint) -> noreturn;

fn foo() {
    const value = bar() %% ExitProcess(1);
    assert(value == 1234);
}

fn bar() -> %u32 {
    return 1234;
}

const assert = @import("std").debug.assert;
```

--------------------------------

TITLE: Improve BigInt Comparison with math.Order
DESCRIPTION: Big integer comparison code has been improved to use `math.Order` instead of `i8`. This change provides a more expressive and type-safe way to represent comparison results.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.big.rational.gcd
```

LANGUAGE: Zig
CODE:
```
std.big.int.gcd
```

LANGUAGE: Zig
CODE:
```
math.Order
```

--------------------------------

TITLE: Zig Inline Assembly Typed Clobbers
DESCRIPTION: Compares the old stringly-typed clobbers in Zig's inline assembly with the new typed clobbers, which improve type safety and readability. The example shows a `syscall1` function.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.15.1/release-notes.html

LANGUAGE: Zig
CODE:
```
pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
        : "rcx", "r11"
    );
}
```

LANGUAGE: Zig
CODE:
```
pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
        : .{ .rcx = true, .r11 = true });
}
```

--------------------------------

TITLE: Zig Linker: Handling Entry Point Stubs
DESCRIPTION: Addresses a specific edge case where the entry point might be a stub entry, ensuring correct linking.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
handle weird case of entry point being a stub entry (`__TEXT,__stubs` entry)
```

--------------------------------

TITLE: Export Zig Enum with C-ABI Compatibility
DESCRIPTION: Demonstrates how to define a Zig enum that is compatible with the C Application Binary Interface (ABI). By default, Zig enums are not C-ABI compatible. To achieve compatibility, an explicit tag type (e.g., `c_int`) must be provided to the enum definition. This allows the enum to be used in functions exported to C.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
const Foo = enum(c_int) {
    a, b, c
};

export fn entry(foo: Foo) void {
    _ = foo;
}
```

--------------------------------

TITLE: Zig: Inline Function Comptime Propagation
DESCRIPTION: Demonstrates how comptime-ness of arguments propagates to the return value of an inlined Zig function. It also shows that runtime side-effects of inlined functions still occur, as tracked by a call counter.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
var call_count: u32 = 0;

inline fn isGreaterThan(x: i32, y: i32) bool {
    call_count += 1;
    return x > y;
}

test "inline call comptime propagation" {
    // Runtime-known parameters to inline function, nothing new here.
    var a: i32 = 1234;
    var b: i32 = 5678;
    try std.testing.expect(!isGreaterThan(a, b));

    // Now it gets interesting...
    const c = 1234;
    const d = 5678;
    if (isGreaterThan(c, d)) {
        @compileError("that wasn't supposed to happen");
    }

    try std.testing.expect(call_count == 2);
}
```

--------------------------------

TITLE: Zig @prefetch Builtin with PrefetchOptions
DESCRIPTION: Zig 0.11.0 introduces the `@prefetch` builtin function, which allows developers to hint to the compiler to emit prefetch instructions for performance optimization. The `PrefetchOptions` struct configures whether the prefetch is for a read or write, its temporal locality, and the cache type (instruction or data).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
@prefetch(ptr: anytype, comptime options: std.builtin.PrefetchOptions)
```

LANGUAGE: Zig
CODE:
```
/// This data structure is used by the Zig language code generation and
/// therefore must be kept in sync with the compiler implementation.
pub const PrefetchOptions = struct {
    /// Whether the prefetch should prepare for a read or a write.
    rw: Rw = .read,
    /// 0 means no temporal locality. That is, the data can be immediately
    /// dropped from the cache after it is accessed.
    ///
    /// 3 means high temporal locality. That is, the data should be kept in
    /// the cache as it is likely to be accessed again soon.
    locality: u2 = 3,
    /// The cache that the prefetch should be preformed on.
    cache: Cache = .data,

    pub const Rw = enum {
        read,
        write,
    };

    pub const Cache = enum {
        instruction,
        data,
    };
};
```

--------------------------------

TITLE: Zig Volatile Memory Access
DESCRIPTION: Explains and demonstrates the use of `volatile` in Zig for memory-mapped input/output (MMIO). It ensures that loads and stores to volatile pointers are preserved and executed in the specified order.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr = @intToPtr(*volatile u8, 0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Progress Protocol: Node API Thread Safety
DESCRIPTION: This section explains the thread-safe and lock-free design of the `Node.start` and `Node.end` APIs within the Zig Progress Protocol. It emphasizes the use of statically allocated buffers and a custom allocator for performance.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.13.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Assume Node and its associated allocator are defined elsewhere
// and are part of the Zig Progress Protocol implementation.

// Example of how Node.start might be called (conceptual):
// var node: Node = ...;
// try node.start(); // This API is designed to be thread-safe and lock-free.

// Example of how Node.end might be called (conceptual):
// try node.end(); // This API is also designed to be thread-safe and lock-free.

```

--------------------------------

TITLE: Rework ResetEvent with pthreads
DESCRIPTION: This update reworks ResetEvent to improve Darwin integration and uses sem_t when linking against pthreads, enhancing synchronization primitives in Zig.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Fix io.Reader.readUntilDelimiter Byte Loss
DESCRIPTION: Corrects a bug in `io.Reader.readUntilDelimiter` where the last byte could be lost if it appeared after the maximum size. This ensures that the delimiter reading function captures all relevant data.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.1/release-notes.html

LANGUAGE: Zig
CODE:
```
`io.Reader.readUntilDelimiter`: fixed functions losing last byte if it is past the max size
```

--------------------------------

TITLE: Generic Call Workaround with Explicit Type Return
DESCRIPTION: Provides a workaround for the generic function type equality issue by introducing a helper function `Make` that explicitly returns the desired struct type. This ensures type consistency across different generic instantiations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "generic call demo" {
    const a = foo(i32, 1234);
    const b = foo(i32, 5678);
    try expect(@TypeOf(a) == @TypeOf(b));
}

fn foo(comptime T: type, init: T) Make(T) {
    return .{ .x = init };
}

fn Make(comptime T: type) type {
    return struct {
        x: T
    };
}
```

--------------------------------

TITLE: Zig Blocking Fetch and File Read
DESCRIPTION: This Zig code snippet demonstrates blocking operations for fetching URL content and reading from a file. It uses `async` and `await` but omits `suspend` points, causing the execution order to be determined by the `async` call sites. Error handling and resource cleanup are managed with `errdefer`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: zig
CODE:
```
    const std = @import("std");
    const Allocator = std.mem.Allocator;
    
    pub fn main() void {
        _ = async amainWrap();
    }
    
    fn amainWrap() void {
        amain() catch |e| {
            std.debug.warn("{}\n", e);
            if (@errorReturnTrace()) |trace| {
                std.debug.dumpStackTrace(trace.*);
            }
            std.process.exit(1);
        };
    }
    
    fn amain() !void {
        const allocator = std.heap.direct_allocator;
        var download_frame = async fetchUrl(allocator, "https://example.com/");
        var awaited_download_frame = false;
        errdefer if (!awaited_download_frame) {
            if (await download_frame) |r| allocator.free(r) else |_| {}
        };
    
        var file_frame = async readFile(allocator, "something.txt");
        var awaited_file_frame = false;
        errdefer if (!awaited_file_frame) {
            if (await file_frame) |r| allocator.free(r) else |_| {}
        };
    
        awaited_file_frame = true;
        const file_text = try await file_frame;
        defer allocator.free(file_text);
    
        awaited_download_frame = true;
        const download_text = try await download_frame;
        defer allocator.free(download_text);
    
        std.debug.warn("download_text: {}\n", download_text);
        std.debug.warn("file_text: {}\n", file_text);
    }
    
    fn fetchUrl(allocator: *Allocator, url: []const u8) ![]u8 {
        const result = try std.mem.dupe(allocator, u8, "this is the downloaded url contents");
        errdefer allocator.free(result);
        std.debug.warn("fetchUrl returning\n");
        return result;
    }
    
    fn readFile(allocator: *Allocator, filename: []const u8) ![]u8 {
        const result = try std.mem.dupe(allocator, u8, "this is the file contents");
        errdefer allocator.free(result);
        std.debug.warn("readFile returning\n");
        return result;
    }

    $ zig build-exe blocking.zig
    $ ./blocking
    fetchUrl returning
    readFile returning
    download_text: this is the downloaded url contents
    file_text: this is the file contents
```

--------------------------------

TITLE: Zig Bitwise XOR Operator
DESCRIPTION: Details the bitwise XOR operator (^) for integers in Zig, noting that it invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
a ^ b
a ^= b

-- Relevant Types:
* Integers

-- Description:
Bitwise XOR.
* Invokes Peer Type Resolution for the operands.

-- Example:
0b011 ^ 0b101 == 0b110
```

--------------------------------

TITLE: Zig cc: Handle /dev/null for Output
DESCRIPTION: Special handling for `/dev/null` as an output target is introduced. `-o /dev/null` is now equivalent to `-fno-emit-bin`, preventing unnecessary file system operations and improving efficiency.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
zig cc -o /dev/null
```

--------------------------------

TITLE: Zig and C cross-language optimization with LTO
DESCRIPTION: This example demonstrates how Zig's LTO can optimize code across Zig and C files. The `main.zig` file calls an external C function `foo1` and exports a Zig function `foo4` which prints to stdout. The `a.c` file defines `foo1` and `foo4`, along with other logic. When compiled with LTO enabled, the Zig `main` function can be optimized to not include calls to `foo1` or exports of `foo4` if they are not used, as shown in the objdump output.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

export fn foo4() void {
    _ = std.c.printf("Hi\n");
}

extern fn foo1() c_int;

pub fn main() u8 {
    return @intCast(u8, foo1());
}
```

LANGUAGE: c
CODE:
```
int foo1(void);
void foo2(void);
void foo4(void);

static signed int i = 0;

void foo2(void) {
  i = -1;
}

static int foo3() {
  foo4();
  return 10;
}

int foo1(void) {
  int data = 0;

  if (i < 0)
    data = foo3();

  data = data + 42;
  return data;
}
```

--------------------------------

TITLE: Testing Opaque Type Interaction
DESCRIPTION: Illustrates the shell output when testing Zig code involving opaque types, highlighting a type mismatch error that occurs due to incompatible opaque type pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: shell
CODE:
```
$ zig test test_opaque.zig
docgen_tmp/test_opaque.zig:6:9: error: expected type '\*test_opaque.Derp', found '\*test_opaque.Wat'
    bar(w);
        ^
docgen_tmp/test_opaque.zig:6:9: note: pointer type child 'test_opaque.Wat' cannot cast into pointer type child 'test_opaque.Derp'
docgen_tmp/test_opaque.zig:2:13: note: opaque declared here
const Wat = opaque {};
            ^~~~~~~~~~
docgen_tmp/test_opaque.zig:1:14: note: opaque declared here
const Derp = opaque {};
             ^~~~~~~~~~
docgen_tmp/test_opaque.zig:4:18: note: parameter type declared here
extern fn bar(d: *Derp) void;
                 ^~~~~
referenced by:
    test.call foo: docgen_tmp/test_opaque.zig:10:5
    remaining reference traces hidden; use '-freference-trace' to see all reference traces
```

--------------------------------

TITLE: Zig: Scope ID Resolution via IPv6
DESCRIPTION: Changes the method for resolving scope IDs to use IPv6 sockets, potentially improving network address handling and compatibility.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
Change to resolving scope IDs using IPv6 sockets.
```

--------------------------------

TITLE: Zig Fully Anonymous List Literal with Type Inference
DESCRIPTION: Explains how anonymous list literals can be used when the type is not specified in the result location, leading to a struct with numbered field names. The 'dump' function accesses these fields using quoted numbers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
test "fully anonymous list literal" {
    dump(.{ @as(u32, 1234), @as(f64, 12.34), true, "hi"});
}

fn dump(args: anytype) void {
    expect(args.@"0" == 1234);
    expect(args.@"1" == 12.34);
    expect(args.@"2");
    expect(args.@"3"[0] == 'h');
    expect(args.@"3"[1] == 'i');
}
```

--------------------------------

TITLE: Zig Packed Struct Atomics
DESCRIPTION: Shows how packed structs can be used directly in atomic operations in Zig, without requiring a `@bitCast` to their underlying integer type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: zig
CODE:
```
#code|packed_struct_atomics.zig#
```

--------------------------------

TITLE: Make ArenaAllocator.deinit Mutable Reference Optional
DESCRIPTION: The `std.heap.ArenaAllocator.deinit` function no longer requires a mutable reference. This change simplifies its usage and allows for more flexible memory management scenarios.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.heap.ArenaAllocator.deinit
```

--------------------------------

TITLE: Zig Blocking Fetch and Read Example
DESCRIPTION: This Zig code demonstrates a blocking approach to fetching URL content and reading a file. It uses `async` and `await` but without `suspend`, showing how the execution order is determined by the call sites. Error handling is included via `amainWrap`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
pub fn main() void {
        _ = async amainWrap();
    }
    
    fn amainWrap() void {
        amain() catch |e| {
            std.debug.warn("{}\n", .{e});
            if (@errorReturnTrace()) |trace| {
                std.debug.dumpStackTrace(trace.*);
            }
            std.process.exit(1);
        };
    }
    
    fn amain() !void {
        const allocator = std.heap.page_allocator;
        var download_frame = async fetchUrl(allocator, "https://example.com/");
        var awaited_download_frame = false;
        errdefer if (!awaited_download_frame) {
            if (await download_frame) |r| allocator.free(r) else |_| {}
        };
    
        var file_frame = async readFile(allocator, "something.txt");
        var awaited_file_frame = false;
        errdefer if (!awaited_file_frame) {
            if (await file_frame) |r| allocator.free(r) else |_| {}
        };
    
        awaited_file_frame = true;
        const file_text = try await file_frame;
        defer allocator.free(file_text);
    
        awaited_download_frame = true;
        const download_text = try await download_frame;
        defer allocator.free(download_text);
    
        std.debug.warn("download_text: {}\n", .{download_text});
        std.debug.warn("file_text: {}\n", .{file_text});
    }
    
    fn fetchUrl(allocator: *Allocator, url: []const u8) ![]u8 {
        const result = try std.mem.dupe(allocator, u8, "this is the downloaded url contents");
        errdefer allocator.free(result);
        std.debug.warn("fetchUrl returning\n", .{});
        return result;
    }
    
    fn readFile(allocator: *Allocator, filename: []const u8) ![]u8 {
        const result = try std.mem.dupe(allocator, u8, "this is the file contents");
        errdefer allocator.free(result);
        std.debug.warn("readFile returning\n", .{});
        return result;
    }
```

--------------------------------

TITLE: Zig: Move Mach-O Load Command Utils to Standard Library
DESCRIPTION: Load command wrappers and parsing utilities for Mach-O files have been moved to the Zig Standard Library. This promotes code reuse and modularity within the Zig ecosystem.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
macho: moved load command wrappers and parsing utils to the {#link|Standard Library#} ([#10310](https://github.com/ziglang/zig/issues/10310)).
```

--------------------------------

TITLE: Zig Packed Struct Equality
DESCRIPTION: Demonstrates direct equality comparison for packed structs in Zig, eliminating the need for `@bitCast` to the underlying integer type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: zig
CODE:
```
#code|packed_struct_equality.zig#
```

--------------------------------

TITLE: Zig Atomic Queue and ArrayList Operations
DESCRIPTION: Improvements to Zig's atomic Queue, including a fix for the `unget` implementation and added documentation. New functions `getLast` and `getLastOrNull` are added to ArrayListAligned/ArrayListAlignedUnmanaged, and `insertAssumeCapacity` is added to ArrayList.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.atomic.Queue: fix unget implementation and add docs
Add fromOwnedSliceSentinel to ArrayList ArrayList and ArrayListUnmanaged, add fromOwnedSlice to ArrayListUnmanaged
Add the two functions 'getLast' and 'getLastOrNull' to ArrayListAligned/ArrayListAlignedUnmanaged.
std: Expose Int parameter in std.PackedInt[Array,Slice]
std: Add ArrayList.insertAssumeCapacity()
ArrayList: Allow const for getLast ([#14522](https://github.com/ziglang/zig/issues/14522))
```

--------------------------------

TITLE: Zig Fully Anonymous List Literal with Type Inference
DESCRIPTION: Explains how anonymous list literals can be used when the type is not specified in the result location, leading to a struct with numbered field names. The 'dump' function accesses these fields using quoted numbers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
test "fully anonymous list literal" {
    dump(.{ @as(u32, 1234), @as(f64, 12.34), true, "hi"});
}

fn dump(args: anytype) void {
    expect(args.@"0" == 1234);
    expect(args.@"1" == 12.34);
    expect(args.@"2");
    expect(args.@"3"[0] == 'h');
    expect(args.@"3"[1] == 'i');
}
```

--------------------------------

TITLE: Testing Opaque Type Interaction
DESCRIPTION: Illustrates the shell output when testing Zig code involving opaque types, highlighting a type mismatch error that occurs due to incompatible opaque type pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: shell
CODE:
```
$ zig test test_opaque.zig
docgen_tmp/test_opaque.zig:6:9: error: expected type '\*test_opaque.Derp', found '\*test_opaque.Wat'
    bar(w);
        ^
docgen_tmp/test_opaque.zig:6:9: note: pointer type child 'test_opaque.Wat' cannot cast into pointer type child 'test_opaque.Derp'
docgen_tmp/test_opaque.zig:2:13: note: opaque declared here
const Wat = opaque {};
            ^~~~~~~~~~
docgen_tmp/test_opaque.zig:1:14: note: opaque declared here
const Derp = opaque {};
             ^~~~~~~~~~
docgen_tmp/test_opaque.zig:4:18: note: parameter type declared here
extern fn bar(d: *Derp) void;
                 ^~~~~
referenced by:
    test.call foo: docgen_tmp/test_opaque.zig:10:5
    remaining reference traces hidden; use '-freference-trace' to see all reference traces
```

--------------------------------

TITLE: Multiline Library Name Parsing and PATH Variable Handling
DESCRIPTION: Fixes for crashes when parsing multiline library names and for std.child_process.ChildProcess.spawnWindows when searching the PATH environment variable.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const child = try std.child_process.ChildProcess.spawnWindows({
    .allocator = allocator,
    .argv = &[_][]const u8{"my_app"},
    .env_map = std.Process.envMap(allocator, .{.inherit = true}),
});
```

--------------------------------

TITLE: Zig Tuple Operations and Testing
DESCRIPTION: Demonstrates the creation and manipulation of anonymous structs (tuples) in Zig. It shows how to access elements, use concatenation and repetition operators, and iterate over tuples. The example includes a test case to verify tuple functionality.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuple" {
    const values = .{
        @as(u32, 1234),
        @as(f64, 12.34),
        true,
        "hi",
    } ++ .{"false"} ** 2;
    try expect(values[0] == 1234);
    try expect(values[4] == false);
    inline for (values, 0..) |v, i| {
        if (i != 2) continue;
        try expect(v);
    }
    try expect(values.len == 6);
    try expect(values.@"3"[0] == 'h');
}
```

--------------------------------

TITLE: Zig Spawn Threads and Atomic Operations
DESCRIPTION: Illustrates Zig's concurrency features by spawning multiple threads that increment a shared counter using atomic operations. This example showcases `std.os.spawnThread` and atomic memory access primitives.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;
const builtin = @import("builtin");
const AtomicRmwOp = builtin.AtomicRmwOp;
const AtomicOrder = builtin.AtomicOrder;

test "spawn threads" {
    var shared_ctx: i32 = 1;

    const thread1 = try std.os.spawnThread({}, start1);
    const thread2 = try std.os.spawnThread(&shared_ctx, start2);
    const thread3 = try std.os.spawnThread(&shared_ctx, start2);
    const thread4 = try std.os.spawnThread(&shared_ctx, start2);

    thread1.wait();
    thread2.wait();
    thread3.wait();
    thread4.wait();

    assert(shared_ctx == 4);
}

fn start1(ctx: void) u8 {
    return 0;
}

fn start2(ctx: *i32) u8 {
    _ = @atomicRmw(i32, ctx, AtomicRmwOp.Add, 1, AtomicOrder.SeqCst);
    return 0;
}
```

--------------------------------

TITLE: Zig: Direct Struct Parameter Passing
DESCRIPTION: Shows how Zig allows structs to be passed directly to functions, rather than requiring pointers. This example defines a struct 'Foo', a function 'callee' that accepts 'Foo' by value, and a test case to demonstrate the functionality.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const assert = @import("std").debug.assert;

const Foo = struct {
    x: i32,
    y: i32,
};

fn callee(foo: Foo) void {
    assert(foo.y == 2);
}

test "pass directly" {
    callee(Foo{ .x = 1, .y = 2 });
}
```

--------------------------------

TITLE: Zig Build System: Add ELF Parse/Dump to CheckObject
DESCRIPTION: Integrates ELF parsing and dumping functionality into std.Build.Step.CheckObject, enhancing its capabilities for analyzing object files.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
std: add ELF parse'n'dump functionality to std.Build.Step.CheckObject
```

--------------------------------

TITLE: C Compatibility and Prototypes
DESCRIPTION: This snippet focuses on C compatibility within the Zig standard library, specifically addressing fixes for C prototypes and the harmonization of `off_t` usage between libc and Zig implementations. It also includes preferring 64-bit libc functions where available.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: c
CODE:
```
c: Fix prototypes for bcmp and memcmp
Prefer 64bit libc functions where possible
Harmonize use of off_t between libc and Zig impls
```

--------------------------------

TITLE: Zig Threading API Updates
DESCRIPTION: This update revises Zig's threading API to be more consistent with built-in call syntax. It introduces customizable thread stack sizes, renames `wait()` to `join()` and `handle()` to `getHandle()`, and adds functionality to `detach()` threads. It also standardizes thread ID retrieval and adds methods for setting and getting thread names.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
#Thread spawning API is now similar to the @call(.{}, function, args_tuple) builtin.
#Spawn options (first arg) with customizable thread stack size.
#Renamed `wait()` to `join()`.
#Renamed `handle()` to `getHandle()` for consistency.
#Added ability to `detach()` thread, allowing it to clean up resources when it exits instead of calling `join()`.
#Moved `getCurrentThreadId()` into `getCurrentId()`.
#Added `Thread.setName` and `Thread.getName` including for DragonFlyBSD
```

--------------------------------

TITLE: Allow ArrayList(u0) Creation
DESCRIPTION: Enables the creation of `ArrayList(u0)` (an ArrayList of void), providing flexibility for empty element types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
ArrayList: Allow `ArrayList(u0)` to be created.
```

--------------------------------

TITLE: Fix Log2Int Type Construction (Zig)
DESCRIPTION: Corrects an issue in the construction of types for the `Log2Int` function, ensuring proper handling of integer types and their bitwise properties.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
// No specific code snippet provided, but the fix relates to Log2Int type construction.
```

--------------------------------

TITLE: Print "Hello, world!" to stdout in Zig
DESCRIPTION: This example demonstrates how to print "Hello, world!" to standard output using the `std.io.getStdOut().writer()` function in Zig. It requires importing the `std` module and handling potential errors during the printing process.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    const stdout = std.io.getStdOut().writer();
    try stdout.print("Hello, {s}!\n", .{ "world" });
}
```

LANGUAGE: Shell
CODE:
```
$ zig build-exe hello.zig
$ ./hello
Hello, world!
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: This Zig code snippet demonstrates how to read and print command-line arguments when compiled for the wasm32-wasi target. It uses `std.process.argsAlloc` to get the arguments and `std.debug.print` for output. The example requires a WASI-compatible runtime like wasmtime to execute.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator: std.heap.GeneralPurposeAllocator(.{}) = .init;
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args, 0..) |arg, i| {
        std.debug.print("{}: {s}
", .{ i, arg });
    }
}
```

LANGUAGE: shell
CODE:
```
$ zig build-exe wasi_args.zig -target wasm32-wasi
```

LANGUAGE: shell
CODE:
```
$ wasmtime wasi_args.wasm 123 hello
0: wasi_args.wasm
1: 123
2: hello
```

--------------------------------

TITLE: Add 0-length buffer checks to os.read and os.write
DESCRIPTION: Implements checks for zero-length buffers in os.read and os.write to prevent potential issues.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Add 0-length buffer checks to os.read and os.write
```

--------------------------------

TITLE: Zig: Function Parameter Type Inference
DESCRIPTION: Illustrates using `anytype` for function parameters in Zig, allowing types to be inferred at compile time. It shows how to use `@TypeOf` to determine the inferred type and includes tests for integer and i64 inputs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    var y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

--------------------------------

TITLE: Add atomic.Bool and atomic operations
DESCRIPTION: Adds atomic.Bool and exposes all atomic operations from atomic.Int, providing more comprehensive atomic types and operations for concurrent programming in Zig.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Zig Atomics: Store
DESCRIPTION: Demonstrates how to perform an atomic store operation in Zig using the `@atomicStore` builtin function, ensuring thread-safe writing to memory.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Atomic(u32) = .{ .value = 0 };
    @atomicStore(u32, &atomic_var, 20, .SeqCst);
    const value = @atomicLoad(u32, &atomic_var, .SeqCst);
    std.debug.print("New atomic value: {d}", .{value});
}
```

--------------------------------

TITLE: Zig: Inline Switch for Type Info
DESCRIPTION: Demonstrates using inline switch prongs with `@typeInfo` to check if a struct field is optional. The `isFieldOptional` function is unrolled by the compiler for each field.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T).Struct.fields;
    return switch (field_index) {
        // This prong is analyzed `fields.len - 1` times with `idx` being an
        // unique comptime known value each time.
        inline 0...fields.len - 1 => |idx| @typeInfo(fields[idx].field_type) == .Optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function: 
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: Zig Atomics: Store
DESCRIPTION: Demonstrates how to perform an atomic store operation in Zig using the `@atomicStore` builtin function, ensuring thread-safe writing to memory.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Atomic(u32) = .{ .value = 0 };
    @atomicStore(u32, &atomic_var, 20, .SeqCst);
    const value = @atomicLoad(u32, &atomic_var, .SeqCst);
    std.debug.print("New atomic value: {d}", .{value});
}
```

--------------------------------

TITLE: Zig: Inline Switch for Type Information
DESCRIPTION: Demonstrates using an inline switch prong to check if a struct field is optional by analyzing type information. It's used with `@typeInfo` to determine field properties at compile time.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T).Struct.fields;
    return switch (field_index) {
        // This prong is analyzed `fields.len - 1` times with `idx` being an
        // unique comptime known value each time.
        inline 0...fields.len - 1 => |idx| @typeInfo(fields[idx].field_type) == .Optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function: 
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: Zig C Interop: External Function Declaration
DESCRIPTION: Provides an example of declaring an external C function, `puts`, within Zig code. This is a fundamental step for interacting with C libraries.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: zig
CODE:
```
extern fn puts([*]const u8) void;
```

--------------------------------

TITLE: Zig: Peer Type Resolution for Empty Array and Slice
DESCRIPTION: Examines peer type resolution involving an empty array literal `[_]u8{}` and a slice. It checks the `.len` property of the resulting slice under different conditions, including compile-time.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
test "peer type resolution: [0]u8 and []const u8" {
    assert(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
    assert(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    comptime {
        assert(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
        assert(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    }
}
fn peerTypeEmptyArrayAndSlice(a: bool, slice: []const u8) []const u8 {
    if (a) {
        return [_]u8{};
    }

    return slice[0..1];
}
```

--------------------------------

TITLE: Zig: Peer Resolve Array and Const Slice Comparison
DESCRIPTION: Tests the peer type resolution for arrays and slices, specifically when comparing them using mem.eql. It includes both runtime and compile-time assertions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
test "peer resolve arrays of different size to const slice" {
    assert(mem.eql(u8, boolToStr(true), "true"));
    assert(mem.eql(u8, boolToStr(false), "false"));
    comptime assert(mem.eql(u8, boolToStr(true), "true"));
    comptime assert(mem.eql(u8, boolToStr(false), "false"));
}
```

--------------------------------

TITLE: Zig Function Reflection with @typeInfo
DESCRIPTION: Demonstrates using `@typeInfo` for function reflection. This example checks the argument types and whether a function is variadic by inspecting the type information of the `expect` function.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "fn reflection" {
    try expect(@typeInfo(@TypeOf(expect)).Fn.args[0].arg_type.? == bool);
    try expect(@typeInfo(@TypeOf(expect)).Fn.is_var_args == false);
}
```

--------------------------------

TITLE: Zig Function Type Inference
DESCRIPTION: Illustrates using `anytype` for function parameters in Zig, allowing types to be inferred at compile time. Shows how to use `@TypeOf` and `@typeInfo` to inspect the inferred types. Includes tests for integer and i64 inputs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    var y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

--------------------------------

TITLE: Zig Noreturn Type Compatibility
DESCRIPTION: Explains the `noreturn` type in Zig and its compatibility with other types in control flow structures like `if` and `switch`. It also shows an example of using `noreturn` with an external function like `ExitProcess`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.0/index.html

LANGUAGE: zig
CODE:
```
fn foo(condition: bool, b: u32) {
    const a = if (condition) b else return;
    bar(a);
}

extern fn bar(value: u32);
```

LANGUAGE: zig
CODE:
```
pub extern "kernel32" stdcallcc fn ExitProcess(exit_code: c_uint) -> noreturn;

fn foo() {
    const value = bar() %% ExitProcess(1);
    assert(value == 1234);
}

fn bar() -> %u32 {
    return 1234;
}

const assert = @import("std").debug.assert;
```

--------------------------------

TITLE: Zig: Using Slices for Strings
DESCRIPTION: Demonstrates how Zig handles string literals as slices of constant unsigned 8-bit integers and performs string concatenation. It shows coercing pointer types to slices and using fmt.bufPrint for buffer printing.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const mem = std.mem;
const fmt = std.fmt;

test "using slices for strings" {
    // Zig has no concept of strings. String literals are const pointers
    // to null-terminated arrays of u8, and by convention parameters
    // that are "strings" are expected to be UTF-8 encoded slices of u8.
    // Here we coerce *const [5:0]u8 and *const [6:0]u8 to []const u8
    const hello: []const u8 = "hello";
    const world: []const u8 = "ä¸–ç•Œ";

    var all_together: [100]u8 = undefined;
    // You can use slice syntax with at least one runtime-known index on an
    // array to convert an array into a slice.
    var start: usize = 0;
    _ = &start;
    const all_together_slice = all_together[start..];
    // String concatenation example.
    const hello_world = try fmt.bufPrint(all_together_slice, "{s} {s}", .{ hello, world });

    // Generally, you can use UTF-8 and not worry about whether something is a
    // string. If you don't need to deal with individual characters, no need
    // to decode.
    try expect(mem.eql(u8, hello_world, "hello ä¸–ç•Œ"));
}
```

--------------------------------

TITLE: Zig Type Reflection
DESCRIPTION: Details the @typeInfo function in Zig, used for type reflection. It provides information about a given type T. For structs, unions, enums, and error sets, field order is guaranteed.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
    @typeInfo(comptime T: type) @import("std").builtin.TypeInfo
```

--------------------------------

TITLE: Zig: Decl Literals for Struct Field Initialization
DESCRIPTION: Shows the practical application of 'decl literals' in initializing struct fields, avoiding redundant type specifications by directly referencing default values.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const S = struct {
    const default = S{};
    value: i32 = 0,
};

fn main() void {
    const instance = S{
        .value = .S.default.value + 1,
    };
    _ = instance;
}
```

--------------------------------

TITLE: Linking: LLD invocation as child process
DESCRIPTION: Addresses issues with the LLD library by invoking it as a child process instead of using its library code directly. This circumvents problems with LLD's `exit()` calls and global state, improving Zig's integration with LLD.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: Zig
CODE:
```
// Conceptual Zig code demonstrating process invocation for LLD
const std = @import("std");

const lld_path = "zig_lld_wrapper"; // Path to a wrapper script or executable
const args = &[_][]const u8{
    "lld",
    "--flavor", "gnu",
    // ... other LLD arguments ...
};

try std.ChildProcess.exec(.{ .argv = args });
```

--------------------------------

TITLE: Zig Tuple Literal and Operations
DESCRIPTION: Shows how to create and manipulate tuples in Zig, which are anonymous structs with numeric field names. Demonstrates concatenation, repetition, indexing, and iteration.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuple" {
    const values = .{ 
        @as(u32, 1234),
        @as(f64, 12.34),
        true,
        "hi",
    } ++ .{false} ** 2;
    try expect(values[0] == 1234);
    try expect(values[4] == false);
    inline for (values) |v, i| {
        if (i != 2) continue;
        try expect(v);
    }
    try expect(values.len == 6);
    try expect(values.@"3"[0] == 'h');
}
```

--------------------------------

TITLE: Zig Tuple Literal and Operations
DESCRIPTION: Shows how to create and manipulate tuples in Zig, which are anonymous structs with numeric field names. Demonstrates concatenation, repetition, indexing, and iteration.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuple" {
    const values = .{ 
        @as(u32, 1234),
        @as(f64, 12.34),
        true,
        "hi",
    } ++ .{false} ** 2;
    try expect(values[0] == 1234);
    try expect(values[4] == false);
    inline for (values) |v, i| {
        if (i != 2) continue;
        try expect(v);
    }
    try expect(values.len == 6);
    try expect(values.@"3"[0] == 'h');
}
```

--------------------------------

TITLE: Zig Function Type Inference
DESCRIPTION: Illustrates using `anytype` for function parameters in Zig, allowing types to be inferred at compile time. Shows how to use `@TypeOf` and `@typeInfo` to inspect the inferred types. Includes tests for integer and i64 inputs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    var y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

--------------------------------

TITLE: Fix setsockopt Syscall on Linux in Zig
DESCRIPTION: Resolves an issue with the `setsockopt` syscall on Linux. This ensures that socket options can be set correctly, improving network programming.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: Zig
CODE:
```
/*
* Duncan fixed the setsockopt syscall on linux.
*/
```

--------------------------------

TITLE: Zig Libc Detection on NixOS
DESCRIPTION: Demonstrates the output of the `zig libc` command on a NixOS system, showing include and library directories for glibc.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: bash
CODE:
```
$ zig libc
# The directory that contains `stdlib.h`.
# On POSIX-like systems, include directories be found with: `cc -E -Wp,-v -xc /dev/null`
include_dir=/nix/store/q2q1sg5sljia8sihhwcpbxir70yw33bw-glibc-2.27-dev/include
# The system-specific include directory. May be the same as `include_dir`.
# On Windows it's the directory that includes `vcruntime.h`.
# On POSIX it's the directory that includes `sys/errno.h`.
sys_include_dir=/nix/store/q2q1sg5sljia8sihhwcpbxir70yw33bw-glibc-2.27-dev/include

# The directory that contains `crt1.o`.
# On POSIX, can be found with `cc -print-file-name=crt1.o`.
# Not needed when targeting MacOS.
crt_dir=/nix/store/fivq0nbggp4y8mhy3ixprqd7qyn1hy2j-glibc-2.27/lib

# The directory that contains `vcruntime.lib`.
# Only needed when targeting MSVC on Windows.
msvc_lib_dir=

# The directory that contains `kernel32.lib`.
# Only needed when targeting MSVC on Windows.
kernel32_lib_dir=
```

--------------------------------

TITLE: Zig Built-in Function: @atomicLoad
DESCRIPTION: The `@atomicLoad` builtin function performs an atomic load operation on a memory location. It ensures that the read operation is indivisible and visible across threads.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Int(u32) = undefined;
    std.atomic.store(&atomic_var, 123, .SeqCst);

    const value = @atomicLoad(&atomic_var, .SeqCst);

    std.debug.print("Atomic value: {d}\\n", .{
        value,
    });
}
```

--------------------------------

TITLE: Zig Target Support - Tier 1
DESCRIPTION: Targets with Tier 1 support are fully integrated and automatically tested. The standard library is expected to work without issues, though some specific APIs might still return an 'Unsupported OS' error. Linking with external libraries like libc might be necessary to fill any gaps.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
x86_64
âœ…
âœ…
âœ…
âœ…
x86
âœ…
ðŸ’€
âœ…
ðŸ”
ðŸ”
âœ…
aarch64
âœ…
âœ…
ðŸ”
ðŸ”
ðŸ”
âœ…
arm
âœ…
ðŸ’€
ðŸ”
ðŸ”
ðŸ”
âœ…
mips64
âœ…
âœ…
N/A
N/A
ðŸ”
ðŸ”
N/A
N/A
mips
âœ…
N/A
N/A
ðŸ”
ðŸ”
N/A
N/A
riscv64
âœ…
N/A
ðŸ”
ðŸ”
N/A
ðŸ”
sparcv9
âœ…
N/A
ðŸ”
ðŸ”
N/A
N/A
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Performs a bitwise AND operation. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
a & b
a &=
b
```

--------------------------------

TITLE: Zig Builtin: @atomicLoad
DESCRIPTION: The `@atomicLoad` builtin function performs an atomic load operation on a memory location, ensuring that the read is indivisible and consistent across threads.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
var counter: atomic(u32) = 0;
const value = @atomicLoad(&counter, .SeqCst);
// value holds the current atomic value of counter
```

--------------------------------

TITLE: Self-Hosted Linker and MachO Improvements in Zig
DESCRIPTION: Progress has been made on the self-hosted linker for ELF and COFF formats, though they are not yet generally usable. The MachO linker has been significantly rewritten for incremental linking, improving efficiency by optimizing section layout and file space allocation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
// Self-hosted linker progress on ELF and COFF.
// Incremental MachO linker rewrite: Zig lays out incrementally linked binary with one section per segment.
// Allows Zig to use quick file space allocation algorithm.
// Disassociates file offsets from allocated virtual memory addresses.
// Enables reordering and moving sections without affecting virtual memory space ordering.
// Handles section growth efficiently without extensive virtual address recalculations.
```

--------------------------------

TITLE: Zig std.os.posixOpen Error Handling
DESCRIPTION: Added error.DeviceBusy as a possible result of std.os.posixOpen. This error can occur when attempting to open a TTY file descriptor for writing that is already in use.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.os.posixOpen
```

--------------------------------

TITLE: Fix Zig Test Case for std.io.InStream
DESCRIPTION: Corrects an erroneous test case related to `std.io.InStream`. This ensures the accuracy of the test suite and the reliability of the `InStream` functionality.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
/* LemonBoy fixed erroneous test case regarding {#syntax#}std.io.InStream{#endsyntax#}. */
```

--------------------------------

TITLE: Write to Standard Output in Zig
DESCRIPTION: Demonstrates how to write a string to standard output using Zig's file I/O. It includes error handling for potential pipe failures during printing. The example shows compilation and execution commands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    // If this program encounters pipe failure when printing to stdout, exit
    // with an error.
    try std.io.getStdOut().writer().writeAll("Hello, world!\n");
}
```

--------------------------------

TITLE: Zig Progress Protocol: Child Process Communication
DESCRIPTION: This snippet demonstrates how the Zig Progress Protocol facilitates communication between parent and child processes. It highlights the use of environment variables and the `std.process.Child` API for progress reporting.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.13.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// ... inside a function or method ...

const child = try std.process.Child.spawn(.{
    .argv = &[_][]const u8{"child_program"},
    // ... other options ...
});

// Attach the child's progress node to the parent's progress tree
// This assumes 'child.progress_node' is a field that can be set.
// The exact mechanism depends on the context where this is used.
// child.progress_node = parent_progress_node;

try child.wait();

```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Details the bitwise AND operator '&' for integers in Zig. It explains that the operation invokes Peer Type Resolution for the operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
a & b
a &= b
```

--------------------------------

TITLE: Zig Build System: Foreign Target Execution Options
DESCRIPTION: The Zig build system provides several command-line switches to enable cross-target testing and execution. These include integrations with Darling, QEMU, Wine, Rosetta, and Wasmtime, allowing macOS programs on Linux, foreign-architecture programs on Linux, Windows programs on Linux, x86_64 on ARM64 macOS, and WASI binaries respectively.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
// Enable Darling for macOS programs on Linux hosts
// build.installDependency(..., .{-fØ¯Ø§Ø±ling});

// Enable QEMU for foreign-architecture programs on Linux hosts
// build.installDependency(..., .{-fqemu});

// Provide glibc runtimes for QEMU integration
// build.installDependency(..., .{"--glibc-runtimes", "/path/to/glibc"});

// Rely on Rosetta for x86_64 programs on ARM64 macOS hosts
// build.installDependency(..., .{-frosetta});

// Enable Wasmtime for WASI binaries
// build.installDependency(..., .{-fwasmtime});

// Enable Wine for Windows programs on Linux hosts
// build.installDependency(..., .{-fwine});

// Prevent cross-target failure from failing the build
// build.skip_foreign_checks = true;

```

--------------------------------

TITLE: Zig @alignOf Example
DESCRIPTION: Explains how to use @alignOf to get the required byte alignment for a type according to the C ABI for the current target. This is useful for ensuring correct memory layout.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

comptime {
    assert(@alignOf(u32) <= @sizeOf(u32));
    assert(&u32 == &align(@alignOf(u32)) u32);
}
```

--------------------------------

TITLE: Zig Builtin: @atomicStore
DESCRIPTION: Demonstrates the `@atomicStore` builtin function in Zig for atomically storing a value to memory. This ensures that writes are visible to other threads correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Atomic(u32) = std.atomic.Atomic(u32){ .value = 0 };
    @atomicStore(u32, &atomic_var, 20, .SeqCst);
    const value = std.atomic.load(u32, &atomic_var, .SeqCst);
    std.debug.print("Stored value: {d}\n", .{value});
}
```

--------------------------------

TITLE: Zig @exp Base-e Exponential Function
DESCRIPTION: Computes the base-e exponential (e^x) for floating-point numbers. Supports scalar and vector types, utilizing hardware instructions when available. Some float operations might not be fully implemented across all types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: Zig
CODE:
```
    @exp(value: var) @TypeOf(value)
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Covers the bitwise AND operator (&) for integers in Zig, noting that it invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
a & b
a &= b

-- Relevant Types:
* Integers

-- Description:
Bitwise AND.
* Invokes Peer Type Resolution for the operands.

-- Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Generic Call Demo with Type Equality
DESCRIPTION: Demonstrates a generic function call that previously passed with Zig 0.10.x but fails in 0.11.0 due to changes in type equality behavior. It highlights the potential unreliability of relying on type equality for generic instances.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "generic call demo" {
    const a = foo(i32, 1234);
    const b = foo(i32, 5678);
    try expect(@TypeOf(a) == @TypeOf(b));
}

fn foo(comptime T: type, init: T) struct {
    x: T
} {
    return .{ .x = init };
}
```

--------------------------------

TITLE: Zig: Base-e Exponential with @exp
DESCRIPTION: Explains the `@exp` function for calculating the base-e exponential of floating-point numbers and vectors. It notes hardware acceleration and potential limitations for specific float types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html



--------------------------------

TITLE: Zig Struct with Methods and Delegation
DESCRIPTION: Demonstrates a Zig struct 'Foo' with member variables, methods, and nested functions. It showcases method delegation and the use of comptime for creating functions with parameters.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

const Foo = struct {
    a: u64 = 10,

    fn one(self: Foo) u64 {
        return self.a + 1;
    }

    const two = __two;
    fn __two(self: Foo) u64 {
        return self.a + 2;
    }

    const three = __three;
    fn __three(self: Foo) u64 {
        return self.a + 3;
    }

    const four = custom(Foo, 4);
};

fn custom(comptime T: type, comptime num: u64) fn (T) u64 {
    return struct {
        fn function(self: T) u64 {
            return self.a + num;
        }
    }.function;
}

test "fn delegation" {
    const foo = Foo{};
    expect(foo.one() == 11);
    expect(foo.two() == 12);
    expect(foo.three() == 13);
    expect(foo.four() == 14);
}
```

--------------------------------

TITLE: Zig Builtin: @atomicStore
DESCRIPTION: Demonstrates the `@atomicStore` builtin function in Zig for atomically storing a value to memory. This ensures that writes are visible to other threads correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Atomic(u32) = std.atomic.Atomic(u32){ .value = 0 };
    @atomicStore(u32, &atomic_var, 20, .SeqCst);
    const value = std.atomic.load(u32, &atomic_var, .SeqCst);
    std.debug.print("Stored value: {d}\n", .{value});
}
```

--------------------------------

TITLE: Windows OVERLAPPED and OVERLAPPED_ENTRY Fixes
DESCRIPTION: Updates the Windows target by fixing the `OVERLAPPED` structure and adding the `OVERLAPPED_ENTRY` structure, improving asynchronous I/O handling.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Windows: fix `OVERLAPPED`, add `OVERLAPPED_ENTRY`.
```

--------------------------------

TITLE: Custom OS Support Improvements
DESCRIPTION: Details enhancements for using Zig to build custom operating systems. This includes making Linux syscalls accessible when targeting non-Linux, overriding `MAX_PATH_BYTES`, and refactoring standard library components like iovec and log levels.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
MAX_PATH_BYTES
root.os.panic
```

--------------------------------

TITLE: Zig Inline Switch with @typeInfo
DESCRIPTION: Demonstrates using Zig's inline switch with @typeInfo to check if struct fields are optional. The prong is analyzed multiple times with unique comptime-known values.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectError = std.testing.expectError;

fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T).Struct.fields;
    return switch (field_index) {
        // This prong is analyzed `fields.len - 1` times with `idx` being a
        // unique comptime-known value each time.
        inline 0...fields.len - 1 => |idx| @typeInfo(fields[idx].type) == .Optional,
        else => return error.IndexOutOfBounds,
    };
}

const Struct1 = struct { a: u32, b: ?u32 };

test "using @typeInfo with runtime values" {
    var index: usize = 0;
    try expect(!try isFieldOptional(Struct1, index));
    index += 1;
    try expect(try isFieldOptional(Struct1, index));
    index += 1;
    try expectError(error.IndexOutOfBounds, isFieldOptional(Struct1, index));
}

// Calls to `isFieldOptional` on `Struct1` get unrolled to an equivalent
// of this function:
fn isFieldOptionalUnrolled(field_index: usize) !bool {
    return switch (field_index) {
        0 => false,
        1 => true,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: Zig: Integer to Integer Cast Example
DESCRIPTION: Demonstrates how to use @intCast to convert an i32 to a usize. This example highlights how Zig enforces type safety, requiring explicit conversion when the type of the variable changes, such as from an integer to a pointer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
fn foo(x: i32) void {
    var i = @intCast(usize, x);
}
```

--------------------------------

TITLE: Zig: Link System LibSystem for Mach-O Native Builds
DESCRIPTION: This update addresses Mach-O file linking by ensuring the system libSystem is linked when building natively. This is important for correct execution of native applications on macOS and other Darwin-based systems.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
#[9242](https://github.com/ziglang/zig/issues/9242)
```

--------------------------------

TITLE: Zig: Base-e Exponential with @exp
DESCRIPTION: Explains the `@exp` function for calculating the base-e exponential of floating-point numbers and vectors. It notes hardware acceleration and potential limitations for specific float types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html



--------------------------------

TITLE: Zig: Resolve Peer Types with Pointers and Slices
DESCRIPTION: Demonstrates Zig's peer type resolution for pointers and slices, including handling errors and empty arrays. This snippet showcases how Zig manages type compatibility in complex scenarios involving pointers and slices.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
fn peerTypeEmptyArrayAndSliceAndError(a: bool, slice: []u8) anyerror![]u8 {
        if (a) {
            return &[_]u8{};
        }
    
        return slice[0..1];
    }
    
    test "peer type resolution: *const T and ?*T" {
        const a = @intToPtr(*const usize, 0x123456780);
        const b = @intToPtr(?*usize, 0x123456780);
        expect(a == b);
        expect(b == a);
    }
```

--------------------------------

TITLE: Zig Translate-C: Handle NAN and INFINITY Macros
DESCRIPTION: Support for `NAN` and `INFINITY` macros has been added to `zig translate-c`, ensuring these common floating-point constants are translated correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: C
CODE:
```
* Handle NAN and INFINITY macros ([#9468](https://github.com/ziglang/zig/issues/9468)).
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Performs a bitwise AND operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
a & b
a &= b
```

--------------------------------

TITLE: Fix C ABI Parameter Handling for Structs in Zig
DESCRIPTION: Corrects the handling of C ABI parameters that are split across multiple registers when flattening structures. This ensures accurate parameter passing for complex types according to the C ABI.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Fix handling of C ABI parameters split in multiple regs by taking into account the increased number of parameters when flattening a structure into one or more SSE registers ([#9061](https://github.com/ziglang/zig/issues/9061)).
```

--------------------------------

TITLE: Migrate RemoveDir Step to Accept LazyPath
DESCRIPTION: The `RemoveDir` step in Zig's build system now expects a `LazyPath` instead of a `[]const u8`. This change requires updating how temporary paths are passed to the `addRemoveDirTree` function.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: zig
CODE:
```
-        const cleanup = b.addRemoveDirTree(tmp_path);
+        const cleanup = b.addRemoveDirTree(.{ .cwd_relative = tmp_path });
```

--------------------------------

TITLE: Zig: Support read-write output constraints in assembly (LLVM backend)
DESCRIPTION: This feature adds support for read-write output constraints in inline assembly within the Zig LLVM backend. This allows for more complex and efficient inline assembly usage by enabling output operands that can be both read and written.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Support read-write output constraints in assembly
```

--------------------------------

TITLE: Zig Noreturn Type Compatibility
DESCRIPTION: Explains that the `noreturn` type in Zig is compatible with all other types when resolving types in conditional constructs like `if` or `switch`. The example shows a function `foo` where the `else` branch returns, making it `noreturn`. This allows the `if` statement to be assigned to a variable `a` without type conflicts.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: Zig
CODE:
```
fn foo(condition: bool, b: u32) void {
    const a = if (condition) b else return;
    @panic("do something with a");
}
test "noreturn" {
    foo(false, 1);
}
```

LANGUAGE: Shell
CODE:
```
$ zig test test.zig
Test [1/1] test "noreturn"... 

All 1 tests passed.
```

--------------------------------

TITLE: Zig Build: Restore Default Stdin Behavior for RunStep
DESCRIPTION: Explains how to revert the `RunStep.stdin_behavior` to its previous default of `.Ignore`. The current default is `.Inherit`, and this code snippet shows how to explicitly set it back.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.build.RunStep.stdin_behavior = .Ignore;
```

--------------------------------

TITLE: Zig Generic List Data Structure Implementation
DESCRIPTION: Demonstrates how Zig's `comptime` feature enables the creation of generic data structures without special syntax. This example defines a `List` type factory that returns a struct with `items` and `len` fields, which can be instantiated with any type `T`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
    fn List(comptime T: type) type {
        return struct {
            items: []T,
            len: usize,
        };
    }
    
    // The generic List data structure can be instantiated by passing in a type:
    var buffer: [10]i32 = undefined;
    var list = List(i32){
        .items = &buffer,
        .len = 0,
    };
```

--------------------------------

TITLE: Zig @TypeOf with multiple parameters
DESCRIPTION: Illustrates the use of `@TypeOf` with multiple parameters, enabling Peer Type Resolution. This allows the `max` function to correctly infer the return type based on the types of its two arguments, `x` and `y`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
// std.math.max
pub fn max(x: var, y: var) @TypeOf(x, y) {
    return if (x > y) x else y;
}
```

--------------------------------

TITLE: Importing Built-in Zig Features
DESCRIPTION: Shows an example of importing and using various built-in features from the 'std.builtin' module, including endianness, output mode, link mode, and architecture details.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
usingnamespace @import("std").builtin;

pub const endian = Endian.Little;
pub const output_mode = OutputMode.Obj;
pub const link_mode = LinkMode.Static;
pub const is_test = false;
pub const single_threaded = false;
/// Deprecated: use `std.Target.cpu.arch`
pub const arch = Arch.x86_64;
pub const abi = Abi.musl;
pub const cpu: Cpu = Cpu{
    .arch = .x86_64,
    .model = &Target.x86.cpu.haswell,
    .features = Target.x86.featureSet(&[_]Target.x86.Feature{
        ."64bit",
        ."aes",
        ."avx",
        ."avx2",
        ."bmi",
        ."bmi2",
        ."cmov",
        ."cx16",
        ."cx8",
        ."ermsb",
        ."f16c",
        ."false_deps_lzcnt_tzcnt",
        ."false_deps_popcnt",
        ."fast_scalar_fsqrt",
        ."fast_shld_rotate",
        ."fast_variable_shuffle",
        ."fma",
        ."fsgsbase",
        ."fxsr",
        ."idivq_to_divl",
        ."invpcid",
        ."lzcnt",
        ."macrofusion",
        ."merge_to_threeway_branch",
        ."mmx",
        ."movbe",
        ."nopl",
        ."pclmul",
        ."popcnt",
        ."rdrnd",
    }),
};
```

--------------------------------

TITLE: Zig: Write LC_FUNCTION_START Data in Mach-O
DESCRIPTION: This update ensures that the `LC_FUNCTION_START` load command data is correctly written into Mach-O files. This information is used by debuggers and other tools to identify function entry points.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
macho: write out `LC_FUNCTION_START` data.
```

--------------------------------

TITLE: Zig: Using Namespace with Standard Library
DESCRIPTION: Demonstrates how to use the `usingnamespace` declaration to import all public declarations from the Zig standard library into a struct. This allows direct access to standard library functions, such as `testing.expect`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
test "using std namespace" {
    const S = struct {
        usingnamespace @import("std");
    };
    try S.testing.expect(true);
}
```

--------------------------------

TITLE: Zig @exp Function
DESCRIPTION: Calculates the base-e exponential of a floating-point number using hardware instructions when available. Supports f32 and f64 types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: zig
CODE:
```
@exp(comptime T: type, value: T) T
```

--------------------------------

TITLE: Zig Function Reflection Test
DESCRIPTION: Provides a Zig test case demonstrating function reflection. It uses '@typeInfo' to inspect function parameters and return types, and checks for generic properties of functions like 'math.Log2Int'.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const math = std.math;
const testing = std.testing;

test "fn reflection" {
    try testing.expect(@typeInfo(@TypeOf(testing.expect))."fn".params[0].type.? == bool);
    try testing.expect(@typeInfo(@TypeOf(testing.tmpDir))."fn".return_type.? == testing.TmpDir);

    try testing.expect(@typeInfo(@TypeOf(math.Log2Int))."fn".is_generic);
}
```

LANGUAGE: shell
CODE:
```
$ zig test test_fn_reflection.zig
1/1 test_fn_reflection.test.fn reflection...OK
All 1 tests passed.
```

--------------------------------

TITLE: Zig Bitwise XOR Operator
DESCRIPTION: Performs a bitwise XOR operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
a ^ b
a ^= b
```

--------------------------------

TITLE: Zig Bitwise XOR Operator
DESCRIPTION: Covers the bitwise XOR operator '^' for integers in Zig. It mentions that the operation invokes Peer Type Resolution for the operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
a ^ b
a ^= b
```

--------------------------------

TITLE: Zig UEFI Protocol Definitions
DESCRIPTION: Provides access to UEFI protocol definitions within Zig's standard library. This allows for cleaner integration with UEFI services and functionalities.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os.uefi.protocols
```

--------------------------------

TITLE: Zig: Generate Foos with Potential Leak
DESCRIPTION: This Zig function `genFoos` attempts to allocate and initialize an array of `Foo` structs. It uses `errdefer` for cleanup, but a leak occurs if the loop terminates early due to an error after allocating `foo.data` but before the `errdefer` for `foo.data` is properly handled in all cases. The test case `genFoos` specifically triggers this leak by requesting more foos than allowed.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const Allocator = std.mem.Allocator;

const Foo = struct {
    data: *u32
};

fn getData() !u32 {
    return 666;
}

fn genFoos(allocator: Allocator, num: usize) ![]Foo {
    var foos = try allocator.alloc(Foo, num);
    errdefer allocator.free(foos);

    for (foos) |*foo| {
        foo.data = try allocator.create(u32);
        // This errdefer does not last between iterations
        errdefer allocator.destroy(foo.data);

        // The data for the first 3 foos will be leaked
        if(i >= 3) return error.TooManyFoos;

        foo.data.* = try getData();
    }

    return foos;
}

test "genFoos" {
    try std.testing.expectError(error.TooManyFoos, genFoos(std.testing.allocator, 5));
}
```

--------------------------------

TITLE: Zig @bytesToSlice: Convert Bytes to Slice
DESCRIPTION: Converts a slice or array of bytes into a slice of a specified element type. The function ensures that the number of bytes is evenly divisible by the size of the element type to avoid undefined behavior. It preserves pointer properties from the input byte slice.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: zig
CODE:
```
comptime Element: type, bytes: []u8) []Element
```

--------------------------------

TITLE: Zig: Initialize Struct Fields with Decl Literals
DESCRIPTION: Shows how to use decl literals in Zig for initializing struct fields, including calling functions that return error unions using 'try'. This provides a cleaner way to initialize complex types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.14.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

const ArrayListUnmanaged = std.ArrayListUnmanaged;

var list = ArrayListUnmanaged([]u8).init(std.heap.page_allocator);

const MyStruct = struct {
    data: []u8,
};

var my_struct = MyStruct{
    .data = ArrayListUnmanaged([]u8).init(std.heap.page_allocator).items,
};

```

--------------------------------

TITLE: Zig Inline Assembly for System Calls (x86_64 Linux)
DESCRIPTION: Provides an example of using Zig's inline assembly feature to directly interact with the operating system via system calls. This implementation demonstrates writing 'hello world' to stdout and exiting the process on x86_64 Linux.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
pub fn main() noreturn {
    const msg = "hello world\\n";
    _ = syscall3(SYS_write, STDOUT_FILENO, @intFromPtr(msg), msg.len);
    _ = syscall1(SYS_exit, 0);
    unreachable;
}

pub const SYS_write = 1;
pub const SYS_exit = 60;

pub const STDOUT_FILENO = 1;

pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "=={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
        : "rcx", "r11"
    );
}

pub fn syscall3(number: usize, arg1: usize, arg2: usize, arg3: usize) usize {
    return asm volatile ("syscall"
        : [ret] "=={rax}" (-> usize),
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
          [arg2] "{rsi}" (arg2),
          [arg3] "{rdx}" (arg3),
        : "rcx", "r11"
    );
}
```

--------------------------------

TITLE: Zig std.atomic.Int.set Method
DESCRIPTION: Added the .set method to std.atomic.Int for atomic integer operations. This allows for safe concurrent modification of integer values.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.atomic.Int.set
```

--------------------------------

TITLE: Add setitimer and getitimer syscalls to std.os.linux
DESCRIPTION: Integrates the setitimer and getitimer syscalls into the std.os.linux module for timer management.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.1/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os.linux: Add setitimer and getitimer syscalls
```

--------------------------------

TITLE: Zig Inline Assembly for syscall3 (write)
DESCRIPTION: Illustrates the use of inline assembly in Zig to perform a `syscall3` operation, specifically for writing to standard output on x86_64 Linux, including necessary constants and function signatures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: Zig
CODE:
```
pub fn main() noreturn {
    const msg = "hello world\n";
    _ = syscall3(SYS_write, STDOUT_FILENO, @ptrToInt(msg), msg.len);
    _ = syscall1(SYS_exit, 0);
    unreachable;
}

pub const SYS_write = 1;
pub const SYS_exit = 60;

pub const STDOUT_FILENO = 1;

pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "=R" (-> usize) // Use =R for output register
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1)
        : "rcx", "r11"
    );
}

pub fn syscall3(number: usize, arg1: usize, arg2: usize, arg3: usize) usize {
    // Assuming a similar structure for syscall3, though the example is cut off
    return asm volatile ("syscall"
        : [ret] "=R" (-> usize)
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
          [arg2] "{rsi}" (arg2),
          [arg3] "{rdx}" (arg3)
        : "rcx", "r11"
    );
}
```

--------------------------------

TITLE: Zig CLI Allocation with ArenaAllocator
DESCRIPTION: Demonstrates using ArenaAllocator for command-line applications where all memory can be freed at once. It initializes an ArenaAllocator, allocates an i32, and prints it. The arena is deinitialized at the end, freeing all associated memory.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var arena = std.heap.ArenaAllocator.init(std.heap.direct_allocator);
    defer arena.deinit();

    const allocator = &arena.allocator;

    const ptr = try allocator.create(i32);
    std.debug.warn("ptr={*}\n", ptr);
}
```

--------------------------------

TITLE: Zig Language Reference: Fix Array/Pointer/Slice Coercion
DESCRIPTION: Corrects the section in the Zig language reference that describes array, pointer, and slice type coercion, addressing an issue identified in [#9392](https://github.com/ziglang/zig/issues/9392).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: zig
CODE:
```
Fix array/pointer/slice type coercion section ([#9392](https://github.com/ziglang/zig/issues/9392)).
```

--------------------------------

TITLE: Zig Base-e Exponential Function
DESCRIPTION: The `@exp` function in Zig computes the base-e exponential of a floating-point number, using hardware instructions if available. It handles both floats and vectors of floats, though some float types may have incomplete support for this operation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
@exp(value: anytype) @TypeOf(value)
```

--------------------------------

TITLE: Zig Function Type Reflection
DESCRIPTION: Demonstrates how to use Zig's reflection capabilities to inspect function types, specifically checking the return type and whether a function uses variable arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "fn reflection" {
    expect(@typeInfo(@TypeOf(expect)).Fn.return_type.? == void);
    expect(@typeInfo(@TypeOf(expect)).Fn.is_var_args == false);
}
```

--------------------------------

TITLE: Exponential with @exp
DESCRIPTION: Describes the @exp function for calculating the base-e exponential of floating-point numbers and vectors of floats, including notes on hardware acceleration and potential limitations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
@exp(value: anytype) @TypeOf(value)
```

--------------------------------

TITLE: Zig: Peer Type Resolution with Error Union Slice
DESCRIPTION: Demonstrates peer type resolution involving an empty array, a slice, and an error union of a slice. It shows how Zig handles these complex types during resolution.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;

test "peer type resolution: [0]u8, []const u8, and anyerror![]u8" {
    {
        var data = "hi";
        const slice = data[0..];
        assert((try peerTypeEmptyArrayAndSliceAndError(true, slice)).len == 0);
        assert((try peerTypeEmptyArrayAndSliceAndError(false, slice)).len == 1);
    }
    comptime {
        var data = "hi";
        const slice = data[0..];
        assert((try peerTypeEmptyArrayAndSliceAndError(true, slice)).len == 0);

```

--------------------------------

TITLE: Macho and Type Import Changes
DESCRIPTION: This snippet includes fixes for typos in Mach-O constants definitions and a change in how built-in modules are imported, from `@import("builtin")` to `std.builtin`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
macho: fix typos in consts defs
change `@import("builtin")` to `std.builtin`
```

--------------------------------

TITLE: Zig: Improved C Pointers with Optional Syntax
DESCRIPTION: Shows an improved C function prototype in Zig with correct pointer types, demonstrating that the optional syntax for C pointers remains functional. This highlights Zig's flexibility in handling C interop, even with refined type definitions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
pub extern fn getenv(name: [*]const u8) ?[*]]u8;
// note: this is just a demo of C pointers with optional syntax. // std.process has better API for getenv.
test "C pointers with optional syntax" {
    const ptr1 = getenv(c"HOME").?; // don't do this ðŸ’¥
    const ptr2 = getenv(c"HOME") orelse return error.Homeless; // OK
    if (getenv(c"HOME")) |ptr3| {
        // also OK
    }
    const ptr4 = getenv(c"HOME");
    if (ptr4 == null) {
        // also works
    }
}
```

--------------------------------

TITLE: Zig @exp for Base-e Exponential Function
DESCRIPTION: Details the @exp built-in function for computing the base-e exponential of a floating-point number. It notes the potential use of hardware instructions and support for floats and float vectors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html



--------------------------------

TITLE: Zig Formatted Printing with Tuples
DESCRIPTION: Demonstrates how Zig uses tuples for formatted printing parameters, replacing variadic functions. This example prints 'Hello, World!' using `std.debug.warn` with a tuple argument.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    std.debug.warn("Hello, {}\n", .{"World!"});
}
```

--------------------------------

TITLE: Zig: Pointer Arithmetic with Many-Item Pointers and Slices
DESCRIPTION: Illustrates pointer arithmetic using many-item pointers ('[*]T') and highlights the potential issues when performing arithmetic directly on slices without updating their length. It shows how to correctly increment a many-item pointer and access elements using indexing.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "pointer arithmetic with many-item pointer" {
    const array = [_]i32{ 1, 2, 3, 4 };
    var ptr: [*]const i32 = &array;

    try expect(ptr[0] == 1);
    ptr += 1;
    try expect(ptr[0] == 2);

    // slicing a many-item pointer without an end is equivalent to
    // pointer arithmetic: `ptr[start..] == ptr + start`
    try expect(ptr[1..] == ptr + 1);
}

test "pointer arithmetic with slices" {
    var array = [_]i32{ 1, 2, 3, 4 };
    var length: usize = 0; // var to make it runtime-known
    _ = &length; // suppress 'var is never mutated' error
    var slice = array[length..array.len];

    try expect(slice[0] == 1);
    try expect(slice.len == 4);

    slice.ptr += 1;
    // now the slice is in an bad state since len has not been updated

    try expect(slice[0] == 2);
    try expect(slice.len == 4);
}
```

--------------------------------

TITLE: Windows and Darwin Dynamic Library Loading
DESCRIPTION: Fixes for Windows dynamic library loading and addition of loading support for Darwin (macOS).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const lib = std.os.dlopen("libexample.so", .{}
);
```

--------------------------------

TITLE: Zig Bitwise OR Operator
DESCRIPTION: Explains the bitwise OR operator (|) for integers in Zig, stating that it invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
a | b
a |= b

-- Relevant Types:
* Integers

-- Description:
Bitwise OR.
* Invokes Peer Type Resolution for the operands.

-- Example:
0b010 | 0b100 == 0b110
```

--------------------------------

TITLE: Zig Tuple Syntax
DESCRIPTION: Illustrates Zig's tuple syntax, which allows omitting the type from array literals. Tuples are presented as structs with auto-numbered field names, enabling array-like access using quoted numbers as field identifiers. Examples show direct population of arrays and accessing tuple elements.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuple syntax" {
    var array: [4]u8 = .{11, 22, 33, 44};
    expect(array[0] == 11);
    expect(array[1] == 22);
    expect(array[2] == 33);
    expect(array[3] == 44);
}
```

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous tuple" {
    dump(.{ @as(u32, 1234), @as(f64, 12.34), true, "hi"});
}

fn dump(args: var) void {
    expect(args.@"0" == 1234);
    expect(args.@"1" == 12.34);
    expect(args.@"2");
    expect(args.@"3"[0] == 'h');
    expect(args.@"3"[1] == 'i');
}
```

--------------------------------

TITLE: Zig WASI Command Line Argument Example
DESCRIPTION: A Zig program demonstrating the use of the WASI target to access and print command-line arguments using the standard library.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    const args = try std.process.argsAlloc(std.heap.wasm_allocator);
    defer std.process.argsFree(std.heap.wasm_allocator, args);

    for (args) |arg, i| {
        std.debug.warn("{}: {}
", i, arg);
    }
}
```

--------------------------------

TITLE: Exponential with @exp
DESCRIPTION: Describes the @exp function for calculating the base-e exponential of floating-point numbers and vectors of floats, including notes on hardware acceleration and potential limitations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
@exp(value: anytype) @TypeOf(value)
```

--------------------------------

TITLE: Zig: Pointer Arithmetic with Many-Item Pointers and Slices
DESCRIPTION: Illustrates pointer arithmetic using many-item pointers ('[*]T') and highlights the potential issues when performing arithmetic directly on slices without updating their length. It shows how to correctly increment a many-item pointer and access elements using indexing.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "pointer arithmetic with many-item pointer" {
    const array = [_]i32{ 1, 2, 3, 4 };
    var ptr: [*]const i32 = &array;

    try expect(ptr[0] == 1);
    ptr += 1;
    try expect(ptr[0] == 2);

    // slicing a many-item pointer without an end is equivalent to
    // pointer arithmetic: `ptr[start..] == ptr + start`
    try expect(ptr[1..] == ptr + 1);
}

test "pointer arithmetic with slices" {
    var array = [_]i32{ 1, 2, 3, 4 };
    var length: usize = 0; // var to make it runtime-known
    _ = &length; // suppress 'var is never mutated' error
    var slice = array[length..array.len];

    try expect(slice[0] == 1);
    try expect(slice.len == 4);

    slice.ptr += 1;
    // now the slice is in an bad state since len has not been updated

    try expect(slice[0] == 2);
    try expect(slice.len == 4);
}
```

--------------------------------

TITLE: Zig: C Standard Library Integration
DESCRIPTION: Expands Zig's integration with the C standard library by adding new functions and fixing existing definitions. This includes functions for signal handling and process control.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
c: add sigfillset, alarm, sigwait.
c: fix waitpid() definition.
```

--------------------------------

TITLE: Zig: Build Artifact Caching Example
DESCRIPTION: This example illustrates Zig's build artifact caching mechanism. When the `--cache on` flag is used, Zig caches build artifacts. The provided manifest file snippet shows the metadata (inode, mtime, hash, path) for a `hello.zig` file, demonstrating how Zig tracks files for caching purposes.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: Shell
CODE:
```
5505089 1548876783 204467480 rcnRrxBgZSiWki_XN9XKlQ2yfWkM6KLYhUWprzniEBtjgmeUSmtlv5mAguA4l2Q1 /home/andy/dev/zig/example/hello_world/hello.zig
```

--------------------------------

TITLE: Zig: `printValue` implementation for different types
DESCRIPTION: Details the `printValue` function within the `Writer` struct, showing how it handles different data types like integers, floats, and pointers. It uses a `switch` statement on the type information to dispatch to type-specific writing functions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
const Writer = struct {
    pub fn printValue(self: *Writer, value: anytype) !void {
        switch (@typeInfo(@TypeOf(value))) {
            .Int => {
                return self.writeInt(value);
            },
            .Float => {
                return self.writeFloat(value);
            },
            .Pointer => {
                return self.write(value);
            },
            else => {
                @compileError("Unable to print type '" ++ @typeName(@TypeOf(value)) ++ "'");
            },
        }
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
};
```

--------------------------------

TITLE: Zig Base-e Exponential Function
DESCRIPTION: The `@exp` function in Zig computes the base-e exponential of a floating-point number, using hardware instructions if available. It handles both floats and vectors of floats, though some float types may have incomplete support for this operation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
@exp(value: anytype) @TypeOf(value)
```

--------------------------------

TITLE: Zig: Linux EALREADY error handling for connect/getsockopt
DESCRIPTION: Enhances the handling of the `EALREADY` error code for `connect()` and `getsockoptError()` system calls on Linux. This improves robustness when dealing with non-blocking socket operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
os/linux: return error on EALREADY for connect() and getsockoptError()
```

--------------------------------

TITLE: Zig @exp for Base-e Exponential Calculation
DESCRIPTION: Covers the @exp built-in function, which calculates the base-e exponential of a floating-point number. It supports both scalar and vector inputs and uses hardware instructions for efficiency.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html



--------------------------------

TITLE: Zig: Expose machine field in ELF header
DESCRIPTION: Exposes the `machine` field from the ELF header structure. This allows programs to directly access and interpret the target architecture information embedded in ELF files.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
expose machine field in ELF header
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Explains the bitwise AND operator '&' and compound assignment '&=' for Integers in Zig. This operation invokes Peer Type Resolution for the operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
a & b
a &= b

// Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig: Add epoll_pwait2 Linux syscall
DESCRIPTION: Adds the `epoll_pwait2` system call to the Linux operating system module. This system call provides enhanced control over epoll event waiting, including timeout precision.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Add epoll_pwait2 Linux syscall
```

--------------------------------

TITLE: Zig: `printValue` Type Handling Implementation
DESCRIPTION: Details the `printValue` function within the Zig `print` implementation, which handles different data types. It uses a `switch` statement on the type information to dispatch to specific writing functions for integers, floats, and pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
const Writer = struct {
    pub fn printValue(self: *Writer, value: anytype) !void {
        switch (@typeInfo(@TypeOf(value))) {
            .int => {
                return self.writeInt(value);
            },
            .float => {
                return self.writeFloat(value);
            },
            .pointer => {
                return self.write(value);
            },
            else => {
                @compileError("Unable to print type '" ++ @typeName(@TypeOf(value)) ++ "'");
            },
        }
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
};
```

--------------------------------

TITLE: Zig Linker: TAPI Parser for System Libraries
DESCRIPTION: Highlights the implementation of a TAPI (Text-based stub files) parser within the Zig MachO linker. This parser is essential for correctly linking system libraries and frameworks, especially in cross-compilation scenarios.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
// The TAPI parser enables the linker to understand Apple's text-based
// definition stub files (e.g., libSystem.B.tbd).

// This is crucial for linking system libraries and frameworks, especially
// when cross-compiling from non-Apple hosts to Apple platforms.

// Example of a TAPI file structure (conceptual):
// --- !tapi
// compatible with:
//   - macos
// install name: /usr/lib/libSystem.B.dylib
// ...

```

--------------------------------

TITLE: Zig builtin @atomicRmw
DESCRIPTION: Illustrates the `@atomicRmw` built-in function in Zig for atomic read-modify-write operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
var atomic_val: atomic(i32) = undefined;
const old_val = @atomicRmw(&atomic_val, .add, .SeqCst, 1);
```

--------------------------------

TITLE: Zig @atomicLoad
DESCRIPTION: Atomically loads a value from a pointer. This function ensures that the read operation is atomic, preventing race conditions in concurrent programming. It supports boolean, float, integer, and enum types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
    @atomicLoad(comptime T: type, ptr: *const T, comptime ordering: builtin.AtomicOrder) T
```

--------------------------------

TITLE: Zig Comptime LLVM IR Output
DESCRIPTION: Presents the generated LLVM Intermediate Representation (IR) for the comptime prime number generation example. It highlights the compile-time computed constants for the prime numbers and their sum, demonstrating the efficiency of Zig's compile-time execution.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: llvm ir
CODE:
```
    @0 = internal unnamed_addr constant [25 x i32] [i32 2, i32 3, i32 5, i32 7, i32 11, i32 13, i32 17, i32 19, i32 23, i32 29, i32 31, i32 37, i32 41, i32 43, i32 47, i32 53, i32 59, i32 61, i32 67, i32 71, i32 73, i32 79, i32 83, i32 89, i32 97]
    @1 = internal unnamed_addr constant i32 1060
```

--------------------------------

TITLE: Add readUntilDelimiterOrEofArrayList and readUntilDelimiterOrEofAlloc
DESCRIPTION: Adds readUntilDelimiterOrEofArrayList and readUntilDelimiterOrEofAlloc to the Zig standard library, providing functions to read from a reader until a delimiter is found or the end of the file is reached, storing the result in an ArrayList or allocating memory for it.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Zig: `mem.split` and `mem.tokenize` Generics
DESCRIPTION: Makes `mem.split` and `mem.tokenize` generic functions instead of assuming `u8` elements, allowing them to work with different data types and improving flexibility.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
`mem.split` and `mem.tokenize` generic instead of assuming u8 ([#9531](https://github.com/ziglang/zig/issues/9531)).
```

--------------------------------

TITLE: Zig Peer Type Resolution: Empty Array and Slice
DESCRIPTION: Illustrates peer type resolution between an empty array literal `[]const u8{}` and a slice of a string. It shows how an empty array can be implicitly cast to a slice, with tests for runtime and compile-time.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;

test "peer type resolution: [0]u8 and []const u8" {
    assert(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
    assert(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    comptime {
        assert(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
        assert(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    }
}
fn peerTypeEmptyArrayAndSlice(a: bool, slice: []const u8) []const u8 {
    if (a) {
        return []const u8{};
    }

    return slice[0..1];
}
```

--------------------------------

TITLE: Zig: Write to File (Blocking)
DESCRIPTION: A simple Zig program demonstrating how to write content to a file using blocking I/O. It creates a file named 'hello.txt', writes 'hello\n' to it, and ensures the file is closed afterwards.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
pub fn main() anyerror!void {
    const file = try std.fs.cwd().createFile("hello.txt", .{
    });
    defer file.close();
    try file.writeAll("hello\n");
}
```

--------------------------------

TITLE: Fix std.atomic.Queue unget and add documentation
DESCRIPTION: Corrects the unget implementation for std.atomic.Queue and adds relevant documentation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.1/release-notes.html

LANGUAGE: Zig
CODE:
```
std.atomic.Queue: fix unget implementation and add doc
```

--------------------------------

TITLE: Zig: Resolve Peer Types
DESCRIPTION: Demonstrates Zig's peer type resolution for pointers and slices, including tests for various pointer and slice combinations. This snippet showcases how Zig handles type compatibility in pointer and slice operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
fn peerTypeEmptyArrayAndSliceAndError(a: bool, slice: []u8) anyerror![]u8 {
    if (a) {
        return &[_]u8{};
    }

    return slice[0..1];
}

test "peer type resolution: *const T and ?*T" {
    const a = @intToPtr(*const usize, 0x123456780);
    const b = @intToPtr(?*usize, 0x123456780);
    expect(a == b);
    expect(b == a);
}
```

--------------------------------

TITLE: Zig Linker Improvements
DESCRIPTION: This snippet details improvements to the Zig linker, including proper implementation of passthrough mode for LLD child processes and fixing the linking order of libc components.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
link: properly implement passthrough mode for LLD child proccess.
Fix libc components' linking order.
```

--------------------------------

TITLE: Zig @alignOf Example
DESCRIPTION: Explains how to use @alignOf to get the required byte alignment for a type according to the C ABI for the current target. This is useful for ensuring correct memory layout.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.0/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

comptime {
    assert(@alignOf(u32) <= @sizeOf(u32));
    assert(&u32 == &align(@alignOf(u32)) u32);
}
```

--------------------------------

TITLE: Zig Fully Anonymous List Literal Inference
DESCRIPTION: Illustrates how fully anonymous list literals (without an explicit type) are treated as structs with numbered field names in Zig. Includes a test case to verify field access.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous list literal" {
    try dump(.{ @as(u32, 1234), @as(f64, 12.34), true, "hi"});
}

fn dump(args: anytype) !void {
    try expect(args.@"0" == 1234);
    try expect(args.@"1" == 12.34);
    try expect(args.@"2");
    try expect(args.@"3"[0] == 'h');
    try expect(args.@"3"[1] == 'i');
}
```

LANGUAGE: Shell
CODE:
```
$ zig test infer_list_literal.zig
1/1 test "fully anonymous list literal"... OK
All 1 tests passed.
```

--------------------------------

TITLE: Zig String Literal to Mutable Slice Error
DESCRIPTION: Illustrates an error scenario in Zig where a string literal is passed to a function expecting a mutable slice (`[]u8`). String literals are immutable and stored in read-only memory, causing a type mismatch.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
fn foo(s: []u8) void {}

test "string literal to mutable slice" {
    foo("hello");
}
```

--------------------------------

TITLE: Zig New Streams API Example
DESCRIPTION: Illustrates reading lines from standard input and calculating a sum using the new, simplified Zig streams API. This version is more ergonomic and efficient.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() anyerror!void {
    const in = std.io.bufferedInStream(std.io.getStdIn().inStream()).inStream();
    var sum: u64 = 0;
    var line_buf: [50]u8 = undefined;
    while (try in.readUntilDelimiterOrEof(&line_buf, '\n')) |line| {
        if (line.len == 0) break;
        const module_mass = try std.fmt.parseInt(u64, line, 10);
        const fuel_required = (module_mass / 3) - 2;
        sum += fuel_required;
    }
    const out = std.io.getStdOut().outStream();
    try out.print("{}\\n", .{sum});
}
```

--------------------------------

TITLE: Zig VDSO Optimization for clock_gettime
DESCRIPTION: On Linux, `clock_gettime` now utilizes the VDSO (Virtual Dynamic Shared Object) for optimization, even in static builds. This can lead to faster time retrieval by avoiding system calls.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: C
CODE:
```
#include <time.h>
struct timespec ts;
clock_gettime(CLOCK_REALTIME, &ts);
```

--------------------------------

TITLE: Zig Volatile Load/Store Example
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig for memory-mapped I/O. It shows how to declare a volatile pointer and assert its type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Build System: Fix Darwin Rpaths and Improve CheckObjectStep
DESCRIPTION: Corrects the addition of rpaths on Darwin systems and enhances `CheckObjectStep` to allow matching `LazyPath` paths, improving cross-platform compatibility and analysis.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
build: fix adding rpaths on darwin, improve CheckObjectStep to allow matching LazyPath paths
```

--------------------------------

TITLE: Zig File System Directory Splitting and Iterator Handling
DESCRIPTION: Splits `Dir` into `IterableDir` and refactors `Iterator.next` for Linux/WASI to handle platform-specific errors like `ENOENT`. Fixes `WalkerEntry.dir` assignment.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Split `Dir` into `IterableDir`
Split Iterator.next on Linux and WASI to allow for handling platform-specific errors
Fixed `WalkerEntry.dir` not always being the containing dir
```

--------------------------------

TITLE: Zig Packed Structs: BitCast Example
DESCRIPTION: Demonstrates using `@bitCast` between different `packed struct` types in Zig. It verifies the size of the structs and the correct interpretation of data based on endianness.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const builtin = std.builtin;
const expect = std.testing.expect;

const Full = packed struct {
    number: u16,
};
const Divided = packed struct {
    half1: u8,
    quarter3: u4,
    quarter4: u4,
};

test "@bitCast between packed structs" {
    doTheTest();
    comptime doTheTest();
}

fn doTheTest() void {
    expect(@sizeOf(Full) == 2);
    expect(@sizeOf(Divided) == 2);
    var full = Full{ .number = 0x1234 };
    var divided = @bitCast(Divided, full);
    switch (builtin.endian) {
        .Big => {
            expect(divided.half1 == 0x12);
            expect(divided.quarter3 == 0x3);
            expect(divided.quarter4 == 0x4);
        },
        .Little => {
            expect(divided.half1 == 0x34);
            expect(divided.quarter3 == 0x2);
            expect(divided.quarter4 == 0x1);
        },
    }
}
```

--------------------------------

TITLE: Zig UEFI Virtual Addressing Helpers
DESCRIPTION: Introduces virtual addressing helper functions for UEFI environments, enhancing memory management capabilities within the boot services.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
# Added virtual addressing helpers ([#10195](https://github.com/ziglang/zig/issues/10195)).
```

--------------------------------

TITLE: Zig @atomicLoad Example
DESCRIPTION: Provides an example of using the @atomicLoad builtin function in Zig for atomic memory operations. It demonstrates how to atomically dereference a pointer to a type T and retrieve its value, specifying the memory ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
@atomicLoad(comptime T: type, ptr: *const T, comptime ordering: AtomicOrder) T
```

--------------------------------

TITLE: Add inotify_rm_watch and Mark Pathname as Null-Terminated
DESCRIPTION: Adds the `inotify_rm_watch` definition to `c/linux.zig` and marks the pathname in `inotify_add_watch` as null-terminated, improving Linux inotify API bindings.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Add inotify_rm_watch definition to c/linux.zig and make inotify_add_watch's pathname marked as nul-terminated.
```

--------------------------------

TITLE: Zig Volatile Load/Store Example
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig for memory-mapped I/O. It shows how to declare a volatile pointer and assert its type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig: Switch Capture Peer Type Resolution
DESCRIPTION: Illustrates how Zig's switch statement captures union payloads, utilizing peer type resolution to handle distinct but compatible types. It also covers pointer captures.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;
const expectEqual = std.testing.expectEqual;

const U1 = union(enum) {
    x: u8,
    y: ?u32,
};

test "switch capture resolves peer types" {
    try f(1, .{ .x = 1 });
    try f(2, .{ .y = 2 });
    try f(0, .{ .y = null });
}

fn f(expected: u32, u: U1) !void {
    switch (u) {
        .x, .y => |val| {
            comptime assert(@TypeOf(val) == ?u32);
            try expectEqual(expected, val orelse 0);
        },
    }
}

const U2 = union(enum) {
    x: c_uint,
    /// This type has the same number of bits as `c_uint`, but is distinct.
    y: @Type(.{ .Int = .{ .signedness = .unsigned, .bits = @bitSizeOf(c_uint) } }),
};

test "switch pointer capture resolves peer types" {
    var a: U2 = .{ .x = 10 };
    var b: U2 = .{ .y = 20 };

    g(&a);
    g(&b);

    try expectEqual(U2{ .x = 11 }, a);
    try expectEqual(U2{ .y = 21 }, b);
}

fn g(u: *U2) void {
    switch (u.*) {
        .x, .y => |*ptr| {
            ptr.* += 1;
        },
    }
}
```

--------------------------------

TITLE: Zig Function Parameter Type Inference
DESCRIPTION: Illustrates how to use `anytype` for function parameters in Zig, allowing types to be inferred at compile time. It shows how to use `@TypeOf` and `@typeInfo` to inspect the inferred types and includes test cases for integer and i64 inputs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    var y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

--------------------------------

TITLE: Zig HTTP Server Connection Lifecycle
DESCRIPTION: Demonstrates the new lifecycle for handling HTTP server connections in Zig. It outlines the sequence of operations from accepting a connection to sending a response, emphasizing the type system's guidance for API users.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: Zig
CODE:
```
1. std.net.Server.accept() gives you a std.net.Server.Connection
2. std.http.Server.init() with the connection
3. Server.receiveHead() gives you a Request
4. Request.reader() gives you a body reader
5. Request.respond() is a one-shot, or Request.respondStreaming() creates a Response
6. Response.writer() gives you a body writer
7. Response.end() finishes the response; Response.endChunked() allows passing response trailers.
```

--------------------------------

TITLE: Zig Fully Anonymous List Literal Inference
DESCRIPTION: Illustrates how fully anonymous list literals (without an explicit type) are treated as structs with numbered field names in Zig. Includes a test case to verify field access.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous list literal" {
    try dump(.{ @as(u32, 1234), @as(f64, 12.34), true, "hi"});
}

fn dump(args: anytype) !void {
    try expect(args.@"0" == 1234);
    try expect(args.@"1" == 12.34);
    try expect(args.@"2");
    try expect(args.@"3"[0] == 'h');
    try expect(args.@"3"[1] == 'i');
}
```

LANGUAGE: Shell
CODE:
```
$ zig test infer_list_literal.zig
1/1 test "fully anonymous list literal"... OK
All 1 tests passed.
```

--------------------------------

TITLE: WebAssembly and WASI Support in Zig
DESCRIPTION: Outlines contributions to Zig's WebAssembly and WASI support, including getting compiler-rt working, tweaking target settings like disabling error return traces, forcing single-threaded mode, and setting the executable file extension to .wasm. It also mentions the addition of standard library support for the WASI target.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
disabling error return traces
forcing single-threaded mode
making the executable file extension ".wasm"
```

--------------------------------

TITLE: Zig noreturn type compatibility example
DESCRIPTION: Demonstrates the compatibility of the `noreturn` type with other types in conditional expressions like `if` statements, showing how `return` can be used as an alternative path.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
fn foo(condition: bool, b: u32) void {
    const a = if (condition) b else return;
    @panic("do something with a");
}
test "noreturn" {
    foo(false, 1);
}
```

--------------------------------

TITLE: Zig: Pointer to Integer Conversion Example
DESCRIPTION: Illustrates the use of @ptrToInt to obtain the integer address of a pointer. This is presented as an alternative when a variable's type changes from an integer to a pointer, requiring a different explicit cast.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
var i = @ptrToInt(x);
```

--------------------------------

TITLE: Zig Cross-Platform Metadata API and File Sync
DESCRIPTION: Implements a cross-platform metadata API and adds `File.sync` functionality. Prevents integer overflow in `Dir.makePath`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Implemented cross-platform metadata API
Added File.sync
Prevent possible integer overflow in Dir.makePath
```

--------------------------------

TITLE: Zig Runtime Struct Field Dump Function
DESCRIPTION: Demonstrates a function `dump` that iterates over the fields of a struct using compile-time reflection and prints their names and values. This example showcases handling mixed comptime and runtime values within a struct.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

fn dump(args: var) void {
    inline for (std.meta.fields(@TypeOf(args))) |field| {
        std.debug.warn("{} = {}\n", .{field.name, @field(args, field.name)});
    }
}

pub fn main() void {
    var runtime_float: f32 = 12.34;
    dump(.{
        .int = 1234,
        .float = runtime_float,
        .b = true,
        .s = "hi",
        .T = [*]f32,
    });
}
```

--------------------------------

TITLE: Zig Standard Library: Windows Specific Improvements
DESCRIPTION: This section outlines various improvements made to Zig's standard library for Windows compatibility. It includes updates to `zig test` for Windows console detection, the addition of `CANNOT_DELETE` error for `os.windows.DeleteFile`, and the use of explicit integer bit widths for Windows GUIDs. Performance improvements for comptime Windows GUID parsing and handling of broken pipes in `child_process` are also mentioned, along with reworkings of `std.Thread.getName/setName` and the addition of `GetProcessTimes` binding.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example: Using os.windows.DeleteFile with CANNOT_DELETE error
const file_path = "protected_file.txt";
var file = std.fs.cwd().openFile(file_path, .{}) catch |err| switch (err) {
    error.AccessDenied -> std.debug.print("Cannot delete file: {s}", .{\nfile_path});
    else -> |e| return e;
};

// Example: Using std.process.EnvMap for case-insensitive environment variables
var env_map = std.process.EnvMap.init(std.heap.page_allocator);
defer env_map.deinit();
const value = env_map.get("PATH");

// Example: GetProcessTimes binding (conceptual)
const kernel32 = std.windows.kernel32;
var creation_time: std.windows.FILETIME = undefined;
var exit_time: std.windows.FILETIME = undefined;
var kernel_time: std.windows.FILETIME = undefined;
var user_time: std.windows.FILETIME = undefined;
try kernel32.GetProcessTimes(std.os.getpid(), &creation_time, &exit_time, &kernel_time, &user_time);

```

--------------------------------

TITLE: Zig Tuple Initialization and Operations
DESCRIPTION: Shows the creation and manipulation of tuples in Zig, which are anonymous structs with numeric field names starting from 0. The example demonstrates concatenation, indexing, iteration, and accessing fields using quoted numeric identifiers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuple" {
    const values = .{ 
        @as(u32, 1234),
        @as(f64, 12.34),
        true,
        "hi",
    } ++ .{false} ** 2;
    try expect(values[0] == 1234);
    try expect(values[4] == false);
    inline for (values) |v, i| {
        if (i != 2) continue;
        try expect(v);
    }
    try expect(values.len == 6);
    try expect(values.@"3"[0] == 'h');
}
```

--------------------------------

TITLE: Zig: Base-2 Exponential with @exp2
DESCRIPTION: Details the `@exp2` function for computing the base-2 exponential of floating-point numbers and vectors. It highlights hardware acceleration and potential implementation gaps for certain float types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html



--------------------------------

TITLE: Zig Volatile Load/Store Example
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig for memory-mapped I/O. It shows how to declare a volatile pointer and assert its type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Function Parameter Type Inference
DESCRIPTION: Illustrates how to use `anytype` for function parameters in Zig, allowing types to be inferred at compile time. It shows how to use `@TypeOf` and `@typeInfo` to inspect the inferred types and includes test cases for integer and i64 inputs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    var y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

--------------------------------

TITLE: Zig Function Type Reflection
DESCRIPTION: Demonstrates how to use Zig's reflection capabilities to inspect function types, specifically checking the return type and whether a function uses variable arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "fn reflection" {
    expect(@typeInfo(@TypeOf(expect)).Fn.return_type.? == void);
    expect(@typeInfo(@TypeOf(expect)).Fn.is_var_args == false);
}
```

--------------------------------

TITLE: Fix WASI IterableDir.nextWasi for large directories
DESCRIPTION: Resolves issues in the WASI implementation of IterableDir.nextWasi when handling very large directories.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.1/release-notes.html

LANGUAGE: Zig
CODE:
```
wasi: fixes IterableDir.nextWasi for large directory
```

--------------------------------

TITLE: Zig: `printValue` Type Handling Implementation
DESCRIPTION: Details the `printValue` function within the Zig `print` implementation, which handles different data types. It uses a `switch` statement on the type information to dispatch to specific writing functions for integers, floats, and pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const Writer = struct {
    pub fn printValue(self: *Writer, value: anytype) !void {
        switch (@typeInfo(@TypeOf(value))) {
            .int => {
                return self.writeInt(value);
            },
            .float => {
                return self.writeFloat(value);
            },
            .pointer => {
                return self.write(value);
            },
            else => {
                @compileError("Unable to print type '" ++ @typeName(@TypeOf(value)) ++ "'");
            },
        }
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
};
```

--------------------------------

TITLE: Linking: LLD Mach-O target-version-gating
DESCRIPTION: Fixes a regression in the Mach-O linker targeting newer macOS systems. It implements target-version-gating for `-syslibroot` on the linker line, ensuring compatibility with updated macOS versions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: Shell
CODE:
```
# Conceptual linker command modification
# Original (potentially problematic):
# zig build ... -target x86_64-macos -syslibroot /path/to/sdk

# Corrected behavior (conditional -syslibroot):
# zig build ... -target x86_64-macos --syslibroot <sdk_path_if_needed>
```

--------------------------------

TITLE: Zig: @ptrToInt and @intToPtr
DESCRIPTION: Demonstrates the use of @ptrToInt and @intToPtr in Zig to convert between pointers and integer representations of memory addresses. It includes assertions to verify the type and value of the converted address.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

test "@ptrToInt and @intToPtr" {
    const ptr = @intToPtr(*i32, 0xdeadbee0);
    const addr = @ptrToInt(ptr);
    assert(@TypeOf(addr) == usize);
    assert(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Zig: Constructing Slice at Comptime (Error)
DESCRIPTION: Shows a Zig function that constructs a slice at comptime, returning a pointer to a comptime variable. This is problematic at runtime and in global declarations, requiring a fix by promoting computed data to a const.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: zig
CODE:
```
fn getName() []const u8 {
    comptime var buf: [9]u8 = undefined;
    @memcpy(&buf, "some name");
    return &buf;
}

test getName {
    try @import("std").testing.expectEqualStrings("some name", getName());
}
```

--------------------------------

TITLE: Zig Builtin: @atomicLoad
DESCRIPTION: Atomically loads a value from a memory location. Ensures that the read operation is not interrupted by other threads.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
const value = @atomicLoad(volatile AtomicType, ptr);
// value is the loaded value
```

--------------------------------

TITLE: Integer Debug Info and Function Alignment
DESCRIPTION: Improvements to debug information for integers, using ABI size * 8 for better GDB compatibility. Also includes a fix for user-defined function alignment not propagating to LLVM IR.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const size_in_bits = @intCast(usize, @bitSizeOf(@typeInfo(i32).Int.signedness));
const abi_size_in_bytes = @bitSizeOf(i32) / 8;
// Use abi_size_in_bytes * 8 for debug info
```

--------------------------------

TITLE: Zig: Convert Integer Address to Pointer and Vice Versa
DESCRIPTION: Demonstrates the use of @intToPtr to convert an integer address to a pointer and @ptrToInt to convert a pointer back to an integer. It also shows how to check the type and value of the converted integer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@ptrToInt and @intToPtr" {
    const ptr = @intToPtr(*i32, 0xdeadbee0);
    const addr = @ptrToInt(ptr);
    expect(@TypeOf(addr) == usize);
    expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Adding a Mirror Entry to community-mirrors.ziggy
DESCRIPTION: An example demonstrating the format for adding a new mirror configuration to the `assets/community-mirrors.ziggy` file. This includes the URL, username, and email for the mirror.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/MIRRORS.md

LANGUAGE: diff
CODE:
```
[
    {
        .url = "https://a.com",
        .username = "a",
        .email = "a@a.com",
    },
    {
        .url = "https://b.com/zig",
        .username = "b",
        .email = "b@b.com",
    },
+    {
+        .url = "https://mymirror.net",
+        .username = "my-github-username",
+        .email = "my@email.com",
+    },
]

```

--------------------------------

TITLE: Zig mingw-w64 ANSI stdio Fix
DESCRIPTION: This snippet addresses a bug fix in Zig's integration with mingw-w64, specifically concerning the `-D__USE_MINGW_ANSI_STDIO=0` flag for crt files, ensuring correct ANSI stdio behavior.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Zig now passes `-D__USE_MINGW_ANSI_STDIO=0` for crt files. This was supposed to be happening all along, and it was a bug that Zig did not do this before.
```

--------------------------------

TITLE: Zig Generics: Linked List Implementation
DESCRIPTION: Illustrates Zig's approach to generics using comptime functions to create a generic linked list. Demonstrates type comparison, instantiation, and accessing struct fields via pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: zig
CODE:
```
// You can return a struct from a function. This is how we do generics
    // in Zig:
    fn LinkedList(comptime T: type) type {
        return struct {
            pub const Node = struct {
                prev: ?*Node,
                next: ?*Node,
                data: T,
            };
    
            first: ?*Node,
            last:  ?*Node,
            len:   usize,
        };
    }
    
    test "linked list" {
        // Functions called at compile-time are memoized. This means you can
        // do this:
        try expect(LinkedList(i32) == LinkedList(i32));
    
        var list = LinkedList(i32) {
            .first = null,
            .last = null,
            .len = 0,
        };
        try expect(list.len == 0);
    
        // Since types are first class values you can instantiate the type
        // by assigning it to a variable:
        const ListOfInts = LinkedList(i32);
        try expect(ListOfInts == LinkedList(i32));
    
        var node = ListOfInts.Node {
            .prev = null,
            .next = null,
            .data = 1234,
        };
        var list2 = LinkedList(i32) {
            .first = &node,
            .last = &node,
            .len = 1,
        };
    
        // When using a pointer to a struct, fields can be accessed directly,
        // without explicitly dereferencing the pointer.
        // So you can do
        try expect(list2.first.?.data == 1234);
        // instead of try expect(list2.first.?.*.data == 1234);
    }
```

--------------------------------

TITLE: Zig: Slice Bounds Checking and Modification
DESCRIPTION: Demonstrates the use of slice syntax `array[start..end]` to create a slice from an array. It shows how slices have a `len` property and allow modification of the underlying array elements through slice indexing, with built-in bounds checking.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "pointer slicing" {
    var array = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
    const slice = array[2..4];
    try expect(slice.len == 2);

    try expect(array[3] == 4);
    slice[1] += 1;
    try expect(array[3] == 5);
}
```

--------------------------------

TITLE: Zig Atomic Read-Modify-Write Operation
DESCRIPTION: Performs an atomic read-modify-write operation on a pointer. It dereferences the pointer, atomically modifies the value, and returns the previous value. Supported types include pointers, bool, floats, integers, enums, and packed structs. Requires specifying the atomic operation type and memory ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
comptime T: type, ptr: *T, comptime op: AtomicRmwOp, operand: T, comptime ordering: AtomicOrder) T
```

--------------------------------

TITLE: Zig: Swap arguments in Thread.spawn
DESCRIPTION: Swaps the arguments for the `Thread.spawn` function, referencing issue #8082. This change might be for API consistency or to improve usability.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Swap arguments in Thread.spawn ([#8082](https://github.com/ziglang/zig/issues/8082)).
```

--------------------------------

TITLE: Zig @intToPtr - Convert Integer to Pointer
DESCRIPTION: The @intToPtr function converts an integer address to a pointer of the specified destination type. If the destination pointer type does not permit address zero and the provided address is zero, it results in safety-checked Undefined Behavior. Use @ptrToInt for the reverse conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
comptime DestType: type, address: usize) DestType
```

--------------------------------

TITLE: Fix Type Mapping for nvptx Architecture in Zig
DESCRIPTION: Corrects type mappings for integers and `c_longdouble` specifically for the nvptx (NVIDIA GPU) architecture. This ensures accurate data representation and manipulation on GPUs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Fix type mapping for integers and c_longdouble for the nvptx architecture.
```

--------------------------------

TITLE: Zig: Refactor Mach-O Constants and Fix Bugs
DESCRIPTION: Constants within `std.macho` have been refactored, and two bugs have been fixed. This improves the maintainability and correctness of the Mach-O handling code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
macho: refactor consts in `std.macho`, and fix two bugs ([#10338](https://github.com/ziglang/zig/issues/10338)).
```

--------------------------------

TITLE: Zig: Atomic Fence
DESCRIPTION: Introduces happens-before edges between operations. The 'order' parameter specifies the atomic memory ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: Zig
CODE:
```
@fence(order: AtomicOrder) void
```

--------------------------------

TITLE: Zig std.os.Thread: Thread ID Support
DESCRIPTION: Adds support for retrieving the current thread ID within the `std.os.Thread` module. This is essential for debugging, logging, and managing concurrent operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const thread_id = std.os.Thread.getCurrentId();
```

--------------------------------

TITLE: Reworked Build Options API in Zig
DESCRIPTION: Illustrates the new `addBuildOption` API in Zig's build system, which offers improved integration with `FileSource`, supports arbitrary package naming, allows mapping to multiple artifacts, and uses content hashing for filenames. This replaces the previous global and hard-coded approach.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
const client = b.addSharedLibrary("client", "src/client/zig/client_main.zig", .unversioned);
client.setTarget(.{
    .cpu_arch = .wasm32,
    .os_tag = .freestanding,
});
client.addPackagePath("shared", "src/shared/index.zig");

const server_options = b.addOptions();
server_options.addOptionArtifact("client_wasm_path", client);
server_options.addOption(u32, "mem_leak_frames", mem_leak_frames);
server_options.addOption(bool, "support_mp3", support_mp3);

const server = b.addExecutable("groovebasin", "src/server/server_main.zig");
server.setTarget(target);
server.setBuildMode(mode);
server.addPackagePath("shared", "src/shared/index.zig");

server.addOptions("build_options", server_options);
server.install();
```

--------------------------------

TITLE: Zig: Fix @bytesToSlice on Packed Struct
DESCRIPTION: Addresses an issue with `@bytesToSlice` when used on packed structs, ensuring correct behavior for memory slicing operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
fix @bytesToSlice on a packed struct ([#1551](https://github.com/ziglang/zig/issues/1551))
```

--------------------------------

TITLE: Zig Anonymous Struct Literal Coercion
DESCRIPTION: Shows how anonymous struct literals can be used and coerced into existing struct types without explicit copying, simplifying initialization.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    const expect = std.testing.expect;
    
    const Point = struct {x: i32, y: i32};
    
    test "anonymous struct literal" {
        var pt: Point = .{ 
            .x = 13,
            .y = 67,
        };
        expect(pt.x == 13);
        expect(pt.y == 67);
    }

    $ zig test struct_result.zig
    1/1 test "anonymous struct literal"... OK
    All 1 tests passed.
    
```

--------------------------------

TITLE: Zig Bitwise XOR Operator
DESCRIPTION: Performs a bitwise XOR operation. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
a ^ b
a ^=
b
```

--------------------------------

TITLE: Zig Bitwise AND
DESCRIPTION: Performs a bitwise AND operation. It invokes Peer Type Resolution for the operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig: Array Literal Syntax Update
DESCRIPTION: Details the change in array literal syntax for size inference. The new syntax `[_]T{...}` is clearer than the old `[]T{...}` which resembled slice instantiation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
[]_]i32{1, 2, 3}
```

--------------------------------

TITLE: Zig Build System: Integrate System Library vs. Fetch Dependency
DESCRIPTION: Shows a Zig build.zig modification to conditionally link against a system library ('groove') or a fetched dependency. The `b.systemIntegrationOption` function checks if a system integration is enabled, defaulting to linking the system library if true, otherwise falling back to the standard dependency fetching and linking process.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: zig
CODE:
```
---
a/build.zig
+++
@@ -5,18 +5,8 @@
         const optimize = b.standardOptimizeOption(.{
             .preferred_optimize_mode = .ReleaseSafe,
         });
-    const libgroove_optimize_mode = b.option(
-        std.builtin.OptimizeMode,
-        "libgroove-optimize",
-        "override optimization mode of libgroove and its dependencies",
-    );
         const use_llvm = b.option(bool, "use-llvm", "LLVM backend");
 
-    const groove_dep = b.dependency("groove", .{
-        .optimize = libgroove_optimize_mode orelse .ReleaseFast,
-        .target = target,
-    });
-
         b.installDirectory(.{
             .source_dir = .{"path" = "public"},
             .install_dir = .lib,
@@ -31,7 +21,22 @@
             .use_llvm = use_llvm,
             .use_lld = use_llvm,
         });
-    server.linkLibrary(groove_dep.artifact("groove"));
+    
+    if (b.systemIntegrationOption("groove", .{})) {
+        server.linkSystemLibrary("groove");
+    } else {
+        const libgroove_optimize_mode = b.option(
+            std.builtin.OptimizeMode,
+            "libgroove-optimize",
+            "override optimization mode of libgroove and its dependencies",
+        );
+        const groove_dep = b.dependency("groove", .{
+            .optimize = libgroove_optimize_mode orelse .ReleaseFast,
+            .target = target,
+        });
+        server.linkLibrary(groove_dep.artifact("groove"));
+    }
+
         b.installArtifact(server);
 
         const run_cmd = b.addRunArtifact(server);

```

--------------------------------

TITLE: Zig: Constructing Slice at Comptime (Fixed)
DESCRIPTION: Presents the corrected Zig code for constructing a slice at comptime. By promoting the computed buffer to a `const` after population, the data has an infinite lifetime and avoids runtime issues.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: zig
CODE:
```
fn getName() []const u8 {
    comptime var buf: [9]u8 = undefined;
    @memcpy(&buf, "some name");
    const final_name = buf;
    return &final_name;
}

test getName {
    try @import("std").testing.expectEqualStrings("some name", getName());
}
```

--------------------------------

TITLE: Zig `printValue` Implementation: Type Handling
DESCRIPTION: This Zig code snippet illustrates the `printValue` function, which is responsible for handling different data types passed to the `print` function. It uses a `switch` statement on the type information to delegate to specific write functions for integers, floats, and pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
const Writer = struct {
    pub fn printValue(self: *Writer, value: anytype) !void {
        switch (@typeInfo(@TypeOf(value))) {
            .Int => {
                return self.writeInt(value);
            },
            .Float => {
                return self.writeFloat(value);
            },
            .Pointer => {
                return self.write(value);
            },
            else => {
                @compileError("Unable to print type '" ++ @typeName(@TypeOf(value)) ++ "'");
            },
        }
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
};
```

--------------------------------

TITLE: Zig: Base-2 Exponential with @exp2
DESCRIPTION: Details the `@exp2` function for computing the base-2 exponential of floating-point numbers and vectors. It highlights hardware acceleration and potential implementation gaps for certain float types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html



--------------------------------

TITLE: Zig Builtin: @atomicStore
DESCRIPTION: The `@atomicStore` builtin function performs an atomic store operation, writing a value to a memory location atomically.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
var flag: atomic(bool) = false;
@atomicStore(&flag, true, .SeqCst);
// flag is now atomically set to true
```

--------------------------------

TITLE: Add Missing Bitcast for Var Ptr Rendering
DESCRIPTION: Includes a missing bitcast operation when rendering variable pointers, addressing an issue identified in issue #7250.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Add missing bitcast when rendering var ptr ([#7250](https://github.com/ziglang/zig/issues/7250)).
```

--------------------------------

TITLE: Zig Anonymous Struct Literal with Type
DESCRIPTION: Demonstrates creating an anonymous struct literal and coercing it to a defined struct type 'Point'. It shows how the literal instantiates the result location directly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

const Point = struct { x: i32, y: i32 };

test "anonymous struct literal" {
    const pt: Point = .{ 
        .x = 13,
        .y = 67,
    };
    try expect(pt.x == 13);
    try expect(pt.y == 67);
}
```

--------------------------------

TITLE: Naked Function Entry Point Example
DESCRIPTION: Demonstrates the updated structure for naked functions in Zig 0.11.0. It shows how to replace the explicit `unreachable` statement with a jump to the actual start of the function, aligning with the new restrictions on naked functions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
pub export fn _start() callconv(.Naked) noreturn {
    asm volatile (
        "push %rbp\n"
        "jmp %[start:P]"
        : 
        : [start] "X" (&start)
    );
    unreachable;
}

fn start() void {}

```

--------------------------------

TITLE: Zig Built-in Function: @bytesToSlice
DESCRIPTION: Demonstrates the @bytesToSlice builtin function in Zig, which converts a byte array into a slice of a specified type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() {
    var bytes: [4]u8 = [_]u8{ 1, 2, 3, 4 };
    const slice = @bytesToSlice(u32, &bytes) catch unreachable;
    std.debug.print("Slice value: {d}\n", .{slice[0]});
}
```

--------------------------------

TITLE: Store Target Info in LLVM Module
DESCRIPTION: Stores target information within the LLVM module for each function. This is necessary for LLVM and libLTO to utilize user-specified target options, as discussed in issue #8803.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Store target info in the LLVM module for every function. This is needed to let LLVM (or, better, libLTO) produce code using the target options specified by the user ([#8803](https://github.com/ziglang/zig/issues/8803)).
```

--------------------------------

TITLE: Zig Self-Hosted Linker Support
DESCRIPTION: The self-hosted linker now supports ELF and COFF formats, removing the dependency on LLD.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
https://lld.llvm.org/
```

--------------------------------

TITLE: macOS: Expose ptrace syscall with errno handling
DESCRIPTION: This update exposes the ptrace syscall on macOS, including proper errno handling. This allows for more robust debugging and process introspection capabilities within Zig applications targeting macOS.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.os.ptrace
```

--------------------------------

TITLE: Zig: `DynamicBitSet` Iterator Modification
DESCRIPTION: Changes the `DynamicBitSet.iterator` to accept `self` by const, improving immutability and potentially performance for iterator operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
`DynamicBitSet.iterator` takes self as const.
```

--------------------------------

TITLE: Type Metadata and Field Handling
DESCRIPTION: This snippet covers improvements related to type metadata and handling of comptime fields. It includes making `meta.alignment` work on more types and fixing logic for duplicate comptime fields.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
fix logic for duplicate comptime fields and avoid freeing comptime fields in parseFree and parseInternal
fix duplicate_field_behavior UseFirst in json.zig
meta.Elem: support all optional types
Make meta.alignment work on more types
```

--------------------------------

TITLE: Zig Function for Base64 Decoding with C ABI
DESCRIPTION: Provides a Zig function signature for decoding base64, designed to be exported with the C ABI. It specifies pointer types for destination and source buffers and their lengths, adhering to C conventions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const base64 = @import("std").base64;

export fn decode_base_64(
    dest_ptr: [*]u8,
    dest_len: usize,
    source_ptr: [*]const u8,
    source_len: usize,
) usize {
    const src = source_ptr[0..source_len];

```

--------------------------------

TITLE: Zig Atomic Store Operation
DESCRIPTION: Atomically stores a given value to a pointer. It dereferences the pointer and writes the value. Supported types are similar to @atomicRmw. Requires specifying the memory ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
comptime T: type, ptr: *T, value: T, comptime ordering: AtomicOrder) void
```

--------------------------------

TITLE: Zig: Inline Switch for Runtime to Compile-time Values
DESCRIPTION: Illustrates the use of the 'inline' keyword with 'switch' statements in Zig. This allows a runtime-known value to be treated as a compile-time-known value, enabling features like comptime array sizing.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var arena_instance = std.heap.ArenaAllocator.init(std.heap.page_allocator);
    const arena = arena_instance.allocator();
    const args = try std.process.argsAlloc(arena);
    const arg = if (args.len >= 2) args[1] else "50";
    const some_number = try std.fmt.parseInt(i32, arg, 10);

    switch (some_number) {
        inline 1...100 => |x| {
            foo(x);
        },
        else => @panic("not in range"),
    }
}

fn foo(comptime x: i32) void {
    // Doing something that requires a comptime number, such as
    // using it as the length of an array:
    var array: [x]u8 = undefined;
    for (array) |*elem, i| {
        elem.* = @intCast(u8, i);
    }
}
```

--------------------------------

TITLE: Zig: Generic List Data Structure
DESCRIPTION: Illustrates how Zig's comptime enables generic data structures. The `List` function takes a type `T` as a comptime parameter and returns an anonymous struct representing a list of elements of type `T`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
    fn List(comptime T: type) type {
        return struct {
            items: []T,
            len: usize,
        };
    }
    
    // The generic List data structure can be instantiated by passing in a type:
    var buffer: [10]i32 = undefined;
    var list = List(i32){
        .items = &buffer,
        .len = 0,
    };
```

--------------------------------

TITLE: Zig: Convert Integer Address to Pointer and Vice Versa
DESCRIPTION: Demonstrates the use of @intToPtr to convert an integer address to a pointer and @ptrToInt to convert a pointer back to an integer. It also shows how to check the type and value of the converted integer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@ptrToInt and @intToPtr" {
    const ptr = @intToPtr(*i32, 0xdeadbee0);
    const addr = @ptrToInt(ptr);
    expect(@TypeOf(addr) == usize);
    expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Zig @intToPtr Function
DESCRIPTION: Converts an integer to a pointer. The destination type must be specified. To convert the other way, use @ptrToInt.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.2.0/index.html

LANGUAGE: zig
CODE:
```
@intToPtr(comptime DestType: type, int: usize) -> DestType
```

--------------------------------

TITLE: Add os.shutdown function for sockets
DESCRIPTION: Adds os.shutdown function for sockets, allowing for controlled shutdown of socket connections in Zig's os module.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Zig: Fix std.fmt.formatInt for Base Int Size
DESCRIPTION: Corrects the `std.fmt.formatInt` function to properly handle upcasting to the base integer size, ensuring accurate formatting across different integer types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
fix std.fmt.formatInt to handle upcasting to base int size
```

--------------------------------

TITLE: Zig: Pointer Slicing and Bounds Checking
DESCRIPTION: Demonstrates how to create a slice from an array using start and end indices and how slices provide bounds checking. The example shows that modifying an element through a slice correctly updates the original array, and implies that out-of-bounds access would be caught.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "pointer slicing" {
    var array = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
    var start: usize = 2; // var to make it runtime-known
    _ = &start; // suppress 'var is never mutated' error
    const slice = array[start..4];
    try expect(slice.len == 2);

    try expect(array[3] == 4);
    slice[1] += 1;
    try expect(array[3] == 5);
}
```

--------------------------------

TITLE: Zig: `printValue` Type Handling Implementation
DESCRIPTION: Details the `printValue` function within the Zig `print` implementation, which handles different data types. It uses a `switch` statement on the type information to dispatch to specific writing functions for integers, floats, and pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const Writer = struct {
    pub fn printValue(self: *Writer, value: anytype) !void {
        switch (@typeInfo(@TypeOf(value))) {
            .int => {
                return self.writeInt(value);
            },
            .float => {
                return self.writeFloat(value);
            },
            .pointer => {
                return self.write(value);
            },
            else => {
                @compileError("Unable to print type '" ++ @typeName(@TypeOf(value)) ++ "'");
            },
        }
    }

    fn write(self: *Writer, value: []const u8) !void {
        _ = self;
        _ = value;
    }
};
```

--------------------------------

TITLE: Zig: Inline Else for Type-Safe Switch Alternatives
DESCRIPTION: Illustrates using `inline else` prongs in Zig as a type-safe alternative to `inline for` loops. This approach ensures all possible cases of a union are handled, allowing the compiler to verify completeness, unlike `inline for` which requires an explicit `unreachable`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

const SliceTypeA = extern struct {
    len: usize,
    ptr: [*]u32,
};
const SliceTypeB = extern struct {
    ptr: [*]SliceTypeA,
    len: usize,
};
const AnySlice = union(enum) {
    a: SliceTypeA,
    b: SliceTypeB,
    c: []const u8,
    d: []AnySlice,
};

fn withFor(any: AnySlice) usize {
    const Tag = @typeInfo(AnySlice).Union.tag_type.?;
    inline for (@typeInfo(Tag).Enum.fields) |field| {
        // With `inline for` the function gets generated as
        // a series of `if` statements relying on the optimizer
        // to convert it to a switch.
        if (field.value == @intFromEnum(any)) {
            return @field(any, field.name).len;
        }
    }
    // When using `inline for` the compiler doesn't know that every
    // possible case has been handled requiring an explicit `unreachable`.
    unreachable;
}

fn withSwitch(any: AnySlice) usize {
    return switch (any) {
        // With `inline else` the function is explicitly generated
        // as the desired switch and the compiler can check that
        // every possible case is handled.
        inline else => |slice| slice.len,
    };
}

test "inline for and inline else similarity" {
    const any = AnySlice{ .c = "hello" };
    try expect(withFor(any) == 5);
    try expect(withSwitch(any) == 5);
}
```

--------------------------------

TITLE: ArrayList and Capacity Management
DESCRIPTION: This section details enhancements to the `ArrayList` data structure, including new functions for capacity management like `clearRetainingCapacity` and `clearAndFree`, and the acceptance of unaligned slices.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
ArrayList: add clearRetainingCapacity and clearAndFree
Accept unaligned slice in several ArrayListAligned ops. Do not impose the internal alignment requirements to the user-supplied parameters ([#8647](https://github.com/ziglang/zig/issues/8647)).
deprecate ensureCapacity, add two other capacity functions
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Performs a bitwise AND operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
a & b
a &= b

// Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig Fully Anonymous List Literal Behavior
DESCRIPTION: Explains how fully anonymous list literals (without a type specified) are treated as structs with numbered fields in Zig. Includes an example of accessing these fields.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous list literal" {
    try dump(.{ @as(u32, 1234), @as(f64, 12.34), true, "hi"});
}

fn dump(args: anytype) !void {
    try expect(args.@"0" == 1234);
    try expect(args.@"1" == 12.34);
    try expect(args.@"2");
    try expect(args.@"3"[0] == 'h');
    try expect(args.@"3"[1] == 'i');
}

```

LANGUAGE: shell
CODE:
```
$ zig test infer_list_literal.zig
1/1 test.fully anonymous list literal... OK
All 1 tests passed.

```

--------------------------------

TITLE: Zig: Struct with Methods (Dot Product)
DESCRIPTION: Defines a `Vec3` struct in Zig with `x`, `y`, and `z` fields, including an `init` constructor and a `dot` method for calculating the dot product. Includes a test case for the dot product functionality.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
// Structs can have methods
// Struct methods are not special, they are only namespaced
// functions that you can call with dot syntax.
const Vec3 = struct {
    x: f32,
    y: f32,
    z: f32,

    pub fn init(x: f32, y: f32, z: f32) Vec3 {
        return Vec3 {
            .x = x,
            .y = y,
            .z = z,
        };
    }

    pub fn dot(self: Vec3, other: Vec3) f32 {
        return self.x * other.x + self.y * other.y + self.z * other.z;
    }
};

const expect = @import("std").testing.expect;
test "dot product" {

```

--------------------------------

TITLE: WASI Runtime Execution: WASMTIME Example
DESCRIPTION: Example of running a WASI-compiled WebAssembly module (`args.wasm`) using the `wasmtime` runtime, passing command line arguments. This demonstrates how the WASI environment receives and processes arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: shell
CODE:
```
wasmtime args.wasm 123 hello
```

--------------------------------

TITLE: Zig: UEFI boot_services locateDevicePath
DESCRIPTION: Implements the `locateDevicePath` function within the UEFI boot services module. This function is used to find devices based on their path information.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
UEFI: boot_services: implement locateDevicePath
```

--------------------------------

TITLE: Zig Noreturn with ExitProcess
DESCRIPTION: Shows how to use the 'noreturn' type with external functions like 'ExitProcess' from kernel32, demonstrating error handling with 'catch ExitProcess'.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const builtin = @import("builtin");
const native_arch = builtin.cpu.arch;
const expect = std.testing.expect;

const WINAPI: std.builtin.CallingConvention = if (native_arch == .i386) .Stdcall else .C;
extern "kernel32" fn ExitProcess(exit_code: c_uint) callconv(WINAPI) noreturn;

test "foo" {
    const value = bar() catch ExitProcess(1);
    try expect(value == 1234);
}

fn bar() anyerror!u32 {
    return 1234;
}
```

--------------------------------

TITLE: Zig: Peer Resolve Array and Slice Conversion
DESCRIPTION: Illustrates peer type resolution for arrays and slices, showing how they can be unified into a common type, such as a const slice. Includes examples for both runtime and compile-time.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;
const mem = std.mem;

test "peer resolve arrays of different size to const slice" {
    assert(mem.eql(u8, boolToStr(true), "true"));
    assert(mem.eql(u8, boolToStr(false), "false"));
    comptime assert(mem.eql(u8, boolToStr(true), "true"));
    comptime assert(mem.eql(u8, boolToStr(false), "false"));
}
fn boolToStr(b: bool) []const u8 {
    return if (b) "true" else "false";
}

test "peer resolve array and const slice" {
    testPeerResolveArrayConstSlice(true);
    comptime testPeerResolveArrayConstSlice(true);
}
fn testPeerResolveArrayConstSlice(b: bool) void {
    const value1 = if (b) "aoeu" else ([]const u8)("zz");
    const value2 = if (b) ([]const u8)("zz") else "aoeu";
    assert(mem.eql(u8, value1, "aoeu"));
    assert(mem.eql(u8, value2, "zz"));
}
```

--------------------------------

TITLE: Zig: Use volatile for Memory Mapped I/O
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig to ensure memory-mapped I/O operations are performed as written and in order. This is crucial for interacting with hardware registers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr = @intToPtr(*volatile u8, 0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Fully Anonymous Struct with Type Inference
DESCRIPTION: Illustrates a fully anonymous struct literal where Zig infers the type because the result location does not include a type. The struct contains integer, float, boolean, and string fields.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous struct" {
    try check(.{
        .int = @as(u32, 1234),
        .float = @as(f64, 12.34),
        .b = true,
        .s = "hi",
    });
}

fn check(args: anytype) !void {
    try expect(args.int == 1234);
    try expect(args.float == 12.34);
    try expect(args.b);
    try expect(args.s[0] == 'h');
    try expect(args.s[1] == 'i');
}
```

--------------------------------

TITLE: Zig: Using Namespace with Standard Library
DESCRIPTION: Demonstrates the 'usingnamespace' feature in Zig by importing the entire standard library into a struct's namespace. This allows direct access to standard library components like testing functions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
test "using std namespace" {
    const S = struct {
        usingnamespace @import("std");
    };
    try S.testing.expect(true);
}
```

--------------------------------

TITLE: Zig: Pointer Slicing and Bounds Checking
DESCRIPTION: Demonstrates how to create a slice from an array using start and end indices and how slices provide bounds checking. The example shows that modifying an element through a slice correctly updates the original array, and implies that out-of-bounds access would be caught.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "pointer slicing" {
    var array = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
    var start: usize = 2; // var to make it runtime-known
    _ = &start; // suppress 'var is never mutated' error
    const slice = array[start..4];
    try expect(slice.len == 2);

    try expect(array[3] == 4);
    slice[1] += 1;
    try expect(array[3] == 5);
}
```

--------------------------------

TITLE: Zig @fence: Introduce Happens-Before Edges
DESCRIPTION: Introduces happens-before edges between operations based on the specified atomic order. AtomicOrder is available via @import("builtin").AtomicOrder.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: zig
CODE:
```
    @fence(order: AtomicOrder)

The `fence` function is used to introduce happens-before edges between operations.

`AtomicOrder` can be found with `@import("builtin").AtomicOrder`.
```

--------------------------------

TITLE: Zig: Use volatile for Memory Mapped I/O
DESCRIPTION: Demonstrates the use of the `volatile` keyword in Zig to ensure memory-mapped I/O operations are performed as written and in order. This is crucial for interacting with hardware registers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr = @intToPtr(*volatile u8, 0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Basic Slices
DESCRIPTION: This Zig code demonstrates basic slice operations. It shows how to create a slice from an array, access elements using slice indexing, and highlights the difference between a slice's pointer field and the address-of operator. It also illustrates Zig's array bounds checking, which causes a runtime error if an out-of-bounds index is accessed.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const assert = @import("std").debug.assert;

test "basic slices" {
    var array = [_]i32{ 1, 2, 3, 4 };
    const slice = array[0..array.len];
    assert(&slice[0] == &array[0]);
    assert(slice.len == array.len);
    assert(@typeOf(slice.ptr) == [*]i32);
    assert(@typeOf(&slice[0]) == *i32);
    assert(@ptrToInt(slice.ptr) == @ptrToInt(&slice[0]));
    slice[10] += 1;
}
```

--------------------------------

TITLE: Zig: Slice ([]T)
DESCRIPTION: Details the Zig syntax for slices, '[]T', which represent a pointer to a runtime-known number of items. Slices support indexing, slicing, and accessing the 'len' property.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
[]T - pointer to runtime-known number of items.
    * Supports index syntax: slice[i]
    * Supports slice syntax: slice[start..end]
    * Supports len property: slice.len

```

--------------------------------

TITLE: Zig Integers: Literals and Runtime Values
DESCRIPTION: Explains how to define integer literals in Zig and work with integer values at runtime, covering different integer types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
const signed_int = -42;
const unsigned_int = 100u;
const sized_int = i64(50);
const hex_int = 0x1A;
const binary_int = 0b1010;

```

--------------------------------

TITLE: Zig Bitwise And Operator
DESCRIPTION: Performs bitwise AND operation on integers. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig Volatile Pointers
DESCRIPTION: Explains the use of the `volatile` keyword in Zig for memory-mapped I/O (MMIO) or other scenarios where loads and stores must not be optimized away. It demonstrates creating a volatile pointer and asserts its type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.2.0/index.html

LANGUAGE: Zig
CODE:
```
test "volatile" {
        // In Zig, loads and stores are assumed to not have side effects.
        // If a given load or store should have side effects, such as
        // Memory Mapped Input/Output (MMIO), use `volatile`:
        const mmio_ptr = @intToPtr(&volatile u8, 0x12345678);
    
        // Now loads and stores with mmio_ptr are guaranteed to all happen
        // and in the same order as in source code.
        assert(@typeOf(mmio_ptr) == &volatile u8);
    }
```

--------------------------------

TITLE: Fix BigInt Add Failures with Aliasing
DESCRIPTION: Resolves issues with the bigint add operation when aliasing occurs, ensuring correct results for arbitrary-precision integer addition.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Fix bigint add failures with aliasing ([#8330](https://github.com/ziglang/zig/issues/8330)).
```

--------------------------------

TITLE: Zig @atomicStore
DESCRIPTION: Atomically stores a value to a pointer. This built-in function guarantees that the write operation is atomic, crucial for concurrent data structures and algorithms. It supports boolean, float, integer, and enum types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
    @atomicStore(comptime T: type, ptr: *T, value: T, comptime ordering: builtin.AtomicOrder) void
```

--------------------------------

TITLE: Zig Noreturn with ExitProcess
DESCRIPTION: Shows how to use the 'noreturn' type with external functions like 'ExitProcess' from kernel32, demonstrating error handling with 'catch ExitProcess'.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const builtin = @import("builtin");
const native_arch = builtin.cpu.arch;
const expect = std.testing.expect;

const WINAPI: std.builtin.CallingConvention = if (native_arch == .i386) .Stdcall else .C;
extern "kernel32" fn ExitProcess(exit_code: c_uint) callconv(WINAPI) noreturn;

test "foo" {
    const value = bar() catch ExitProcess(1);
    try expect(value == 1234);
}

fn bar() anyerror!u32 {
    return 1234;
}
```

--------------------------------

TITLE: Zig Integer Bitwise AND
DESCRIPTION: Performs a bitwise AND operation on integers. Invokes peer type resolution.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
a & b
a &= b

// Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Operating System and POSIX Wrappers
DESCRIPTION: This snippet covers improvements related to operating system interactions and POSIX wrappers. It includes fixes for `munmap`, `WSAStartup` calls, `atfork` handler robustness, and handling specific error codes like `ECONNRESET`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
os: munmap takes a const pointer
os: WSAStartup is now called upon socket creation when needed
Make atfork handler more robust ([#8841](https://github.com/ziglang/zig/issues/8841)).
Call pthread_atfork only once
add android __SIZEOF_PTHREAD_MUTEX_T ([#8384](https://github.com/ziglang/zig/issues/8384))
add missing EBADF error code for openat
dragonfly: fix duplicate definition of sockaddr_storage
dragonfly: fix duplicate definition of sockaddr_storage
os: add missing sockaddr_storage defs
os: fix sockaddr_storage padding size
Handle EPERM and ELOOP in os.fstatat()
thread: simplify and remove useless return in spawn ([#8621](https://github.com/ziglang/zig/issues/8621))
Fix thread creation with field-less context type ([#8524](https://github.com/ziglang/zig/issues/8524)).
Split syscall parameters for PowerPC targets
os/posix: handle ECONNRESET for write/writev
Add process_vm_readv/writev wrappers
Add pidfd wrappers
linux: fix number of arguments for tgkill syscall
```

--------------------------------

TITLE: Resolve usingnamespace for @typeInfo
DESCRIPTION: Fixes an issue in stage1 where `usingnamespace` declarations were not correctly resolved when calling `@typeInfo`, ensuring accurate type information retrieval.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: zig
CODE:
```
usingnamespace MyNamespace;
const info = @typeInfo(MyType);
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Performs a bitwise AND operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
a & b
a &= b

// Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Fix Zig Switch with Null and T Peer Types
DESCRIPTION: Resolves issues with `switch` statements involving `null` and `T` peer types, including inferred result location types. This ensures correct control flow and type handling in complex switch cases.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
/* Fixed {#syntax#}switch{#endsyntax#} with {#syntax#}null{#endsyntax#} and T peer types and inferred result location type. [#2762](https://github.com/ziglang/zig/issues/2762) */
```

--------------------------------

TITLE: Zig: Print 'Hello, world!' using std.debug.print
DESCRIPTION: This Zig code snippet demonstrates how to print 'Hello, world!' to standard output using the std.debug.print function. It imports the print function from the standard library's debug module and defines a main function that executes the print statement. The second argument to print is an empty anonymous struct literal, indicating no additional arguments for formatting.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
const print = @import("std").debug.print;

pub fn main() void {
    print("Hello, world!\n", .{});
}
```

--------------------------------

TITLE: Zig Builtin: @bitOffsetOf
DESCRIPTION: Demonstrates the `@bitOffsetOf` builtin function in Zig, which returns the bit offset of a field within a struct. This is useful for manual memory layout manipulation and serialization.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

const MyStruct = struct {
    a: u8,
    b: u16,
};

pub fn main() void {
    const offset_b = @bitOffsetOf(MyStruct, "b");
    std.debug.print("Bit offset of field 'b': {d}", .{offset_b});
}

```

--------------------------------

TITLE: Zig @atomicLoad Example
DESCRIPTION: Provides an example of using the @atomicLoad builtin function in Zig for atomic memory operations. It demonstrates how to atomically dereference a pointer to a type T and retrieve its value, specifying the memory ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
@atomicLoad(comptime T: type, ptr: *const T, comptime ordering: AtomicOrder) T
```

--------------------------------

TITLE: Zig Generics with Compile-Time Parameters
DESCRIPTION: Demonstrates a generic `max` function using `comptime` parameters for different types (f32, u64). This showcases compile-time duck typing in Zig.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: zig
CODE:
```
fn max(comptime T: type, a: T, b: T) T {
    return if (a > b) a else b;
}
fn gimmeTheBiggerFloat(a: f32, b: f32) f32 {
    return max(f32, a, b);
}
fn gimmeTheBiggerInteger(a: u64, b: u64) u64 {
    return max(u64, a, b);
}
```

--------------------------------

TITLE: POSIX Terminals Progress Indicator
DESCRIPTION: For POSIX-compliant terminals, a progress indicator has been implemented to provide visual feedback during long compilation processes. This helps users monitor the build status and estimate completion time.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: shell
CODE:
```
# During a long compilation, a progress indicator will be displayed.
# Example output:
# [=====>.............] 25% compiled
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Performs a bitwise AND operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: Zig
CODE:
```
a & b
a &= b

// Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Shell: Test Inline Switch for Type Information
DESCRIPTION: Executes the Zig test file that demonstrates the inline switch functionality for checking optional fields using `@typeInfo`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: shell
CODE:
```
$ zig test test_inline_switch.zig
1/1 test.using @typeInfo with runtime values... OK
All 1 tests passed.
```

--------------------------------

TITLE: Zig: Peer Type Resolution for Empty Array and Slice
DESCRIPTION: Explains peer type resolution involving an empty array ('[0]u8') and a slice ('[]const u8'). It shows how Zig can unify these into a common slice type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;

test "peer type resolution: [0]u8 and []const u8" {
    assert(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
    assert(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    comptime {
        assert(peerTypeEmptyArrayAndSlice(true, "hi").len == 0);
        assert(peerTypeEmptyArrayAndSlice(false, "hi").len == 1);
    }
}
fn peerTypeEmptyArrayAndSlice(a: bool, slice: []const u8) []const u8 {
    if (a) {
        return []const u8{};
    }

    return slice[0..1];
}
```

--------------------------------

TITLE: Zig Hello World with Stdout
DESCRIPTION: A basic Zig program that prints 'Hello, world!' to standard output. It demonstrates importing the standard library and handling potential I/O errors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    // If this program is run without stdout attached, exit with an error.
    const stdout_file = try std.io.getStdOut();
    // If this program encounters pipe failure when printing to stdout, exit
    // with an error.
    try stdout_file.write("Hello, world!\n");
}
```

--------------------------------

TITLE: Zig Generics with Compile-Time Parameters
DESCRIPTION: Demonstrates a generic `max` function using `comptime` parameters for type-agnostic comparison. It shows how to use this generic function with different types like `f32` and `u64`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
fn max(comptime T: type, a: T, b: T) T {
    return if (a > b) a else b;
}
fn gimmeTheBiggerFloat(a: f32, b: f32) f32 {
    return max(f32, a, b);
}
fn gimmeTheBiggerInteger(a: u64, b: u64) u64 {
    return max(u64, a, b);
}
```

--------------------------------

TITLE: Zig: Inline Assembly for Hello World
DESCRIPTION: Illustrates the use of inline assembly in Zig to perform system calls on x86_64 Linux. This example implements a 'Hello, World!' program by directly invoking `write` and `exit` system calls.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
pub fn main() noreturn {
    const msg = "hello world\n";
    _ = syscall3(SYS_write, STDOUT_FILENO, @ptrToInt(msg), msg.len);
    _ = syscall1(SYS_exit, 0);
    unreachable;
}

pub const SYS_write = 1;
pub const SYS_exit = 60;

pub const STDOUT_FILENO = 1;

pub fn syscall1(number: usize, arg1: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize)
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1)
        : "rcx", "r11"
    );
}

pub fn syscall3(number: usize, arg1: usize, arg2: usize, arg3: usize) usize {
    return asm volatile ("syscall"
        : [ret] "={rax}" (-> usize)
        : [number] "{rax}" (number),
          [arg1] "{rdi}" (arg1),
          [arg2] "{rsi}" (arg2),
          [arg3] "{rdx}" (arg3)
        : "rcx", "r11"
    );
}
```

--------------------------------

TITLE: Zig Builtin: @bitOffsetOf
DESCRIPTION: The `@bitOffsetOf` builtin function returns the bit offset of a field within a struct.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
const MyStruct = struct {
    a: u8,
    b: u16,
};
const offset_b = @bitOffsetOf(MyStruct, "b");
// offset_b is 8 (since 'a' is 8 bits)
```

--------------------------------

TITLE: Zig: Implicit @intCast for @intToEnum
DESCRIPTION: Explains a backward-compatible change where `@intToEnum` now implicitly performs `@intCast`. This simplifies calls by allowing any integer operand to be passed, removing the need for explicit `@intCast` in many cases.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const Tag = enum(u8) {
    A,
    B,
};

const number: u32 = 1;

// Old way:
// return @intToEnum(Tag, @intCast(@typeInfo(Tag).Enum.tag_type, number));

// New way:
return @intToEnum(Tag, number);
```

--------------------------------

TITLE: Zig builtin @bytesToSlice
DESCRIPTION: Illustrates the `@bytesToSlice` built-in function in Zig, converting a byte array to a slice.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const bytes: [4]u8 = .{1, 2, 3, 4};
const slice = @bytesToSlice(u8, bytes[0..]);
```

--------------------------------

TITLE: Fix os.rusage Linking with C on Linux
DESCRIPTION: Corrects an issue with `os.rusage` when linking with the C library on Linux. This ensures that resource usage information can be correctly retrieved and linked.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Fixed `os.rusage` when linking with c library on Linux
```

--------------------------------

TITLE: Zig Explicitly Annotate Result Type with @as
DESCRIPTION: Shows how to explicitly define the result type of a cast operation using the @as builtin, which is a fallback when type inference is not sufficient or desired.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
test "explicitly annotate result type with @as" {
  const E = enum(u8) {
    a,
    b
  };
  const x: u8 = 1;
  _ = @as(E, @enumFromInt(x));
}
```

--------------------------------

TITLE: Zig: Anonymous union literal syntax
DESCRIPTION: Shows how to use anonymous struct literal syntax in Zig to initialize unions without explicitly stating the type. Includes a test case to verify initialization.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

const Number = union {
    int: i32,
    float: f64,
};

test "anonymous union literal syntax" {
    var i: Number = .{.int = 42};
    var f = makeNumber();
    try expect(i.int == 42);
    try expect(f.float == 12.34);
}

fn makeNumber() Number {
    return .{.float = 12.34};
}
```

--------------------------------

TITLE: Zig Struct Naming Inference
DESCRIPTION: Demonstrates how Zig infers type names for anonymous structs based on their context, such as variable initialization, return expressions, or default anonymous naming.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    
    pub fn main() void {
        const Foo = struct {};
        std.debug.print("variable: {}\n", .{@typeName(Foo)});
        std.debug.print("anonymous: {}\n", .{@typeName(struct {})});
        std.debug.print("function: {}\n", .{@typeName(List(i32))});
    }
    
    fn List(comptime T: type) type {
        return struct {
            x: T,
        };
    }

    $ zig build-exe struct_name.zig
    $ ./struct_name
    variable: Foo
    anonymous: struct:6:52
    function: List(i32)
    
```

--------------------------------

TITLE: Zig Function Reflection Example
DESCRIPTION: Demonstrates function reflection in Zig by inspecting the type information of the `expect` function. It checks the argument type and whether the function accepts variable arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "fn reflection" {
    try expect(@typeInfo(@TypeOf(expect)).Fn.args[0].arg_type.? == bool);
    try expect(@typeInfo(@TypeOf(expect)).Fn.is_var_args == false);
}
```

--------------------------------

TITLE: Zig Builtin: @bitOffsetOf
DESCRIPTION: Demonstrates how to get the bit offset of a field within a struct using `@bitOffsetOf`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    const MyStruct = struct {
        a: u8,
        b: u16,
    };
    const offset_b = @bitOffsetOf(MyStruct, .b);
    std.debug.print("Bit offset of field 'b': {d}\\n", .{offset_b});
}
```

--------------------------------

TITLE: Zig: Linux IPv6 socket options
DESCRIPTION: Adds support for IPv6 socket options within the Linux operating system module. This allows for more granular control over IPv6 network communication.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
os/bits/linux: add IPv6 socket options
```

--------------------------------

TITLE: Zig: Add Compile Error for Slice.*.len
DESCRIPTION: Implements a compile-time error check for accessing the `.len` property on slices of undefined types, improving compile-time safety.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
add compile error for slice.*.len ([#1372](https://github.com/ziglang/zig/issues/1372))
```

--------------------------------

TITLE: Zig Bitwise And Operator
DESCRIPTION: Performs bitwise AND operation on integers. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig WASI: Populate and List Preopens
DESCRIPTION: Shows how to populate and iterate through preopened file descriptors in Zig for WASI environments using `std.fs.wasi.PreopenList`. This allows interaction with the host filesystem.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const PreopenList = std.fs.wasi.PreopenList;

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();

    var preopens = PreopenList.init(gpa);
    defer preopens.deinit();

    try preopens.populate();

    for (preopens.asSlice()) |preopen, i| {
        std.debug.print("{}: {}\n", .{ i, preopen });
    }
}
```

--------------------------------

TITLE: Test Fully Anonymous Struct with Type Inference
DESCRIPTION: Shell command to test the Zig code example for a fully anonymous struct where the type is inferred.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: shell
CODE:
```
$ zig test test_anonymous_struct.zig
1/1 test_anonymous_struct.test.fully anonymous struct... OK
All 1 tests passed.
```

--------------------------------

TITLE: Zig String Literals and Slice Manipulation
DESCRIPTION: Demonstrates how Zig treats string literals as null-terminated byte arrays and how to use slice syntax to manipulate them. It includes an example of string concatenation using fmt.bufPrint.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const fmt = std.fmt;
const mem = std.mem;
const expect = std.testing.expect;

test "using slices for strings" {
        // Zig has no concept of strings. String literals are const pointers
        // to null-terminated arrays of u8, and by convention parameters
        // that are "strings" are expected to be UTF-8 encoded slices of u8.
        // Here we coerce *const [5:0]u8 and *const [6:0]u8 to []const u8
        const hello: []const u8 = "hello";
        const world: []const u8 = "ä¸–ç•Œ";
    
        var all_together: [100]u8 = undefined;
        // You can use slice syntax on an array to convert an array into a slice.
        const all_together_slice = all_together[0..];
        // String concatenation example.
        const hello_world = try fmt.bufPrint(all_together_slice, "{s} {s}", .{ hello, world });
    
        // Generally, you can use UTF-8 and not worry about whether something is a
        // string. If you don't need to deal with individual characters, no need
        // to decode.
        try expect(mem.eql(u8, hello_world, "hello ä¸–ç•Œ"));
    }
    
test "slice pointer" {
        var array: [10]u8 = undefined;
        const ptr = &array;
    
        // You can use slicing syntax to convert a pointer into a slice:
        const slice = ptr[0..5];
        slice[2] = 3;
        try expect(slice[2] == 3);
        // The slice is mutable because we sliced a mutable pointer.
        // Furthermore, it is actually a pointer to an array, since the start
        // and end indexes were both comptime-known.
        try expect(@TypeOf(slice) == *[5]u8);
    
        // You can also slice a slice:
        const slice2 = slice[2..3];
        try expect(slice2.len == 1);
        try expect(slice2[0] == 3);
    }
```

--------------------------------

TITLE: Empty Switch Compile Error and Channel Buffer Wrapping
DESCRIPTION: A compile error is now emitted for an empty switch on an integer. Also, correct buffer wrapping logic is implemented in std.event.Channel.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

var channel = std.event.Channel(i32).init(std.heap.page_allocator, 10);
channel.sendSync(1);
const value = channel.receiveSync();
```

--------------------------------

TITLE: Zig: Type-Safe Union Iteration with `inline else`
DESCRIPTION: Compares Zig's `inline for` with `inline else` for iterating over union fields. `inline else` provides a type-safe alternative, ensuring all cases are handled, unlike `inline for` which requires an explicit `unreachable`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

const SliceTypeA = extern struct {
    len: usize,
    ptr: [*]u32,
};
const SliceTypeB = extern struct {
    ptr: [*]SliceTypeA,
    len: usize,
};
const AnySlice = union(enum) {
    a: SliceTypeA,
    b: SliceTypeB,
    c: []const u8,
    d: []AnySlice,
};

fn withFor(any: AnySlice) usize {
    const Tag = @typeInfo(AnySlice)."union".tag_type.?;
    inline for (@typeInfo(Tag)."enum".fields) |field| {
        // With `inline for` the function gets generated as
        // a series of `if` statements relying on the optimizer
        // to convert it to a switch.
        if (field.value == @intFromEnum(any)) {
            return @field(any, field.name).len;
        }
    }
    // When using `inline for` the compiler doesn't know that every
    // possible case has been handled requiring an explicit `unreachable`.
    unreachable;
}

fn withSwitch(any: AnySlice) usize {
    return switch (any) {
        // With `inline else` the function is explicitly generated
        // as the desired switch and the compiler can check that
        // every possible case is handled.
        inline else => |slice| slice.len,
    };
}

test "inline for and inline else similarity" {
    const any = AnySlice{ .c = "hello" };
    try expect(withFor(any) == 5);
    try expect(withSwitch(any) == 5);
}
```

--------------------------------

TITLE: Zig Struct Naming Inference
DESCRIPTION: Demonstrates how Zig infers type names for anonymous structs based on their context, such as variable initialization, return expressions, or default anonymous naming.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
    
    pub fn main() void {
        const Foo = struct {};
        std.debug.print("variable: {}\n", .{@typeName(Foo)});
        std.debug.print("anonymous: {}\n", .{@typeName(struct {})});
        std.debug.print("function: {}\n", .{@typeName(List(i32))});
    }
    
    fn List(comptime T: type) type {
        return struct {
            x: T,
        };
    }

    $ zig build-exe struct_name.zig
    $ ./struct_name
    variable: Foo
    anonymous: struct:6:52
    function: List(i32)
    
```

--------------------------------

TITLE: Fix tanh for Negative Inputs and NaN
DESCRIPTION: Corrects the `tanh` function to produce accurate results for negative inputs and also addresses the NaN codepath by adding missing logic. This rectifies issues stemming from an incorrect C port.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Fix tanh for negative inputs ([#9047](https://github.com/ziglang/zig/issues/9047)).  
    It turns out the code was not ported correctly from C and produced wrong results for negative input values. As a bonus fix the NaN codepath by adding yet another missing piece of code.
```

--------------------------------

TITLE: Zig Doc Comments for Structs and Functions
DESCRIPTION: Demonstrates the use of documentation comments (`///`) in Zig for documenting structs, their fields, and their methods. It shows how multiple `///` comments are merged into a multiline doc comment.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: Zig
CODE:
```
/// A structure for storing a timestamp, with nanosecond precision (this is a
/// multiline doc comment).
const Timestamp = struct {
    /// The number of seconds since the epoch (this is also a doc comment).
    seconds: i64,  // signed so we can represent pre-1970 (not a doc comment)
    /// The number of nanoseconds past the second (doc comment again).
    nanos: u32,

    /// Returns a `Timestamp` struct representing the Unix epoch; that is, the
    /// moment of 1970 Jan 1 00:00:00 UTC (this is a doc comment too).
    pub fn unixEpoch() Timestamp {
        return Timestamp{
            .seconds = 0,
            .nanos = 0,
        };
    }
};
```

--------------------------------

TITLE: Zig: Generic List Data Structure using Comptime
DESCRIPTION: Illustrates how Zig uses comptime to create generic data structures without special syntax. The `List` function takes a type `T` as a comptime parameter and returns an anonymous struct representing a list of elements of type `T`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
fn List(comptime T: type) type {
    return struct {
        items: []T,
        len: usize,
    };
}

// The generic List data structure can be instantiated by passing in a type:
var buffer: [10]i32 = undefined;
var list = List(i32){
    .items = &buffer,
    .len = 0,
};
```

--------------------------------

TITLE: Zig: Atomic Fence
DESCRIPTION: Introduces happens-before edges between operations. The 'order' parameter specifies the atomic memory ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
@fence(order: AtomicOrder) void
```

--------------------------------

TITLE: Zig Formatted Printing Enhancements
DESCRIPTION: This section details improvements to Zig's formatted printing capabilities, including better formatting for tuple types, clearer compile errors for unsupported format strings, support for printing slice strings, and fixes for float formatting. It also introduces case sensitivity for format functions and improved error messages for missing arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
#std.fmt: Better formatting of tuple types by skipping the useless type name and the numeric field names.
#Added name of type in unsupport format string compile error.
#Added support for printing slices strings
#Fixed float formatting for 0.0 when precision is 0
#Format functions take case as an enum.
#Improved error message when there are missing arguments.
#Added `fmt.fmtDurationSigned`.
#Fixed endless loop, working around a Bootstrap Compiler bug that may sometimes lead to an endless loop being generated when unrolling the fmt impl
```

--------------------------------

TITLE: Zig Function for Base64 Decoding with C ABI
DESCRIPTION: Provides a Zig function signature for decoding base64, designed to be exported with the C ABI. It specifies pointer types for destination and source buffers and their lengths, adhering to C conventions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
const base64 = @import("std").base64;

export fn decode_base_64(
    dest_ptr: [*]u8,
    dest_len: usize,
    source_ptr: [*]const u8,
    source_len: usize,
) usize {
    const src = source_ptr[0..source_len];

```

--------------------------------

TITLE: Zig Old Streams API Example
DESCRIPTION: Demonstrates reading lines from standard input and calculating a sum using the older Zig streams API. This code is provided as a reference for comparison with the new API.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() anyerror!void {
    var stdin_unbuf = std.io.getStdIn().inStream();
    // damn that is pretty painful, isn't it?
    const in = &std.io.BufferedInStream(@TypeOf(stdin_unbuf).Error).init(&stdin_unbuf.stream).stream;

    var sum: u64 = 0;
    var line_buf: [50]u8 = undefined;
    while (try in.readUntilDelimiterOrEof(&line_buf, '\n')) |line| {
        if (line.len == 0) break;
        const module_mass = try std.fmt.parseInt(u64, line, 10);
        const fuel_required = (module_mass / 3) - 2;
        sum += fuel_required;
    }

    const out = &std.io.getStdOut().outStream().stream;
    try out.print("{}\\n", .{sum});
}
```

--------------------------------

TITLE: Convert integer to pointer with @intToPtr
DESCRIPTION: The @intToPtr function converts an integer address to a pointer of a specified destination type. If the destination pointer type disallows address zero and the provided address is zero, it results in safety-checked Undefined Behavior. Use @ptrToInt for the reverse conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
@intToPtr(comptime DestType: type, address: usize) DestType
```

--------------------------------

TITLE: Zig Bitwise And Operator
DESCRIPTION: Performs bitwise AND operation on integers. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig Fully Anonymous Struct Literal
DESCRIPTION: Illustrates a fully anonymous struct literal where the type is inferred by Zig. The literal is passed to a generic 'check' function, demonstrating type inference and value validation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous struct" {
    try check(.{
        .int = @as(u32, 1234),
        .float = @as(f64, 12.34),
        .b = true,
        .s = "hi",
    });
}

fn check(args: anytype) !void {
    try expect(args.int == 1234);
    try expect(args.float == 12.34);
    try expect(args.b);
    try expect(args.s[0] == 'h');
    try expect(args.s[1] == 'i');
}
```

--------------------------------

TITLE: Fix alignment in std.Thread.Futex.PosixImpl.Address.from
DESCRIPTION: Ensures correct alignment for the Address.from method within std.Thread.Futex.PosixImpl.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.1/release-notes.html

LANGUAGE: Zig
CODE:
```
std.Thread.Futex.PosixImpl.Address.from: fix `alignment` type
```

--------------------------------

TITLE: Zig: Print 'Hello, world!' using std.debug.print
DESCRIPTION: This Zig code snippet demonstrates how to print 'Hello, world!' to standard output using the std.debug.print function. It imports the print function from the standard library's debug module and defines a main function that executes the print statement. The second argument to print is an empty anonymous struct literal, indicating no additional arguments for formatting.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
const print = @import("std").debug.print;

pub fn main() void {
    print("Hello, world!\n", .{});
}
```

--------------------------------

TITLE: Zig Translate-C: Dereference Pointers Implicitly
DESCRIPTION: The translation process now implicitly handles pointer dereferencing, removing the need for manual unwrapping and preventing potential bugs.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: C
CODE:
```
* Don't bother with unwrapping pointers. Dereferencing a c pointer implicitly includes an unwrap, manually adding it just causes bugs.
```

--------------------------------

TITLE: Zig Compiler Command Line Options for Code Emission
DESCRIPTION: A set of command-line flags for controlling the Zig compiler's output. These options allow users to specify whether to emit binary code, assembly, LLVM IR, or C header files, or to disable these emissions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
-femit-bin
```

LANGUAGE: Zig
CODE:
```
-fno-emit-bin
```

LANGUAGE: Zig
CODE:
```
-femit-asm
```

LANGUAGE: Zig
CODE:
```
-fno-emit-asm
```

LANGUAGE: Zig
CODE:
```
-femit-llvm-ir
```

LANGUAGE: Zig
CODE:
```
-fno-emit-llvm-ir
```

LANGUAGE: Zig
CODE:
```
-femit-h
```

LANGUAGE: Zig
CODE:
```
-fno-emit-h
```

--------------------------------

TITLE: GET /zig/<filename>
DESCRIPTION: Retrieves Zig tarballs from a community mirror. The mirror's behavior depends on the filename and the Zig version it represents.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/MIRRORS.md

LANGUAGE: APIDOC
CODE:
```
## GET /zig/<filename>

### Description
Fetches a Zig tarball from the mirror. The mirror determines whether to serve the file directly, respond with a 404, or fetch it from the official Zig repository based on the filename and version.

### Method
GET

### Endpoint
`X/<filename>`

Where `X` is the base URL of the mirror.

### Parameters
#### Path Parameters
- **filename** (string) - Required - The name of the tarball file to retrieve (e.g., `zig-0.14.1.tar.xz`, `zig-x86_64-linux-0.15.0-dev.671+c907866d5.tar.xz`).

#### Query Parameters
- **source** (string) - Optional - Identifies the origin of the request (e.g., `github-mlugg-setup-zig`).

### Request Example
```http
GET /zig/zig-0.14.1.tar.xz HTTP/1.1
Host: mirror.example.com
```

### Response
#### Success Response (200 OK)
- The requested tarball file.

#### Error Responses
- **404 Not Found**: If the filename does not match the expected schema or if the mirror chooses not to serve a specific version (especially older pre-releases).
- **429 Too Many Requests**: If the request is rate-limited.
- **503 Unavailable**: If the mirror is down for scheduled maintenance.
- **504 Gateway Timeout**: If the mirror fails to retrieve the file from the official Zig repository.
```

--------------------------------

TITLE: Zig Bitwise AND Operator
DESCRIPTION: Performs a bitwise AND operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
a & b
a &= b

// Example:
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Add Missing Bitcast for Var Ptr Rendering
DESCRIPTION: Includes a missing `bitcast` when rendering a variable pointer in stage1. This is necessary for types like structs or unions with compiler-inserted padding fields.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: zig
CODE:
```
const ptr = &my_struct;
const void_ptr = @bitcast(*void, ptr);
```

--------------------------------

TITLE: Zig POSIX API Adjustments for Cross-Platform Compatibility
DESCRIPTION: This code snippet demonstrates how Zig's POSIX APIs are adjusted based on the native architecture. It defines a packed struct `tc_lflag_t` with different field layouts for various architectures like powerpc, mips, and others, ensuring compatibility and correct interpretation of terminal flags across different systems.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: zig
CODE:
```
pub const tc_lflag_t = switch (native_arch) {
    .powerpc, .powerpcle, .powerpc64, .powerpc64le => packed struct(u32) {
        _0: u1 = 0,
        ECHOE: bool = false,
        ECHOK: bool = false,
        ECHO: bool = false,
        ECHONL: bool = false,
        _5: u2 = 0,
        ISIG: bool = false,
        ICANON: bool = false,
        _9: u1 = 0,
        IEXTEN: bool = false,
        _11: u11 = 0,
        TOSTOP: bool = false,
        _23: u8 = 0,
        NOFLSH: bool = false,
    },
    .mips, .mipsel, .mips64, .mips64el => packed struct(u32) {
        ISIG: bool = false,
        ICANON: bool = false,
        _2: u1 = 0,
        ECHO: bool = false,
        ECHOE: bool = false,
        ECHOK: bool = false,
        ECHONL: bool = false,
        NOFLSH: bool = false,
        IEXTEN: bool = false,
        _9: u6 = 0,
        TOSTOP: bool = false,
        _: u16 = 0,
    },
    else => packed struct(u32) {
        ISIG: bool = false,
        ICANON: bool = false,
        _2: u1 = 0,
        ECHO: bool = false,
        ECHOE: bool = false,
        ECHOK: bool = false,
        ECHONL: bool = false,
        NOFLSH: bool = false,
        TOSTOP: bool = false,
        _9: u6 = 0,
        IEXTEN: bool = false,
        _: u16 = 0,
    },
};
```

--------------------------------

TITLE: Zig Function Reflection Example
DESCRIPTION: Demonstrates function reflection in Zig by inspecting the type information of the `expect` function. It checks the argument type and whether the function accepts variable arguments.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "fn reflection" {
    try expect(@typeInfo(@TypeOf(expect)).Fn.args[0].arg_type.? == bool);
    try expect(@typeInfo(@TypeOf(expect)).Fn.is_var_args == false);
}
```

--------------------------------

TITLE: Zig: @memcpy Usage with Slices and Many-Pointers
DESCRIPTION: Demonstrates the usage of Zig's built-in `@memcpy` function. This example shows copying between two slices of equal length and copying from a many-pointer to a slice, highlighting the function's flexibility.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
test "@memcpy usage" {
    const a: [4]u32 = .{ 1, 2, 3, 4 };
    var b: [4]u32 = undefined;
    @memcpy(&b, &a);
    try std.testing.expectEqualSlices(u32, &a, &b);
    // If the second operand is a many-ptr, the length is taken from the first operand
    var c: [4]u32 = undefined;
    const a_manyptr: [*]const u32 = (&a).ptr;
    @memcpy(&c, a_manyptr);
    try std.testing.expectEqualSlices(u32, &a, &c);
}
```

--------------------------------

TITLE: Zig: Pointer Casting and Type Information
DESCRIPTION: Demonstrates unsafe pointer casting using @ptrCast and alternative methods like slice narrowing and @bitCast for type conversions. It also shows how to access pointer type information, specifically the 'child' field which indicates the type the pointer points to.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "pointer casting" {
    const bytes align(@alignOf(u32)) = [_]u8{ 0x12, 0x12, 0x12, 0x12 };
    const u32_ptr = @ptrCast(*const u32, &bytes);
    expect(u32_ptr.* == 0x12121212);

    // Even this example is contrived - there are better ways to do the above than
    // pointer casting. For example, using a slice narrowing cast:
    const u32_value = std.mem.bytesAsSlice(u32, bytes[0..])[0];
    expect(u32_value == 0x12121212);

    // And even another way, the most straightforward way to do it:
    expect(@bitCast(u32, bytes) == 0x12121212);
}

test "pointer child type" {
    // pointer types have a `child` field which tells you the type they point to.
    expect(@typeInfo(*u32).Pointer.child == u32);
}
```

--------------------------------

TITLE: Zig Opaque Type Declaration and C Interop
DESCRIPTION: Illustrates the use of `opaque {}` in Zig to declare types with unknown size and alignment, primarily for safe interaction with C code. It shows how to define opaque types and use them in function signatures, highlighting potential type casting errors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Fix Invalid LLVM IR for ?*void Const Casts
DESCRIPTION: Addresses the generation of invalid LLVM IR for constant casts involving `?*void`. This ensures that the generated intermediate representation is correct for type conversions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
/* Fix invalid LLVM IR generated for {#syntax#}?*void{#endsyntax#} const casts. [#2578](https://github.com/ziglang/zig/issues/2578) */
```

--------------------------------

TITLE: Convert integer to pointer with @intToPtr
DESCRIPTION: The @intToPtr function converts an integer address to a pointer of a specified destination type. If the destination pointer type disallows address zero and the provided address is zero, it results in safety-checked Undefined Behavior. Use @ptrToInt for the reverse conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
@intToPtr(comptime DestType: type, address: usize) DestType
```

--------------------------------

TITLE: Zig Volatile Memory Access
DESCRIPTION: Illustrates the use of the `volatile` keyword in Zig for memory-mapped input/output (MMIO). Loads and stores marked as volatile are guaranteed to occur and maintain their order as specified in the source code.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr: *volatile u8 = @ptrFromInt(0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig Result Type Propagation in Struct Initializer
DESCRIPTION: Demonstrates how Zig's result type propagation works within a struct initializer. The example shows how the type of a struct field influences the expected type of an expression assigned to it, utilizing @intCast for type conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
const expectEqual = @import("std").testing.expectEqual;
test "result type propagates through struct initializer" {
    const S = struct { x: u32 };
    const val: u64 = 123;
    const s: S = .{ .x = @intCast(val) };
    // .{ .x = @intCast(val) }   has result type `S` due to the type annotation
    //         @intCast(val)     has result type `u32` due to the type of the field `S.x`
    //                  val      has no result type, as it is permitted to be any integer type
    try expectEqual(@as(u32, 123), s.x);
}
```

--------------------------------

TITLE: Zig Bitwise OR Operator
DESCRIPTION: Performs a bitwise OR operation for integers. Invokes Peer Type Resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
a | b
a |= b
```

--------------------------------

TITLE: Fix BigInt Shift Operation
DESCRIPTION: Addresses a bug in the bigint_shl function, ensuring correct behavior for bitwise shift operations on arbitrary-precision integers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
Fix bigint_shl ([#9305](https://github.com/ziglang/zig/issues/9305)).
```

--------------------------------

TITLE: Zig Atomics Store
DESCRIPTION: Illustrates atomic store operations in Zig, enabling thread-safe writing to memory. This guarantees that writes are completed indivisibly, maintaining data integrity in concurrent scenarios.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Atomic(u32) = .{ .value = 0 };
    std.atomic.store(u32, &atomic_var, 20, .SeqCst);
    const value = std.atomic.load(u32, &atomic_var, .SeqCst);
    std.debug.print("Atomic store value: {d}", .{value});
}

```

--------------------------------

TITLE: Zig Integer Literals and Runtime Values
DESCRIPTION: Demonstrates various ways to represent integer literals in Zig, including decimal, hexadecimal, binary, and octal formats, and how these translate to runtime values.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.1/index.html

LANGUAGE: zig
CODE:
```
const decimal_int = 100;
const hex_int = 0x64;
const binary_int = 0b01100100;
const octal_int = 0144;

const signed_int = -50;
const unsigned_int: u8 = 200;
```

--------------------------------

TITLE: Linking: Avoid passing -l arguments for .a or .o files
DESCRIPTION: Corrects the linker behavior by preventing the passing of `-l` arguments when building `.a` (static library) or `.o` (object) files. This ensures that the linker is not incorrectly instructed to link against libraries when creating these intermediate artifacts.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: Shell
CODE:
```
# Conceptual build command for a static library
# zig build-lib --name mylib src/mylib.c -lotherlib (Incorrect behavior)

# Corrected behavior: -lotherlib is not passed when building .a or .o
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read command-line arguments when compiling for WASI. It allocates memory for arguments, iterates through them, and prints each argument with its index. The example includes the shell commands to build and run the Zig code using `wasm32-wasi` target and `wasmtime`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig Generic List Data Structure
DESCRIPTION: Defines a generic List data structure in Zig that can hold elements of any type. It returns an anonymous struct with an items slice and a length.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
fn List(comptime T: type) type {
    return struct {
        items: []T,
        len: usize,
    };
}
```

--------------------------------

TITLE: Zig @atomicLoad: Atomic Pointer Dereference
DESCRIPTION: The @atomicLoad function atomically dereferences a pointer, returning its value. It supports various types and atomic ordering constraints.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: zig
CODE:
```
T @atomicLoad(comptime T: type, ptr: *const T, comptime ordering: builtin.AtomicOrder) T
```

--------------------------------

TITLE: Zig @export with Field Access
DESCRIPTION: The `@export` feature in Zig now allows field access expressions in addition to identifiers for declarations. This means you can export functions or variables accessed through struct fields, like `x.y`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
x
```

LANGUAGE: Zig
CODE:
```
x.y
```

--------------------------------

TITLE: Fix Linux stdlib RW Flags
DESCRIPTION: Corrects the definition of Read/Write (RW) flags in the Linux standard library bindings, ensuring proper file access control.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.1/release-notes.html

LANGUAGE: Zig
CODE:
```
linux stdlib: fix definition of RW flags ([#9428](https://github.com/ziglang/zig/issues/9428)).
```

--------------------------------

TITLE: Zig C String Literals
DESCRIPTION: Shows how to use C-style string literals in Zig, which are null-terminated and can span multiple lines. These are useful for interacting with C functions that expect C strings.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.1.1/index.html

LANGUAGE: c
CODE:
```
extern fn puts(&const u8);

pub fn main() -> %void {
    puts(c"this has a null terminator");
    puts(
        c\and so
        c\does this
        c\multiline C string literal
    );
}
```

--------------------------------

TITLE: Zig: Linux ECONNRESET handling for connect/recv
DESCRIPTION: Implements handling for the `ECONNRESET` error code in the Linux operating system module for both `connect()` and `recv()` system calls. This improves the robustness of network operations by correctly managing connection reset events.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: Zig
CODE:
```
os: handle ECONNRESET for connect() syscall
```

LANGUAGE: Zig
CODE:
```
os.linux: handle ECONNRESET for recv
```

--------------------------------

TITLE: Base-2 Exponential with @exp2
DESCRIPTION: Details the @exp2 function for calculating the base-2 exponential of floating-point numbers and vectors of floats, mentioning hardware acceleration and potential implementation caveats.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
@exp2(value: anytype) @TypeOf(value)
```

--------------------------------

TITLE: Zig Translate-C: Fix Args for clang::ASTUnit::LoadFromCommandLine
DESCRIPTION: Arguments passed to `clang::ASTUnit::LoadFromCommandLine` have been fixed, resolving issues with how the Clang AST is loaded.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: C
CODE:
```
* Fix args when calling `clang::ASTUnit::LoadFromCommandLine`.
```

--------------------------------

TITLE: Zig Opaque Type Declaration and C Interop
DESCRIPTION: Illustrates the use of `opaque {}` in Zig to declare types with unknown size and alignment, primarily for safe interaction with C code. It shows how to define opaque types and use them in function signatures, highlighting potential type casting errors.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const Derp = opaque {};
const Wat = opaque {};

extern fn bar(d: *Derp) void;
fn foo(w: *Wat) callconv(.C) void {
    bar(w);
}

test "call foo" {
    foo(undefined);
}
```

--------------------------------

TITLE: Handle Structs with Comptime Fields Correctly (Zig)
DESCRIPTION: Improves how structs containing fields that require comptime (like `type`) are handled. Such structs are now correctly marked as requiring comptime themselves, affecting function calls that return them.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
// Example demonstrating the behavior change:
const std = @import("std");

// Struct requiring comptime due to 'type' field
const ComptimeStruct = struct {
    data: type,
};

// Function returning a struct that requires comptime
fn getComptimeStruct() ComptimeStruct {
    return ComptimeStruct{ .data = u32 };
}

// This function would implicitly be called at comptime
fn processComptimeStruct() void {
    const instance = getComptimeStruct();
    // ... process instance ...
}

```

--------------------------------

TITLE: Testing Inferred Anonymous Struct Execution
DESCRIPTION: Shell command to execute the Zig test file 'struct_anon.zig', which verifies the functionality of inferred anonymous struct literals.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: shell
CODE:
```
$ zig test struct_anon.zig
1/1 test "fully anonymous struct"... OK
All 1 tests passed.
```

--------------------------------

TITLE: Zig: Data Structure Enhancements
DESCRIPTION: Improves the functionality and consistency of data structures like `ArrayList` and `PriorityQueue`. This includes making `ArrayList(u0)` functional and refining the API for `PriorityQueue`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
ArrayList(u0) works now. This type still tracks length, but does not allocate additional memory.
DynamicBitSet.iterator takes self as const.
PriorityQueue: moved compareFn from init to type constructor in PriorityQueue and PriorityDequeue. This change significantly improves performance for simple compare functions and modifies the API to be more consistent with e.g. HashMap.
MultiArrayList: get function take self by value.
```

--------------------------------

TITLE: Zig: Swap Array Elements with Result Location Interference
DESCRIPTION: Demonstrates how Zig's result location system can interfere with direct array element swapping using an aggregate initializer. The test case shows that the naive swap fails because the intermediate assignment overwrites the original value before it can be used in the second assignment.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;
test "attempt to swap array elements with array initializer" {
    var arr: [2]u32 = .{ 1, 2 };
    arr = .{ arr[1], arr[0] };
    // The previous line is equivalent to the following two lines:
    //   arr[0] = arr[1];
    //   arr[1] = arr[0];
    // So this fails!
    try expect(arr[0] == 2); // succeeds
    try expect(arr[1] == 1); // fails
}
```

--------------------------------

TITLE: Zig Struct Methods (Dot Product Example)
DESCRIPTION: Demonstrates how to define and use methods within Zig structs. This example defines a Vec3 struct with an init constructor and a dot product method, showing both direct method calls and referencing methods as declarations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.2.0/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

// Structs can have methods
// Struct methods are not special, they are only namespaced
// functions that you can call with dot syntax.
const Vec3 = struct {
    x: f32,
    y: f32,
    z: f32,

    pub fn init(x: f32, y: f32, z: f32) Vec3 {
        return Vec3 {
            .x = x,
            .y = y,
            .z = z,
        };
    }

    pub fn dot(self: &const Vec3, other: &const Vec3) f32 {
        return self.x * other.x + self.y * other.y + self.z * other.z;
    }
};

test "dot product" {
    const v1 = Vec3.init(1.0, 0.0, 0.0);
    const v2 = Vec3.init(0.0, 1.0, 0.0);
    assert(v1.dot(v2) == 0.0);

    // Other than being available to call with dot syntax, struct methods are
    // not special. You can reference them as any other declaration inside
    // the struct:
    assert(Vec3.dot(v1, v2) == 0.0);
}
```

--------------------------------

TITLE: Zig Integer-to-Pointer and Pointer-to-Integer Conversion
DESCRIPTION: Explains and demonstrates the use of `@intToPtr` to convert an integer address to a pointer and `@ptrToInt` to convert a pointer back to an integer (usize).

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@ptrToInt and @intToPtr" {
    const ptr = @intToPtr(*i32, 0xdeadbee0);
    const addr = @ptrToInt(ptr);
    try expect(@TypeOf(addr) == usize);
    try expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Zig Packed Structs: BitCast Between Packed Structs
DESCRIPTION: Demonstrates using @bitCast to reinterpret data between different packed struct layouts in Zig. It includes tests for both runtime and compile-time execution, verifying the correct interpretation of fields based on endianness.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const native_endian = @import("builtin").target.cpu.arch.endian();
const expect = std.testing.expect;

const Full = packed struct {
    number: u16,
};
const Divided = packed struct {
    half1: u8,
    quarter3: u4,
    quarter4: u4,
};

test "@bitCast between packed structs" {
    try doTheTest();
    try comptime doTheTest();
}

fn doTheTest() !void {
    try expect(@sizeOf(Full) == 2);
    try expect(@sizeOf(Divided) == 2);
    const full = Full{ .number = 0x1234 };
    const divided: Divided = @bitCast(full);
    try expect(divided.half1 == 0x34);
    try expect(divided.quarter3 == 0x2);
    try expect(divided.quarter4 == 0x1);

    const ordered: [2]u8 = @bitCast(full);
    switch (native_endian) {
        .big => {
            try expect(ordered[0] == 0x12);
            try expect(ordered[1] == 0x34);
        },
        .little => {
            try expect(ordered[0] == 0x34);
            try expect(ordered[1] == 0x12);
        },
    }
}
```

--------------------------------

TITLE: Zig Volatile Memory Access
DESCRIPTION: Demonstrates the use of `volatile` for memory-mapped I/O in Zig. It shows how to create a volatile pointer and check its type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "volatile" {
    const mmio_ptr = @intToPtr(*volatile u8, 0x12345678);
    try expect(@TypeOf(mmio_ptr) == *volatile u8);
}
```

--------------------------------

TITLE: Zig: String Literals and Concatenation with Slices
DESCRIPTION: Illustrates how Zig handles string literals as null-terminated UTF-8 byte slices and demonstrates concatenation using `std.fmt.bufPrint` with slices. It shows coercing pointer types to slices and performing operations on them.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const mem = std.mem;
const fmt = std.fmt;

test "using slices for strings" {
    // Zig has no concept of strings. String literals are const pointers
    // to null-terminated arrays of u8, and by convention parameters
    // that are "strings" are expected to be UTF-8 encoded slices of u8.
    // Here we coerce *const [5:0]u8 and *const [6:0]u8 to []const u8
    const hello: []const u8 = "hello";
    const world: []const u8 = "ä¸–ç•Œ";

    var all_together: [100]u8 = undefined;
    // You can use slice syntax with at least one runtime-known index on an
    // array to convert an array into a slice.
    var start : usize = 0;
    const all_together_slice = all_together[start..];
    // String concatenation example.
    const hello_world = try fmt.bufPrint(all_together_slice, "{s} {s}", .{ hello, world });

    // Generally, you can use UTF-8 and not worry about whether something is a
    // string. If you don't need to deal with individual characters, no need
    // to decode.
    try expect(mem.eql(u8, hello_world, "hello ä¸–ç•Œ"));
}
```

--------------------------------

TITLE: Zig: Integer-Pointer Conversion Functions
DESCRIPTION: Demonstrates the use of `@ptrFromInt` to convert an integer address to a pointer and `@intFromPtr` to convert a pointer back to a `usize` integer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "@intFromPtr and @ptrFromInt" {
    const ptr: *i32 = @ptrFromInt(0xdeadbee0);
    const addr = @intFromPtr(ptr);
    try expect(@TypeOf(addr) == usize);
    try expect(addr == 0xdeadbee0);
}
```

--------------------------------

TITLE: Zig Tuple Operations Test
DESCRIPTION: Demonstrates the usage of tuples in Zig, which are anonymous structs without field names. The example showcases tuple creation, concatenation, indexing using numeric field names (prefixed with '@'), inline iteration, and checking the tuple's length.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "tuple" {
    const values = .{ 
        @as(u32, 1234),
        @as(f64, 12.34),
        true,
        "hi",
    } ++ .{false} ** 2;
    try expect(values[0] == 1234);
    try expect(values[4] == false);
    inline for (values, 0..) |v, i| {
        if (i != 2) continue;
        try expect(v);
    }
    try expect(values.len == 6);
    try expect(values.@"3"[0] == 'h');
}
```

--------------------------------

TITLE: Zig Bitwise And Operator
DESCRIPTION: Performs bitwise AND operation on integers. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig Bitwise And Operator
DESCRIPTION: Performs bitwise AND operation on integers. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Introduce Memory Fences with @fence
DESCRIPTION: The @fence function is used to establish happens-before relationships between operations. The 'order' parameter specifies the atomic memory ordering, which can be imported from '@import("builtin").AtomicOrder'.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: zig
CODE:
```
@fence(order: AtomicOrder)
```

--------------------------------

TITLE: NetBSD: Add termios constants to std.c.netbsd
DESCRIPTION: Essential termios constants have been added to `std.c.netbsd`. These constants are used for controlling terminal I/O characteristics, enabling more advanced terminal manipulation in Zig programs on NetBSD.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.c.netbsd.termios
```

--------------------------------

TITLE: Zig Extern Struct for C ABI Compatibility
DESCRIPTION: Explains the use of `extern struct` in Zig for ensuring memory layout compatibility with the C ABI. It's recommended only for C interoperability, with `packed struct` or normal `struct` preferred for other use cases.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: Zig
CODE:
```
An `extern struct` has in-memory layout guaranteed to match the C ABI for the target.

This kind of struct should only be used for compatibility with the C ABI. Every other use case should be solved with [packed struct](#packed-struct) or normal [struct](#struct).

See also:

*   [extern union](#extern-union)
*   [extern enum](#extern-enum)
```

--------------------------------

TITLE: Zig: Destructure Array, Vector, and Tuple
DESCRIPTION: This Zig code provides examples of destructuring aggregates like arrays, vectors, and tuples. It demonstrates how to assign elements of these aggregates to multiple lvalues or local variables on the left side of an assignment.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.12.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;
const expectEqual = std.testing.expectEqual;

test "destructure array" {
    var z: u32 = undefined;
    const x, var y, z = [3]u32{ 1, 2, 3 };
    y += 10;
    try expectEqual(1, x);
    try expectEqual(12, y);
    try expectEqual(3, z);
}

test "destructure vector" {
    // Comptime-known values are propagated as you would expect.
    const x, const y = @Vector(2, u32){ 1, 2 };
    comptime assert(x == 1);
    comptime assert(y == 2);
}

test "destructure tuple" {
    var runtime: u32 = undefined;
    runtime = 123;
    const x, const y = .{ 42, runtime };
    // The first tuple field is a `comptime` field, so `x` is comptime-known even
    // though `y` is runtime-known.
    comptime assert(x == 42);
    try expectEqual(123, y);
}
```

--------------------------------

TITLE: Zig: Pointer Casting and Type Information
DESCRIPTION: Demonstrates unsafe pointer casting using @ptrCast and alternative methods like slice narrowing and @bitCast for type conversions. It also shows how to access pointer type information, specifically the 'child' field which indicates the type the pointer points to.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "pointer casting" {
    const bytes align(@alignOf(u32)) = [_]u8{ 0x12, 0x12, 0x12, 0x12 };
    const u32_ptr = @ptrCast(*const u32, &bytes);
    expect(u32_ptr.* == 0x12121212);

    // Even this example is contrived - there are better ways to do the above than
    // pointer casting. For example, using a slice narrowing cast:
    const u32_value = std.mem.bytesAsSlice(u32, bytes[0..])[0];
    expect(u32_value == 0x12121212);

    // And even another way, the most straightforward way to do it:
    expect(@bitCast(u32, bytes) == 0x12121212);
}

test "pointer child type" {
    // pointer types have a `child` field which tells you the type they point to.
    expect(@typeInfo(*u32).Pointer.child == u32);
}
```

--------------------------------

TITLE: Zig: BitCast Between Packed Structs with Different Field Sizes
DESCRIPTION: Demonstrates using @bitCast to reinterpret memory between two packed structs with different field arrangements and sizes. It includes checks for both Big and Little endian systems to ensure correct data interpretation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const builtin = std.builtin;
const assert = std.debug.assert;

const Full = packed struct {
    number: u16,
};
const Divided = packed struct {
    half1: u8,
    quarter3: u4,
    quarter4: u4,
};

test "@bitCast between packed structs" {
    doTheTest();
    comptime doTheTest();
}

fn doTheTest() void {
    assert(@sizeOf(Full) == 2);
    assert(@sizeOf(Divided) == 2);
    var full = Full{ .number = 0x1234 };
    var divided = @bitCast(Divided, full);
    switch (builtin.endian) {
        .Big => {
            assert(divided.half1 == 0x12);
            assert(divided.quarter3 == 0x3);
            assert(divided.quarter4 == 0x4);
        },
        .Little => {
            assert(divided.half1 == 0x34);
            assert(divided.quarter3 == 0x2);
            assert(divided.quarter4 == 0x1);
        },
    }
}
```

--------------------------------

TITLE: Zig Address Of Syntax and Pointer Dereferencing
DESCRIPTION: Illustrates how to obtain pointers to variables using the address-of operator (&) in Zig. It shows dereferencing pointers to read and modify values, and explains the difference between const and mutable pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "address of syntax" {
    // Get the address of a variable:
    const x: i32 = 1234;
    const x_ptr = &x;

    // Dereference a pointer:
    expect(x_ptr.* == 1234);

    // When you get the address of a const variable, you get a const pointer to a single item.
    expect(@TypeOf(x_ptr) == *const i32);

    // If you want to mutate the value, you'd need an address of a mutable variable:
    var y: i32 = 5678;
    const y_ptr = &y;
    expect(@TypeOf(y_ptr) == *i32);
    y_ptr.* += 1;
    expect(y_ptr.* == 5679);
}
```

--------------------------------

TITLE: Zig Async Functions
DESCRIPTION: Zig supports asynchronous programming with `async` and `await` keywords, enabling non-blocking I/O and concurrent task execution. Suspend blocks allow for cooperative multitasking.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example of an async function
async fn fetchData(url: []const u8) ![]const u8 {
    // Simulate fetching data asynchronously
    std.time.sleep(1 * std.time.ns_per_second);
    return "Some fetched data";
}

// Example of using async/await
async fn processData() void {
    const data = await fetchData("http://example.com");
    std.debug.print("Received: {s}\\n", .{data});
}

// To run an async function, you typically need an event loop or a runner.
// For simplicity, this example just shows the function definition.

```

--------------------------------

TITLE: Zig: Basic Slice Operations and Bounds Checking
DESCRIPTION: Illustrates the creation and usage of slices in Zig, comparing them to arrays. It shows how slices are pointers with lengths, how to access elements, and demonstrates Zig's runtime bounds checking, which causes a panic when accessing an out-of-bounds index.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.1/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;

test "basic slices" {
    var array = [_]i32{ 1, 2, 3, 4 };
    // A slice is a pointer and a length. The difference between an array and
    // a slice is that the array's length is part of the type and known at
    // compile-time, whereas the slice's length is known at runtime.
    // Both can be accessed with the `len` field.
    var known_at_runtime_zero: usize = 0;
    const slice = array[known_at_runtime_zero..array.len];
    try expect(@TypeOf(slice) == []i32);
    try expect(&slice[0] == &array[0]);
    try expect(slice.len == array.len);

    // If you slice with comptime-known start and end positions, the result is
    // a pointer to an array, rather than a slice.
    const array_ptr = array[0..array.len];
    try expect(@TypeOf(array_ptr) == *[array.len]i32);

    // Using the address-of operator on a slice gives a single-item pointer,
    // while using the `ptr` field gives a many-item pointer.
    try expect(@TypeOf(slice.ptr) == [*]i32);
    try expect(@TypeOf(&slice[0]) == *i32);
    try expect(@ptrToInt(slice.ptr) == @ptrToInt(&slice[0]));

    // Slices have array bounds checking. If you try to access something out
    // of bounds, you'll get a safety check failure:
    slice[10] += 1;

    // Note that `slice.ptr` does not invoke safety checking, while `&slice[0]`
    // asserts that the slice has len >= 1.
}
```

--------------------------------

TITLE: Zig Basic Slice Operations and Bounds Checking
DESCRIPTION: Explains the fundamental properties of Zig slices, including their composition of a pointer and a length. It demonstrates accessing slice elements and highlights the runtime array bounds checking mechanism with an example that causes a failure.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.2.0/index.html

LANGUAGE: zig
CODE:
```
const assert = @import("std").debug.assert;

test "basic slices" {
    var array = []i32{1, 2, 3, 4};
    const slice = array[0..array.len];
    assert(slice.ptr == &array[0]);
    assert(slice.len == array.len);

    slice[10] += 1; // Triggers bounds checking failure
}
```

--------------------------------

TITLE: Zig std.os.linux.getpid() Type Error Fix
DESCRIPTION: Resolves a type error in the `std.os.linux.getpid()` function. This ensures that the process ID is correctly retrieved and typed within the Linux operating system module.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const pid = std.os.linux.getpid();
```

--------------------------------

TITLE: Zig Comptime Slicing with Indexes
DESCRIPTION: Demonstrates how to slice arrays using comptime-known indexes in Zig. Prior to this change, an @ptrCast was required for such operations. This example shows the type inference for slices with both comptime and runtime indices.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;

test "slicing with comptime indexes" {
    var a = "abcdefgh".*;
    assert(@TypeOf(a) == [8:0]u8); // both indices are comptime, thus length is comptime
    var b = a[3..6];
    assert(@TypeOf(b) == *[3]u8); // length is runtime
    var runtime_i: usize = 3;
    var c = a[runtime_i..6];
    assert(@TypeOf(c) == []u8); // copy
    a[0..3].* = a[5..8].*;
    assert(std.mem.eql(u8, &a, "fghdefgh"));
}
```

--------------------------------

TITLE: Zig: ELF Module Updates
DESCRIPTION: Adds support for additional ELF section indexes and AMD64 relocation types, enhancing Zig's capabilities for working with executable and object files.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: zig
CODE:
```
elf: added a couple missing special section indexes.
elf: add amd64 relocation types.
```

--------------------------------

TITLE: Zig builtin @bitOffsetOf
DESCRIPTION: Shows the `@bitOffsetOf` built-in function in Zig, which returns the bit offset of a field within a struct.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const MyStruct = struct {
    a: u8,
    b: u16,
};
const offset_b = @bitOffsetOf(MyStruct, "b");
// offset_b is 8
```

--------------------------------

TITLE: Verbose C import with caching
DESCRIPTION: Shows how to use the `--verbose-cimport` flag with `zig build-exe` to see the cache locations for C imports. It also demonstrates a basic Zig program that uses `@cImport` to include stdio.h.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: Zig
CODE:
```
const c = @cImport({
    @cDefine("_NO_CRT_STDIO_INLINE", "1");
    @cInclude("stdio.h");
});
pub fn main() void {
    _ = c;
}
```

--------------------------------

TITLE: Zig Buffer Stream Replacement
DESCRIPTION: Details the removal of certain functions from `std.Buffer` in favor of `std.io.BufferOutStream`. Functions like `appendFormat`, `appendByte`, and `appendByteNTimes` are no longer available on `std.Buffer`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.Buffer.appendFormat
std.Buffer.appendByte
std.Buffer.appendByteNTimes
```

--------------------------------

TITLE: Zig noreturn type compatibility
DESCRIPTION: Demonstrates that the 'noreturn' type is compatible with all other types when resolving types in conditional expressions like if clauses or switch prongs. The example shows a function `foo` where a variable `a` is assigned a value from an `if` expression, with the `else` branch using `return`, which has the 'noreturn' type.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: Zig
CODE:
```
fn foo(condition: bool, b: u32) void {
    const a = if (condition) b else return;
    @panic("do something with a");
}
test "noreturn" {
    foo(false, 1);
}
```

--------------------------------

TITLE: Zig Type Reflection with @typeInfo
DESCRIPTION: Describes the @typeInfo built-in function in Zig, which provides type reflection capabilities. It guarantees that for structs, unions, enums, and error sets, the fields are ordered as declared. For other declarations, the order is unspecified.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html



--------------------------------

TITLE: Zig Result Type Propagation in Struct Initializer
DESCRIPTION: Demonstrates how result types propagate through a struct initializer in Zig. The example shows how the type of a struct field influences the expected type of an expression assigned to it, utilizing @intCast for type conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: zig
CODE:
```
const expectEqual = @import("std").testing.expectEqual;
test "result type propagates through struct initializer" {
    const S = struct { x: u32 };
    const val: u64 = 123;
    const s: S = .{ .x = @intCast(val) };
    // .{ .x = @intCast(val) }   has result type `S` due to the type annotation
    //         @intCast(val)     has result type `u32` due to the type of the field `S.x`
    //                  val      has no result type, as it is permitted to be any integer type
    try expectEqual(@as(u32, 123), s.x);
}
```

--------------------------------

TITLE: Zig @cImport for C Header Inclusion
DESCRIPTION: Shows how to use the `@cImport` builtin function to import symbols from C header files. This example includes `stdio.h` and defines `_NO_CRT_STDIO_INLINE` to use `printf`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: zig
CODE:
```
const c = @cImport({
    // See https://github.com/ziglang/zig/issues/515
    @cDefine("_NO_CRT_STDIO_INLINE", "1");
    @cInclude("stdio.h");
});
pub fn main() void {
    _ = c.printf(c"hello\n");
}

```

--------------------------------

TITLE: Zig @memset with Undefined Values
DESCRIPTION: Describes an optimization in Zig's `@memset` function. When used to set bytes to `undefined`, Zig now employs a single Valgrind client request, improving performance in safe modes.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
@memset
```

LANGUAGE: Zig
CODE:
```
undefined
```

--------------------------------

TITLE: Zig Thread Spawning
DESCRIPTION: Introduces the `std.os.spawnThread` function, which allows for the creation of new threads across all supported targets. On Linux, its implementation leverages pthreads when linking with libc, and directly uses system calls when not linking with libc.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: zig
CODE:
```
std.os.spawnThread
```

--------------------------------

TITLE: Zig @exp for Floating-Point Exponential (e^x)
DESCRIPTION: Documents the `@exp` built-in function in Zig, which computes the base-e exponential of a floating-point number. It mentions hardware acceleration and support for float vectors, along with the general limitation regarding full implementation across all float types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html



--------------------------------

TITLE: Zig Packed Structs: Bit Casting Example
DESCRIPTION: Demonstrates using @bitCast between different packed structs, showcasing how memory is reinterpreted based on the packed layout. It includes checks for endianness to ensure correct field extraction.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const builtin = std.builtin;
const expect = std.testing.expect;

const Full = packed struct {
    number: u16,
};
const Divided = packed struct {
    half1: u8,
    quarter3: u4,
    quarter4: u4,
};

test "@bitCast between packed structs" {
    doTheTest();
    comptime doTheTest();
}

fn doTheTest() void {
    expect(@sizeOf(Full) == 2);
    expect(@sizeOf(Divided) == 2);
    var full = Full{ .number = 0x1234 };
    var divided = @bitCast(Divided, full);
    switch (builtin.endian) {
        .Big => {
            expect(divided.half1 == 0x12);
            expect(divided.quarter3 == 0x3);
            expect(divided.quarter4 == 0x4);
        },
        .Little => {
            expect(divided.half1 == 0x34);
            expect(divided.quarter3 == 0x2);
            expect(divided.quarter4 == 0x1);
        },
    }
}
```

--------------------------------

TITLE: Zig Bitwise OR Operator
DESCRIPTION: Explains the bitwise OR operator '|' for integers in Zig. It notes that the operation invokes Peer Type Resolution for the operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
a | b
a |= b
```

--------------------------------

TITLE: Base-2 Exponential with @exp2
DESCRIPTION: Details the @exp2 function for calculating the base-2 exponential of floating-point numbers and vectors of floats, mentioning hardware acceleration and potential implementation caveats.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: Zig
CODE:
```
@exp2(value: anytype) @TypeOf(value)
```

--------------------------------

TITLE: Zig Noreturn Type Compatibility Example
DESCRIPTION: Illustrates the compatibility of the 'noreturn' type in Zig. The 'noreturn' type is compatible with all other types when resolving types together, such as in 'if' clauses or 'switch' prongs. This example shows how 'return' can be used as an alternative in an 'if' statement.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: zig
CODE:
```
fn foo(condition: bool, b: u32) void {
    const a = if (condition) b else return;
    _ = a;
    @panic("do something with a");
}

test "noreturn" {
    foo(false, 1);
}
```

--------------------------------

TITLE: Zig Test: Result Location Semantics
DESCRIPTION: Demonstrates result location semantics in Zig tests. It defines structs for objects and points, and includes functions for conditional execution, object creation, and point generation. The test verifies the correct assignment and access of values within the Object struct.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

const Object = struct {
    tag: i32,
    pt: [2]Point,
};

const Point = struct {
    x: i32,
    y: i32,
};

test "result location semantics" {
    const result = if (condition()) foo(10) else bar();
    std.testing.expect(result.tag == 10);
    std.testing.expect(result.pt[0].x == 69);
    std.testing.expect(result.pt[1].y == 420);
}

fn condition() bool {
    return true;
}

fn foo(arg: i32) Object {
    return baz(arg);
}

fn bar() Object {
    return Object{
        .tag = 1,
        .pt = undefined,
    };
}

fn baz(arg: i32) Object {
    return Object{
        .tag = arg,
        .pt = [_]Point{
            nice(),
            blazet(),
        },
    };
}

fn nice() Point {
    return Point{
        .x = 69,
        .y = 69,
    };
}

fn blazet() Point {
    return Point{
        .x = 420,
        .y = 420,
    };
}
```

--------------------------------

TITLE: Zig @exp for Floating-Point Exponential (e^x)
DESCRIPTION: Documents the `@exp` built-in function in Zig, which computes the base-e exponential of a floating-point number. It mentions hardware acceleration and support for float vectors, along with the general limitation regarding full implementation across all float types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.0/index.html



--------------------------------

TITLE: Zig Result Type Propagation in Struct Initializer
DESCRIPTION: Demonstrates how Zig's result type propagation works within a struct initializer. The example shows how the type of a struct field influences the expected type of an expression assigned to it, utilizing @intCast for type conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const expectEqual = @import("std").testing.expectEqual;
test "result type propagates through struct initializer" {
    const S = struct { x: u32 };
    const val: u64 = 123;
    const s: S = .{ .x = @intCast(val) };
    // .{ .x = @intCast(val) }   has result type `S` due to the type annotation
    //         @intCast(val)     has result type `u32` due to the type of the field `S.x`
    //                  val      has no result type, as it is permitted to be any integer type
    try expectEqual(@as(u32, 123), s.x);
}
```

--------------------------------

TITLE: Zig Async Functions
DESCRIPTION: Zig supports asynchronous programming with `async` and `await` keywords, enabling non-blocking I/O and concurrent task execution. Suspend blocks allow for cooperative multitasking.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

// Example of an async function
async fn fetchData(url: []const u8) ![]const u8 {
    // Simulate fetching data asynchronously
    std.time.sleep(1 * std.time.ns_per_second);
    return "Some fetched data";
}

// Example of using async/await
async fn processData() void {
    const data = await fetchData("http://example.com");
    std.debug.print("Received: {s}\\n", .{data});
}

// To run an async function, you typically need an event loop or a runner.
// For simplicity, this example just shows the function definition.

```

--------------------------------

TITLE: Zig Builtin: @bitOffsetOf
DESCRIPTION: Demonstrates the `@bitOffsetOf` builtin function in Zig, which returns the bit offset of a field within a struct. This is useful for manual memory layout manipulation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    const MyStruct = struct {
        a: u8,
        b: u16,
    };
    const offset = @bitOffsetOf(MyStruct, "b");
    std.debug.print("Bit offset of field 'b': {d}\n", .{offset});
}
```

--------------------------------

TITLE: Zig Builtin: @bitOffsetOf
DESCRIPTION: Returns the byte offset of a field within a struct. Useful for manual memory layout manipulation and reflection.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.4.0/index.html

LANGUAGE: Zig
CODE:
```
const offset = @bitOffsetOf(StructType, .fieldName);
// offset is a usize value
```

--------------------------------

TITLE: Zig Anonymous Struct Literals
DESCRIPTION: Demonstrates Zig's anonymous struct literals, which allow omitting the struct type in a literal. The struct can be directly instantiated into a location if type coercion is possible, or the type can be inferred if not explicitly provided. Examples show usage with explicit and inferred types, and initializing unions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "anonymous struct literal" {
    checkPoint(.{ .x = 13, .y = 67 });
}

fn checkPoint(pt: struct {x: i32, y: i32}) void {
    expect(pt.x == 13);
    expect(pt.y == 67);
}
```

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "fully anonymous struct" {
    dump(.{ .int = 1234, .float = 12.34, .b = true, .s = "hi" });
}

fn dump(args: var) void {
    expect(args.int == 1234);
    expect(args.float == 12.34);
    expect(args.b);
    expect(args.s[0] == 'h');
    expect(args.s[1] == 'i');
}
```

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

const Number = union {
    int: i32,
    float: f64,
};

test "anonymous union literal syntax" {
    var i: Number = .{.int = 42};
    var f = makeNumber();
    expect(i.int == 42);
    expect(f.float == 12.34);
}

fn makeNumber() Number {
    return .{.float = 12.34};
}
```

--------------------------------

TITLE: Zig Linker: Incremental Codepath TLS Variable Emission
DESCRIPTION: Implements the emission of Thread-Local Storage (TLS) variables within the incremental codepath.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
implement emitting TLS variables in incremental codepath
```

--------------------------------

TITLE: Thread Local Storage in Zig
DESCRIPTION: Demonstrates the use of `threadlocal` variables in Zig, ensuring that each thread has its own independent copy of the variable.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const assert = std.debug.assert;

threadlocal var x: i32 = 1234;

test "thread local storage" {
    const thread1 = try std.Thread.spawn({}, testTls);
    const thread2 = try std.Thread.spawn({}, testTls);
    testTls({});
    thread1.wait();
    thread2.wait();
}

fn testTls(context: void) void {
    assert(x == 1234);
    x += 1;
    assert(x == 1235);
}
```

--------------------------------

TITLE: Zig Builtin: @bitOffsetOf
DESCRIPTION: Demonstrates the `@bitOffsetOf` builtin function in Zig, which returns the bit offset of a field within a struct. This is useful for manual memory layout manipulation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    const MyStruct = struct {
        a: u8,
        b: u16,
    };
    const offset = @bitOffsetOf(MyStruct, "b");
    std.debug.print("Bit offset of field 'b': {d}\n", .{offset});
}
```

--------------------------------

TITLE: Zig Standard Library: Memory Utilities
DESCRIPTION: The memory module has received several new functions and improvements. These include indexOfMin and indexOfMax, enhancements to mem.zeroes to work with comptime struct fields and extern unions, and fixes for mem.zeroInit with empty initializers. New functions like concatWithSentinel and splitBackwards have been added, along with first and reset methods for SplitIterator and SplitBackwardsIterator. The ascii.indexOfIgnoreCase function is now enhanced with the Boyer-Moore-Horspool algorithm.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

// Example: Using mem.zeroes with comptime struct fields
var data = std.mem.zeroes(struct {
    field1: u32,
    comptime field2: bool,
});

// Example: Using mem.indexOfMin
const numbers = [_]i32{ 5, 2, 8, 1, 9 };
const min_index = std.mem.indexOfMin(i32, &numbers) orelse unreachable;
std.debug.print("Index of minimum value: {d}\\n", .{min_index});

// Example: Using ascii.indexOfIgnoreCase
const haystack = "Hello World";
const needle = "world";
const index = std.ascii.indexOfIgnoreCase(u8, haystack, needle);

```

--------------------------------

TITLE: Zig: Slice Bounds Checking and Manipulation
DESCRIPTION: Demonstrates how to create slices from arrays using start and end indices and how these slices provide bounds checking. The example shows modifying an element through a slice and verifying the change in the original array.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.13.0/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

test "pointer slicing" {
    var array = [_]u8{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
    var start: usize = 2; // var to make it runtime-known
    _ = &start; // suppress 'var is never mutated' error
    const slice = array[start..4];
    try expect(slice.len == 2);

    try expect(array[3] == 4);
    slice[1] += 1;
    try expect(array[3] == 5);
}
```

--------------------------------

TITLE: Zig Tuple Type Declarations
DESCRIPTION: Demonstrates the new syntax for declaring tuple types using struct declaration syntax without field types. It also shows tuple concatenation and multiplication operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;
const expectEqualStrings = std.testing.expectEqualStrings;

test "tuple declarations" {
  const T = struct { u32, []const u8 };
  var t: T = .{ 1, "foo" };
  try expect(t[0] == 1);
  try expectEqualStrings(t[1], "foo");

  var mul = t ** 3;
  try expect(@TypeOf(mul) != T);
  try expect(mul.len == 6);
  try expect(mul[2] == 1);
  try expectEqualStrings(mul[3], "foo");

  var t2: T = .{ 2, "bar" };
  var cat = t ++ t2;
  try expect(@TypeOf(cat) != T);
  try expect(cat.len == 4);
  try expect(cat[2] == 2);
  try expectEqualStrings(cat[3], "bar");
}
```

--------------------------------

TITLE: Enable Thread Sanitizer in Zig
DESCRIPTION: The `-fsanitize-thread` option is now available to detect data races in Zig programs. This feature is based on Clang's ThreadSanitizer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html

LANGUAGE: zig
CODE:
```
#zig build-exe src/main.zig -fsanitize-thread
```

--------------------------------

TITLE: Zig WASI: Read Command Line Arguments
DESCRIPTION: Demonstrates how to use Zig's standard library to read command-line arguments when compiling for WASI. It allocates memory for arguments, iterates through them, and prints each argument with its index. The example includes the shell commands to build and run the Zig code using `wasm32-wasi` target and `wasmtime`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.10.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() !void {
    var general_purpose_allocator = std.heap.GeneralPurposeAllocator(.{}){};
    const gpa = general_purpose_allocator.allocator();
    const args = try std.process.argsAlloc(gpa);
    defer std.process.argsFree(gpa, args);

    for (args) |arg, i| {
        std.debug.print("{}: {s}\n", .{ i, arg });
    }
}
```

--------------------------------

TITLE: Zig Pointer Address Syntax and Mutability
DESCRIPTION: Demonstrates how to get the address of variables in Zig, distinguishing between constant and mutable pointers. It shows how to assert pointer types and mutate values through mutable pointers.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.5.0/index.html

LANGUAGE: Zig
CODE:
```
const assert = @import("std").debug.assert;

test "address of syntax" {
        // When you get the address of a const variable, you get a const pointer to a single item.
        var x: i32 = 1234;
        const x_ptr = &x;
        assert(@typeOf(x_ptr) == *const i32);
    
        // If you want to mutate the value, you'd need an address of a mutable variable:
        var y: i32 = 5678;
        const y_ptr = &y;
        assert(@typeOf(y_ptr) == *i32);
        y_ptr.* += 1;
        assert(y_ptr.* == 5679);
    }
```

--------------------------------

TITLE: Zig Cast Return Type and Big Int Allocator
DESCRIPTION: Modifies `cast` to return an optional value instead of an error. Updates `big.int.toString()` to use a provided allocator and addresses potential UAF issues.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Made `cast` return optional instead of an error.
big int: update Managed.toString() to use provided allocator
big int: breaking API changes to prevent UAF
```

--------------------------------

TITLE: Zig: Inline Switch Prongs with Ranges
DESCRIPTION: Illustrates the use of Zig's `inline` keyword with ranges in switch prongs. The `isFieldOptional` function is adapted to handle a range of field indices, checking for optional fields within that range.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.0/index.html

LANGUAGE: zig
CODE:
```
fn isFieldOptional(comptime T: type, field_index: usize) !bool {
    const fields = @typeInfo(T)."struct".fields;
    return switch (field_index) {
        inline 0...fields.len - 1 => |idx| @typeInfo(fields[idx].type) == .optional,
        else => return error.IndexOutOfBounds,
    };
}
```

--------------------------------

TITLE: Fix BufferedInStream Not Reading Delayed Input in Zig
DESCRIPTION: Corrects a bug in `BufferedInStream` where it was not reading delayed input. This ensures that buffered input streams function correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.4.0/release-notes.html

LANGUAGE: Zig
CODE:
```
/*
* sjdh02 fixed BufferedInStream not reading delayed input.
*/
```

--------------------------------

TITLE: Zig String Literal to Constant Slice Success
DESCRIPTION: Shows the correct way to pass a string literal to a function in Zig by using a constant slice (`[]const u8`). This avoids type errors as string literals are inherently constant.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.6.0/index.html

LANGUAGE: zig
CODE:
```
fn foo(s: []const u8) void {}

test "string literal to constant slice" {
    foo("hello");
}
```

--------------------------------

TITLE: Improve Big-Endian Compatibility
DESCRIPTION: LemonBoy improved big-endian compatibility within the Zig project. This enhances the language's ability to function correctly on systems that use big-endian byte ordering.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html



--------------------------------

TITLE: Zig Result Type Propagation in Struct Initializer
DESCRIPTION: Demonstrates how result types propagate through a struct initializer in Zig. The example shows how the type of a struct field influences the expected type of an expression assigned to it, utilizing @intCast for type conversion.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.0/index.html

LANGUAGE: zig
CODE:
```
const expectEqual = @import("std").testing.expectEqual;
test "result type propagates through struct initializer" {
    const S = struct { x: u32 };
    const val: u64 = 123;
    const s: S = .{ .x = @intCast(val) };
    // .{ .x = @intCast(val) }   has result type `S` due to the type annotation
    //         @intCast(val)     has result type `u32` due to the type of the field `S.x`
    //                  val      has no result type, as it is permitted to be any integer type
    try expectEqual(@as(u32, 123), s.x);
}
```

--------------------------------

TITLE: Zig String Literal Properties
DESCRIPTION: Demonstrates the properties of Zig string literals, including their type, length, null termination, and character access. It also shows the conversion of character literals to integer types and the comparison of string literals.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: zig
CODE:
```
const expect = @import("std").testing.expect;
const mem = @import("std").mem;

test "string literals" {
    const bytes = "hello";
    expect(@TypeOf(bytes) == *const [5:0]u8);
    expect(bytes.len == 5);
    expect(bytes[1] == 'e');
    expect(bytes[5] == 0);
    expect('e' == '\x65');
    expect('\u{1f4a9}' == 128169);
    expect('ðŸ’¯' == 128175);
    expect(mem.eql(u8, "hello", "h\x65llo"));
}
```

--------------------------------

TITLE: Zig: Implement Writer Functions
DESCRIPTION: Demonstrates how to define functions `writeInt` and `writeFloat` that accept a writer and a value, with placeholders for actual implementation logic.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
fn writeInt(self: *Writer, value: anytype) !void {
    _ = self;
    _ = value;
}
fn writeFloat(self: *Writer, value: anytype) !void {
    _ = self;
    _ = value;
}
```

--------------------------------

TITLE: Zig: Add mem.alignPointerOffset
DESCRIPTION: Introduces `mem.alignPointerOffset`, a utility for calculating the offset needed to align a pointer.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.9.0/release-notes.html

LANGUAGE: Zig
CODE:
```
mem.alignPointerOffset
```

--------------------------------

TITLE: Zig: Support returning !u8 from main()
DESCRIPTION: Zig now supports returning `!u8` from the `main` function. This allows the main function to return an error code, providing more flexibility in program termination.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.5.0/release-notes.html

LANGUAGE: Zig
CODE:
```
!u8
```

--------------------------------

TITLE: Zig Function Parameter Type Inference
DESCRIPTION: Illustrates Zig's function parameter type inference using `anytype`. The example defines an `addFortyTwo` function that accepts any type for its parameter `x` and returns the same type. It demonstrates calling this function with an integer literal and an `i64`, verifying the return type using `@TypeOf`.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.14.1/index.html

LANGUAGE: Zig
CODE:
```
const expect = @import("std").testing.expect;

fn addFortyTwo(x: anytype) @TypeOf(x) {
    return x + 42;
}

test "fn type inference" {
    try expect(addFortyTwo(1) == 43);
    try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
    const y: i64 = 2;
    try expect(addFortyTwo(y) == 44);
    try expect(@TypeOf(addFortyTwo(y)) == i64);
}
```

LANGUAGE: Shell
CODE:
```
$ zig test test_fn_type_inference.zig
1/1 test_fn_type_inference.test.fn type inference...OK
All 1 tests passed.
```

--------------------------------

TITLE: Zig: Coerce Pointers to Slices and Arrays
DESCRIPTION: Demonstrates coercing constant pointers to arrays into slices, including handling different destination types like error unions and optionals. It also shows how array lengths map to slice lengths and how single-item pointers can be coerced to arrays.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

// You can assign constant pointers to arrays to a slice with
// const modifier on the element type. Useful in particular for
// String literals.
test "*const [N]T to []const T" {
    var x1: []const u8 = "hello";
    var x2: []const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, x1, x2));

    var y: []const f32 = &[2]f32{ 1.2, 3.4 };
    try expect(y[0] == 1.2);
}

// Likewise, it works when the destination type is an error union.
test "*const [N]T to E![]const T" {
    var x1: anyerror![]const u8 = "hello";
    var x2: anyerror![]const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, try x1, try x2));

    var y: anyerror![]const f32 = &[2]f32{ 1.2, 3.4 };
    try expect((try y)[0] == 1.2);
}

// Likewise, it works when the destination type is an optional.
test "*const [N]T to ?[]const T" {
    var x1: ?[]const u8 = "hello";
    var x2: ?[]const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, x1.?, x2.?));

    var y: ?[]const f32 = &[2]f32{ 1.2, 3.4 };
    try expect(y.?[0] == 1.2);
}

// In this cast, the array length becomes the slice length.
test "*[N]T to []T" {
    var buf: [5]u8 = "hello".*;
    const x: []u8 = &buf;
    try expect(std.mem.eql(u8, x, "hello"));

    const buf2 = [2]f32{ 1.2, 3.4 };
    const x2: []const f32 = &buf2;
    try expect(std.mem.eql(f32, x2, &[2]f32{ 1.2, 3.4 }));
}

// Single-item pointers to arrays can be coerced to many-item pointers.
test "*[N]T to [*]T" {
    var buf: [5]u8 = "hello".*;
    const x: [*]u8 = &buf;
    try expect(x[4] == 'o');
    // x[5] would be an uncaught out of bounds pointer dereference!
}

// Likewise, it works when the destination type is an optional.
test "*[N]T to ?[*]T" {
    var buf: [5]u8 = "hello".*;
    const x: ?[*]u8 = &buf;
    try expect(x.?[4] == 'o');
}

// Single-item pointers can be cast to len-1 single-item arrays.
test "*T to *[1]T" {
    var x: i32 = 1234;
    const y: *[1]i32 = &x;
    const z: [*]i32 = y;
    try expect(z[0] == 1234);
}
```

--------------------------------

TITLE: Zig BitSet and EnumSet Pure Functions
DESCRIPTION: Introduction of pure functions for StaticBitSet and EnumSet in Zig's standard library. These functions enable set operations like equality checking, subset, superset, complement, union, intersection, XOR, and difference.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.11.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std: added pure functions to StaticBitSet and EnumSet

fn eql(self: Self, other: Self) bool
fn subsetOf(self: Self, other: Self) bool
fn supersetOf(self: Self, other: Self) bool
fn complement(self: Self) Self
fn unionWith(self: Self, other: Self) Self
fn intersectWith(self: Self, other: Self) Self
fn xorWith(self: Self, other: Self) Self
fn differenceWith(self: Self, other: Self) Self
```

--------------------------------

TITLE: Zig Bitwise And Operator
DESCRIPTION: Performs bitwise AND operation on integers. Invokes peer type resolution for operands.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
0b011 & 0b101 == 0b001
```

--------------------------------

TITLE: Zig Atomic Read-Modify-Write Operations
DESCRIPTION: Defines the types of read-modify-write operations supported for atomic variables in Zig, such as Xchg, Add, Sub, and bitwise operations.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.3.0/index.html

LANGUAGE: zig
CODE:
```
pub const AtomicRmwOp = enum {
        Xchg,
        Add,
        Sub,
        And,
        Nand,
        Or,
        Xor,
        Max,
        Min,
    };

```

--------------------------------

TITLE: Zig File System Error Sets and Iterator Fixes
DESCRIPTION: Defines a static error set for `fs.Dir.copyFile` and fixes issues with using `fs.Dir.Iterator` twice. Improves handling of `ENOENT` errors during iteration.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.10.0/release-notes.html

LANGUAGE: Zig
CODE:
```
Define static error set for fs.Dir.copyFile
Fixed using fs.Dir.Iterator twice
End iteration on Linux/WASI during Iterator.next when hitting `ENOENT`
```

--------------------------------

TITLE: Zig: Field Access by Compile-Time String
DESCRIPTION: Demonstrates accessing struct fields and declarations using a compile-time string with the `@field` built-in function. This function works for both instance fields and static declarations within types.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

const Point = struct {
    x: u32,
    y: u32,

    pub var z: u32 = 1;
};

test "field access by string" {
    const expect = std.testing.expect;
    var p = Point{ .x = 0, .y = 0 };

    @field(p, "x") = 4;
    @field(p, "y") = @field(p, "x") + 1;

    try expect(@field(p, "x") == 4);
    try expect(@field(p, "y") == 5);
}

test "decl access by string" {
    const expect = std.testing.expect;

    try expect(@field(Point, "z") == 1);

    @field(Point, "z") = 2;
    try expect(@field(Point, "z") == 2);
}
```

--------------------------------

TITLE: Zig Doc Comments for Structs and Functions
DESCRIPTION: Demonstrates the use of documentation comments (`///`) in Zig for documenting structs, their fields, and their methods. It shows how multiple `///` comments are merged into a multiline doc comment.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.7.0/index.html

LANGUAGE: Zig
CODE:
```
/// A structure for storing a timestamp, with nanosecond precision (this is a
/// multiline doc comment).
const Timestamp = struct {
    /// The number of seconds since the epoch (this is also a doc comment).
    seconds: i64,  // signed so we can represent pre-1970 (not a doc comment)
    /// The number of nanoseconds past the second (doc comment again).
    nanos: u32,

    /// Returns a `Timestamp` struct representing the Unix epoch; that is, the
    /// moment of 1970 Jan 1 00:00:00 UTC (this is a doc comment too).
    pub fn unixEpoch() Timestamp {
        return Timestamp{
            .seconds = 0,
            .nanos = 0,
        };
    }
};
```

--------------------------------

TITLE: Zig Built-in Function: @atomicLoad
DESCRIPTION: Introduces the `@atomicLoad` built-in function for atomic memory operations. This function is crucial for concurrent programming, ensuring that memory reads are performed atomically to prevent race conditions.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.3.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const value = @atomicLoad(pointer_to_atomic_variable, .Acquire);
```

--------------------------------

TITLE: Zig Builtin: @byteOffsetOf
DESCRIPTION: The `@byteOffsetOf` builtin function returns the byte offset of a field within a struct.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: Zig
CODE:
```
const MyStruct = struct {
    a: u8,
    b: u16,
};
const offset_b = @byteOffsetOf(MyStruct, "b");
// offset_b is 1 (since 'a' is 1 byte)
```

--------------------------------

TITLE: Zig Test Execution for Anonymous Struct
DESCRIPTION: Displays the shell command to test the Zig code with a fully anonymous struct literal and the successful test execution output.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Shell
CODE:
```
$ zig test test_anonymous_struct.zig
1/1 test_anonymous_struct.test.fully anonymous struct... OK
All 1 tests passed.
```

--------------------------------

TITLE: Define and Use dev_t for Linux x86_64
DESCRIPTION: This change introduces the definition and usage of `dev_t` for the Linux x86_64 architecture within the OS module. `dev_t` is a data type used to represent device IDs in Unix-like systems.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.7.1/release-notes.html

LANGUAGE: zig
CODE:
```
const dev_t = c_long;
```

--------------------------------

TITLE: Zig @TypeOf Example: No Runtime Side Effects
DESCRIPTION: Demonstrates the usage of the @TypeOf builtin function in Zig, showing that it has no runtime side effects. It verifies the type of a function's return value and checks that a variable remains unchanged.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.15.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "no runtime side effects" {
    var data: i32 = 0;
    const T = @TypeOf(foo(i32, &data));
    try comptime expect(T == i32);
    try expect(data == 0);
}

fn foo(comptime T: type, ptr: *T) T {
    ptr.* += 1;
    return ptr.*;
}
```

--------------------------------

TITLE: Zig @TypeOf Example with No Runtime Side Effects
DESCRIPTION: Demonstrates the usage of the @TypeOf built-in function in Zig, specifically highlighting that expressions passed to it are guaranteed to have no runtime side-effects. This example tests that the type of a function's return value is correctly inferred and that a variable passed by pointer is not modified.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.1/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "no runtime side effects" {
    var data: i32 = 0;
    const T = @TypeOf(foo(i32, &data));
    comptime try expect(T == i32);
    try expect(data == 0);
}

fn foo(comptime T: type, ptr: *T) T {
    ptr.* += 1;
    return ptr.*;
}
```

--------------------------------

TITLE: Optimize Timer.lap to Read System Time Once
DESCRIPTION: The `std.time.Timer.lap` function has been optimized to read the system time only once per call. This reduces overhead and improves the accuracy of timing measurements.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
std.time.Timer.lap
```

--------------------------------

TITLE: Integrate Mutex with pthreads
DESCRIPTION: Integrates Mutex with pthreads for improved thread synchronization in Zig, providing a more robust and standard-compliant mutex implementation.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.8.0/release-notes.html



--------------------------------

TITLE: Zig SIMD: Vector Element Access
DESCRIPTION: Shows how Zig's SIMD vectors support element access syntax, allowing individual elements to be read or modified. This example demonstrates setting and retrieving values from a vector.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/download/0.6.0/release-notes.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

test "vector element access" {
    var v: @Vector(4, i32) = [_]i32{ 1, 5, 3, undefined };
    v[2] = 42;
    expect(v[1] == 5);
    v[3] = -364;
    expect(v[2] == 42);
    expect(-364 == v[3]);

    storev(&v[0], 100);
    expect(v[0] == 100);
}

fn storev(ptr: var, x: i32) void {
    ptr.* = x;
}
```

--------------------------------

TITLE: Zig Atomics: Atomic Store
DESCRIPTION: Shows how to perform an atomic store operation in Zig, writing a value to memory atomically. This ensures that writes to shared memory are indivisible and visible to other threads correctly.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.12.1/index.html

LANGUAGE: Zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_var: std.atomic.Atomic(u32) = std.atomic.Atomic(u32){ .value = 0 };
    std.atomic.store(u32, &atomic_var, 20, .SeqCst);
    const value = std.atomic.load(u32, &atomic_var, .SeqCst);
    std.debug.print("Stored value: {d}\n", .{value});
}
```

--------------------------------

TITLE: Zig Function Type Inference Test
DESCRIPTION: Tests the type inference capabilities of Zig functions. It verifies that the return type of `addFortyTwo` is correctly inferred as `comptime_int` for a literal input and `i64` for a variable input.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.8.1/index.html

LANGUAGE: zig
CODE:
```
test "fn type inference" {
        try expect(addFortyTwo(1) == 43);
        try expect(@TypeOf(addFortyTwo(1)) == comptime_int);
        var y: i64 = 2;
        try expect(addFortyTwo(y) == 44);
        try expect(@TypeOf(addFortyTwo(y)) == i64);
    }
```

--------------------------------

TITLE: Zig: Coerce Pointers to Slices and Arrays
DESCRIPTION: Demonstrates coercing constant pointers to arrays into slices, including handling different destination types like error unions and optionals. It also shows how array lengths map to slice lengths and how single-item pointers can be coerced to arrays.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.9.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");
const expect = std.testing.expect;

// You can assign constant pointers to arrays to a slice with
// const modifier on the element type. Useful in particular for
// String literals.
test "*const [N]T to []const T" {
    var x1: []const u8 = "hello";
    var x2: []const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, x1, x2));

    var y: []const f32 = &[2]f32{ 1.2, 3.4 };
    try expect(y[0] == 1.2);
}

// Likewise, it works when the destination type is an error union.
test "*const [N]T to E![]const T" {
    var x1: anyerror![]const u8 = "hello";
    var x2: anyerror![]const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, try x1, try x2));

    var y: anyerror![]const f32 = &[2]f32{ 1.2, 3.4 };
    try expect((try y)[0] == 1.2);
}

// Likewise, it works when the destination type is an optional.
test "*const [N]T to ?[]const T" {
    var x1: ?[]const u8 = "hello";
    var x2: ?[]const u8 = &[5]u8{ 'h', 'e', 'l', 'l', 111 };
    try expect(std.mem.eql(u8, x1.?, x2.?));

    var y: ?[]const f32 = &[2]f32{ 1.2, 3.4 };
    try expect(y.?[0] == 1.2);
}

// In this cast, the array length becomes the slice length.
test "*[N]T to []T" {
    var buf: [5]u8 = "hello".*;
    const x: []u8 = &buf;
    try expect(std.mem.eql(u8, x, "hello"));

    const buf2 = [2]f32{ 1.2, 3.4 };
    const x2: []const f32 = &buf2;
    try expect(std.mem.eql(f32, x2, &[2]f32{ 1.2, 3.4 }));
}

// Single-item pointers to arrays can be coerced to many-item pointers.
test "*[N]T to [*]T" {
    var buf: [5]u8 = "hello".*;
    const x: [*]u8 = &buf;
    try expect(x[4] == 'o');
    // x[5] would be an uncaught out of bounds pointer dereference!
}

// Likewise, it works when the destination type is an optional.
test "*[N]T to ?[*]T" {
    var buf: [5]u8 = "hello".*;
    const x: ?[*]u8 = &buf;
    try expect(x.?[4] == 'o');
}

// Single-item pointers can be cast to len-1 single-item arrays.
test "*T to *[1]T" {
    var x: i32 = 1234;
    const y: *[1]i32 = &x;
    const z: [*]i32 = y;
    try expect(z[0] == 1234);
}
```

--------------------------------

TITLE: Zig Builtin: @atomicLoad
DESCRIPTION: Illustrates the `@atomicLoad` builtin function in Zig for performing atomic reads from memory. This ensures that values are read without interference from other threads.

SOURCE: https://github.com/ziglang/www.ziglang.org/blob/main/src/documentation/0.11.0/index.html

LANGUAGE: zig
CODE:
```
const std = @import("std");

pub fn main() void {
    var atomic_counter: std.atomic.Atomic(u64) = .{ .value = 100 };
    const current_value = @atomicLoad(u64, &atomic_counter, .SeqCst);
    std.debug.print("Current atomic value: {d}", .{current_value});
}

```
</file>

<file path=".gitignore">
# Zig build artifacts
.zig-cache/
zig-out/
zig-cache/

# Object files
*.o
*.obj

# Editor/IDE specific
.vscode/
.idea/
*.swp
*.swo
*~

# OS specific
.DS_Store
Thumbs.db

# Debug files
*.pdb

# Temporary files
*.tmp
*.temp

# Build outputs
*.exe
*.dll
*.so
*.dylib
*.a
*.lib
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 bkataru (Baalateja Kataru)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="build.zig.zon">
.{
    .fingerprint = 0x6623cd56cb3aca0c,
    .name = .pocketflow,
    .version = "0.2.0",
    .minimum_zig_version = "0.15.0",
    .paths = .{
        "build.zig",
        "build.zig.zon",
        "src",
        "examples",
        "README.md",
        "LICENSE",
    },
}
</file>

<file path="src/flow.zig">
/// The `Flow` manages the execution of nodes in a graph.
const std = @import("std");
const Allocator = std.mem.Allocator;
const testing = std.testing;

const Action = @import("node.zig").Action;
const BaseNode = @import("node.zig").BaseNode;
const Context = @import("context.zig").Context;
const Node = @import("node.zig").Node;

pub const Flow = struct {
    start_node: Node,
    allocator: Allocator,

    pub fn init(allocator: Allocator, start_node: Node) Flow {
        return .{
            .allocator = allocator,
            .start_node = start_node,
        };
    }

    pub fn run(self: *Flow, context: *Context) !void {
        var current_node: ?Node = self.start_node;

        while (current_node) |node| {
            const prep_res = try node.prep(self.allocator, context);
            defer node.cleanupPrep(self.allocator, prep_res);

            const exec_res = try node.exec(self.allocator, prep_res);
            defer node.cleanupExec(self.allocator, exec_res);

            const action = try node.post(self.allocator, context, prep_res, exec_res);

            const base_node: *BaseNode = @ptrCast(@alignCast(node.self));

            current_node = base_node.successors.get(action);
        }
    }
};

// ============================================================================
// TESTS
// ============================================================================

/// A counter node for testing flow execution
const CounterNode = struct {
    base: BaseNode,
    exec_count: *usize,
    node_id: usize,
    return_action: []const u8,

    pub fn init(allocator: Allocator, exec_count: *usize, node_id: usize, return_action: []const u8) *CounterNode {
        const self = allocator.create(CounterNode) catch @panic("oom");
        self.* = .{
            .base = BaseNode.init(allocator),
            .exec_count = exec_count,
            .node_id = node_id,
            .return_action = return_action,
        };
        return self;
    }

    pub fn deinit(self: *CounterNode, allocator: Allocator) void {
        self.base.deinit();
        allocator.destroy(self);
    }

    pub fn prep(_: *anyopaque, allocator: Allocator, _: *Context) !*anyopaque {
        const result = try allocator.create(u8);
        result.* = 0;
        return @ptrCast(result);
    }

    pub fn exec(self_ptr: *anyopaque, allocator: Allocator, _: *anyopaque) !*anyopaque {
        const self: *CounterNode = @ptrCast(@alignCast(self_ptr));
        self.exec_count.* += 1;

        const result = try allocator.create(usize);
        result.* = self.node_id;
        return @ptrCast(result);
    }

    pub fn post(self_ptr: *anyopaque, _: Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) !Action {
        const self: *CounterNode = @ptrCast(@alignCast(self_ptr));
        const node_id: *usize = @ptrCast(@alignCast(exec_res));

        // Store the last executed node id in context
        try context.set("last_node", node_id.*);

        return self.return_action;
    }

    pub fn cleanup_prep(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) void {
        const ptr: *u8 = @ptrCast(@alignCast(prep_res));
        allocator.destroy(ptr);
    }

    pub fn cleanup_exec(_: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void {
        const ptr: *usize = @ptrCast(@alignCast(exec_res));
        allocator.destroy(ptr);
    }

    pub const VTABLE = Node.VTable{
        .prep = prep,
        .exec = exec,
        .post = post,
        .cleanup_prep = cleanup_prep,
        .cleanup_exec = cleanup_exec,
    };
};

test "Flow - init" {
    var exec_count: usize = 0;
    const node = CounterNode.init(testing.allocator, &exec_count, 1, "default");
    defer node.deinit(testing.allocator);

    const wrapper = Node{ .self = node, .vtable = &CounterNode.VTABLE };
    const flow = Flow.init(testing.allocator, wrapper);

    try testing.expect(flow.start_node.self == @as(*anyopaque, @ptrCast(node)));
}

test "Flow - run single node" {
    var exec_count: usize = 0;
    const node = CounterNode.init(testing.allocator, &exec_count, 1, "end");
    defer node.deinit(testing.allocator);

    const wrapper = Node{ .self = node, .vtable = &CounterNode.VTABLE };
    var flow = Flow.init(testing.allocator, wrapper);

    var context = Context.init(testing.allocator);
    defer context.deinit();

    try flow.run(&context);

    try testing.expectEqual(@as(usize, 1), exec_count);
    try testing.expectEqual(@as(?usize, 1), context.get(usize, "last_node"));
}

test "Flow - run chain of nodes" {
    var exec_count: usize = 0;

    const node1 = CounterNode.init(testing.allocator, &exec_count, 1, "default");
    defer node1.deinit(testing.allocator);

    const node2 = CounterNode.init(testing.allocator, &exec_count, 2, "default");
    defer node2.deinit(testing.allocator);

    const node3 = CounterNode.init(testing.allocator, &exec_count, 3, "end");
    defer node3.deinit(testing.allocator);

    const wrapper1 = Node{ .self = node1, .vtable = &CounterNode.VTABLE };
    const wrapper2 = Node{ .self = node2, .vtable = &CounterNode.VTABLE };
    const wrapper3 = Node{ .self = node3, .vtable = &CounterNode.VTABLE };

    // Chain: node1 -> node2 -> node3
    try node1.base.next("default", wrapper2);
    try node2.base.next("default", wrapper3);

    var flow = Flow.init(testing.allocator, wrapper1);

    var context = Context.init(testing.allocator);
    defer context.deinit();

    try flow.run(&context);

    try testing.expectEqual(@as(usize, 3), exec_count);
    try testing.expectEqual(@as(?usize, 3), context.get(usize, "last_node"));
}

test "Flow - branching based on action" {
    var exec_count: usize = 0;

    const node1 = CounterNode.init(testing.allocator, &exec_count, 1, "branch_a");
    defer node1.deinit(testing.allocator);

    const node_a = CounterNode.init(testing.allocator, &exec_count, 10, "end");
    defer node_a.deinit(testing.allocator);

    const node_b = CounterNode.init(testing.allocator, &exec_count, 20, "end");
    defer node_b.deinit(testing.allocator);

    const wrapper1 = Node{ .self = node1, .vtable = &CounterNode.VTABLE };
    const wrapper_a = Node{ .self = node_a, .vtable = &CounterNode.VTABLE };
    const wrapper_b = Node{ .self = node_b, .vtable = &CounterNode.VTABLE };

    // Branch: node1 can go to node_a or node_b
    try node1.base.next("branch_a", wrapper_a);
    try node1.base.next("branch_b", wrapper_b);

    var flow = Flow.init(testing.allocator, wrapper1);

    var context = Context.init(testing.allocator);
    defer context.deinit();

    try flow.run(&context);

    // Should have executed node1 and node_a (branch_a path)
    try testing.expectEqual(@as(usize, 2), exec_count);
    try testing.expectEqual(@as(?usize, 10), context.get(usize, "last_node"));
}

test "Flow - stops when no successor found" {
    var exec_count: usize = 0;

    const node1 = CounterNode.init(testing.allocator, &exec_count, 1, "nonexistent");
    defer node1.deinit(testing.allocator);

    const wrapper1 = Node{ .self = node1, .vtable = &CounterNode.VTABLE };

    var flow = Flow.init(testing.allocator, wrapper1);

    var context = Context.init(testing.allocator);
    defer context.deinit();

    try flow.run(&context);

    // Should only execute the first node, then stop (no successor for "nonexistent")
    try testing.expectEqual(@as(usize, 1), exec_count);
}
</file>

<file path="build.zig">
const std = @import("std");

pub fn build(b: *std.Build) void {
    const target = b.standardTargetOptions(.{});
    const optimize = b.standardOptimizeOption(.{});

    // Create and export the PocketFlow module (public, accessible to dependents)
    const pocketflow_mod = b.addModule("pocketflow", .{
        .root_source_file = b.path("src/pocketflow.zig"),
        .target = target,
        .optimize = optimize,
    });

    // Create and export the Ollama module (public, accessible to dependents)
    const ollama_mod = b.addModule("ollama", .{
        .root_source_file = b.path("src/ollama.zig"),
        .target = target,
        .optimize = optimize,
    });

    // Build the PocketFlow library
    const lib = b.addLibrary(.{
        .name = "pocketflow",
        .root_module = pocketflow_mod,
    });

    b.installArtifact(lib);

    // Build the document generator example executable
    const example_mod = b.createModule(.{
        .root_source_file = b.path("examples/document_generator.zig"),
        .target = target,
        .optimize = optimize,
    });

    // Add imports to the example module
    example_mod.addImport("pocketflow", pocketflow_mod);
    example_mod.addImport("ollama", ollama_mod);

    const example_exe = b.addExecutable(.{
        .name = "document_generator",
        .root_module = example_mod,
    });

    b.installArtifact(example_exe);

    const run_cmd = b.addRunArtifact(example_exe);
    run_cmd.step.dependOn(b.getInstallStep());

    if (b.args) |args| {
        run_cmd.addArgs(args);
    }

    const run_step = b.step("run", "Run the document generator example");
    run_step.dependOn(&run_cmd.step);

    // Unit tests for core modules
    const node_tests = b.addTest(.{
        .root_module = b.createModule(.{
            .root_source_file = b.path("src/node.zig"),
            .target = target,
            .optimize = optimize,
        }),
    });

    const context_tests = b.addTest(.{
        .root_module = b.createModule(.{
            .root_source_file = b.path("src/context.zig"),
            .target = target,
            .optimize = optimize,
        }),
    });

    const flow_tests = b.addTest(.{
        .root_module = b.createModule(.{
            .root_source_file = b.path("src/flow.zig"),
            .target = target,
            .optimize = optimize,
        }),
    });

    const ollama_tests = b.addTest(.{
        .root_module = b.createModule(.{
            .root_source_file = b.path("src/ollama.zig"),
            .target = target,
            .optimize = optimize,
        }),
    });

    const run_node_tests = b.addRunArtifact(node_tests);
    const run_context_tests = b.addRunArtifact(context_tests);
    const run_flow_tests = b.addRunArtifact(flow_tests);
    const run_ollama_tests = b.addRunArtifact(ollama_tests);

    // Test step to run all unit tests
    const test_step = b.step("test", "Run all unit tests");
    test_step.dependOn(&run_node_tests.step);
    test_step.dependOn(&run_context_tests.step);
    test_step.dependOn(&run_flow_tests.step);
    test_step.dependOn(&run_ollama_tests.step);

    // Integration test step (requires Ollama server)
    const integration_test_step = b.step("test-integration", "Run integration tests (requires Ollama server)");
    integration_test_step.dependOn(&run_ollama_tests.step);
}
</file>

<file path="README.md">
# PocketFlow-Zig

A Zig implementation of [PocketFlow](https://github.com/The-Pocket/PocketFlow), a minimalist flow-based programming framework for building LLM-powered workflows.

## Overview

PocketFlow-Zig is a port of the original Python PocketFlow framework, redesigned to leverage Zig's unique capabilities:

- **Compile-time polymorphism**: Uses vtables for type-erased node interfaces without runtime overhead
- **Explicit memory management**: No hidden allocations; all memory is managed through Zig allocators
- **Thread-safe context**: Built-in mutex protection for shared state between nodes
- **Zero dependencies**: Core framework has no external dependencies (Ollama client is optional)

## Features

- **Node-based architecture**: Define workflows as a graph of interconnected nodes
- **Type-erased interfaces**: Generic `Node` interface via vtables enables heterogeneous node types
- **Flow execution engine**: Automatic traversal and execution of node graphs
- **Thread-safe shared context**: Safe data passing between nodes with mutex protection
- **Ollama integration**: Built-in client for local LLM inference (optional)
- **Action-based routing**: Nodes return actions that determine the next node in the flow

## Installation

### Method 1: Using `zig fetch` (Recommended)

The easiest way to add PocketFlow-Zig to your project is using `zig fetch --save`:

```bash
# Fetch from a GitHub release tarball (recommended for stability)
zig fetch --save https://github.com/The-Pocket/PocketFlow-Zig/archive/refs/tags/v0.2.0.tar.gz

# Or fetch directly from a git repository
zig fetch --save git+https://github.com/The-Pocket/PocketFlow-Zig.git

# You can also specify a custom name for the dependency
zig fetch --save=pocketflow https://github.com/The-Pocket/PocketFlow-Zig/archive/refs/tags/v0.2.0.tar.gz
```

This automatically:
1. Downloads the package to Zig's global cache
2. Computes the package hash
3. Adds the dependency to your `build.zig.zon` file

After running `zig fetch --save`, your `build.zig.zon` will contain something like:

```zig
.dependencies = .{
    .pocketflow = .{
        .url = "https://github.com/The-Pocket/PocketFlow-Zig/archive/refs/tags/v0.2.0.tar.gz",
        .hash = "1220...", // Auto-generated hash
    },
},
```

Then add the import in your `build.zig`:

```zig
const std = @import("std");

pub fn build(b: *std.Build) void {
    const target = b.standardTargetOptions(.{});
    const optimize = b.standardOptimizeOption(.{});

    // Fetch the pocketflow dependency
    const pocketflow_dep = b.dependency("pocketflow", .{
        .target = target,
        .optimize = optimize,
    });

    // Get the module from the dependency
    const pocketflow_mod = pocketflow_dep.module("pocketflow");

    // Create your executable
    const exe = b.addExecutable(.{
        .name = "my_app",
        .root_module = b.createModule(.{
            .root_source_file = b.path("src/main.zig"),
            .target = target,
            .optimize = optimize,
        }),
    });

    // Add the pocketflow import to your executable
    exe.root_module.addImport("pocketflow", pocketflow_mod);

    // Optional: also add the ollama module for LLM integration
    const ollama_mod = pocketflow_dep.module("ollama");
    exe.root_module.addImport("ollama", ollama_mod);

    b.installArtifact(exe);
}
```

### Method 2: Manual `build.zig.zon` Configuration

If you prefer to manually configure your dependencies, add the following to your `build.zig.zon`:

```zig
.{
    .name = .my_project,
    .version = "0.1.0",
    .minimum_zig_version = "0.15.0",
    .dependencies = .{
        .pocketflow = .{
            .url = "https://github.com/The-Pocket/PocketFlow-Zig/archive/refs/tags/v0.2.0.tar.gz",
            // Get the hash by running: zig fetch https://github.com/The-Pocket/PocketFlow-Zig/archive/refs/tags/v0.2.0.tar.gz
            .hash = "1220...",
        },
    },
    .paths = .{
        "build.zig",
        "build.zig.zon",
        "src",
    },
}
```

To get the correct hash, run:

```bash
zig fetch https://github.com/The-Pocket/PocketFlow-Zig/archive/refs/tags/v0.2.0.tar.gz
```

This prints the hash without modifying any files.

### Method 3: Git-based Dependency

For development or to track the latest changes:

```zig
.dependencies = .{
    .pocketflow = .{
        .url = "git+https://github.com/The-Pocket/PocketFlow-Zig.git",
        .hash = "1220...",
    },
},
```

Or with `zig fetch`:

```bash
zig fetch --save git+https://github.com/The-Pocket/PocketFlow-Zig.git
```

### Method 4: Local Path Dependency

For local development or when vendoring:

```zig
.dependencies = .{
    .pocketflow = .{
        .path = "../PocketFlow-Zig",
    },
},
```

### Building from Source

```bash
# Clone the repository
git clone https://github.com/The-Pocket/PocketFlow-Zig.git
cd PocketFlow-Zig

# Build the library
zig build

# Run the example (requires Ollama running locally)
zig build run

# Run tests
zig build test
```

## Quick Start

### 1. Define a Custom Node

Each node implements prep, exec, and post phases:

```zig
const std = @import("std");
const pocketflow = @import("pocketflow");
const Node = pocketflow.Node;
const BaseNode = pocketflow.BaseNode;
const Context = pocketflow.Context;

const MyNode = struct {
    base: BaseNode,

    pub fn init(allocator: std.mem.Allocator) *MyNode {
        const self = allocator.create(MyNode) catch @panic("oom");
        self.* = .{ .base = BaseNode.init(allocator) };
        return self;
    }

    pub fn deinit(self: *MyNode, allocator: std.mem.Allocator) void {
        self.base.deinit();
        allocator.destroy(self);
    }

    // Prepare: read from context, prepare data for execution
    pub fn prep(_: *anyopaque, allocator: std.mem.Allocator, context: *Context) !*anyopaque {
        const input = context.get([]const u8, "input") orelse "default";
        const result = try allocator.create([]const u8);
        result.* = input;
        return @ptrCast(result);
    }

    // Execute: perform the main work (can be CPU-intensive)
    pub fn exec(_: *anyopaque, allocator: std.mem.Allocator, prep_res: *anyopaque) !*anyopaque {
        const input: *[]const u8 = @ptrCast(@alignCast(prep_res));
        const output = try std.fmt.allocPrint(allocator, "Processed: {s}", .{input.*});
        const result = try allocator.create([]const u8);
        result.* = output;
        return @ptrCast(result);
    }

    // Post: save results to context, return action for routing
    pub fn post(_: *anyopaque, _: std.mem.Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) ![]const u8 {
        const output: *[]const u8 = @ptrCast(@alignCast(exec_res));
        try context.set("output", output.*);
        return "default"; // Action determines next node
    }

    pub fn cleanup_prep(_: *anyopaque, allocator: std.mem.Allocator, prep_res: *anyopaque) void {
        const ptr: *[]const u8 = @ptrCast(@alignCast(prep_res));
        allocator.destroy(ptr);
    }

    pub fn cleanup_exec(_: *anyopaque, allocator: std.mem.Allocator, exec_res: *anyopaque) void {
        const ptr: *[]const u8 = @ptrCast(@alignCast(exec_res));
        allocator.destroy(ptr);
    }

    pub const VTABLE = Node.VTable{
        .prep = prep,
        .exec = exec,
        .post = post,
        .cleanup_prep = cleanup_prep,
        .cleanup_exec = cleanup_exec,
    };
};
```

### 2. Build and Run a Flow

```zig
const pocketflow = @import("pocketflow");
const Flow = pocketflow.Flow;
const Context = pocketflow.Context;
const Node = pocketflow.Node;

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Create nodes
    const node1 = MyNode.init(allocator);
    defer node1.deinit(allocator);
    
    const node2 = MyNode.init(allocator);
    defer node2.deinit(allocator);

    // Wrap nodes with their vtables
    const wrapper1 = Node{ .self = node1, .vtable = &MyNode.VTABLE };
    const wrapper2 = Node{ .self = node2, .vtable = &MyNode.VTABLE };

    // Connect nodes: node1 --"default"--> node2
    try node1.base.next("default", wrapper2);

    // Create and run flow
    var flow = Flow.init(allocator, wrapper1);
    
    var context = Context.init(allocator);
    defer context.deinit();
    
    try context.set("input", @as([]const u8, "Hello, PocketFlow!"));
    try flow.run(&context);
    
    if (context.get([]const u8, "output")) |output| {
        std.debug.print("Result: {s}\n", .{output});
    }
}
```

### 3. Branching Flows

Nodes can return different actions to route to different successors:

```zig
pub fn post(_: *anyopaque, _: Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) ![]const u8 {
    const result: *i32 = @ptrCast(@alignCast(exec_res));
    try context.set("result", result.*);
    
    // Branch based on result
    if (result.* > 100) {
        return "high";
    } else {
        return "low";
    }
}

// Later, when connecting nodes:
try node1.base.next("high", high_value_handler);
try node1.base.next("low", low_value_handler);
```

## Architecture

### Core Components

| Component | Description |
|-----------|-------------|
| `Node` | Type-erased interface for workflow steps (via vtable) |
| `BaseNode` | Provides successor management for routing |
| `Flow` | Executes nodes in sequence, following action-based routing |
| `Context` | Thread-safe key-value store for sharing data between nodes |

### Node Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  prep   â”‚ --> â”‚  exec   â”‚ --> â”‚  post   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     v               v               v
 Read from       Process         Write to
 Context         Data            Context
                                     â”‚
                                     v
                              Return Action
                              (routes to next node)
```

## Examples

See the `examples/` directory:

- **document_generator.zig**: Multi-node flow that generates documents using Ollama LLM
  - Generates an outline for a topic
  - Writes content for each outline point
  - Assembles the final document

Run the example:

```bash
# Requires Ollama running locally on port 11434
zig build run
```

## API Reference

### Context

```zig
// Initialize a new context
var ctx = Context.init(allocator);
defer ctx.deinit();

// Store a value
try ctx.set("key", value);

// Retrieve a value (returns null if not found)
if (ctx.get(MyType, "key")) |value| {
    // use value
}
```

### Flow

```zig
// Create a flow starting at a node
var flow = Flow.init(allocator, start_node);

// Run the flow with a context
try flow.run(&context);
```

### BaseNode

```zig
// Add a successor for an action
try node.base.next("action_name", successor_node);
```

## Testing

```bash
# Run all unit tests
zig build test

# Run integration tests (requires Ollama server)
zig build test-integration
```

## Project Structure

```
PocketFlow-Zig/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pocketflow.zig    # Main library exports
â”‚   â”œâ”€â”€ node.zig          # Node interface and BaseNode
â”‚   â”œâ”€â”€ flow.zig          # Flow execution engine
â”‚   â”œâ”€â”€ context.zig       # Thread-safe context storage
â”‚   â””â”€â”€ ollama.zig        # Ollama LLM client (optional)
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ document_generator.zig
â”œâ”€â”€ build.zig
â”œâ”€â”€ build.zig.zon
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## Contributing

Contributions are welcome! Areas of interest:

1. **Async support**: Implement async node execution using Zig 0.15+ async I/O
2. **Batch processing**: Add BatchNode for processing multiple items
3. **More examples**: Additional workflow examples (RAG, agents, etc.)
4. **Performance**: Benchmarks and optimizations

Please submit pull requests or open issues for discussion.

## Requirements

- Zig 0.15.0 or later
- (Optional) Ollama for LLM integration

## License

[MIT License](LICENSE)
</file>

<file path="src/context.zig">
/// The `Context` is a thread-safe hash map that holds the shared state between nodes.
const std = @import("std");
const Allocator = std.mem.Allocator;
const StringHashMap = std.StringHashMap;
const testing = std.testing;

const StoredValue = struct {
    ptr: *anyopaque,
    destructor: *const fn (allocator: Allocator, ptr: *anyopaque) void,
};

pub const Context = struct {
    allocator: Allocator,
    data: StringHashMap(StoredValue),
    mutex: std.Thread.Mutex,

    pub fn init(allocator: Allocator) Context {
        return .{
            .allocator = allocator,
            .data = StringHashMap(StoredValue).init(allocator),
            .mutex = .{},
        };
    }

    pub fn deinit(self: *Context) void {
        self.mutex.lock();
        defer self.mutex.unlock();

        // Free all stored values using their destructors
        var key_it = self.data.iterator();
        while (key_it.next()) |entry| {
            entry.value_ptr.destructor(self.allocator, entry.value_ptr.ptr);
        }

        self.data.deinit();
    }

    pub fn get(self: *Context, comptime T: type, key: []const u8) ?T {
        self.mutex.lock();
        defer self.mutex.unlock();

        if (self.data.get(key)) |stored_value| {
            const typed_ptr: *const T = @ptrCast(@alignCast(stored_value.ptr));
            return typed_ptr.*;
        }
        return null;
    }

    pub fn set(self: *Context, key: []const u8, value: anytype) !void {
        self.mutex.lock();
        defer self.mutex.unlock();

        const T = @TypeOf(value);

        // Check if we're replacing an existing value
        if (self.data.get(key)) |old_stored_value| {
            old_stored_value.destructor(self.allocator, old_stored_value.ptr);
        }

        // Always allocate space for T and store a pointer to it
        // This ensures consistent storage regardless of T being a pointer, struct, slice, etc.
        const ptr = try self.allocator.create(T);
        ptr.* = value;

        // Create a destructor function for this type
        const destructor = struct {
            fn destroy(allocator: Allocator, p: *anyopaque) void {
                const typed_ptr: *T = @ptrCast(@alignCast(p));
                allocator.destroy(typed_ptr);
            }
        }.destroy;

        const stored_value = StoredValue{
            .ptr = @ptrCast(ptr),
            .destructor = destructor,
        };

        try self.data.put(key, stored_value);
    }
};

// ============================================================================
// TESTS
// ============================================================================

test "Context - init and deinit" {
    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    // Context should be usable after init
    try ctx.set("test", @as(i32, 42));
    const value = ctx.get(i32, "test");
    try testing.expectEqual(@as(?i32, 42), value);
}

test "Context - set and get basic types" {
    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    // Test i32
    try ctx.set("int_val", @as(i32, 123));
    try testing.expectEqual(@as(?i32, 123), ctx.get(i32, "int_val"));

    // Test u64
    try ctx.set("u64_val", @as(u64, 999999999999));
    try testing.expectEqual(@as(?u64, 999999999999), ctx.get(u64, "u64_val"));

    // Test bool
    try ctx.set("bool_val", true);
    try testing.expectEqual(@as(?bool, true), ctx.get(bool, "bool_val"));

    // Test f32
    try ctx.set("float_val", @as(f32, 3.14));
    try testing.expectEqual(@as(?f32, 3.14), ctx.get(f32, "float_val"));
}

test "Context - get non-existent key returns null" {
    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    const result = ctx.get(i32, "non_existent");
    try testing.expectEqual(@as(?i32, null), result);
}

test "Context - set replaces existing value" {
    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    try ctx.set("key", @as(i32, 100));
    try testing.expectEqual(@as(?i32, 100), ctx.get(i32, "key"));

    try ctx.set("key", @as(i32, 200));
    try testing.expectEqual(@as(?i32, 200), ctx.get(i32, "key"));
}

test "Context - set and get slices" {
    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    const original = "Hello, PocketFlow!";
    const slice: []const u8 = original;
    try ctx.set("message", slice);

    const retrieved = ctx.get([]const u8, "message");
    try testing.expect(retrieved != null);
    try testing.expectEqualStrings(original, retrieved.?);
}

test "Context - multiple keys" {
    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    try ctx.set("key1", @as(i32, 1));
    try ctx.set("key2", @as(i32, 2));
    try ctx.set("key3", @as(i32, 3));

    try testing.expectEqual(@as(?i32, 1), ctx.get(i32, "key1"));
    try testing.expectEqual(@as(?i32, 2), ctx.get(i32, "key2"));
    try testing.expectEqual(@as(?i32, 3), ctx.get(i32, "key3"));
}

test "Context - set and get struct" {
    const TestStruct = struct {
        x: i32,
        y: i32,
        name: []const u8,
    };

    var ctx = Context.init(testing.allocator);
    defer ctx.deinit();

    const value = TestStruct{ .x = 10, .y = 20, .name = "test" };
    try ctx.set("struct_val", value);

    const retrieved = ctx.get(TestStruct, "struct_val");
    try testing.expect(retrieved != null);
    try testing.expectEqual(@as(i32, 10), retrieved.?.x);
    try testing.expectEqual(@as(i32, 20), retrieved.?.y);
    try testing.expectEqualStrings("test", retrieved.?.name);
}
</file>

<file path="src/node.zig">
/// The `Node` defines the basic unit of work with `prep`, `exec`, and `post` methods.
/// We use a vtable to create a generic interface.
const std = @import("std");
const Allocator = std.mem.Allocator;
const testing = std.testing;

const Context = @import("context.zig").Context;

pub const Action = []const u8;

pub const Node = struct {
    self: *anyopaque,
    vtable: *const VTable,

    pub const VTable = struct {
        prep: *const fn (self: *anyopaque, allocator: Allocator, context: *Context) anyerror!*anyopaque,
        exec: *const fn (self: *anyopaque, allocator: Allocator, prep_res: *anyopaque) anyerror!*anyopaque,
        post: *const fn (self: *anyopaque, allocator: Allocator, context: *Context, prep_res: *anyopaque, exec_res: *anyopaque) anyerror!Action,
        cleanup_prep: *const fn (self: *anyopaque, allocator: Allocator, prep_res: *anyopaque) void,
        cleanup_exec: *const fn (self: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void,
    };

    pub fn prep(self: Node, allocator: Allocator, context: *Context) !*anyopaque {
        return self.vtable.prep(self.self, allocator, context);
    }

    pub fn exec(self: Node, allocator: Allocator, prep_res: *anyopaque) !*anyopaque {
        return self.vtable.exec(self.self, allocator, prep_res);
    }

    pub fn post(self: Node, allocator: Allocator, context: *Context, prep_res: *anyopaque, exec_res: *anyopaque) !Action {
        return self.vtable.post(self.self, allocator, context, prep_res, exec_res);
    }

    pub fn cleanupPrep(self: Node, allocator: Allocator, prep_res: *anyopaque) void {
        self.vtable.cleanup_prep(self.self, allocator, prep_res);
    }

    pub fn cleanupExec(self: Node, allocator: Allocator, exec_res: *anyopaque) void {
        self.vtable.cleanup_exec(self.self, allocator, exec_res);
    }
};

pub const BaseNode = struct {
    successors: std.StringHashMap(Node),

    pub fn init(allocator: Allocator) BaseNode {
        return .{
            .successors = std.StringHashMap(Node).init(allocator),
        };
    }

    pub fn deinit(self: *BaseNode) void {
        self.successors.deinit();
    }

    pub fn next(self: *BaseNode, action: Action, node: Node) !void {
        try self.successors.put(action, node);
    }
};

// ============================================================================
// TESTS
// ============================================================================

/// A simple test node implementation for unit testing
const TestNode = struct {
    base: BaseNode,
    prep_called: bool = false,
    exec_called: bool = false,
    post_called: bool = false,
    prep_value: i32 = 0,
    exec_value: i32 = 0,

    pub fn init(allocator: Allocator) *TestNode {
        const self = allocator.create(TestNode) catch @panic("oom");
        self.* = .{
            .base = BaseNode.init(allocator),
        };
        return self;
    }

    pub fn deinit(self: *TestNode, allocator: Allocator) void {
        self.base.deinit();
        allocator.destroy(self);
    }

    pub fn prep(self_ptr: *anyopaque, allocator: Allocator, context: *Context) !*anyopaque {
        const self: *TestNode = @ptrCast(@alignCast(self_ptr));
        self.prep_called = true;

        // Get value from context if available
        const input = context.get(i32, "input") orelse 0;
        self.prep_value = input;

        const result = try allocator.create(i32);
        result.* = input * 2;
        return @ptrCast(result);
    }

    pub fn exec(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) !*anyopaque {
        const input: *i32 = @ptrCast(@alignCast(prep_res));

        const result = try allocator.create(i32);
        result.* = input.* + 10;
        return @ptrCast(result);
    }

    pub fn post(self_ptr: *anyopaque, _: Allocator, context: *Context, _: *anyopaque, exec_res: *anyopaque) !Action {
        const self: *TestNode = @ptrCast(@alignCast(self_ptr));
        self.post_called = true;

        const result: *i32 = @ptrCast(@alignCast(exec_res));
        self.exec_value = result.*;
        try context.set("output", result.*);

        return "next";
    }

    pub fn cleanup_prep(_: *anyopaque, allocator: Allocator, prep_res: *anyopaque) void {
        const ptr: *i32 = @ptrCast(@alignCast(prep_res));
        allocator.destroy(ptr);
    }

    pub fn cleanup_exec(_: *anyopaque, allocator: Allocator, exec_res: *anyopaque) void {
        const ptr: *i32 = @ptrCast(@alignCast(exec_res));
        allocator.destroy(ptr);
    }

    pub const VTABLE = Node.VTable{
        .prep = prep,
        .exec = exec,
        .post = post,
        .cleanup_prep = cleanup_prep,
        .cleanup_exec = cleanup_exec,
    };
};

test "BaseNode - init and deinit" {
    var base = BaseNode.init(testing.allocator);
    defer base.deinit();

    try testing.expectEqual(@as(usize, 0), base.successors.count());
}

test "BaseNode - add successors" {
    const node1 = TestNode.init(testing.allocator);
    defer node1.deinit(testing.allocator);

    const node2 = TestNode.init(testing.allocator);
    defer node2.deinit(testing.allocator);

    const wrapper2 = Node{ .self = node2, .vtable = &TestNode.VTABLE };

    try node1.base.next("default", wrapper2);

    try testing.expectEqual(@as(usize, 1), node1.base.successors.count());
    try testing.expect(node1.base.successors.get("default") != null);
}

test "BaseNode - multiple successors" {
    const node1 = TestNode.init(testing.allocator);
    defer node1.deinit(testing.allocator);

    const node2 = TestNode.init(testing.allocator);
    defer node2.deinit(testing.allocator);

    const node3 = TestNode.init(testing.allocator);
    defer node3.deinit(testing.allocator);

    const wrapper2 = Node{ .self = node2, .vtable = &TestNode.VTABLE };
    const wrapper3 = Node{ .self = node3, .vtable = &TestNode.VTABLE };

    try node1.base.next("success", wrapper2);
    try node1.base.next("failure", wrapper3);

    try testing.expectEqual(@as(usize, 2), node1.base.successors.count());
    try testing.expect(node1.base.successors.get("success") != null);
    try testing.expect(node1.base.successors.get("failure") != null);
}

test "Node - vtable prep/exec/post cycle" {
    const test_node = TestNode.init(testing.allocator);
    defer test_node.deinit(testing.allocator);

    const wrapper = Node{ .self = test_node, .vtable = &TestNode.VTABLE };

    var context = Context.init(testing.allocator);
    defer context.deinit();

    try context.set("input", @as(i32, 5));

    // Run prep
    const prep_res = try wrapper.prep(testing.allocator, &context);
    defer wrapper.cleanupPrep(testing.allocator, prep_res);

    try testing.expect(test_node.prep_called);
    try testing.expectEqual(@as(i32, 5), test_node.prep_value);

    // Run exec
    const exec_res = try wrapper.exec(testing.allocator, prep_res);
    defer wrapper.cleanupExec(testing.allocator, exec_res);

    // Run post
    const action = try wrapper.post(testing.allocator, &context, prep_res, exec_res);

    try testing.expect(test_node.post_called);
    try testing.expectEqualStrings("next", action);

    // Check output: input(5) * 2 = 10, then + 10 = 20
    const output = context.get(i32, "output");
    try testing.expectEqual(@as(?i32, 20), output);
}

test "Node - successor lookup returns null for unknown action" {
    const test_node = TestNode.init(testing.allocator);
    defer test_node.deinit(testing.allocator);

    const result = test_node.base.successors.get("unknown");
    try testing.expectEqual(@as(?Node, null), result);
}
</file>

<file path="src/ollama.zig">
const std = @import("std");
const http = std.http;
const json = std.json;
const mem = std.mem;
const testing = std.testing;

/// Ollama API client for local LLM inference
pub const Ollama = struct {
    allocator: mem.Allocator,
    client: http.Client,
    base_url: []const u8,

    pub const Error = error{
        RequestFailed,
        InvalidResponse,
        NetworkError,
        EndOfStream,
        ReadFailed,
        JsonParseError,
    } || mem.Allocator.Error || http.Client.RequestError || http.Client.FetchError || http.Client.ConnectError || json.ParseError(json.Scanner);

    /// Initialize a new Ollama client
    pub fn init(allocator: mem.Allocator, base_url: ?[]const u8) !Ollama {
        return .{
            .allocator = allocator,
            // allocate client on heap rather than stack
            .client = http.Client{ .allocator = allocator },
            .base_url = base_url orelse "http://localhost:11434",
        };
    }

    /// Clean up resources
    pub fn deinit(self: *Ollama) void {
        self.client.deinit();
    }

    /// Message structure for chat requests
    pub const Message = struct {
        content: []const u8,
        role: []const u8,
    };

    /// Common options for LLM requests (generate and chat)
    pub const LLMOptions = struct {
        model: []const u8 = "granite4:tiny-h",
        temperature: ?f32 = null,
        top_p: ?f32 = null,
        top_k: ?i32 = null,
        num_predict: ?i32 = null,
        stop: ?[]const []const u8 = null,
        seed: ?i32 = null,
        stream: bool = false,
    };

    /// Alias for backwards compatibility
    pub const GenerateOptions = LLMOptions;
    pub const ChatOptions = LLMOptions;

    /// Response from text generation
    pub const GenerateResponse = struct {
        response: []const u8,
        model: []const u8,
        done: bool,
        allocator: mem.Allocator,

        pub fn deinit(self: GenerateResponse) void {
            self.allocator.free(self.response);
            self.allocator.free(self.model);
        }
    };

    /// Response from chat
    pub const ChatResponse = struct {
        message: Message,
        done: bool,
        allocator: mem.Allocator,

        pub fn deinit(self: ChatResponse) void {
            self.allocator.free(self.message.role);
            self.allocator.free(self.message.content);
        }
    };

    /// Model information
    pub const ModelInfo = struct {
        name: []const u8,
        modified_at: []const u8,
        size: i64,
        allocator: mem.Allocator,

        pub fn deinit(self: ModelInfo) void {
            self.allocator.free(self.name);
            self.allocator.free(self.modified_at);
        }
    };

    /// List response
    pub const ListResponse = struct {
        models: []ModelInfo,
        allocator: mem.Allocator,

        pub fn deinit(self: ListResponse) void {
            for (self.models) |model| {
                model.deinit();
            }
            self.allocator.free(self.models);
        }
    };

    /// Generate text completion using the Ollama API
    pub fn generate(
        self: *Ollama,
        prompt: []const u8,
        options: GenerateOptions,
    ) Error!GenerateResponse {
        var url_buffer = std.ArrayListUnmanaged(u8){};
        defer url_buffer.deinit(self.allocator);

        try url_buffer.writer(self.allocator).print("{s}/api/generate", .{self.base_url});
        const url = url_buffer.items;

        // Build JSON request body
        var request_body_buffer = std.ArrayListUnmanaged(u8){};
        defer request_body_buffer.deinit(self.allocator);

        try request_body_buffer.appendSlice(self.allocator, "{\"model\":\"");
        try request_body_buffer.appendSlice(self.allocator, options.model);
        try request_body_buffer.appendSlice(self.allocator, "\",\"prompt\":\"");
        try self.jsonEscape(&request_body_buffer, prompt);
        try request_body_buffer.appendSlice(self.allocator, "\"");

        if (options.temperature) |temp| {
            try request_body_buffer.writer(self.allocator).print(",\"temperature\":{d}", .{temp});
        }
        if (options.top_p) |top_p| {
            try request_body_buffer.writer(self.allocator).print(",\"top_p\":{d}", .{top_p});
        }
        if (options.top_k) |top_k| {
            try request_body_buffer.writer(self.allocator).print(",\"top_k\":{d}", .{top_k});
        }
        if (options.num_predict) |num| {
            try request_body_buffer.writer(self.allocator).print(",\"num_predict\":{d}", .{num});
        }
        if (options.seed) |seed| {
            try request_body_buffer.writer(self.allocator).print(",\"seed\":{d}", .{seed});
        }
        try request_body_buffer.writer(self.allocator).print(",\"stream\":{}", .{options.stream});
        try request_body_buffer.appendSlice(self.allocator, "}");

        const request_body = request_body_buffer.items;

        const uri = try std.Uri.parse(url);

        var req = try self.client.request(.POST, uri, .{
            .extra_headers = &.{
                .{ .name = "Content-Type", .value = "application/json" },
            },
        });
        defer req.deinit();

        // Set transfer encoding to content-length (automatically sets Content-Length header)
        req.transfer_encoding = .{ .content_length = request_body.len };

        std.debug.print("Generate: Sending body of length: {}\n", .{request_body.len});

        // Send the request body using the BodyWriter API
        var body_writer = try req.sendBody(&.{});
        try body_writer.writer.writeAll(request_body);
        try body_writer.end();

        std.debug.print("Generate: Body sent successfully\n", .{});

        var response = try req.receiveHead(&.{});

        if (response.head.status != .ok) {
            std.debug.print("Generate request failed with status: {}\n", .{response.head.status});
            return Error.RequestFailed;
        }

        // Read response body
        var response_buffer: [1024 * 1024 * 10]u8 = undefined;
        var response_writer: std.Io.Writer = .fixed(&response_buffer);
        var read_buffer: [4096]u8 = undefined;
        const body_reader: *std.Io.Reader = response.reader(&read_buffer);

        const n = try body_reader.stream(&response_writer, @enumFromInt(response_buffer.len));

        // Parse JSON response
        const response_json = response_buffer[0..n];
        std.debug.print("Generate response body ({} bytes): {s}\n", .{ n, response_json });
        const parsed = try json.parseFromSlice(
            json.Value,
            self.allocator,
            response_json,
            .{},
        );
        defer parsed.deinit();

        const obj = parsed.value.object;
        const response_text = obj.get("response").?.string;
        const model_name = obj.get("model").?.string;
        const done = obj.get("done").?.bool;

        return GenerateResponse{
            .response = try self.allocator.dupe(u8, response_text),
            .model = try self.allocator.dupe(u8, model_name),
            .done = done,
            .allocator = self.allocator,
        };
    }

    /// Chat with the model using conversation history
    pub fn chat(
        self: *Ollama,
        messages: []const Message,
        options: ChatOptions,
    ) Error!ChatResponse {
        var url_buffer = std.ArrayListUnmanaged(u8){};
        defer url_buffer.deinit(self.allocator);

        try url_buffer.writer(self.allocator).print("{s}/api/chat", .{self.base_url});
        const url = url_buffer.items;

        // Build JSON request body
        var request_body_buffer = std.ArrayListUnmanaged(u8){};
        defer request_body_buffer.deinit(self.allocator);

        try request_body_buffer.appendSlice(self.allocator, "{\"model\":\"");
        try request_body_buffer.appendSlice(self.allocator, options.model);
        try request_body_buffer.appendSlice(self.allocator, "\",\"messages\":[");

        for (messages, 0..) |msg, i| {
            if (i > 0) try request_body_buffer.appendSlice(self.allocator, ",");
            try request_body_buffer.appendSlice(self.allocator, "{\"role\":\"");
            try request_body_buffer.appendSlice(self.allocator, msg.role);
            try request_body_buffer.appendSlice(self.allocator, "\",\"content\":\"");
            try self.jsonEscape(&request_body_buffer, msg.content);
            try request_body_buffer.appendSlice(self.allocator, "\"}");
        }

        try request_body_buffer.appendSlice(self.allocator, "]");

        if (options.temperature) |temp| {
            try request_body_buffer.writer(self.allocator).print(",\"temperature\":{d}", .{temp});
        }
        if (options.top_p) |top_p| {
            try request_body_buffer.writer(self.allocator).print(",\"top_p\":{d}", .{top_p});
        }
        if (options.top_k) |top_k| {
            try request_body_buffer.writer(self.allocator).print(",\"top_k\":{d}", .{top_k});
        }
        if (options.num_predict) |num| {
            try request_body_buffer.writer(self.allocator).print(",\"num_predict\":{d}", .{num});
        }
        if (options.seed) |seed| {
            try request_body_buffer.writer(self.allocator).print(",\"seed\":{d}", .{seed});
        }
        try request_body_buffer.writer(self.allocator).print(",\"stream\":{}", .{options.stream});
        try request_body_buffer.appendSlice(self.allocator, "}");

        const request_body = request_body_buffer.items;

        std.debug.print("Chat request body: {s}\n", .{request_body});

        const uri = try std.Uri.parse(url);

        var req = try self.client.request(.POST, uri, .{
            .extra_headers = &.{
                .{ .name = "Content-Type", .value = "application/json" },
            },
        });
        defer req.deinit();

        // Set transfer encoding to content-length (automatically sets Content-Length header)
        req.transfer_encoding = .{ .content_length = request_body.len };

        std.debug.print("Chat: Sending body of length: {}\n", .{request_body.len});

        // Send the request body using the BodyWriter API
        var body_writer = try req.sendBody(&.{});
        try body_writer.writer.writeAll(request_body);
        try body_writer.end();

        std.debug.print("Chat: Body sent successfully\n", .{});

        // Receive response
        var response = try req.receiveHead(&.{});

        // Read response body - allocate large buffer for response
        const response_buffer = try self.allocator.alloc(u8, 10 * 1024 * 1024);
        defer self.allocator.free(response_buffer);
        var response_writer: std.Io.Writer = .fixed(response_buffer);
        var read_buffer: [4096]u8 = undefined;
        const body_reader: *std.Io.Reader = response.reader(&read_buffer);

        const n = try body_reader.streamRemaining(&response_writer);

        // Parse JSON response
        const response_json = response_buffer[0..n];
        std.debug.print("Chat response status: {}, body ({} bytes): {s}\n", .{ response.head.status, n, response_json });

        if (response.head.status != .ok) {
            std.debug.print("Chat request failed with status: {}\n", .{response.head.status});
            return Error.RequestFailed;
        }
        const parsed = try json.parseFromSlice(
            json.Value,
            self.allocator,
            response_json,
            .{},
        );
        defer parsed.deinit();

        const obj = parsed.value.object;
        const msg_obj = obj.get("message").?.object;
        const role = msg_obj.get("role").?.string;
        const content = msg_obj.get("content").?.string;
        const done = obj.get("done").?.bool;

        return ChatResponse{
            .message = Message{
                .role = try self.allocator.dupe(u8, role),
                .content = try self.allocator.dupe(u8, content),
            },
            .done = done,
            .allocator = self.allocator,
        };
    }

    /// List available models
    pub fn listModels(self: *Ollama) Error!ListResponse {
        var url_buffer = std.ArrayListUnmanaged(u8){};
        defer url_buffer.deinit(self.allocator);

        try url_buffer.writer(self.allocator).print("{s}/api/tags", .{self.base_url});
        const url = url_buffer.items;

        const uri = try std.Uri.parse(url);

        var req = try self.client.request(.GET, uri, .{});
        defer req.deinit();

        try req.sendBodiless();
        var response = try req.receiveHead(&.{});

        if (response.head.status != .ok) {
            std.debug.print("ListModels request failed with status: {}\n", .{response.head.status});
            return Error.RequestFailed;
        }

        // Read response body - allocate large buffer for response
        const response_buffer = try self.allocator.alloc(u8, 10 * 1024 * 1024);
        defer self.allocator.free(response_buffer);
        var response_writer: std.Io.Writer = .fixed(response_buffer);
        var read_buffer: [4096]u8 = undefined;
        const body_reader: *std.Io.Reader = response.reader(&read_buffer);

        const n = try body_reader.streamRemaining(&response_writer);

        // Parse JSON response
        const response_json = response_buffer[0..n];
        std.debug.print("ListModels response body ({} bytes): {s}\n", .{ n, response_json });
        const parsed = try json.parseFromSlice(
            json.Value,
            self.allocator,
            response_json,
            .{},
        );
        defer parsed.deinit();

        const obj = parsed.value.object;
        const models_array = obj.get("models").?.array;

        var models = try self.allocator.alloc(ModelInfo, models_array.items.len);
        for (models_array.items, 0..) |model_val, i| {
            const model_obj = model_val.object;

            models[i] = ModelInfo{
                .name = try self.allocator.dupe(u8, model_obj.get("name").?.string),
                .modified_at = try self.allocator.dupe(u8, model_obj.get("modified_at").?.string),
                .size = model_obj.get("size").?.integer,
                .allocator = self.allocator,
            };
        }

        return ListResponse{
            .models = models,
            .allocator = self.allocator,
        };
    }

    /// Helper function to escape JSON strings
    fn jsonEscape(self: *Ollama, buffer: *std.ArrayListUnmanaged(u8), text: []const u8) !void {
        for (text) |c| {
            switch (c) {
                '"' => try buffer.appendSlice(self.allocator, "\\\""),
                '\\' => try buffer.appendSlice(self.allocator, "\\\\"),
                '\n' => try buffer.appendSlice(self.allocator, "\\n"),
                '\r' => try buffer.appendSlice(self.allocator, "\\r"),
                '\t' => try buffer.appendSlice(self.allocator, "\\t"),
                else => try buffer.append(self.allocator, c),
            }
        }
    }
};

// ============================================================================
// TESTS
// ============================================================================

test "Ollama - init and deinit" {
    var gpa = std.testing.allocator_instance;
    var client = try Ollama.init(gpa.allocator(), null);
    defer client.deinit();

    try testing.expect(client.base_url.len > 0);
    try testing.expectEqualStrings("http://localhost:11434", client.base_url);
}

test "Ollama - init with custom URL" {
    var gpa = std.testing.allocator_instance;
    var client = try Ollama.init(gpa.allocator(), "http://192.168.1.100:11434");
    defer client.deinit();

    try testing.expectEqualStrings("http://192.168.1.100:11434", client.base_url);
}

test "Ollama - Message structure" {
    const msg = Ollama.Message{
        .role = "user",
        .content = "Hello, Ollama!",
    };

    try testing.expectEqualStrings("user", msg.role);
    try testing.expectEqualStrings("Hello, Ollama!", msg.content);
}

test "Ollama - LLMOptions defaults" {
    const opts = Ollama.LLMOptions{};

    try testing.expectEqualStrings("granite4:tiny-h", opts.model);
    try testing.expect(opts.temperature == null);
    try testing.expect(opts.seed == null);
    try testing.expect(opts.stream == false);
}

test "Ollama - LLMOptions custom" {
    const opts = Ollama.LLMOptions{
        .model = "granite4:tiny-h",
        .temperature = 0.7,
        .seed = 123,
        .stream = false,
        .num_predict = 100,
    };

    try testing.expectEqualStrings("granite4:tiny-h", opts.model);
    try testing.expect(opts.temperature.? == 0.7);
    try testing.expect(opts.seed.? == 123);
    try testing.expect(opts.stream == false);
    try testing.expect(opts.num_predict.? == 100);
}

test "Ollama - backwards compatible aliases" {
    // Verify that GenerateOptions and ChatOptions are aliases for LLMOptions
    const gen_opts = Ollama.GenerateOptions{};
    const chat_opts = Ollama.ChatOptions{};
    const llm_opts = Ollama.LLMOptions{};

    try testing.expectEqualStrings(gen_opts.model, llm_opts.model);
    try testing.expectEqualStrings(chat_opts.model, llm_opts.model);
}

// Integration tests - require a running Ollama server
test "Ollama - listModels integration" {
    var gpa = std.testing.allocator_instance;
    var client = try Ollama.init(gpa.allocator(), null);
    defer client.deinit();

    const list_response = try client.listModels();
    defer list_response.deinit();

    std.debug.print("\nAvailable models: {d}\n", .{list_response.models.len});
    for (list_response.models) |model| {
        std.debug.print("  - {s} (size: {d})\n", .{ model.name, model.size });
    }

    try testing.expect(list_response.models.len >= 0);
}

test "Ollama - generate integration" {
    if (true) return error.SkipZigTest; // Skip by default

    var gpa = std.testing.allocator_instance;
    var client = try Ollama.init(gpa.allocator(), null);
    defer client.deinit();

    const response = try client.generate("Say hello in one word", .{
        .model = "granite4:tiny-h",
    });
    defer response.deinit();

    std.debug.print("\nGenerate Response: {s}\n", .{response.response});
    std.debug.print("Model: {s}, Done: {}\n", .{ response.model, response.done });

    try testing.expect(response.response.len > 0);
    try testing.expect(response.done);
}

test "Ollama - chat integration" {
    var gpa = std.testing.allocator_instance;
    var client = try Ollama.init(gpa.allocator(), null);
    defer client.deinit();

    const messages = [_]Ollama.Message{
        .{ .role = "user", .content = "What is the capital of France?" },
    };

    const response = try client.chat(&messages, .{
        .model = "granite4:tiny-h",
    });
    defer response.deinit();

    std.debug.print("\nChat Response: {s}\n", .{response.message.content});
    std.debug.print("Role: {s}, Done: {}\n", .{ response.message.role, response.done });

    try testing.expect(response.message.content.len > 0);
    try testing.expectEqualStrings("assistant", response.message.role);
    try testing.expect(response.done);
}
</file>

</files>
